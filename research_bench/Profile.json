{
  "d55bcdd5-515f-4fe5-a865-ebb9094cf6bb": {
    "pk": "d55bcdd5-515f-4fe5-a865-ebb9094cf6bb",
    "name": "Shengnan An",
    "bio": "I am a researcher dedicated to enhancing the capabilities of large language models (LLMs) in code generation and reasoning tasks. My recent work focuses on addressing the challenges of library-oriented code generation through innovative approaches like CAPIR (Compositional API Recommendation), which effectively breaks down coarse-grained requirements into detailed subtasks for improved API recommendations. I also developed Skill-KNN, a skill-based few-shot selection method that optimizes in-context learning without the need for model retraining, showcasing my commitment to practical and efficient solutions.\n\nMy exploration of abstraction capabilities in deep learning models has led to significant insights into how models like T5 and GPT-2 can induce abstract concepts, revealing a \"memorize-then-abstract\" process during training. Additionally, I have investigated compositional generalization, a critical reasoning capability, through frameworks like CoFe and LeAR, which model semantic parsing as algebraic recombination.\n\nI am particularly interested in the potential of LLMs to learn from their mistakes, as demonstrated in my LEMA framework, which incorporates error-driven learning to enhance reasoning capabilities. My research also extends to improving prompt-tuning for natural language generation tasks through input-tuning, demonstrating my focus on practical applications and advancements in the field.\n\nOverall, my work aims to bridge the gap between theoretical advancements and real-world applications, contributing to the evolving landscape of AI and machine learning.",
    "collaborators": [
      "Zeqi Lin",
      "Nanning Zheng",
      "Jian-Guang Lou",
      "Qiang Fu",
      "B. Chen",
      "Weizhu Chen",
      "Qian Liu",
      "Zexiong Ma",
      "Dongmei Zhang",
      "Bing Xie",
      "Bo Zhou",
      "D. Zhang",
      "Yifei Li",
      "Chenyao Liu",
      "L. Wen",
      "Yan Gao",
      "Bin Zhou"
    ],
    "pub_titles": [
      "Compositional API Recommendation for Library-Oriented Code Generation",
      "Skill-Based Few-Shot Selection for In-Context Learning",
      "Does Deep Learning Learn to Abstract? A Systematic Probing Framework",
      "How Do In-Context Examples Affect Compositional Generalization?",
      "Learning From Mistakes Makes LLM Better Reasoner",
      "Input-Tuning: Adapting Unfamiliar Inputs to Frozen Pretrained Models",
      "Learning Algebraic Recombination for Compositional Generalization",
      "Compositional Generalization by Learning Analytical Expressions"
    ],
    "pub_abstracts": [
      "Large language models (LLMs) have achieved exceptional performance in code generation. However, the performance remains unsatisfactory in generating library-oriented code, especially for the libraries not present in the training data of LLMs. Previous work utilizes API recommendation technology to help LLMs use libraries: it retrieves APIs related to the user requirements, then leverages them as context to prompt LLMs. However, developmental requirements can be coarse-grained, requiring a combination of multiple fine-grained APIs. This granularity inconsistency makes API recommendation a challenging task. To address this, we propose CAPIR (Compositional API Recommendation), which adopts a \u201cdivide-and-conquer\u201d strategy to recommend APIs for coarse-grained requirements. Specifically, CAPIR employs an LLM-based Decomposer to break down a coarse-grained task description into several detailed subtasks. Then, CAPIR applies an embedding-based Retriever to identify relevant APIs corresponding to each subtask. Moreover, CAPIR leverages an LLM-based Reranker to filter out redundant APIs and provides the final recommendation. To facilitate the evaluation of API recommendation methods on coarse-grained requirements, we present two challenging benchmarks, RAPID (Recommend APIs based on Documentation) and LOCG (Library-Oriented Code Generation). Experimental results on these benchmarks, demonstrate the effectiveness of CAPIR in comparison to existing baselines. Specifically, on RAPID\u2019s TorchdataAR dataset, compared to the state-of-the-art API recommendation approach, CAPIR improves recall@5 from 18.7% to 43.2% and precision@5 from 15.5% to 37.1%. On LOCG\u2019s Torchdata-Code dataset, compared to code generation without API recommendation, CAPIR improves pass@100 from 16.0% to 28.0%.Ccs Concepts \u2022 Software and its engineering $\\rightarrow$ Search-based software engineering; Software development techniques.",
      "In-context learning is the paradigm that adapts large language models to downstream tasks by providing a few examples. Few-shot selection -- selecting appropriate examples for each test instance separately -- is important for in-context learning. In this paper, we propose Skill-KNN, a skill-based few-shot selection method for in-context learning. The key advantages of Skill-KNN include: (1) it addresses the problem that existing methods based on pre-trained embeddings can be easily biased by surface natural language features that are not important for the target task; (2) it does not require training or fine-tuning of any models, making it suitable for frequently expanding or changing example banks. The key insight is to optimize the inputs fed into the embedding model, rather than tuning the model itself. Technically, Skill-KNN generates the skill-based descriptions for each test case and candidate example by utilizing a pre-processing few-shot prompting, thus eliminating unimportant surface features. Experimental results across five cross-domain semantic parsing datasets and six backbone models show that Skill-KNN significantly outperforms existing methods.",
      "Abstraction is a desirable capability for deep learning models, which means to induce abstract concepts from concrete instances and flexibly apply them beyond the learning context. At the same time, there is a lack of clear understanding about both the presence and further characteristics of this capability in deep learning models. In this paper, we introduce a systematic probing framework to explore the abstraction capability of deep learning models from a transferability perspective. A set of controlled experiments are conducted based on this framework, providing strong evidence that two probed pre-trained language models (PLMs), T5 and GPT2, have the abstraction capability. We also conduct in-depth analysis, thus shedding further light: (1) the whole training phase exhibits a\"memorize-then-abstract\"two-stage process; (2) the learned abstract concepts are gathered in a few middle-layer attention heads, rather than being evenly distributed throughout the model; (3) the probed abstraction capabilities exhibit robustness against concept mutations, and are more robust to low-level/source-side mutations than high-level/target-side ones; (4) generic pre-training is critical to the emergence of abstraction capability, and PLMs exhibit better abstraction with larger model sizes and data scales.",
      "Compositional generalization\u2013understanding unseen combinations of seen primitives\u2013is an essential reasoning capability in human intelligence.The AI community mainly studies this capability by fine-tuning neural networks on lots of training samples, while it is still unclear whether and how in-context learning\u2013the prevailing few-shot paradigm based on large language models\u2013exhibits compositional generalization.In this paper, we present CoFe, a test suite to investigate in-context compositional generalization.We find that the compositional generalization performance can be easily affected by the selection of in-context examples, thus raising the research question what the key factors are to make good in-context examples for compositional generalization.We study three potential factors: similarity, diversity and complexity. Our systematic experiments indicate that in-context examples should be structurally similar to the test case, diverse from each other, and individually simple.Furthermore, two strong limitations are observed: in-context compositional generalization on fictional words is much weaker than that on commonly used ones; it is still critical that the in-context examples should cover required linguistic structures, even though the backbone model has been pre-trained on large corpus.We hope our analysis would facilitate the understanding and utilization of in-context learning paradigm.",
      "Large language models (LLMs) recently exhibited remarkable reasoning capabilities on solving math problems. To further improve their reasoning capabilities, this work explores whether LLMs can LEarn from MistAkes (LEMA), akin to the human learning process. Consider a human student who failed to solve a math problem, he will learn from what mistake he has made and how to correct it. Mimicking this error-driven learning process, LEMA incorporates mistake-correction data pairs during fine-tuning LLMs. Specifically, we first collect inaccurate reasoning paths from various LLMs, and then employ GPT-4 as a ''corrector'' to identify the mistake step, explain the reason for the mistake, correct the mistake and generate the final answer. In addition, we apply a correction-centric evolution strategy that effectively expands the question set for generating correction data. Experiments across various LLMs and reasoning tasks show that LEMA effectively improves CoT-alone fine-tuning. Our further ablations shed light on the non-homogeneous effectiveness between CoT data and correction data. These results suggest a significant potential for LLMs to improve through learning from their mistakes. Our code, models and prompts are publicly available at https://github.com/microsoft/LEMA.",
      "Recently the prompt-tuning paradigm has attracted significant attention. By only tuning continuous prompts with a frozen pre-trained language model (PLM), prompt-tuning takes a step towards deploying a shared frozen PLM to serve numerous downstream tasks. Although prompt-tuning shows good performance on certain natural language understanding (NLU) tasks, its effectiveness on natural language generation (NLG) tasks is still under-explored. In this paper, we argue that one of the factors hindering the development of prompt-tuning on NLG tasks is the unfamiliar inputs (i.e., inputs are linguistically different from the pretraining corpus). For example, our preliminary exploration reveals a large performance gap between prompt-tuning and fine-tuning when unfamiliar inputs occur frequently in NLG tasks. This motivates us to propose input-tuning, which fine-tunes both the continuous prompts and the input representations, leading to a more effective way to adapt unfamiliar inputs to frozen PLMs. Our proposed input-tuning is conceptually simple and empirically powerful. Experimental results on seven NLG tasks demonstrate that input-tuning is significantly and consistently better than prompt-tuning. Furthermore, on three of these tasks, input-tuning can achieve a comparable or even better performance than fine-tuning.",
      "Neural sequence models exhibit limited compositional generalization ability in semantic parsing tasks. Compositional generalization requires algebraic recombination, i.e., dynamically recombining structured expressions in a recursive manner. However, most previous studies mainly concentrate on recombining lexical units, which is an important but not sufficient part of algebraic recombination. In this paper, we propose LeAR, an end-to-end neural model to learn algebraic recombination for compositional generalization. The key insight is to model the semantic parsing task as a homomorphism between a latent syntactic algebra and a semantic algebra, thus encouraging algebraic recombination. Specifically, we learn two modules jointly: a Composer for producing latent syntax, and an Interpreter for assigning semantic operations. Experiments on two realistic and comprehensive compositional generalization benchmarks demonstrate the effectiveness of our model. The source code is publicly available at https://github.com/microsoft/ContextualSP.",
      "Compositional generalization is a basic but essential intellective capability of human beings, which allows us to recombine known parts readily. However, existing neural network based models have been proven to be extremely deficient in such a capability. Inspired by work in cognition which argues compositionality can be captured by variable slots with symbolic functions, we present a refreshing view that connects a memory-augmented neural model with analytical expressions, to achieve compositional generalization. Our model consists of two cooperative neural modules Composer and Solver, fitting well with the cognitive argument while still being trained in an end-to-end manner via a hierarchical reinforcement learning algorithm. Experiments on a well-known benchmark SCAN demonstrate that our model seizes a great ability of compositional generalization, solving all challenges addressed by previous works with 100% accuracies."
    ],
    "domain": [
      "Natural Language Processing",
      "Code Generation",
      "Machine Learning",
      "Compositional Generalization"
    ],
    "is_leader_candidate": true,
    "is_member_candidate": true,
    "is_reviewer_candidate": true,
    "is_chair_candidate": true
  },
  "8a942d4b-bfee-4c7e-812d-194a7ed1cd49": {
    "pk": "8a942d4b-bfee-4c7e-812d-194a7ed1cd49",
    "name": "Zexiong Ma",
    "bio": "I am a researcher dedicated to enhancing the capabilities of large language models (LLMs) in the realm of code generation and reasoning. My recent work focuses on addressing the challenges associated with generating library-oriented code, particularly when the required libraries are not included in the training data. To tackle this, I developed CAPIR (Compositional API Recommendation), which employs a \"divide-and-conquer\" strategy to effectively recommend APIs for coarse-grained requirements. By breaking down complex tasks into detailed subtasks and leveraging advanced retrieval and ranking techniques, CAPIR significantly improves API recommendation performance, as demonstrated by our benchmarks.\n\nAdditionally, I have explored the potential of LLMs to learn from their mistakes through a framework I call LEMA (LEarn from MistAkes). This approach mimics human learning by incorporating mistake-correction data pairs during the fine-tuning process, allowing LLMs to enhance their reasoning capabilities. My experiments have shown that LEMA effectively boosts performance across various reasoning tasks, highlighting the importance of error-driven learning in AI.\n\nI am passionate about pushing the boundaries of what LLMs can achieve, and I strive to make my research accessible by sharing code, models, and prompts with the community. My goal is to contribute to the development of more robust and intelligent systems that can better understand and generate code, ultimately improving software development practices.",
    "collaborators": [
      "Shengnan An",
      "Zeqi Lin",
      "Bing Xie",
      "Nanning Zheng",
      "Jian-Guang Lou",
      "Weizhu Chen"
    ],
    "pub_titles": [
      "Compositional API Recommendation for Library-Oriented Code Generation",
      "Learning From Mistakes Makes LLM Better Reasoner"
    ],
    "pub_abstracts": [
      "Large language models (LLMs) have achieved exceptional performance in code generation. However, the performance remains unsatisfactory in generating library-oriented code, especially for the libraries not present in the training data of LLMs. Previous work utilizes API recommendation technology to help LLMs use libraries: it retrieves APIs related to the user requirements, then leverages them as context to prompt LLMs. However, developmental requirements can be coarse-grained, requiring a combination of multiple fine-grained APIs. This granularity inconsistency makes API recommendation a challenging task. To address this, we propose CAPIR (Compositional API Recommendation), which adopts a \u201cdivide-and-conquer\u201d strategy to recommend APIs for coarse-grained requirements. Specifically, CAPIR employs an LLM-based Decomposer to break down a coarse-grained task description into several detailed subtasks. Then, CAPIR applies an embedding-based Retriever to identify relevant APIs corresponding to each subtask. Moreover, CAPIR leverages an LLM-based Reranker to filter out redundant APIs and provides the final recommendation. To facilitate the evaluation of API recommendation methods on coarse-grained requirements, we present two challenging benchmarks, RAPID (Recommend APIs based on Documentation) and LOCG (Library-Oriented Code Generation). Experimental results on these benchmarks, demonstrate the effectiveness of CAPIR in comparison to existing baselines. Specifically, on RAPID\u2019s TorchdataAR dataset, compared to the state-of-the-art API recommendation approach, CAPIR improves recall@5 from 18.7% to 43.2% and precision@5 from 15.5% to 37.1%. On LOCG\u2019s Torchdata-Code dataset, compared to code generation without API recommendation, CAPIR improves pass@100 from 16.0% to 28.0%.Ccs Concepts \u2022 Software and its engineering $\\rightarrow$ Search-based software engineering; Software development techniques.",
      "Large language models (LLMs) recently exhibited remarkable reasoning capabilities on solving math problems. To further improve their reasoning capabilities, this work explores whether LLMs can LEarn from MistAkes (LEMA), akin to the human learning process. Consider a human student who failed to solve a math problem, he will learn from what mistake he has made and how to correct it. Mimicking this error-driven learning process, LEMA incorporates mistake-correction data pairs during fine-tuning LLMs. Specifically, we first collect inaccurate reasoning paths from various LLMs, and then employ GPT-4 as a ''corrector'' to identify the mistake step, explain the reason for the mistake, correct the mistake and generate the final answer. In addition, we apply a correction-centric evolution strategy that effectively expands the question set for generating correction data. Experiments across various LLMs and reasoning tasks show that LEMA effectively improves CoT-alone fine-tuning. Our further ablations shed light on the non-homogeneous effectiveness between CoT data and correction data. These results suggest a significant potential for LLMs to improve through learning from their mistakes. Our code, models and prompts are publicly available at https://github.com/microsoft/LEMA."
    ],
    "domain": [
      "Natural Language Processing",
      "Code Generation",
      "Large Language Models",
      "API Recommendation"
    ],
    "is_leader_candidate": true,
    "is_member_candidate": true,
    "is_reviewer_candidate": true,
    "is_chair_candidate": true
  },
  "fecd0021-b356-49fc-94c6-38cd12dde210": {
    "pk": "fecd0021-b356-49fc-94c6-38cd12dde210",
    "name": "Zeqi Lin",
    "bio": "I am a researcher dedicated to enhancing the capabilities of large language models (LLMs) in the realm of code generation, particularly focusing on library-oriented code. My recent work introduces CAPIR (Compositional API Recommendation), a novel approach that addresses the challenges of API recommendation for coarse-grained development requirements. By employing a \"divide-and-conquer\" strategy, CAPIR effectively decomposes complex tasks into manageable subtasks, allowing for more precise API recommendations.\n\nThrough the integration of an LLM-based Decomposer, an embedding-based Retriever, and an LLM-based Reranker, I have developed a system that significantly improves the accuracy and relevance of API suggestions. My research has led to the creation of two challenging benchmarks, RAPID and LOCG, which facilitate the evaluation of API recommendation methods. The experimental results demonstrate CAPIR's superiority over existing baselines, achieving notable improvements in recall and precision metrics.\n\nI am passionate about bridging the gap between user requirements and effective code generation, and I strive to contribute to the ongoing evolution of software development techniques through innovative research in this field.",
    "collaborators": [
      "Zexiong Ma",
      "Shengnan An",
      "Bing Xie"
    ],
    "pub_titles": [
      "Compositional API Recommendation for Library-Oriented Code Generation"
    ],
    "pub_abstracts": [
      "Large language models (LLMs) have achieved exceptional performance in code generation. However, the performance remains unsatisfactory in generating library-oriented code, especially for the libraries not present in the training data of LLMs. Previous work utilizes API recommendation technology to help LLMs use libraries: it retrieves APIs related to the user requirements, then leverages them as context to prompt LLMs. However, developmental requirements can be coarse-grained, requiring a combination of multiple fine-grained APIs. This granularity inconsistency makes API recommendation a challenging task. To address this, we propose CAPIR (Compositional API Recommendation), which adopts a \u201cdivide-and-conquer\u201d strategy to recommend APIs for coarse-grained requirements. Specifically, CAPIR employs an LLM-based Decomposer to break down a coarse-grained task description into several detailed subtasks. Then, CAPIR applies an embedding-based Retriever to identify relevant APIs corresponding to each subtask. Moreover, CAPIR leverages an LLM-based Reranker to filter out redundant APIs and provides the final recommendation. To facilitate the evaluation of API recommendation methods on coarse-grained requirements, we present two challenging benchmarks, RAPID (Recommend APIs based on Documentation) and LOCG (Library-Oriented Code Generation). Experimental results on these benchmarks, demonstrate the effectiveness of CAPIR in comparison to existing baselines. Specifically, on RAPID\u2019s TorchdataAR dataset, compared to the state-of-the-art API recommendation approach, CAPIR improves recall@5 from 18.7% to 43.2% and precision@5 from 15.5% to 37.1%. On LOCG\u2019s Torchdata-Code dataset, compared to code generation without API recommendation, CAPIR improves pass@100 from 16.0% to 28.0%.Ccs Concepts \u2022 Software and its engineering $\\rightarrow$ Search-based software engineering; Software development techniques."
    ],
    "domain": [
      "Natural Language Processing",
      "Code Generation",
      "API Recommendation"
    ],
    "is_leader_candidate": true,
    "is_member_candidate": true,
    "is_reviewer_candidate": true,
    "is_chair_candidate": true
  },
  "e9aeca48-4a96-4aca-bc00-0c41715d9d6e": {
    "pk": "e9aeca48-4a96-4aca-bc00-0c41715d9d6e",
    "name": "Nanning Zheng",
    "bio": "I am a researcher dedicated to enhancing the reasoning capabilities of large language models (LLMs), particularly in the context of solving mathematical problems. My recent work introduces the concept of Learning from Mistakes (LEMA), which draws inspiration from human learning processes. Just as a student learns from errors, I explore how LLMs can benefit from mistake-correction data pairs during their fine-tuning phase.\n\nIn my research, I collect inaccurate reasoning paths from various LLMs and utilize GPT-4 as a \"corrector\" to identify mistakes, explain them, and generate accurate answers. This innovative approach not only improves the performance of LLMs but also highlights the importance of correction-centric strategies in expanding the question set for generating correction data. My experiments demonstrate that LEMA significantly enhances the effectiveness of chain-of-thought (CoT) fine-tuning, revealing the potential for LLMs to learn and grow from their errors.\n\nI am passionate about making my findings accessible to the broader research community, which is why I have made the code, models, and prompts from my work publicly available. I believe that by fostering collaboration and sharing knowledge, we can unlock new possibilities in the field of artificial intelligence.",
    "collaborators": [
      "Shengnan An",
      "Zexiong Ma",
      "Zeqi Lin",
      "Jian-Guang Lou",
      "Weizhu Chen"
    ],
    "pub_titles": [
      "Learning From Mistakes Makes LLM Better Reasoner"
    ],
    "pub_abstracts": [
      "Large language models (LLMs) recently exhibited remarkable reasoning capabilities on solving math problems. To further improve their reasoning capabilities, this work explores whether LLMs can LEarn from MistAkes (LEMA), akin to the human learning process. Consider a human student who failed to solve a math problem, he will learn from what mistake he has made and how to correct it. Mimicking this error-driven learning process, LEMA incorporates mistake-correction data pairs during fine-tuning LLMs. Specifically, we first collect inaccurate reasoning paths from various LLMs, and then employ GPT-4 as a ''corrector'' to identify the mistake step, explain the reason for the mistake, correct the mistake and generate the final answer. In addition, we apply a correction-centric evolution strategy that effectively expands the question set for generating correction data. Experiments across various LLMs and reasoning tasks show that LEMA effectively improves CoT-alone fine-tuning. Our further ablations shed light on the non-homogeneous effectiveness between CoT data and correction data. These results suggest a significant potential for LLMs to improve through learning from their mistakes. Our code, models and prompts are publicly available at https://github.com/microsoft/LEMA."
    ],
    "domain": [
      "Natural Language Processing",
      "Large Language Models",
      "Machine Learning"
    ],
    "is_leader_candidate": true,
    "is_member_candidate": true,
    "is_reviewer_candidate": true,
    "is_chair_candidate": true
  },
  "9f2bab97-1eeb-4d1e-a319-4bd64fa9824d": {
    "pk": "9f2bab97-1eeb-4d1e-a319-4bd64fa9824d",
    "name": "Jian-Guang Lou",
    "bio": "I am a researcher dedicated to advancing the capabilities of large language models (LLMs) and their applications in natural language processing. My recent work has focused on enhancing in-context learning, particularly through the development of Skill-KNN, a skill-based few-shot selection method that optimizes input representations to improve performance across various tasks. I have also explored the abstraction capabilities of deep learning models, revealing insights into how these models learn and apply abstract concepts.\n\nMy research extends to compositional generalization, where I introduced CoFe, a test suite that investigates how in-context examples influence reasoning capabilities. Additionally, I have contributed to hybrid question-answering systems with the TACR model, which effectively aligns questions with table data for improved evidence retrieval.\n\nI am particularly concerned with the social biases present in LLMs, especially in sensitive applications like Text-to-SQL. My work aims to uncover and mitigate these biases, ensuring fairer outcomes in automated decision-making processes. Furthermore, I have developed innovative frameworks like LEMA, which allows LLMs to learn from their mistakes, and AutoSD, which aligns automated debugging with human reasoning.\n\nThrough my research, I strive to bridge the gap between theoretical advancements and practical applications, ensuring that LLMs are not only powerful but also responsible and effective in real-world scenarios. My goal is to continue exploring the intersection of language understanding, reasoning, and ethical AI, contributing to a more nuanced understanding of how these technologies can be harnessed for good.",
    "collaborators": [
      "Zeqi Lin",
      "B. Chen",
      "Shengnan An",
      "Qiang Fu",
      "Nanning Zheng",
      "Weizhu Chen",
      "Yan Gao",
      "Qian Liu",
      "D. Zhang",
      "Y. Liu",
      "Zhe Su",
      "Xiaokang Chen",
      "Fengji Zhang",
      "Daoguang Zan",
      "Bo Zhou",
      "Jian Wu",
      "B\u00f6rje F. Karlsson",
      "M. Okumura",
      "Elliott Ash",
      "Zexiong Ma",
      "Xinyu Zhu",
      "Cheng Yang",
      "Siheng Li",
      "Yujiu Yang",
      "Yue Zhang",
      "Jin Liu",
      "Yi Mao",
      "Longxu Dou",
      "Xuqi Liu",
      "Mingyang Pan",
      "Dingzirui Wang",
      "Wanxiang Che",
      "Min-Yen Kan",
      "Dechen Zhan",
      "Pin-Yu Chen",
      "Tsung-Yi Ho",
      "Sungmin Kang",
      "S. Yoo",
      "Junyi Zhang",
      "Jiaqi Guo",
      "Shizhao Sun",
      "Xinyu Pi",
      "Morteza Ziyadi",
      "Gustavo Aguilar",
      "T. Solorio",
      "Xiaodong Gu",
      "Kang Min",
      "Yoo Jung-Woo",
      "Jiwei Li",
      "Will Monroe",
      "Alan Ritter",
      "Dan Jurafsky",
      "Yanran Li",
      "Hui Su",
      "Xiaoyu Shen",
      "Ziqiang Wenjie Li",
      "Yihong Chen",
      "Bei Chen",
      "Mingbo Ma",
      "Liang Huang",
      "Bowen Zhou",
      "Yifei Li"
    ],
    "pub_titles": [
      "Skill-Based Few-Shot Selection for In-Context Learning",
      "Does Deep Learning Learn to Abstract? A Systematic Probing Framework",
      "How Do In-Context Examples Affect Compositional Generalization?",
      "TACR: A Table-alignment-based Cell-selection and Reasoning Model for Hybrid Question-Answering",
      "Uncovering and Categorizing Social Biases in Text-to-SQL",
      "Learning From Mistakes Makes LLM Better Reasoner",
      "Question Answering as Programming for Solving Time-Sensitive Questions",
      "RepoCoder: Repository-Level Code Completion Through Iterative Retrieval and Generation",
      "Towards Knowledge-Intensive Text-to-SQL Semantic Parsing with Formulaic Knowledge",
      "Uncovering and Quantifying Social Biases in Code Generation",
      "Explainable Automated Debugging via Large Language Model-driven Scientific Debugging",
      "LayoutDiffusion: Improving Graphic Layout Generation by Discrete Diffusion Probabilistic Models",
      "Reasoning Like Program Executors",
      "Incorporate Directed Dependency Relation Graph into Transformer Block for Multi-turn Dialogue Generation",
      "Input-Tuning: Adapting Unfamiliar Inputs to Frozen Pretrained Models"
    ],
    "pub_abstracts": [
      "In-context learning is the paradigm that adapts large language models to downstream tasks by providing a few examples. Few-shot selection -- selecting appropriate examples for each test instance separately -- is important for in-context learning. In this paper, we propose Skill-KNN, a skill-based few-shot selection method for in-context learning. The key advantages of Skill-KNN include: (1) it addresses the problem that existing methods based on pre-trained embeddings can be easily biased by surface natural language features that are not important for the target task; (2) it does not require training or fine-tuning of any models, making it suitable for frequently expanding or changing example banks. The key insight is to optimize the inputs fed into the embedding model, rather than tuning the model itself. Technically, Skill-KNN generates the skill-based descriptions for each test case and candidate example by utilizing a pre-processing few-shot prompting, thus eliminating unimportant surface features. Experimental results across five cross-domain semantic parsing datasets and six backbone models show that Skill-KNN significantly outperforms existing methods.",
      "Abstraction is a desirable capability for deep learning models, which means to induce abstract concepts from concrete instances and flexibly apply them beyond the learning context. At the same time, there is a lack of clear understanding about both the presence and further characteristics of this capability in deep learning models. In this paper, we introduce a systematic probing framework to explore the abstraction capability of deep learning models from a transferability perspective. A set of controlled experiments are conducted based on this framework, providing strong evidence that two probed pre-trained language models (PLMs), T5 and GPT2, have the abstraction capability. We also conduct in-depth analysis, thus shedding further light: (1) the whole training phase exhibits a\"memorize-then-abstract\"two-stage process; (2) the learned abstract concepts are gathered in a few middle-layer attention heads, rather than being evenly distributed throughout the model; (3) the probed abstraction capabilities exhibit robustness against concept mutations, and are more robust to low-level/source-side mutations than high-level/target-side ones; (4) generic pre-training is critical to the emergence of abstraction capability, and PLMs exhibit better abstraction with larger model sizes and data scales.",
      "Compositional generalization\u2013understanding unseen combinations of seen primitives\u2013is an essential reasoning capability in human intelligence.The AI community mainly studies this capability by fine-tuning neural networks on lots of training samples, while it is still unclear whether and how in-context learning\u2013the prevailing few-shot paradigm based on large language models\u2013exhibits compositional generalization.In this paper, we present CoFe, a test suite to investigate in-context compositional generalization.We find that the compositional generalization performance can be easily affected by the selection of in-context examples, thus raising the research question what the key factors are to make good in-context examples for compositional generalization.We study three potential factors: similarity, diversity and complexity. Our systematic experiments indicate that in-context examples should be structurally similar to the test case, diverse from each other, and individually simple.Furthermore, two strong limitations are observed: in-context compositional generalization on fictional words is much weaker than that on commonly used ones; it is still critical that the in-context examples should cover required linguistic structures, even though the backbone model has been pre-trained on large corpus.We hope our analysis would facilitate the understanding and utilization of in-context learning paradigm.",
      "Hybrid Question-Answering (HQA), which targets reasoning over tables and passages linked from table cells, has witnessed significant research in recent years. A common challenge in HQA and other passage-table QA datasets is that it is generally unrealistic to iterate over all table rows, columns, and linked passages to retrieve evidence. Such a challenge made it difficult for previous studies to show their reasoning ability in retrieving answers. To bridge this gap, we propose a novel Table-alignment-based Cell-selection and Reasoning model (TACR) for hybrid text and table QA, evaluated on the HybridQA and WikiTableQuestions datasets. In evidence retrieval, we design a table-question-alignment enhanced cell-selection method to retrieve fine-grained evidence. In answer reasoning, we incorporate a QA module that treats the row containing selected cells as context. Experimental results over the HybridQA and WikiTableQuestions (WTQ) datasets show that TACR achieves state-of-the-art results on cell selection and outperforms fine-grained evidence retrieval baselines on HybridQA, while achieving competitive performance on WTQ. We also conducted a detailed analysis to demonstrate that being able to align questions to tables in the cell-selection stage can result in important gains from experiments of over 90\\% table row and column selection accuracy, meanwhile also improving output explainability.",
      "Large pre-trained language models are acknowledged to carry social bias towards different demographics, which can further amplify existing stereotypes in our society and cause even more harm. Text-to-SQL is an important task, models of which are mainly adopted by administrative industries, where unfair decisions may lead to catastrophic consequences. However, existing Text-to-SQL models are trained on clean, neutral datasets, such as Spider and WikiSQL. This, to some extent, cover up social bias in models under ideal conditions, which nevertheless may emerge in real application scenarios. In this work, we aim to uncover and mitigate social bias in Text-to-SQL models. We summarize the categories of social bias that may occur in structural data for Text-to-SQL models. We build test benchmarks and reveal that models with similar task accuracy can contain social bias at very different rates. We show how to take advantage of our methodology to assess and mitigate social bias in the downstream Text-to-SQL task.",
      "Large language models (LLMs) recently exhibited remarkable reasoning capabilities on solving math problems. To further improve their reasoning capabilities, this work explores whether LLMs can LEarn from MistAkes (LEMA), akin to the human learning process. Consider a human student who failed to solve a math problem, he will learn from what mistake he has made and how to correct it. Mimicking this error-driven learning process, LEMA incorporates mistake-correction data pairs during fine-tuning LLMs. Specifically, we first collect inaccurate reasoning paths from various LLMs, and then employ GPT-4 as a ''corrector'' to identify the mistake step, explain the reason for the mistake, correct the mistake and generate the final answer. In addition, we apply a correction-centric evolution strategy that effectively expands the question set for generating correction data. Experiments across various LLMs and reasoning tasks show that LEMA effectively improves CoT-alone fine-tuning. Our further ablations shed light on the non-homogeneous effectiveness between CoT data and correction data. These results suggest a significant potential for LLMs to improve through learning from their mistakes. Our code, models and prompts are publicly available at https://github.com/microsoft/LEMA.",
      "Question answering plays a pivotal role in human daily life because it involves our acquisition of knowledge about the world. However, due to the dynamic and ever-changing nature of real-world facts, the answer can be completely different when the time constraint in the question changes. Recently, Large Language Models (LLMs) have shown remarkable intelligence in question answering, while our experiments reveal that the aforementioned problems still pose a significant challenge to existing LLMs. This can be attributed to the LLMs' inability to perform rigorous reasoning based on surface-level text semantics. To overcome this limitation, rather than requiring LLMs to directly answer the question, we propose a novel approach where we reframe the $\\textbf{Q}$uestion $\\textbf{A}$nswering task $\\textbf{a}$s $\\textbf{P}$rogramming ($\\textbf{QAaP}$). Concretely, by leveraging modern LLMs' superior capability in understanding both natural language and programming language, we endeavor to harness LLMs to represent diversely expressed text as well-structured code and select the best matching answer from multiple candidates through programming. We evaluate our QAaP framework on several time-sensitive question answering datasets and achieve decent improvement, up to $14.5$% over strong baselines. Our codes and data are available at https://github.com/TianHongZXY/qaap",
      "The task of repository-level code completion is to continue writing the unfinished code based on a broader context of the repository. While for automated code completion tools, it is difficult to utilize the useful information scattered in different files. We propose RepoCoder, a simple, generic, and effective framework to address the challenge. It streamlines the repository-level code completion process by incorporating a similarity-based retriever and a pre-trained code language model in an iterative retrieval-generation pipeline. RepoCoder makes effective utilization of repository-level information for code completion and has the ability to generate code at various levels of granularity. Moreover, we propose a new benchmark RepoEval, which consists of the latest and high-quality real-world repositories covering line, API invocation, and function body completion scenarios. Experimental results indicate that RepoCoder significantly improves the In-File completion baseline by over 10% in all settings and consistently outperforms the vanilla retrieval-augmented code completion approach. Furthermore, we validate the effectiveness of RepoCoder through comprehensive analysis, providing valuable insights for future research. Our source code and benchmark are publicly available: https://github.com/microsoft/CodeT/tree/main/RepoCoder",
      "In this paper, we study the problem of knowledge-intensive text-to-SQL, in which domain knowledge is necessary to parse expert questions into SQL queries over domain-specific tables. We formalize this scenario by building a new benchmark KnowSQL consisting of domain-specific questions covering various domains. We then address this problem by representing formulaic knowledge rather than by annotating additional data examples. More concretely, we construct a formulaic knowledge bank as a domain knowledge base and propose a framework (ReGrouP) to leverage this formulaic knowledge during parsing. Experiments using ReGrouP demonstrate a significant 28.2% improvement overall on KnowSQL.",
      "With the popularity of automatic code generation tools, such as Copilot, the study of the potential hazards of these tools is gaining importance. In this work, we explore the social bias problem in pre-trained code generation models. We propose a new paradigm to construct code prompts and successfully uncover social biases in code generation models. To quantify the severity of social biases in generated code, we develop a dataset along with three metrics to evaluate the overall social bias and fine-grained unfairness across different demographics. Experimental results on three pre-trained code generation models (Codex, InCoder, and CodeGen) with varying sizes, reveal severe social biases. Moreover, we conduct analysis to provide useful insights for further choice of code generation models with low social bias. (This work contains examples that potentially implicate stereotypes, associations, and other harms that could be offensive to individuals in certain social groups.)",
      "Automated debugging techniques have the potential to reduce developer effort in debugging, and have matured enough to be adopted by industry. However, one critical issue with existing techniques is that, while developers want rationales for the provided automatic debugging results, existing techniques are ill-suited to provide them, as their deduction process differs significantly from that of human developers. Inspired by the way developers interact with code when debugging, we propose Automated Scientific Debugging (AutoSD), a technique that given buggy code and a bug-revealing test, prompts large language models to automatically generate hypotheses, uses debuggers to actively interact with buggy code, and thus automatically reach conclusions prior to patch generation. By aligning the reasoning of automated debugging more closely with that of human developers, we aim to produce intelligible explanations of how a specific patch has been generated, with the hope that the explanation will lead to more efficient and accurate developer decisions. Our empirical analysis on three program repair benchmarks shows that AutoSD performs competitively with other program repair baselines, and that it can indicate when it is confident in its results. Furthermore, we perform a human study with 20 participants, including six professional developers, to evaluate the utility of explanations from AutoSD. Participants with access to explanations could judge patch correctness in roughly the same time as those without, but their accuracy improved for five out of six real-world bugs studied: 70% of participants answered that they wanted explanations when using repair tools, while 55% answered that they were satisfied with the Scientific Debugging presentation.",
      "Creating graphic layouts is a fundamental step in graphic designs. In this work, we present a novel generative model named LayoutDiffusion for automatic layout generation. As layout is typically represented as a sequence of discrete tokens, LayoutDiffusion models layout generation as a discrete denoising diffusion process. It learns to reverse a mild forward process, in which layouts become increasingly chaotic with the growth of forward steps and layouts in the neighboring steps do not differ too much. Designing such a mild forward process is however very challenging as layout has both categorical attributes and ordinal attributes. To tackle the challenge, we summarize three critical factors for achieving a mild forward process for the layout, i.e., legality, coordinate proximity and type disruption. Based on the factors, we propose a block-wise transition matrix coupled with a piece-wise linear noise schedule. Experiments on RICO and PubLayNet datasets show that LayoutDiffusion outperforms state-of-the-art approaches significantly. Moreover, it enables two conditional layout generation tasks in a plug-and-play manner without re-training and achieves better performance than existing methods. Project page: https://layoutdiffusion.github.io.",
      "Reasoning over natural language is a long-standing goal for the research community. However, studies have shown that existing language models are inadequate in reasoning. To address the issue, we present POET, a novel reasoning pre-training paradigm. Through pre-training language models with programs and their execution results, POET empowers language models to harvest the reasoning knowledge possessed by program executors via a data-driven approach. POET is conceptually simple and can be instantiated by different kinds of program executors. In this paper, we showcase two simple instances POET-Math and POET-Logic, in addition to a complex instance, POET-SQL. Experimental results on six benchmarks demonstrate that POET can significantly boost model performance in natural language reasoning, such as numerical reasoning, logical reasoning, and multi-hop reasoning. POET opens a new gate on reasoning-enhancement pre-training, and we hope our analysis would shed light on the future research of reasoning like program executors.",
      "Because of the compositionality of natural lan-001 guage, syntactic structure is a key factor for 002 semantic understanding in dialogue generation 003 tasks. However, the widely adopted Trans-004 former is hard to learn the compositionaity ef-005 fectively, because the position embeddings con-006 tain less semantic relation information. To ex-007 plicit model the compositionaity of language, 008 we limit the information flow between words 009 by constructing directed dependency relation 010 graph and propose Dependency Relation At-011 tention (DRA) to replace position embeddings. 012 Experimental results demonstrate that DRA can 013 further improve the performance of state-of-the-014 art models for multi-turn dialogue generation. 015",
      "Recently the prompt-tuning paradigm has attracted significant attention. By only tuning continuous prompts with a frozen pre-trained language model (PLM), prompt-tuning takes a step towards deploying a shared frozen PLM to serve numerous downstream tasks. Although prompt-tuning shows good performance on certain natural language understanding (NLU) tasks, its effectiveness on natural language generation (NLG) tasks is still under-explored. In this paper, we argue that one of the factors hindering the development of prompt-tuning on NLG tasks is the unfamiliar inputs (i.e., inputs are linguistically different from the pretraining corpus). For example, our preliminary exploration reveals a large performance gap between prompt-tuning and fine-tuning when unfamiliar inputs occur frequently in NLG tasks. This motivates us to propose input-tuning, which fine-tunes both the continuous prompts and the input representations, leading to a more effective way to adapt unfamiliar inputs to frozen PLMs. Our proposed input-tuning is conceptually simple and empirically powerful. Experimental results on seven NLG tasks demonstrate that input-tuning is significantly and consistently better than prompt-tuning. Furthermore, on three of these tasks, input-tuning can achieve a comparable or even better performance than fine-tuning."
    ],
    "domain": [
      "Natural Language Processing",
      "Few-Shot Learning",
      "Reasoning",
      "Code Generation"
    ],
    "is_leader_candidate": true,
    "is_member_candidate": true,
    "is_reviewer_candidate": true,
    "is_chair_candidate": true
  },
  "5d82cf41-44f9-43d4-ba1f-15d34f898f35": {
    "pk": "5d82cf41-44f9-43d4-ba1f-15d34f898f35",
    "name": "Chenyi Zi",
    "bio": "I am a researcher dedicated to advancing the fields of protein structure prediction and anomaly detection through innovative machine learning techniques. My recent work has focused on developing a Generative Adversarial Policy Network (GAPN) for protein complex modeling (PCM), where I tackle the challenges of combinatorial optimization and distribution shifts in protein complexes. By framing each protein chain as a node and assembly actions as edges, I have created a framework that efficiently navigates the complex assembly space, achieving significant improvements in accuracy and efficiency over existing PCM software.\n\nIn addition to my work in protein modeling, I have also made strides in the realm of anomaly detection. I introduced the Knowledge-Data Alignment (KDAlign) framework, which integrates expert rule knowledge to enhance weakly supervised anomaly detection. By employing Optimal Transport techniques, I align knowledge with data to improve model generalization on unseen anomalies. My experiments across various real-world datasets demonstrate that KDAlign significantly outperforms state-of-the-art methods, showcasing my commitment to bridging the gap between theoretical advancements and practical applications.\n\nThrough my research, I aim to contribute to the understanding of complex biological systems and improve the reliability of anomaly detection in critical applications. I am passionate about leveraging machine learning to solve real-world challenges and continuously seek to push the boundaries of what is possible in these domains.",
    "collaborators": [
      "Yan Zhou",
      "Chen Zhang",
      "Jia Li",
      "Ziqi Gao",
      "Tao Feng",
      "Jiaxuan You",
      "Haihong Zhao",
      "Yang Liu"
    ],
    "pub_titles": [
      "Deep Reinforcement Learning for Modelling Protein Complexes",
      "Weakly Supervised Anomaly Detection via Knowledge-Data Alignment"
    ],
    "pub_abstracts": [
      "AlphaFold can be used for both single-chain and multi-chain protein structure prediction, while the latter becomes extremely challenging as the number of chains increases. In this work, by taking each chain as a node and assembly actions as edges, we show that an acyclic undirected connected graph can be used to predict the structure of multi-chain protein complexes (a.k.a., protein complex modelling, PCM). However, there are still two challenges: 1) The huge combinatorial optimization space of $N^{N-2}$ ($N$ is the number of chains) for the PCM problem can easily lead to high computational cost. 2) The scales of protein complexes exhibit distribution shift due to variance in chain numbers, which calls for the generalization in modelling complexes of various scales. To address these challenges, we propose GAPN, a Generative Adversarial Policy Network powered by domain-specific rewards and adversarial loss through policy gradient for automatic PCM prediction. Specifically, GAPN learns to efficiently search through the immense assembly space and optimize the direct docking reward through policy gradient. Importantly, we design an adversarial reward function to enhance the receptive field of our model. In this way, GAPN will simultaneously focus on a specific batch of complexes and the global assembly rules learned from complexes with varied chain numbers. Empirically, we have achieved both significant accuracy (measured by RMSD and TM-Score) and efficiency improvements compared to leading PCM softwares.",
      "Anomaly detection (AD) plays a pivotal role in numerous web-based applications, including malware detection, anti-money laundering, device failure detection, and network fault analysis. Most methods, which rely on unsupervised learning, are hard to reach satisfactory detection accuracy due to the lack of labels. Weakly Supervised Anomaly Detection (WSAD) has been introduced with a limited number of labeled anomaly samples to enhance model performance. Nevertheless, it is still challenging for models, trained on an inadequate amount of labeled data, to generalize to unseen anomalies. In this paper, we introduce a novel framework, Knowledge-Data Alignment (KDAlign), to integrate rule knowledge, typically summarized by human experts, to supplement the limited labeled data. Specifically, we transpose these rules into the knowledge space and subsequently recast the incorporation of knowledge as the alignment of knowledge and data. To facilitate this alignment, we employ the Optimal Transport (OT) technique. We then incorporate the OT distance as an additional loss term to the original objective function of WSAD methodologies. Comprehensive experimental results on five real-world datasets demonstrate that our proposed KDAlign framework markedly surpasses its state-of-the-art counterparts, achieving superior performance across various anomaly types. Our codes are released at https://github.com/cshhzhao/KDAlign."
    ],
    "domain": [
      "Protein Structure Prediction",
      "Anomaly Detection",
      "Weakly Supervised Learning",
      "Graph Neural Network"
    ],
    "is_leader_candidate": true,
    "is_member_candidate": true,
    "is_reviewer_candidate": true,
    "is_chair_candidate": true
  },
  "6651f973-ca64-4ed9-857c-fee2f99afa93": {
    "pk": "6651f973-ca64-4ed9-857c-fee2f99afa93",
    "name": "Haihong Zhao",
    "bio": "I am a researcher dedicated to advancing the intersection of graph neural networks (GNNs) and machine learning, particularly in the context of multi-object physical systems and anomaly detection. My recent work explores the transformative potential of large language models (LLMs) in the graph domain, culminating in the development of Graph COordinators for PrEtraining (GCOPE). This innovative approach leverages the commonalities across diverse graph datasets to enhance few-shot learning, marking a significant step forward in the field.\n\nIn addition to GCOPE, I have introduced the Knowledge-Data Alignment (KDAlign) framework, which integrates expert rule knowledge to bolster weakly supervised anomaly detection. By employing Optimal Transport techniques, KDAlign effectively aligns knowledge with data, achieving superior performance across various anomaly types.\n\nMy research also delves into the dynamics of multi-object physical systems through the Physics-Inspired Neural Graph ODE (PINGO) and the Second-order Equivariant Graph Neural Ordinary Differential Equation (SEGNO). These models address the limitations of existing GNNs by incorporating physical inductive biases, such as continuity and second-order motion laws, to improve generalization and long-term prediction accuracy.\n\nThrough my work, I aim to bridge theoretical insights with practical applications, pushing the boundaries of what is possible in graph-based learning and modeling. I am passionate about developing methodologies that not only advance academic understanding but also have real-world implications across various domains.",
    "collaborators": [
      "Jia Li",
      "Yang Liu",
      "Jiashun Cheng",
      "Tingyang Xu",
      "F. Tsung",
      "Yu Rong",
      "Aochuan Chen",
      "Xiangguo Sun",
      "Hong Cheng",
      "Chenyi Zi",
      "Chen Zhang",
      "Yan Zhou",
      "Peilin Zhao",
      "P. Zhao"
    ],
    "pub_titles": [
      "All in One and One for All: A Simple yet Effective Method towards Cross-domain Graph Pretraining",
      "Weakly Supervised Anomaly Detection via Knowledge-Data Alignment",
      "Physics-Inspired Neural Graph ODE for Long-term Dynamical Simulation",
      "SEGNO: Generalizing Equivariant Graph Neural Networks with Physical Inductive Biases"
    ],
    "pub_abstracts": [
      "Large Language Models (LLMs) have revolutionized the fields of computer vision (CV) and natural language processing (NLP). One of the most notable advancements of LLMs is that a single model is trained on vast and diverse datasets spanning multiple domains -- a paradigm we term `All in One'. This methodology empowers LLMs with super generalization capabilities, facilitating an encompassing comprehension of varied data distributions. Leveraging these capabilities, a single LLM demonstrates remarkable versatility across a variety of domains -- a paradigm we term `One for All'. However, applying this idea to the graph field remains a formidable challenge, with cross-domain pretraining often resulting in negative transfer. This issue is particularly important in few-shot learning scenarios, where the paucity of training data necessitates the incorporation of external knowledge sources. In response to this challenge, we propose a novel approach called Graph COordinators for PrEtraining (GCOPE), that harnesses the underlying commonalities across diverse graph datasets to enhance few-shot learning. Our novel methodology involves a unification framework that amalgamates disparate graph datasets during the pretraining phase to distill and transfer meaningful knowledge to target tasks. Extensive experiments across multiple graph datasets demonstrate the superior efficacy of our approach. By successfully leveraging the synergistic potential of multiple graph datasets for pretraining, our work stands as a pioneering contribution to the realm of graph foundational model.",
      "Anomaly detection (AD) plays a pivotal role in numerous web-based applications, including malware detection, anti-money laundering, device failure detection, and network fault analysis. Most methods, which rely on unsupervised learning, are hard to reach satisfactory detection accuracy due to the lack of labels. Weakly Supervised Anomaly Detection (WSAD) has been introduced with a limited number of labeled anomaly samples to enhance model performance. Nevertheless, it is still challenging for models, trained on an inadequate amount of labeled data, to generalize to unseen anomalies. In this paper, we introduce a novel framework, Knowledge-Data Alignment (KDAlign), to integrate rule knowledge, typically summarized by human experts, to supplement the limited labeled data. Specifically, we transpose these rules into the knowledge space and subsequently recast the incorporation of knowledge as the alignment of knowledge and data. To facilitate this alignment, we employ the Optimal Transport (OT) technique. We then incorporate the OT distance as an additional loss term to the original objective function of WSAD methodologies. Comprehensive experimental results on five real-world datasets demonstrate that our proposed KDAlign framework markedly surpasses its state-of-the-art counterparts, achieving superior performance across various anomaly types. Our codes are released at https://github.com/cshhzhao/KDAlign.",
      "Simulating and modeling the long-term dynamics of multi-object physical systems is an essential and challenging task. Current studies model the physical systems utilizing Graph Neural Networks (GNNs) with equivariant properties. Specifically, they model the dynamics as a sequence of discrete states with a fixed time interval and learn a direct mapping for all the two adjacent states. However, this direct mapping overlooks the continuous nature between the two states. Namely, we have verified that there are countless possible trajectories between two discrete dynamic states in current GNN-based direct mapping models. This issue greatly hinders the model generalization ability, leading to poor performance of the long-term simulation. In this paper, to better model the latent trajectory through discrete supervision signals, we propose a P hysics-I nspired N eural G raph O DE (PINGO) algorithm. In PINGO, to ensure the uniqueness of the trajectory, we construct a Physics-Inspired Neural ODE framework to update the latent trajectory. Meanwhile, to effectively capture intricate interactions among objects, we use a GNN-based model to parameterize Neural ODE in a plug-and-play manner. Furthermore, we prove that the discrepancy between the learned trajectory of PIGNO and the true trajectory can be theoretically bounded. Extensive experiments verify our theoretical findings and demonstrate that our model yields an order-of-magnitude improvement over the state-of-the-art baselines, especially on long-term predictions and roll-out errors.",
      "Graph Neural Networks (GNNs) with equivariant properties have emerged as powerful tools for modeling complex dynamics of multi-object physical systems. However, their generalization ability is limited by the inadequate consideration of physical inductive biases: (1) Existing studies overlook the continuity of transitions among system states, opting to employ several discrete transformation layers to learn the direct mapping between two adjacent states; (2) Most models only account for first-order velocity information, despite the fact that many physical systems are governed by second-order motion laws. To incorporate these inductive biases, we propose the Second-order Equivariant Graph Neural Ordinary Differential Equation (SEGNO). Specifically, we show how the second-order continuity can be incorporated into GNNs while maintaining the equivariant property. Furthermore, we offer theoretical insights into SEGNO, highlighting that it can learn a unique trajectory between adjacent states, which is crucial for model generalization. Additionally, we prove that the discrepancy between this learned trajectory of SEGNO and the true trajectory is bounded. Extensive experiments on complex dynamical systems including molecular dynamics and motion capture demonstrate that our model yields a significant improvement over the state-of-the-art baselines."
    ],
    "domain": [
      "Graph Neural Network",
      "Anomaly Detection",
      "Physics-Inspired Modeling",
      "Few-Shot Learning"
    ],
    "is_leader_candidate": true,
    "is_member_candidate": true,
    "is_reviewer_candidate": true,
    "is_chair_candidate": true
  },
  "aa4a554d-21e1-4dea-9664-d82be9ed27c6": {
    "pk": "aa4a554d-21e1-4dea-9664-d82be9ed27c6",
    "name": "Xiangguo Sun",
    "bio": "I am a researcher deeply engaged in the intersection of large language models (LLMs) and graph-based methodologies, with a focus on enhancing performance across diverse domains such as recommendation systems, social network analysis, and biological data interpretation. My recent work has pioneered the concept of \"All in One\" and \"One for All\" paradigms, which leverage the super generalization capabilities of LLMs to tackle challenges in graph data, particularly in few-shot learning scenarios. \n\nOne of my notable contributions is the Graph COordinators for PrEtraining (GCOPE) framework, which unifies disparate graph datasets to enhance knowledge transfer and improve few-shot learning outcomes. Additionally, I have developed HAGO, a framework that dynamically integrates multi-domain graphs to optimize recommendation systems while mitigating negative transfer effects. \n\nMy research also delves into temporal interaction graphs, where I introduced TIGPrompt, a framework that bridges temporal and semantic gaps in representation learning. I am particularly interested in the application of prompt learning techniques to various graph tasks, as evidenced by my work on PromptMSP for multimer structure prediction and a hierarchical approach to drug-drug interaction prediction.\n\nThrough my extensive publications, I aim to advance the understanding of how LLMs can be effectively integrated with graph data, exploring both theoretical foundations and practical applications. I am committed to pushing the boundaries of graph prompting and its implications for artificial general intelligence, ultimately striving to reshape how we approach complex data relationships in the digital age.",
    "collaborators": [
      "Hong Cheng",
      "Jia Li",
      "Yun Xiong",
      "Xixi Wu",
      "Jiawei Zhang",
      "Haihong Zhao",
      "Aochuan Chen",
      "Zhiyao Shu",
      "Hengyu Zhang",
      "Chunxu Shen",
      "Jie Tan",
      "Yu Rong",
      "Chengzhi Piao",
      "Lingling Yi",
      "Xi Chen",
      "Siwei Zhang",
      "Yao Zhang",
      "Feng Zhao",
      "Yulin Kang",
      "Ziqi Gao",
      "Zijing Liu",
      "Yu Li",
      "Yingying Wang",
      "Bo Liu",
      "Jihong Guan",
      "Qunzhong Wang",
      "Yuhan Li",
      "Zhixun Li",
      "Peisong Wang",
      "Hongtao Cheng",
      "Jeffrey Xu Yu",
      "Jiawen Zhang",
      "Hang Dong",
      "Bo Qiao",
      "Si Qin",
      "Qingwei Lin"
    ],
    "pub_titles": [
      "All in One and One for All: A Simple yet Effective Method towards Cross-domain Graph Pretraining",
      "When LLM Meets Hypergraph: A Sociological Analysis on Personality via Online Social Networks",
      "Adaptive Coordinators and Prompts on Heterogeneous Graphs for Cross-Domain Recommendations",
      "Prompt Learning on Temporal Interaction Graphs",
      "Protein Multimer Structure Prediction via Prompt Learning",
      "Advanced Drug Interaction Event Prediction",
      "All in One: Multi-Task Prompting for Graph Neural Networks (Extended Abstract)",
      "Does Graph Prompt Work? A Data Operation Perspective with Theoretical Analysis",
      "A Survey of Graph Meets Large Language Model: Progress and Future Directions",
      "Graph Prompt Learning: A Comprehensive Survey and Beyond",
      "Counter-Empirical Attacking Based on Adversarial Reinforcement Learning for Time-Relevant Scoring System"
    ],
    "pub_abstracts": [
      "Large Language Models (LLMs) have revolutionized the fields of computer vision (CV) and natural language processing (NLP). One of the most notable advancements of LLMs is that a single model is trained on vast and diverse datasets spanning multiple domains -- a paradigm we term `All in One'. This methodology empowers LLMs with super generalization capabilities, facilitating an encompassing comprehension of varied data distributions. Leveraging these capabilities, a single LLM demonstrates remarkable versatility across a variety of domains -- a paradigm we term `One for All'. However, applying this idea to the graph field remains a formidable challenge, with cross-domain pretraining often resulting in negative transfer. This issue is particularly important in few-shot learning scenarios, where the paucity of training data necessitates the incorporation of external knowledge sources. In response to this challenge, we propose a novel approach called Graph COordinators for PrEtraining (GCOPE), that harnesses the underlying commonalities across diverse graph datasets to enhance few-shot learning. Our novel methodology involves a unification framework that amalgamates disparate graph datasets during the pretraining phase to distill and transfer meaningful knowledge to target tasks. Extensive experiments across multiple graph datasets demonstrate the superior efficacy of our approach. By successfully leveraging the synergistic potential of multiple graph datasets for pretraining, our work stands as a pioneering contribution to the realm of graph foundational model.",
      "Individual personalities significantly influence our perceptions, decisions, and social interactions, which is particularly crucial for gaining insights into human behavior patterns in online social network analysis. Many psychological studies have observed that personalities are strongly reflected in their social behaviors and social environments. In light of these problems, this paper proposes a sociological analysis framework for one's personality in an environment-based view instead of individual-level data mining. Specifically, to comprehensively understand an individual's behavior from low-quality records, we leverage the powerful associative ability of LLMs by designing an effective prompt. In this way, LLMs can integrate various scattered information with their external knowledge to generate higher-quality profiles, which can significantly improve the personality analysis performance. To explore the interactive mechanism behind the users and their online environments, we design an effective hypergraph neural network where the hypergraph nodes are users and the hyperedges in the hypergraph are social environments. We offer a useful dataset with user profile data, personality traits, and several detected environments from the real-world social platform. To the best of our knowledge, this is the first network-based dataset containing both hypergraph structure and social information, which could push forward future research in this area further. By employing the framework on this dataset, we can effectively capture the nuances of individual personalities and their online behaviors, leading to a deeper understanding of human interactions in the digital world.",
      "In the online digital world, users frequently engage with diverse items across multiple domains (e.g., e-commerce platforms, streaming services, and social media networks), forming complex heterogeneous interaction graphs. Leveraging this multi-domain information can undoubtedly enhance the performance of recommendation systems by providing more comprehensive user insights and alleviating data sparsity in individual domains. However, integrating multi-domain knowledge for the cross-domain recommendation is very hard due to inherent disparities in user behavior and item characteristics and the risk of negative transfer, where irrelevant or conflicting information from the source domains adversely impacts the target domain's performance. To address these challenges, we offer HAGO, a novel framework with $\\textbf{H}$eterogeneous $\\textbf{A}$daptive $\\textbf{G}$raph co$\\textbf{O}$rdinators, which dynamically integrate multi-domain graphs into a cohesive structure by adaptively adjusting the connections between coordinators and multi-domain graph nodes, thereby enhancing beneficial inter-domain interactions while mitigating negative transfer effects. Additionally, we develop a universal multi-domain graph pre-training strategy alongside HAGO to collaboratively learn high-quality node representations across domains. To effectively transfer the learned multi-domain knowledge to the target domain, we design an effective graph prompting method, which incorporates pre-trained embeddings with learnable prompts for the recommendation task. Our framework is compatible with various graph-based models and pre-training techniques, demonstrating broad applicability and effectiveness. Further experimental results show that our solutions outperform state-of-the-art methods in multi-domain recommendation scenarios and highlight their potential for real-world applications.",
      "Temporal Interaction Graphs (TIGs) are widely utilized to represent real-world systems. To facilitate representation learning on TIGs, researchers have proposed a series of TIG models. However, these models are still facing two tough gaps between the pre-training and downstream predictions in their ``pre-train, predict'' training paradigm. First, the temporal discrepancy between the pre-training and inference data severely undermines the models' applicability in distant future predictions on the dynamically evolving data. Second, the semantic divergence between pretext and downstream tasks hinders their practical applications, as they struggle to align with their learning and prediction capabilities across application scenarios. Recently, the ``pre-train, prompt'' paradigm has emerged as a lightweight mechanism for model generalization. Applying this paradigm is a potential solution to solve the aforementioned challenges. However, the adaptation of this paradigm to TIGs is not straightforward. The application of prompting in static graph contexts falls short in temporal settings due to a lack of consideration for time-sensitive dynamics and a deficiency in expressive power. To address this issue, we introduce Temporal Interaction Graph Prompting (TIGPrompt), a versatile framework that seamlessly integrates with TIG models, bridging both the temporal and semantic gaps. In detail, we propose a temporal prompt generator to offer temporally-aware prompts for different tasks. These prompts stand out for their minimalistic design, relying solely on the tuning of the prompt generator with very little supervision data. To cater to varying computational resource demands, we propose an extended ``pre-train, prompt-based fine-tune'' paradigm, offering greater flexibility. Through extensive experiments, the TIGPrompt demonstrates the SOTA performance and remarkable efficiency advantages.",
      "Understanding the 3D structures of protein multimers is crucial, as they play a vital role in regulating various cellular processes. It has been empirically confirmed that the multimer structure prediction~(MSP) can be well handled in a step-wise assembly fashion using provided dimer structures and predicted protein-protein interactions~(PPIs). However, due to the biological gap in the formation of dimers and larger multimers, directly applying PPI prediction techniques can often cause a \\textit{poor generalization} to the MSP task. To address this challenge, we aim to extend the PPI knowledge to multimers of different scales~(i.e., chain numbers). Specifically, we propose \\textbf{\\textsc{PromptMSP}}, a pre-training and \\textbf{Prompt} tuning framework for \\textbf{M}ultimer \\textbf{S}tructure \\textbf{P}rediction. First, we tailor the source and target tasks for effective PPI knowledge learning and efficient inference, respectively. We design PPI-inspired prompt learning to narrow the gaps of two task formats and generalize the PPI knowledge to multimers of different scales. We provide a meta-learning strategy to learn a reliable initialization of the prompt model, enabling our prompting framework to effectively adapt to limited data for large-scale multimers. Empirically, we achieve both significant accuracy (RMSD and TM-Score) and efficiency improvements compared to advanced MSP models. The code, data and checkpoints are released at \\url{https://github.com/zqgao22/PromptMSP}.",
      "Predicting drug-drug interaction adverse events, so-called DDI events, is increasingly valuable as it facilitates the study of mechanisms underlying drug use or adverse reactions. Existing models often neglect the distinctive characteristics of individual event classes when integrating multi-source features, which contributes to systematic unfairness when dealing with highly imbalanced event samples. Moreover, the limited capacity of these models to abstract the unique attributes of each event subclass considerably hampers their application in predicting rare drug-drug interaction events with a limited sample size. Reducing dataset bias and abstracting event subclass characteristics are two unresolved challenges. Recently, prompt tuning with frozen pre-trained graph models, namely\"pre-train, prompt, fine-tune\"strategy, has demonstrated impressive performance in few-shot tasks. Motivated by this, we propose an advanced method as a solution to address these aforementioned challenges. Specifically, our proposed approach entails a hierarchical pre-training task that aims to capture crucial aspects of drug molecular structure and intermolecular interactions while effectively mitigating implicit dataset bias within the node embeddings. Furthermore, we construct a prototypical graph by strategically sampling data from distinct event types and design subgraph prompts utilizing pre-trained node features. Through comprehensive benchmark experiments, we validate the efficacy of our subgraph prompts in accurately representing event classes and achieve exemplary results in both overall and subclass prediction tasks.",
      "This paper is an extended abstract of our original work published in KDD23, where we won the best research paper award. The paper introduces a novel approach to bridging the gap between pre-trained graph models and the diverse tasks they\u2019re applied to, inspired by the success of prompt learning in NLP. Recognizing the challenge of aligning pre-trained models with varied graph tasks (node level, edge level, and graph level), which can lead to negative transfer and poor performance, we propose a multi-task prompting method for graphs. This method involves unifying graph and language prompt formats, enabling NLP\u2019s prompting strategies to be adapted for graph tasks. By analyzing the task space of graph applications, we reformulate problems to fit graph-level tasks and apply meta-learning to improve prompt initialization for multiple tasks. Experiments show our method\u2019s effectiveness in enhancing model performance across different graph tasks. Beyond the original work, in this extended abstract, we further discuss the graph prompt from a bigger picture and provide some of the latest work toward this area.",
      "In recent years, graph prompting has emerged as a promising research direction, enabling the learning of additional tokens or subgraphs appended to the original graphs without requiring retraining of pre-trained graph models across various applications. This novel paradigm, shifting from the traditional pretraining and finetuning to pretraining and prompting has shown significant empirical success in simulating graph data operations, with applications ranging from recommendation systems to biological networks and graph transferring. However, despite its potential, the theoretical underpinnings of graph prompting remain underexplored, raising critical questions about its fundamental effectiveness. The lack of rigorous theoretical proof of why and how much it works is more like a dark cloud over the graph prompt area to go further. To fill this gap, this paper introduces a theoretical framework that rigorously analyzes graph prompting from a data operation perspective. Our contributions are threefold: First, we provide a formal guarantee theorem, demonstrating graph prompts capacity to approximate graph transformation operators, effectively linking upstream and downstream tasks. Second, we derive upper bounds on the error of these data operations by graph prompts for a single graph and extend this discussion to batches of graphs, which are common in graph model training. Third, we analyze the distribution of data operation errors, extending our theoretical findings from linear graph models (e.g., GCN) to non-linear graph models (e.g., GAT). Extensive experiments support our theoretical results and confirm the practical implications of these guarantees.",
      "Graph plays a significant role in representing and analyzing complex relationships in real-world applications such as citation networks, social networks, and biological data. Recently, Large Language Models (LLMs), which have achieved tremendous success in various domains, have also been leveraged in graph-related tasks to surpass traditional Graph Neural Networks (GNNs) based methods and yield state-of-the-art performance. In this survey, we first present a comprehensive review and analysis of existing methods that integrate LLMs with graphs. First of all, we propose a new taxonomy, which organizes existing methods into three categories based on the role (i.e., enhancer, predictor, and alignment component) played by LLMs in graph-related tasks. Then we systematically survey the representative methods along the three categories of the taxonomy. Finally, we discuss the remaining limitations of existing studies and highlight promising avenues for future research. The relevant papers are summarized and will be consistently updated at: https://github.com/yhLeeee/Awesome-LLMs-in-Graph-tasks.",
      "Artificial General Intelligence (AGI) has revolutionized numerous fields, yet its integration with graph data, a cornerstone in our interconnected world, remains nascent. This paper presents a pioneering survey on the emerging domain of graph prompts in AGI, addressing key challenges and opportunities in harnessing graph data for AGI applications. Despite substantial advancements in AGI across natural language processing and computer vision, the application to graph data is relatively underexplored. This survey critically evaluates the current landscape of AGI in handling graph data, highlighting the distinct challenges in cross-modality, cross-domain, and cross-task applications specific to graphs. Our work is the first to propose a unified framework for understanding graph prompt learning, offering clarity on prompt tokens, token structures, and insertion patterns in the graph domain. We delve into the intrinsic properties of graph prompts, exploring their flexibility, expressiveness, and interplay with existing graph models. A comprehensive taxonomy categorizes over 100 works in this field, aligning them with pre-training tasks across node-level, edge-level, and graph-level objectives. Additionally, we present, ProG, a Python library, and an accompanying website, to support and advance research in graph prompting. The survey culminates in a discussion of current challenges and future directions, offering a roadmap for research in graph prompting within AGI. Through this comprehensive analysis, we aim to catalyze further exploration and practical applications of AGI in graph data, underlining its potential to reshape AGI fields and beyond. ProG and the website can be accessed by \\url{https://github.com/WxxShirley/Awesome-Graph-Prompt}, and \\url{https://github.com/sheldonresearch/ProG}, respectively.",
      "Scoring systems are commonly seen for platforms in the era of Big Data. From credit scoring systems in financial services to membership scores in E-commerce shopping platforms, platform managers use such systems to guide users towards the encouraged activity pattern, and manage resources more effectively and efficiently. To establish such scoring systems, several \u201cempirical criteria\u201d are first determined, followed by a dedicated top-down design for each score factor, which usually requires enormous effort to adjust and tune the scoring function in the new application scenario. What's worse, many fresh projects usually have no ground truth or any experience to evaluate a reasonable scoring system, making the designing even harder. To reduce the effort of manual adjustment of the scoring function in every new scoring system, we innovatively study the scoring system from the preset empirical criteria without any ground truth and propose a novel framework to improve the system from scratch. In this paper, we propose a \u201ccounter-empirical attacking\u201d mechanism that can generate \u201cattacking\u201d behavior traces and try to break the empirical rules of the scoring system. Then an adversarial \u201cenhancer\u201d is applied to evaluate the scoring system and find the improvement strategy. By training the adversarial learning problem, a proper scoring function can be learned to be robust to the attacking activity traces that are trying to violate the empirical criteria. Extensive experiments have been conducted on two scoring systems, including a shared computing resource platform and a financial credit system. The experimental results have validated the effectiveness of our proposed framework."
    ],
    "domain": [
      "Graph Neural Network",
      "Large Language Models",
      "Multi-domain Learning",
      "Drug Interaction Prediction"
    ],
    "is_leader_candidate": true,
    "is_member_candidate": true,
    "is_reviewer_candidate": true,
    "is_chair_candidate": true
  },
  "096e2ae0-1b8f-438c-a792-7b4da5a4b521": {
    "pk": "096e2ae0-1b8f-438c-a792-7b4da5a4b521",
    "name": "Kostadin Cvejoski",
    "bio": "I am a researcher dedicated to advancing the understanding and application of dynamical systems, particularly through the lens of ordinary differential equations (ODEs). My recent work focuses on developing innovative machine learning frameworks for zero-shot inference and time series imputation, leveraging neural networks to extract meaningful insights from noisy data. I have introduced foundational inference models (FIM) that outperform traditional methods by enabling zero-shot learning across various dynamical systems, demonstrating the power of pretrained models in diverse applications.\n\nIn addition to my work on ODEs, I explore the intersection of language models and temporal dynamics, addressing challenges in natural language processing and recommender systems. My research includes developing dynamic topic models that account for the evolution of topics over time, as well as enhancing the performance of large pretrained language models in the face of distribution shifts.\n\nI am also passionate about knowledge-augmented machine learning, where I integrate expert knowledge into model training to improve generalization and robustness, especially in scenarios with limited data. My work spans various domains, from environmental modeling to password generation, and I strive to create methodologies that are not only effective but also interpretable and accessible.\n\nThrough my research, I aim to bridge the gap between theoretical foundations and practical applications, contributing to the development of robust, scalable, and insightful machine learning models that can tackle real-world challenges.",
    "collaborators": [
      "Rams\u00e9s J. S\u00e1nchez",
      "C. Ojeda",
      "C. Bauckhage",
      "B. Georgiev",
      "Patrick Seifner",
      "Laura von Rueden",
      "Sebastian Houben",
      "D. Biesner",
      "R. Sifa",
      "J. Sch\u00fccker",
      "Jannis Schuecker",
      "Antonia Korner",
      "N. Piatkowski",
      "Julian Wormann",
      "Daniel Bogdoll",
      "Etienne Buhrle",
      "Hang Chen",
      "E. F. Chuo",
      "L. V. Elst",
      "Tobias Glei\u00dfner",
      "Philip Gottschall",
      "Stefan Griesche",
      "Christian Hellert",
      "Christian Hesels",
      "Tim Joseph",
      "Niklas Keil",
      "J. Kelsch",
      "Hendrik Konigshof",
      "E. Kraft",
      "Leonie Kreuser",
      "Kevin Krone",
      "T. Latka",
      "Denny Mattern",
      "Stefan Matthes",
      "Mohsin Munir",
      "Moritz Nekolla",
      "A. Paschke",
      "Maximilian Pintz",
      "Tianming Qiu",
      "Faraz Qureishi",
      "Syed Tahseen Raza Rizvi",
      "J. Reichardt",
      "Stefan Rudolph",
      "A. Sagel",
      "G. Schunk",
      "Hao Shen",
      "Hendrik Stapelbroek",
      "V. Stehr",
      "G. Srinivas",
      "Anh Tuan Tran",
      "A.K. Vivekanandan",
      "Y. Wang",
      "Florian Wasserrab",
      "Tino Werner",
      "Christian Wirth",
      "Stefan Zwicklbauer",
      "L. Conrads",
      "Pascal Welke",
      "Anne-Katrin Mahlein",
      "Viktoriya Olari",
      "\u00d8yvind Eide",
      "Erik Krupicka"
    ],
    "pub_titles": [
      "Foundational Inference Models for Dynamical Systems",
      "Foundational Inference Models for Dynamical Systems",
      "Neural Dynamic Focused Topic Model",
      "The future is different: Large pre-trained language models fail in prediction tasks",
      "Informed Pre-Training on Prior Knowledge",
      "Knowledge Augmented Machine Learning with Applications in Autonomous Driving: A Survey",
      "Hidden Schema Networks",
      "Combining Variational Autoencoders and Transformer Language Models for Improved Password Generation",
      "Switching Dynamical Systems with Deep Neural Networks",
      "Combining expert knowledge and neural networks to model environmental stresses in agriculture",
      "Introduction to Machine Learning with Robots and Playful Learning",
      "Dynamic Review-based Recommenders",
      "Learning Deep Generative Models for Queuing Systems",
      "Auto Encoding Explanatory Examples with Stochastic Paths",
      "Generative Deep Learning Techniques for Password Generation",
      "Recurrent Point Review Models"
    ],
    "pub_abstracts": [
      "Ordinary differential equations (ODEs) underlie dynamical systems which serve as models for a vast number of natural and social phenomena. Yet inferring the ODE that best describes a set of noisy observations on one such phenomenon can be remarkably challenging, and the models available to achieve it tend to be highly specialized and complex too. In this work we propose a novel supervised learning framework for zero-shot inference of ODEs from noisy data. We first generate large datasets of one-dimensional ODEs, by sampling distributions over the space of initial conditions, and the space of vector fields defining them. We then learn neural maps between noisy observations on the solutions of these equations, and their corresponding initial condition and vector fields. The resulting models, which we call foundational inference models (FIM), can be (i) copied and matched along the time dimension to increase their resolution; and (ii) copied and composed to build inference models of any dimensionality, without the need of any finetuning. We use FIM to model both ground-truth dynamical systems of different dimensionalities and empirical time series data in a zero-shot fashion , and outperform state-of-the-art models which are fine-tuned to these systems. Our (pretrained) FIMs are available online.",
      "Dynamical systems governed by ordinary differential equations (ODEs) serve as models for a vast number of natural and social phenomena. In this work, we offer a fresh perspective on the classical problem of imputing missing time series data, whose underlying dynamics are assumed to be determined by ODEs. Specifically, we revisit ideas from amortized inference and neural operators, and propose a novel supervised learning framework for zero-shot time series imputation, through parametric functions satisfying some (hidden) ODEs. Our proposal consists of two components. First, a broad probability distribution over the space of ODE solutions, observation times and noise mechanisms, with which we generate a large, synthetic dataset of (hidden) ODE solutions, along with their noisy and sparse observations. Second, a neural recognition model that is trained offline, to map the generated time series onto the spaces of initial conditions and time derivatives of the (hidden) ODE solutions, which we then integrate to impute the missing data. We empirically demonstrate that one and the same (pretrained) recognition model can perform zero-shot imputation across 63 distinct time series with missing values, each sampled from widely different dynamical systems. Likewise, we demonstrate that it can perform zero-shot imputation of missing high-dimensional data in 10 vastly different settings, spanning human motion, air quality, traffic and electricity studies, as well as Navier-Stokes simulations -- without requiring any fine-tuning. What is more, our proposal often outperforms state-of-the-art methods, which are trained on the target datasets. Our pretrained model will be available online soon.",
      "Topic models and all their variants analyse text by learning meaningful representations through word co-occurrences. As pointed out by previous work, such models implicitly assume that the probability of a topic to be active and its proportion within each document are positively correlated. This correlation can be strongly detrimental in the case of documents created over time, simply because recent documents are likely better described by new and hence rare topics. In this work we leverage recent advances in neural variational inference and present an alternative neural approach to the dynamic Focused Topic Model. Indeed, we develop a neural model for topic evolution which exploits sequences of Bernoulli random variables in order to track the appearances of topics, thereby decoupling their activities from their proportions. We evaluate our model on three different datasets (the UN general debates, the collection of NeurIPS papers, and the ACL Anthology dataset) and show that it (i) outperforms state-of-the-art topic models in generalization tasks and (ii) performs comparably to them on prediction tasks, while employing roughly the same number of parameters, and converging about two times faster.",
      "Large pre-trained language models (LPLM) have shown spectacular success when fine-tuned on downstream supervised tasks. Yet, it is known that their performance can drastically drop when there is a distribution shift between the data used during training and that used at inference time. In this paper we focus on data distributions that naturally change over time and introduce four new REDDIT datasets, namely the WALLSTREETBETS, ASKSCIENCE, THE DONALD, and POLITICS sub-reddits. First, we empirically demonstrate that LPLM can display average performance drops of about 88% (in the best case!) when predicting the popularity of future posts from sub-reddits whose topic distribution changes with time. We then introduce a simple methodology that leverages neural variational dynamic topic models and attention mechanisms to infer temporal language model representations for regression tasks. Our models display performance drops of only about 40% in the worst cases (2% in the best ones) when predicting the popularity of future posts, while using only about 7% of the total number of parameters of LPLM and providing interpretable representations that offer insight into real-world events, like the GameStop short squeeze of 2021",
      "When training data is scarce, the incorporation of additional prior knowledge can assist the learning process. While it is common to initialize neural networks with weights that have been pre-trained on other large data sets, pre-training on more concise forms of knowledge has rather been overlooked. In this paper, we propose a novel informed machine learning approach and suggest to pre-train on prior knowledge. Formal knowledge representations, e.g. graphs or equations, are first transformed into a small and condensed data set of knowledge prototypes. We show that informed pre-training on such knowledge prototypes (i) speeds up the learning processes, (ii) improves generalization capabilities in the regime where not enough training data is available, and (iii) increases model robustness. Analyzing which parts of the model are affected most by the prototypes reveals that improvements come from deeper layers that typically represent high-level features. This confirms that informed pre-training can indeed transfer semantic knowledge. This is a novel effect, which shows that knowledge-based pre-training has additional and complementary strengths to existing approaches.",
      "The availability of representative datasets is an essential prerequisite for many successful artificial intelligence and machine learning models. However, in real life applications these models often encounter scenarios that are inadequately represented in the data used for training. There are various reasons for the absence of sufficient data, ranging from time and cost constraints to ethical considerations. As a consequence, the reliable usage of these models, especially in safety-critical applications, is still a tremendous challenge. Leveraging additional, already existing sources of knowledge is key to overcome the limitations of purely data-driven approaches. Knowledge augmented machine learning approaches offer the possibility of compensating for deficiencies, errors, or ambiguities in the data, thus increasing the generalization capability of the applied models. Even more, predictions that conform with knowledge are crucial for making trustworthy and safe decisions even in underrepresented scenarios. This work provides an overview of existing techniques and methods in the literature that combine data-driven models with existing knowledge. The identified approaches are structured according to the categories knowledge integration, extraction and conformity. In particular, we address the application of the presented methods in the field of autonomous driving.",
      "Large, pretrained language models infer powerful representations that encode rich semantic and syntactic content, albeit implicitly. In this work we introduce a novel neural language model that enforces, via inductive biases, explicit relational structures which allow for compositionality onto the output representations of pretrained language models. Specifically, the model encodes sentences into sequences of symbols (composed representations), which correspond to the nodes visited by biased random walkers on a global latent graph, and infers the posterior distribution of the latter. We first demonstrate that the model is able to uncover ground-truth graphs from artificially generated datasets of random token sequences. Next, we leverage pretrained BERT and GPT-2 language models as encoder and decoder, respectively, to infer networks of symbols (schemata) from natural language datasets. Our experiments show that (i) the inferred symbols can be interpreted as encoding different aspects of language, as e.g. topics or sentiments, and that (ii) GPT-2-like models can effectively be conditioned on symbolic representations. Finally, we explore training autoregressive, random walk \u201creasoning\u201d models on schema networks inferred from commonsense knowledge databases, and using the sampled paths to enhance the performance of pretrained language models on commonsense If-Then reasoning tasks.",
      "Password generation techniques have recently been explored by leveraging deep-learning natural language processing (NLP) algorithms. Previous work has raised the state of the art for password guessing algorithms significantly, by approaching the problem using either variational autoencoders with CNN-based encoder and decoder architectures or transformer-based architectures (namely GPT2) for text generation. In this work we aim to combine both paradigms, introducing a novel architecture that leverages the expressive power of transformers with the natural sampling approach to text generation of variational autoencoders. We show how our architecture generates state-of-the-art results in password matching performance across multiple benchmark datasets.",
      "The problem of uncovering different dynamical regimes is of pivotal importance in time series analysis. Switching dynamical systems provide a solution for modeling physical phenomena whose time series data exhibit different dynamical modes. In this work we propose a novel variational RNN model for switching dynamics allowing for both non-Markovian and nonlinear dynamical behavior between and within dynamic modes. Attention mechanisms are provided to inform the switching distribution. We evaluate our model on synthetic and empirical datasets of diverse nature and successfully uncover different dynamical regimes and predict the switching dynamics.",
      "In this work we combine representation learning capabilities of neural network with agricultural knowledge from experts to model environmental heat and drought stresses. We first design deterministic expert models which serve as a benchmark and inform the design of flexible neural-network architectures. Finally, a sensitivity analysis of the latter allows a clustering of hybrids into susceptible and resistant ones.",
      "Inspired by explanations of machine learning concepts in children\u2019s books, we developed an approach to introduce supervised, unsupervised, and reinforcement learning using a block-based programming language in combination with the benefits of educational robotics. Instead of using blocks as high-end APIs to access AI cloud services or to reproduce the machine learning algorithms, we use them as a means to put the student \u201cin the algorithm\u2019s shoes.\u201d We adapt the training of neural networks, Q-learning, and k-means algorithms to a design and format suitable for children and equip the students with hands-on tools for playful experimentation. The children learn about direct supervision by modifying the weights in the neural networks and immediately observing the effects on the simulated robot. Following the ideas of constructionism, they experience how the algorithms and underlying machine learning concepts work in practice. We conducted and evaluated this approach with students in primary, middle, and high school. All the age groups perceived the topics to be very easy to moderately hard to grasp. Younger students experienced direct supervision as challenging, whereas they found Q-learning and k-means algorithms much more accessible. Most high-school students could cope with all the topics without particular difficulties.",
      "Just as user preferences change with time, item reviews also reflect those same preference changes. In a nutshell, if one is to sequentially incorporate review content knowledge into recommender systems, one is naturally led to dynamical models of text. In the present work we leverage the known power of reviews to enhance rating predictions in a way that (i) respects the causality of review generation and (ii) includes, in a bidirectional fashion, the ability of ratings to inform language review models and vice-versa, language representations that help predict ratings end-to-end. Moreover, our representations are time-interval aware and thus yield a continuous-time representation of the dynamics. We provide experiments on real-world datasets and show that our methodology is able to outperform several state-of-the-art models. Source code for all models can be found at [1].",
      "Modern society is heavily dependent on large scale client-server systems with applications ranging from Internet and Communication Services to sophisticated logistics and deployment of goods. To maintain and improve such a system, a careful study of client and server dynamics is needed \u2013 e.g. response/service times, aver-age number of clients at given times, etc. To this end, one traditionally relies, within the queuing theory formalism,on parametric analysis and explicit distribution forms.However, parametric forms limit the model\u2019s expressiveness and could struggle on extensively large datasets. We propose a novel data-driven approach towards queuing systems: the Deep Generative Service Times. Our methodology delivers a flexible and scalable model for service and response times. We leverage the representation capabilities of Recurrent Marked Point Processes for the temporal dynamics of clients, as well as Wasserstein Generative Adversarial Network techniques, to learn deep generative models which are able to represent complex conditional service time distributions. We provide extensive experimental analysis on both empirical and synthetic datasets, showing the effectiveness of the proposed models",
      "In this paper we ask for the main factors that determine a classifier\u2019s decision making process and uncover such factors by studying latent codes produced by auto-encoding frameworks. To deliver an explanation of a classifier\u2019s behaviour, we propose a method that provides series of examples highlighting semantic differences between the classifier\u2019s decisions. These examples are generated through interpolations in latent space. We introduce and formalize the notion of a semantic stochastic path, as a suitable stochastic process defined in feature (data) space via latent code interpolations. We then introduce the concept of semantic Lagrangians as a way to incorporate the desired classifier\u2019s behaviour and find that the solution of the associated variational problem allows for highlighting differences in the classifier decision. Very importantly, within our framework the classifier is used as a black-box, and only its evaluation is required.",
      "Password guessing approaches via deep learning have recently been investigated with significant breakthroughs in their ability to generate novel, realistic password candidates. In the present work we study a broad collection of deep learning and probabilistic based models in the light of password guessing: attention-based deep neural networks, autoencoding mechanisms and generative adversarial networks. We provide novel generative deep-learning models in terms of variational autoencoders exhibiting state-of-art sampling performance, yielding additional latent-space features such as interpolations and targeted sampling. Lastly, we perform a thorough empirical analysis in a unified controlled framework over well-known datasets (RockYou, LinkedIn, Youku, Zomato, Pwnd). Our results not only identify the most promising schemes driven by deep neural networks, but also illustrate the strengths of each approach in terms of generation variability and sample uniqueness.",
      "Deep neural network models represent the state-of-the-art methodologies for natural language processing. Here we build on top of these methodologies to incorporate temporal information and model how review data changes with time. Specifically, we use the dynamic representations of recurrent point process models, which encode the history of how business or service reviews are received in time, to generate instantaneous language models with improved prediction capabilities. Simultaneously, our methodologies enhance the predictive power of our point process models by incorporating summarized review content representations. We provide recurrent network and temporal convolution solutions for modeling the review content. We deploy our methodologies in the context of recommender systems, effectively characterizing the change in preference and taste of users as time evolves. Source code is available at [1]."
    ],
    "domain": [
      "Dynamical Systems",
      "Machine Learning",
      "Natural Language Processing",
      "Time Series Analysis"
    ],
    "is_leader_candidate": true,
    "is_member_candidate": true,
    "is_reviewer_candidate": true,
    "is_chair_candidate": true
  },
  "f16fb819-8800-4795-927a-04b39f4fa535": {
    "pk": "f16fb819-8800-4795-927a-04b39f4fa535",
    "name": "Patrick Seifner",
    "bio": "I am a researcher dedicated to the intersection of dynamical systems and machine learning, particularly focusing on ordinary differential equations (ODEs) and their applications in modeling complex phenomena. My recent work has led to the development of foundational inference models (FIMs) that enable zero-shot inference of ODEs from noisy observations. This innovative framework allows for the generation of large datasets of one-dimensional ODEs, facilitating the learning of neural maps that can accurately predict initial conditions and vector fields without the need for fine-tuning.\n\nIn addition to ODE inference, I have explored the challenge of imputing missing time series data governed by ODEs. By leveraging concepts from amortized inference and neural operators, I proposed a novel approach that demonstrates remarkable zero-shot imputation capabilities across diverse datasets, outperforming state-of-the-art methods.\n\nMy research also extends to Markov jump processes, where I introduced a variational inference algorithm that utilizes neural ODEs for efficient and effective inference. This work highlights my commitment to developing scalable and robust methodologies that can be applied across various domains, from human motion analysis to molecular dynamics simulations.\n\nOverall, my goal is to bridge the gap between theoretical dynamical systems and practical machine learning applications, providing tools that enhance our understanding and prediction of complex systems.",
    "collaborators": [
      "Rams\u00e9s J. S\u00e1nchez",
      "K. Cvejoski",
      "Antonia Korner"
    ],
    "pub_titles": [
      "Foundational Inference Models for Dynamical Systems",
      "Foundational Inference Models for Dynamical Systems",
      "Neural Markov Jump Processes"
    ],
    "pub_abstracts": [
      "Ordinary differential equations (ODEs) underlie dynamical systems which serve as models for a vast number of natural and social phenomena. Yet inferring the ODE that best describes a set of noisy observations on one such phenomenon can be remarkably challenging, and the models available to achieve it tend to be highly specialized and complex too. In this work we propose a novel supervised learning framework for zero-shot inference of ODEs from noisy data. We first generate large datasets of one-dimensional ODEs, by sampling distributions over the space of initial conditions, and the space of vector fields defining them. We then learn neural maps between noisy observations on the solutions of these equations, and their corresponding initial condition and vector fields. The resulting models, which we call foundational inference models (FIM), can be (i) copied and matched along the time dimension to increase their resolution; and (ii) copied and composed to build inference models of any dimensionality, without the need of any finetuning. We use FIM to model both ground-truth dynamical systems of different dimensionalities and empirical time series data in a zero-shot fashion , and outperform state-of-the-art models which are fine-tuned to these systems. Our (pretrained) FIMs are available online.",
      "Dynamical systems governed by ordinary differential equations (ODEs) serve as models for a vast number of natural and social phenomena. In this work, we offer a fresh perspective on the classical problem of imputing missing time series data, whose underlying dynamics are assumed to be determined by ODEs. Specifically, we revisit ideas from amortized inference and neural operators, and propose a novel supervised learning framework for zero-shot time series imputation, through parametric functions satisfying some (hidden) ODEs. Our proposal consists of two components. First, a broad probability distribution over the space of ODE solutions, observation times and noise mechanisms, with which we generate a large, synthetic dataset of (hidden) ODE solutions, along with their noisy and sparse observations. Second, a neural recognition model that is trained offline, to map the generated time series onto the spaces of initial conditions and time derivatives of the (hidden) ODE solutions, which we then integrate to impute the missing data. We empirically demonstrate that one and the same (pretrained) recognition model can perform zero-shot imputation across 63 distinct time series with missing values, each sampled from widely different dynamical systems. Likewise, we demonstrate that it can perform zero-shot imputation of missing high-dimensional data in 10 vastly different settings, spanning human motion, air quality, traffic and electricity studies, as well as Navier-Stokes simulations -- without requiring any fine-tuning. What is more, our proposal often outperforms state-of-the-art methods, which are trained on the target datasets. Our pretrained model will be available online soon.",
      "Markov jump processes are continuous-time stochastic processes with a wide range of applications in both natural and social sciences. Despite their widespread use, inference in these models is highly non-trivial and typically proceeds via either Monte Carlo or expectation-maximization methods. In this work we introduce an alternative, variational inference algorithm for Markov jump processes which relies on neural ordinary differential equations, and is trainable via back-propagation. Our methodology learns neural, continuous-time representations of the observed data, that are used to approximate the initial distribution and time-dependent transition probability rates of the posterior Markov jump process. The time-independent rates of the prior process are in contrast trained akin to generative adversarial networks. We test our approach on synthetic data sampled from ground-truth Markov jump processes, experimental switching ion channel data and molecular dynamics simulations. Source code to reproduce our experiments is available online."
    ],
    "domain": [
      "Dynamical Systems",
      "Neural Networks",
      "Variational Inference",
      "Time Series Analysis"
    ],
    "is_leader_candidate": true,
    "is_member_candidate": true,
    "is_reviewer_candidate": true,
    "is_chair_candidate": true
  },
  "a42f1038-0e6d-465a-b5c1-66ef7558bcd6": {
    "pk": "a42f1038-0e6d-465a-b5c1-66ef7558bcd6",
    "name": "Cesar Ojeda",
    "bio": "I am a researcher deeply engaged in the intersection of machine learning, time series analysis, and natural language processing. My work primarily focuses on developing innovative models that capture the dynamics of data over time, particularly in the context of topic modeling, recommender systems, and service systems. \n\nIn my recent publications, I have introduced novel approaches such as a neural model for dynamic topic evolution that decouples topic activity from their proportions, significantly improving generalization tasks. I have also explored the application of Schr\u00f6dinger bridges for generative data modeling, enhancing the understanding of complex data distributions. My research on large pre-trained language models has revealed their vulnerabilities to distribution shifts, leading to the development of methodologies that leverage neural variational dynamic topic models for more robust predictions.\n\nI am particularly interested in the dynamics of user preferences and how they evolve, which has driven my work on recommender systems that integrate review content with rating predictions in a causally aware manner. Additionally, I have contributed to the field of queuing theory by proposing deep generative models that effectively capture client-server dynamics without the constraints of traditional parametric forms.\n\nThrough my research, I aim to provide interpretable and scalable solutions that not only advance theoretical understanding but also have practical implications across various domains, from social media analysis to service system optimization. My work is characterized by a commitment to leveraging advanced statistical techniques and deep learning methodologies to address real-world challenges.",
    "collaborators": [
      "Rams\u00e9s J. S\u00e1nchez",
      "K. Cvejoski",
      "C. Bauckhage",
      "B. Georgiev",
      "M. Opper",
      "W. Palma",
      "S. Eyheramendy",
      "F. Elorrieta",
      "J. Sch\u00fccker",
      "Ludwig Winkler",
      "Noa Malem-Shinitski",
      "L. Conrads",
      "Pascal Welke",
      "Jannis Schuecker",
      "Kostadin Cvejosky"
    ],
    "pub_titles": [
      "Neural Dynamic Focused Topic Model",
      "A Score-Based Approach for Training Schr\u00f6dinger Bridges for Data Modelling",
      "The future is different: Large pre-trained language models fail in prediction tasks",
      "Stochastic Control for Bayesian Neural Network Training",
      "A Novel First-Order Autoregressive Moving Average Model to Analyze Discrete-Time Series Irregularly Observed",
      "Hidden Schema Networks",
      "Nonlinear Hawkes Process with Gaussian Process Self Effects",
      "A novel bivariate autoregressive model for predicting and forecasting irregularly observed time series",
      "An irregularly spaced first-order moving average model",
      "Switching Dynamical Systems with Deep Neural Networks",
      "Flexible Temporal Point Processes Modeling with Nonlinear Hawkes Processes with Gaussian Processes Excitations and Inhibitions",
      "Dynamic Review-based Recommenders",
      "Learning Deep Generative Models for Queuing Systems",
      "Auto Encoding Explanatory Examples with Stochastic Paths",
      "Recurrent Point Review Models",
      "Recurrent Adversarial Service Times"
    ],
    "pub_abstracts": [
      "Topic models and all their variants analyse text by learning meaningful representations through word co-occurrences. As pointed out by previous work, such models implicitly assume that the probability of a topic to be active and its proportion within each document are positively correlated. This correlation can be strongly detrimental in the case of documents created over time, simply because recent documents are likely better described by new and hence rare topics. In this work we leverage recent advances in neural variational inference and present an alternative neural approach to the dynamic Focused Topic Model. Indeed, we develop a neural model for topic evolution which exploits sequences of Bernoulli random variables in order to track the appearances of topics, thereby decoupling their activities from their proportions. We evaluate our model on three different datasets (the UN general debates, the collection of NeurIPS papers, and the ACL Anthology dataset) and show that it (i) outperforms state-of-the-art topic models in generalization tasks and (ii) performs comparably to them on prediction tasks, while employing roughly the same number of parameters, and converging about two times faster.",
      "A Schr\u00f6dinger bridge is a stochastic process connecting two given probability distributions over time. It has been recently applied as an approach for generative data modelling. The computational training of such bridges requires the repeated estimation of the drift function for a time-reversed stochastic process using samples generated by the corresponding forward process. We introduce a modified score- function-based method for computing such reverse drifts, which can be efficiently implemented by a feed-forward neural network. We applied our approach to artificial datasets with increasing complexity. Finally, we evaluated its performance on genetic data, where Schr\u00f6dinger bridges can be used to model the time evolution of single-cell RNA measurements.",
      "Large pre-trained language models (LPLM) have shown spectacular success when fine-tuned on downstream supervised tasks. Yet, it is known that their performance can drastically drop when there is a distribution shift between the data used during training and that used at inference time. In this paper we focus on data distributions that naturally change over time and introduce four new REDDIT datasets, namely the WALLSTREETBETS, ASKSCIENCE, THE DONALD, and POLITICS sub-reddits. First, we empirically demonstrate that LPLM can display average performance drops of about 88% (in the best case!) when predicting the popularity of future posts from sub-reddits whose topic distribution changes with time. We then introduce a simple methodology that leverages neural variational dynamic topic models and attention mechanisms to infer temporal language model representations for regression tasks. Our models display performance drops of only about 40% in the worst cases (2% in the best ones) when predicting the popularity of future posts, while using only about 7% of the total number of parameters of LPLM and providing interpretable representations that offer insight into real-world events, like the GameStop short squeeze of 2021",
      "In this paper, we propose to leverage the Bayesian uncertainty information encoded in parameter distributions to inform the learning procedure for Bayesian models. We derive a first principle stochastic differential equation for the training dynamics of the mean and uncertainty parameter in the variational distributions. On the basis of the derived Bayesian stochastic differential equation, we apply the methodology of stochastic optimal control on the variational parameters to obtain individually controlled learning rates. We show that the resulting optimizer, StochControlSGD, is significantly more robust to large learning rates and can adaptively and individually control the learning rates of the variational parameters. The evolution of the control suggests separate and distinct dynamical behaviours in the training regimes for the mean and uncertainty parameters in Bayesian neural networks.",
      "A novel first-order autoregressive moving average model for analyzing discrete-time series observed at irregularly spaced times is introduced. Under Gaussianity, it is established that the model is strictly stationary and ergodic. In the general case, it is shown that the model is weakly stationary. The lowest dimension of the state-space representation is given along with the one-step linear predictors and their mean squared errors. The maximum likelihood estimation procedure is discussed, and their finite-sample behavior is assessed through Monte Carlo experiments. These experiments show that bias, root mean squared error, and coefficient of variation are smaller when the length of the series increases. Further, the method provides good estimations for the standard errors, even with relatively small sample sizes. Also, the irregularly spaced times seem to increase the estimation variability. The application of the proposed model is made through two real-life examples. The first is concerned with medical data, whereas the second describes an astronomical data set analysis.",
      "Large, pretrained language models infer powerful representations that encode rich semantic and syntactic content, albeit implicitly. In this work we introduce a novel neural language model that enforces, via inductive biases, explicit relational structures which allow for compositionality onto the output representations of pretrained language models. Specifically, the model encodes sentences into sequences of symbols (composed representations), which correspond to the nodes visited by biased random walkers on a global latent graph, and infers the posterior distribution of the latter. We first demonstrate that the model is able to uncover ground-truth graphs from artificially generated datasets of random token sequences. Next, we leverage pretrained BERT and GPT-2 language models as encoder and decoder, respectively, to infer networks of symbols (schemata) from natural language datasets. Our experiments show that (i) the inferred symbols can be interpreted as encoding different aspects of language, as e.g. topics or sentiments, and that (ii) GPT-2-like models can effectively be conditioned on symbolic representations. Finally, we explore training autoregressive, random walk \u201creasoning\u201d models on schema networks inferred from commonsense knowledge databases, and using the sampled paths to enhance the performance of pretrained language models on commonsense If-Then reasoning tasks.",
      "Traditionally, Hawkes processes are used to model time--continuous point processes with history dependence. Here we propose an extended model where the self--effects are of both excitatory and inhibitory type and follow a Gaussian Process. Whereas previous work either relies on a less flexible parameterization of the model, or requires a large amount of data, our formulation allows for both a flexible model and learning when data are scarce. We continue the line of work of Bayesian inference for Hawkes processes, and our approach dispenses with the necessity of estimating a branching structure for the posterior, as we perform inference on an aggregated sum of Gaussian Processes. Efficient approximate Bayesian inference is achieved via data augmentation, and we describe a mean--field variational inference approach to learn the model parameters. To demonstrate the flexibility of the model we apply our methodology on data from three different domains and compare it to previously reported results.",
      "In several disciplines it is common to find time series measured at irregular observational times. In particular, in astronomy there are a large number of surveys that gather information over irregular time gaps and in more than one passband. Some examples are Pan-STARRS, ZTF and also the LSST. However, current commonly used time series models that estimate the time dependency in astronomical light curves consider the information of each band separately (e.g, CIAR, IAR and CARMA models) disregarding the dependency that might exist between different passbands. In this paper we propose a novel bivariate model for irregularly sampled time series, called the bivariate irregular autoregressive (BIAR) model. The BIAR model assumes an autoregressive structure on each time series, it is stationary, and it allows to estimate the autocorrelation, the cross-correlation and the contemporary correlation between two unequally spaced time series. We implemented the BIAR model on light curves, in the g and r bands, obtained from the ZTF alerts processed by the ALeRCE broker. We show that if the light curves of the two bands are highly correlated, the model has more accurate forecast and prediction using the bivariate model than a similar method that uses only univariate information. Further, the estimated parameters of the BIAR are useful to characterize Long Period Variable Stars and to distinguish between classes of stochastic objects, providing promising features that can be used for classification purposes.",
      "A novel first-order moving-average model for analyzing time series observed at irregularly spaced intervals is introduced. Two definitions are presented, which are equivalent under Gaussianity. The first one relies on normally distributed data and the specification of second-order moments. The second definition provided is more flexible in the sense that it allows for considering other distributional assumptions. The statistical properties are investigated along with the one-step linear predictors and their mean squared errors. It is established that the process is strictly stationary under normality and weakly stationary in the general case. Maximum likelihood and bootstrap estimation procedures are discussed and the finite-sample behavior of these estimates is assessed through Monte Carlo experiments. In these simulations, both methods perform well in terms of estimation bias and standard errors, even with relatively small sample sizes. Moreover, we show that for non-Gaussian data, for t-Student and Generalized errors distributions, the parameters of the model can be estimated precisely by maximum likelihood. The proposed IMA model is compared to the continuous autoregressive moving average (CARMA) models, exhibiting good performance. Finally, the practical application and usefulness of the proposed model are illustrated with two real-life data examples.",
      "The problem of uncovering different dynamical regimes is of pivotal importance in time series analysis. Switching dynamical systems provide a solution for modeling physical phenomena whose time series data exhibit different dynamical modes. In this work we propose a novel variational RNN model for switching dynamics allowing for both non-Markovian and nonlinear dynamical behavior between and within dynamic modes. Attention mechanisms are provided to inform the switching distribution. We evaluate our model on synthetic and empirical datasets of diverse nature and successfully uncover different dynamical regimes and predict the switching dynamics.",
      "We propose an extended Hawkes process model where the self\u2013effects are of both excitatory and inhibitory type and follow a Gaussian Process. Whereas previous work either relies on a less \ufb02ex-ible parameterization of the model, or requires a large amount of data, our formulation allows for both a \ufb02exible model and learning when data are scarce. Ef\ufb01cient approximate Bayesian inference is achieved via data augmentation, and we describe a mean\u2013\ufb01eld variational inference approach to learn the model parameters. To demonstrate the \ufb02exibility of the model we apply our methodology on data from two different domains and compare it to previously reported results.",
      "Just as user preferences change with time, item reviews also reflect those same preference changes. In a nutshell, if one is to sequentially incorporate review content knowledge into recommender systems, one is naturally led to dynamical models of text. In the present work we leverage the known power of reviews to enhance rating predictions in a way that (i) respects the causality of review generation and (ii) includes, in a bidirectional fashion, the ability of ratings to inform language review models and vice-versa, language representations that help predict ratings end-to-end. Moreover, our representations are time-interval aware and thus yield a continuous-time representation of the dynamics. We provide experiments on real-world datasets and show that our methodology is able to outperform several state-of-the-art models. Source code for all models can be found at [1].",
      "Modern society is heavily dependent on large scale client-server systems with applications ranging from Internet and Communication Services to sophisticated logistics and deployment of goods. To maintain and improve such a system, a careful study of client and server dynamics is needed \u2013 e.g. response/service times, aver-age number of clients at given times, etc. To this end, one traditionally relies, within the queuing theory formalism,on parametric analysis and explicit distribution forms.However, parametric forms limit the model\u2019s expressiveness and could struggle on extensively large datasets. We propose a novel data-driven approach towards queuing systems: the Deep Generative Service Times. Our methodology delivers a flexible and scalable model for service and response times. We leverage the representation capabilities of Recurrent Marked Point Processes for the temporal dynamics of clients, as well as Wasserstein Generative Adversarial Network techniques, to learn deep generative models which are able to represent complex conditional service time distributions. We provide extensive experimental analysis on both empirical and synthetic datasets, showing the effectiveness of the proposed models",
      "In this paper we ask for the main factors that determine a classifier\u2019s decision making process and uncover such factors by studying latent codes produced by auto-encoding frameworks. To deliver an explanation of a classifier\u2019s behaviour, we propose a method that provides series of examples highlighting semantic differences between the classifier\u2019s decisions. These examples are generated through interpolations in latent space. We introduce and formalize the notion of a semantic stochastic path, as a suitable stochastic process defined in feature (data) space via latent code interpolations. We then introduce the concept of semantic Lagrangians as a way to incorporate the desired classifier\u2019s behaviour and find that the solution of the associated variational problem allows for highlighting differences in the classifier decision. Very importantly, within our framework the classifier is used as a black-box, and only its evaluation is required.",
      "Deep neural network models represent the state-of-the-art methodologies for natural language processing. Here we build on top of these methodologies to incorporate temporal information and model how review data changes with time. Specifically, we use the dynamic representations of recurrent point process models, which encode the history of how business or service reviews are received in time, to generate instantaneous language models with improved prediction capabilities. Simultaneously, our methodologies enhance the predictive power of our point process models by incorporating summarized review content representations. We provide recurrent network and temporal convolution solutions for modeling the review content. We deploy our methodologies in the context of recommender systems, effectively characterizing the change in preference and taste of users as time evolves. Source code is available at [1].",
      "Service system dynamics occur at the interplay between customer behaviour and a service provider's response. This kind of dynamics can effectively be modeled within the framework of queuing theory where customers' arrivals are described by point process models. However, these approaches are limited by parametric assumptions as to, for example, inter-event time distributions. In this paper, we address these limitations and propose a novel, deep neural network solution to the queuing problem. Our solution combines a recurrent neural network that models the arrival process with a recurrent generative adversarial network which models the service time distribution. We evaluate our methodology on various empirical datasets ranging from internet services (Blockchain, GitHub, Stackoverflow) to mobility service systems (New York taxi cab)."
    ],
    "domain": [
      "Natural Language Processing",
      "Time Series Analysis",
      "Bayesian Inference",
      "Deep Learning"
    ],
    "is_leader_candidate": true,
    "is_member_candidate": true,
    "is_reviewer_candidate": true,
    "is_chair_candidate": true
  },
  "e73be071-9a9a-4d65-bd99-fe97646ea967": {
    "pk": "e73be071-9a9a-4d65-bd99-fe97646ea967",
    "name": "Ramses J. Sanchez",
    "bio": "I am a researcher dedicated to exploring the intersection of dynamical systems and machine learning, particularly in the context of time series data. My recent work focuses on the challenge of imputing missing values in time series governed by ordinary differential equations (ODEs). By leveraging concepts from amortized inference and neural operators, I have developed a novel supervised learning framework for zero-shot time series imputation. \n\nThis framework is built on two key components: a broad probability distribution that generates synthetic datasets of ODE solutions, and a neural recognition model that maps these time series to the initial conditions and derivatives of the underlying ODEs. Remarkably, this pretrained model can perform zero-shot imputation across a diverse range of 63 distinct time series, spanning various domains such as human motion, air quality, and traffic studies, without the need for fine-tuning. \n\nMy approach not only demonstrates the versatility of the model but also consistently outperforms state-of-the-art methods that require training on specific datasets. I am excited about the potential applications of this work and look forward to making my pretrained model publicly available to further advance research in this area.",
    "collaborators": [
      "Patrick Seifner",
      "K. Cvejoski",
      "Antonia Korner"
    ],
    "pub_titles": [
      "Foundational Inference Models for Dynamical Systems"
    ],
    "pub_abstracts": [
      "Dynamical systems governed by ordinary differential equations (ODEs) serve as models for a vast number of natural and social phenomena. In this work, we offer a fresh perspective on the classical problem of imputing missing time series data, whose underlying dynamics are assumed to be determined by ODEs. Specifically, we revisit ideas from amortized inference and neural operators, and propose a novel supervised learning framework for zero-shot time series imputation, through parametric functions satisfying some (hidden) ODEs. Our proposal consists of two components. First, a broad probability distribution over the space of ODE solutions, observation times and noise mechanisms, with which we generate a large, synthetic dataset of (hidden) ODE solutions, along with their noisy and sparse observations. Second, a neural recognition model that is trained offline, to map the generated time series onto the spaces of initial conditions and time derivatives of the (hidden) ODE solutions, which we then integrate to impute the missing data. We empirically demonstrate that one and the same (pretrained) recognition model can perform zero-shot imputation across 63 distinct time series with missing values, each sampled from widely different dynamical systems. Likewise, we demonstrate that it can perform zero-shot imputation of missing high-dimensional data in 10 vastly different settings, spanning human motion, air quality, traffic and electricity studies, as well as Navier-Stokes simulations -- without requiring any fine-tuning. What is more, our proposal often outperforms state-of-the-art methods, which are trained on the target datasets. Our pretrained model will be available online soon."
    ],
    "domain": [
      "Time Series Analysis",
      "Ordinary Differential Equations",
      "Machine Learning",
      "Neural Networks"
    ],
    "is_leader_candidate": true,
    "is_member_candidate": true,
    "is_reviewer_candidate": true,
    "is_chair_candidate": true
  }
}
