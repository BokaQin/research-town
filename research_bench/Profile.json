{
  "5d82cf41-44f9-43d4-ba1f-15d34f898f35": {
    "pk": "5d82cf41-44f9-43d4-ba1f-15d34f898f35",
    "name": "Chenyi Zi",
    "bio": "I am a researcher dedicated to advancing the fields of protein structure prediction and anomaly detection through innovative machine learning techniques. My recent work has focused on developing a Generative Adversarial Policy Network (GAPN) for protein complex modeling (PCM), where I tackle the challenges of combinatorial optimization and distribution shifts in protein complexes. By framing each protein chain as a node and assembly actions as edges, I have created a framework that efficiently navigates the complex assembly space, achieving significant improvements in accuracy and efficiency over existing PCM software.\n\nIn addition to my work in protein modeling, I have also made strides in the realm of anomaly detection. I introduced the Knowledge-Data Alignment (KDAlign) framework, which integrates expert rule knowledge to enhance weakly supervised anomaly detection. By employing Optimal Transport techniques, I align knowledge with data to improve model generalization on unseen anomalies. My experiments across various real-world datasets demonstrate that KDAlign significantly outperforms state-of-the-art methods, showcasing my commitment to bridging the gap between theoretical advancements and practical applications.\n\nThrough my research, I aim to contribute to the understanding of complex biological systems and improve the reliability of anomaly detection in critical applications. I am passionate about leveraging machine learning to solve real-world challenges and continuously seek to push the boundaries of what is possible in these domains.",
    "collaborators": [
      "Yan Zhou",
      "Chen Zhang",
      "Jia Li",
      "Ziqi Gao",
      "Tao Feng",
      "Jiaxuan You",
      "Haihong Zhao",
      "Yang Liu"
    ],
    "pub_titles": [
      "Deep Reinforcement Learning for Modelling Protein Complexes",
      "Weakly Supervised Anomaly Detection via Knowledge-Data Alignment"
    ],
    "pub_abstracts": [
      "AlphaFold can be used for both single-chain and multi-chain protein structure prediction, while the latter becomes extremely challenging as the number of chains increases. In this work, by taking each chain as a node and assembly actions as edges, we show that an acyclic undirected connected graph can be used to predict the structure of multi-chain protein complexes (a.k.a., protein complex modelling, PCM). However, there are still two challenges: 1) The huge combinatorial optimization space of $N^{N-2}$ ($N$ is the number of chains) for the PCM problem can easily lead to high computational cost. 2) The scales of protein complexes exhibit distribution shift due to variance in chain numbers, which calls for the generalization in modelling complexes of various scales. To address these challenges, we propose GAPN, a Generative Adversarial Policy Network powered by domain-specific rewards and adversarial loss through policy gradient for automatic PCM prediction. Specifically, GAPN learns to efficiently search through the immense assembly space and optimize the direct docking reward through policy gradient. Importantly, we design an adversarial reward function to enhance the receptive field of our model. In this way, GAPN will simultaneously focus on a specific batch of complexes and the global assembly rules learned from complexes with varied chain numbers. Empirically, we have achieved both significant accuracy (measured by RMSD and TM-Score) and efficiency improvements compared to leading PCM softwares.",
      "Anomaly detection (AD) plays a pivotal role in numerous web-based applications, including malware detection, anti-money laundering, device failure detection, and network fault analysis. Most methods, which rely on unsupervised learning, are hard to reach satisfactory detection accuracy due to the lack of labels. Weakly Supervised Anomaly Detection (WSAD) has been introduced with a limited number of labeled anomaly samples to enhance model performance. Nevertheless, it is still challenging for models, trained on an inadequate amount of labeled data, to generalize to unseen anomalies. In this paper, we introduce a novel framework, Knowledge-Data Alignment (KDAlign), to integrate rule knowledge, typically summarized by human experts, to supplement the limited labeled data. Specifically, we transpose these rules into the knowledge space and subsequently recast the incorporation of knowledge as the alignment of knowledge and data. To facilitate this alignment, we employ the Optimal Transport (OT) technique. We then incorporate the OT distance as an additional loss term to the original objective function of WSAD methodologies. Comprehensive experimental results on five real-world datasets demonstrate that our proposed KDAlign framework markedly surpasses its state-of-the-art counterparts, achieving superior performance across various anomaly types. Our codes are released at https://github.com/cshhzhao/KDAlign."
    ],
    "domain": [
      "Protein Structure Prediction",
      "Anomaly Detection",
      "Weakly Supervised Learning",
      "Graph Neural Network"
    ],
    "is_leader_candidate": true,
    "is_member_candidate": true,
    "is_reviewer_candidate": true,
    "is_chair_candidate": true
  },
  "6651f973-ca64-4ed9-857c-fee2f99afa93": {
    "pk": "6651f973-ca64-4ed9-857c-fee2f99afa93",
    "name": "Haihong Zhao",
    "bio": "I am a researcher dedicated to advancing the intersection of graph neural networks (GNNs) and machine learning, particularly in the context of multi-object physical systems and anomaly detection. My recent work explores the transformative potential of large language models (LLMs) in the graph domain, culminating in the development of Graph COordinators for PrEtraining (GCOPE). This innovative approach leverages the commonalities across diverse graph datasets to enhance few-shot learning, marking a significant step forward in the field.\n\nIn addition to GCOPE, I have introduced the Knowledge-Data Alignment (KDAlign) framework, which integrates expert rule knowledge to bolster weakly supervised anomaly detection. By employing Optimal Transport techniques, KDAlign effectively aligns knowledge with data, achieving superior performance across various anomaly types.\n\nMy research also delves into the dynamics of multi-object physical systems through the Physics-Inspired Neural Graph ODE (PINGO) and the Second-order Equivariant Graph Neural Ordinary Differential Equation (SEGNO). These models address the limitations of existing GNNs by incorporating physical inductive biases, such as continuity and second-order motion laws, to improve generalization and long-term prediction accuracy.\n\nThrough my work, I aim to bridge theoretical insights with practical applications, pushing the boundaries of what is possible in graph-based learning and modeling. I am passionate about developing methodologies that not only advance academic understanding but also have real-world implications across various domains.",
    "collaborators": [
      "Jia Li",
      "Yang Liu",
      "Jiashun Cheng",
      "Tingyang Xu",
      "F. Tsung",
      "Yu Rong",
      "Aochuan Chen",
      "Xiangguo Sun",
      "Hong Cheng",
      "Chenyi Zi",
      "Chen Zhang",
      "Yan Zhou",
      "Peilin Zhao",
      "P. Zhao"
    ],
    "pub_titles": [
      "All in One and One for All: A Simple yet Effective Method towards Cross-domain Graph Pretraining",
      "Weakly Supervised Anomaly Detection via Knowledge-Data Alignment",
      "Physics-Inspired Neural Graph ODE for Long-term Dynamical Simulation",
      "SEGNO: Generalizing Equivariant Graph Neural Networks with Physical Inductive Biases"
    ],
    "pub_abstracts": [
      "Large Language Models (LLMs) have revolutionized the fields of computer vision (CV) and natural language processing (NLP). One of the most notable advancements of LLMs is that a single model is trained on vast and diverse datasets spanning multiple domains -- a paradigm we term `All in One'. This methodology empowers LLMs with super generalization capabilities, facilitating an encompassing comprehension of varied data distributions. Leveraging these capabilities, a single LLM demonstrates remarkable versatility across a variety of domains -- a paradigm we term `One for All'. However, applying this idea to the graph field remains a formidable challenge, with cross-domain pretraining often resulting in negative transfer. This issue is particularly important in few-shot learning scenarios, where the paucity of training data necessitates the incorporation of external knowledge sources. In response to this challenge, we propose a novel approach called Graph COordinators for PrEtraining (GCOPE), that harnesses the underlying commonalities across diverse graph datasets to enhance few-shot learning. Our novel methodology involves a unification framework that amalgamates disparate graph datasets during the pretraining phase to distill and transfer meaningful knowledge to target tasks. Extensive experiments across multiple graph datasets demonstrate the superior efficacy of our approach. By successfully leveraging the synergistic potential of multiple graph datasets for pretraining, our work stands as a pioneering contribution to the realm of graph foundational model.",
      "Anomaly detection (AD) plays a pivotal role in numerous web-based applications, including malware detection, anti-money laundering, device failure detection, and network fault analysis. Most methods, which rely on unsupervised learning, are hard to reach satisfactory detection accuracy due to the lack of labels. Weakly Supervised Anomaly Detection (WSAD) has been introduced with a limited number of labeled anomaly samples to enhance model performance. Nevertheless, it is still challenging for models, trained on an inadequate amount of labeled data, to generalize to unseen anomalies. In this paper, we introduce a novel framework, Knowledge-Data Alignment (KDAlign), to integrate rule knowledge, typically summarized by human experts, to supplement the limited labeled data. Specifically, we transpose these rules into the knowledge space and subsequently recast the incorporation of knowledge as the alignment of knowledge and data. To facilitate this alignment, we employ the Optimal Transport (OT) technique. We then incorporate the OT distance as an additional loss term to the original objective function of WSAD methodologies. Comprehensive experimental results on five real-world datasets demonstrate that our proposed KDAlign framework markedly surpasses its state-of-the-art counterparts, achieving superior performance across various anomaly types. Our codes are released at https://github.com/cshhzhao/KDAlign.",
      "Simulating and modeling the long-term dynamics of multi-object physical systems is an essential and challenging task. Current studies model the physical systems utilizing Graph Neural Networks (GNNs) with equivariant properties. Specifically, they model the dynamics as a sequence of discrete states with a fixed time interval and learn a direct mapping for all the two adjacent states. However, this direct mapping overlooks the continuous nature between the two states. Namely, we have verified that there are countless possible trajectories between two discrete dynamic states in current GNN-based direct mapping models. This issue greatly hinders the model generalization ability, leading to poor performance of the long-term simulation. In this paper, to better model the latent trajectory through discrete supervision signals, we propose a P hysics-I nspired N eural G raph O DE (PINGO) algorithm. In PINGO, to ensure the uniqueness of the trajectory, we construct a Physics-Inspired Neural ODE framework to update the latent trajectory. Meanwhile, to effectively capture intricate interactions among objects, we use a GNN-based model to parameterize Neural ODE in a plug-and-play manner. Furthermore, we prove that the discrepancy between the learned trajectory of PIGNO and the true trajectory can be theoretically bounded. Extensive experiments verify our theoretical findings and demonstrate that our model yields an order-of-magnitude improvement over the state-of-the-art baselines, especially on long-term predictions and roll-out errors.",
      "Graph Neural Networks (GNNs) with equivariant properties have emerged as powerful tools for modeling complex dynamics of multi-object physical systems. However, their generalization ability is limited by the inadequate consideration of physical inductive biases: (1) Existing studies overlook the continuity of transitions among system states, opting to employ several discrete transformation layers to learn the direct mapping between two adjacent states; (2) Most models only account for first-order velocity information, despite the fact that many physical systems are governed by second-order motion laws. To incorporate these inductive biases, we propose the Second-order Equivariant Graph Neural Ordinary Differential Equation (SEGNO). Specifically, we show how the second-order continuity can be incorporated into GNNs while maintaining the equivariant property. Furthermore, we offer theoretical insights into SEGNO, highlighting that it can learn a unique trajectory between adjacent states, which is crucial for model generalization. Additionally, we prove that the discrepancy between this learned trajectory of SEGNO and the true trajectory is bounded. Extensive experiments on complex dynamical systems including molecular dynamics and motion capture demonstrate that our model yields a significant improvement over the state-of-the-art baselines."
    ],
    "domain": [
      "Graph Neural Network",
      "Anomaly Detection",
      "Physics-Inspired Modeling",
      "Few-Shot Learning"
    ],
    "is_leader_candidate": true,
    "is_member_candidate": true,
    "is_reviewer_candidate": true,
    "is_chair_candidate": true
  },
  "aa4a554d-21e1-4dea-9664-d82be9ed27c6": {
    "pk": "aa4a554d-21e1-4dea-9664-d82be9ed27c6",
    "name": "Xiangguo Sun",
    "bio": "I am a researcher deeply engaged in the intersection of large language models (LLMs) and graph-based methodologies, with a focus on enhancing performance across diverse domains such as recommendation systems, social network analysis, and biological data interpretation. My recent work has pioneered the concept of \"All in One\" and \"One for All\" paradigms, which leverage the super generalization capabilities of LLMs to tackle challenges in graph data, particularly in few-shot learning scenarios. \n\nOne of my notable contributions is the Graph COordinators for PrEtraining (GCOPE) framework, which unifies disparate graph datasets to enhance knowledge transfer and improve few-shot learning outcomes. Additionally, I have developed HAGO, a framework that dynamically integrates multi-domain graphs to optimize recommendation systems while mitigating negative transfer effects. \n\nMy research also delves into temporal interaction graphs, where I introduced TIGPrompt, a framework that bridges temporal and semantic gaps in representation learning. I am particularly interested in the application of prompt learning techniques to various graph tasks, as evidenced by my work on PromptMSP for multimer structure prediction and a hierarchical approach to drug-drug interaction prediction.\n\nThrough my extensive publications, I aim to advance the understanding of how LLMs can be effectively integrated with graph data, exploring both theoretical foundations and practical applications. I am committed to pushing the boundaries of graph prompting and its implications for artificial general intelligence, ultimately striving to reshape how we approach complex data relationships in the digital age.",
    "collaborators": [
      "Hong Cheng",
      "Jia Li",
      "Yun Xiong",
      "Xixi Wu",
      "Jiawei Zhang",
      "Haihong Zhao",
      "Aochuan Chen",
      "Zhiyao Shu",
      "Hengyu Zhang",
      "Chunxu Shen",
      "Jie Tan",
      "Yu Rong",
      "Chengzhi Piao",
      "Lingling Yi",
      "Xi Chen",
      "Siwei Zhang",
      "Yao Zhang",
      "Feng Zhao",
      "Yulin Kang",
      "Ziqi Gao",
      "Zijing Liu",
      "Yu Li",
      "Yingying Wang",
      "Bo Liu",
      "Jihong Guan",
      "Qunzhong Wang",
      "Yuhan Li",
      "Zhixun Li",
      "Peisong Wang",
      "Hongtao Cheng",
      "Jeffrey Xu Yu",
      "Jiawen Zhang",
      "Hang Dong",
      "Bo Qiao",
      "Si Qin",
      "Qingwei Lin"
    ],
    "pub_titles": [
      "All in One and One for All: A Simple yet Effective Method towards Cross-domain Graph Pretraining",
      "When LLM Meets Hypergraph: A Sociological Analysis on Personality via Online Social Networks",
      "Adaptive Coordinators and Prompts on Heterogeneous Graphs for Cross-Domain Recommendations",
      "Prompt Learning on Temporal Interaction Graphs",
      "Protein Multimer Structure Prediction via Prompt Learning",
      "Advanced Drug Interaction Event Prediction",
      "All in One: Multi-Task Prompting for Graph Neural Networks (Extended Abstract)",
      "Does Graph Prompt Work? A Data Operation Perspective with Theoretical Analysis",
      "A Survey of Graph Meets Large Language Model: Progress and Future Directions",
      "Graph Prompt Learning: A Comprehensive Survey and Beyond",
      "Counter-Empirical Attacking Based on Adversarial Reinforcement Learning for Time-Relevant Scoring System"
    ],
    "pub_abstracts": [
      "Large Language Models (LLMs) have revolutionized the fields of computer vision (CV) and natural language processing (NLP). One of the most notable advancements of LLMs is that a single model is trained on vast and diverse datasets spanning multiple domains -- a paradigm we term `All in One'. This methodology empowers LLMs with super generalization capabilities, facilitating an encompassing comprehension of varied data distributions. Leveraging these capabilities, a single LLM demonstrates remarkable versatility across a variety of domains -- a paradigm we term `One for All'. However, applying this idea to the graph field remains a formidable challenge, with cross-domain pretraining often resulting in negative transfer. This issue is particularly important in few-shot learning scenarios, where the paucity of training data necessitates the incorporation of external knowledge sources. In response to this challenge, we propose a novel approach called Graph COordinators for PrEtraining (GCOPE), that harnesses the underlying commonalities across diverse graph datasets to enhance few-shot learning. Our novel methodology involves a unification framework that amalgamates disparate graph datasets during the pretraining phase to distill and transfer meaningful knowledge to target tasks. Extensive experiments across multiple graph datasets demonstrate the superior efficacy of our approach. By successfully leveraging the synergistic potential of multiple graph datasets for pretraining, our work stands as a pioneering contribution to the realm of graph foundational model.",
      "Individual personalities significantly influence our perceptions, decisions, and social interactions, which is particularly crucial for gaining insights into human behavior patterns in online social network analysis. Many psychological studies have observed that personalities are strongly reflected in their social behaviors and social environments. In light of these problems, this paper proposes a sociological analysis framework for one's personality in an environment-based view instead of individual-level data mining. Specifically, to comprehensively understand an individual's behavior from low-quality records, we leverage the powerful associative ability of LLMs by designing an effective prompt. In this way, LLMs can integrate various scattered information with their external knowledge to generate higher-quality profiles, which can significantly improve the personality analysis performance. To explore the interactive mechanism behind the users and their online environments, we design an effective hypergraph neural network where the hypergraph nodes are users and the hyperedges in the hypergraph are social environments. We offer a useful dataset with user profile data, personality traits, and several detected environments from the real-world social platform. To the best of our knowledge, this is the first network-based dataset containing both hypergraph structure and social information, which could push forward future research in this area further. By employing the framework on this dataset, we can effectively capture the nuances of individual personalities and their online behaviors, leading to a deeper understanding of human interactions in the digital world.",
      "In the online digital world, users frequently engage with diverse items across multiple domains (e.g., e-commerce platforms, streaming services, and social media networks), forming complex heterogeneous interaction graphs. Leveraging this multi-domain information can undoubtedly enhance the performance of recommendation systems by providing more comprehensive user insights and alleviating data sparsity in individual domains. However, integrating multi-domain knowledge for the cross-domain recommendation is very hard due to inherent disparities in user behavior and item characteristics and the risk of negative transfer, where irrelevant or conflicting information from the source domains adversely impacts the target domain's performance. To address these challenges, we offer HAGO, a novel framework with $\\textbf{H}$eterogeneous $\\textbf{A}$daptive $\\textbf{G}$raph co$\\textbf{O}$rdinators, which dynamically integrate multi-domain graphs into a cohesive structure by adaptively adjusting the connections between coordinators and multi-domain graph nodes, thereby enhancing beneficial inter-domain interactions while mitigating negative transfer effects. Additionally, we develop a universal multi-domain graph pre-training strategy alongside HAGO to collaboratively learn high-quality node representations across domains. To effectively transfer the learned multi-domain knowledge to the target domain, we design an effective graph prompting method, which incorporates pre-trained embeddings with learnable prompts for the recommendation task. Our framework is compatible with various graph-based models and pre-training techniques, demonstrating broad applicability and effectiveness. Further experimental results show that our solutions outperform state-of-the-art methods in multi-domain recommendation scenarios and highlight their potential for real-world applications.",
      "Temporal Interaction Graphs (TIGs) are widely utilized to represent real-world systems. To facilitate representation learning on TIGs, researchers have proposed a series of TIG models. However, these models are still facing two tough gaps between the pre-training and downstream predictions in their ``pre-train, predict'' training paradigm. First, the temporal discrepancy between the pre-training and inference data severely undermines the models' applicability in distant future predictions on the dynamically evolving data. Second, the semantic divergence between pretext and downstream tasks hinders their practical applications, as they struggle to align with their learning and prediction capabilities across application scenarios. Recently, the ``pre-train, prompt'' paradigm has emerged as a lightweight mechanism for model generalization. Applying this paradigm is a potential solution to solve the aforementioned challenges. However, the adaptation of this paradigm to TIGs is not straightforward. The application of prompting in static graph contexts falls short in temporal settings due to a lack of consideration for time-sensitive dynamics and a deficiency in expressive power. To address this issue, we introduce Temporal Interaction Graph Prompting (TIGPrompt), a versatile framework that seamlessly integrates with TIG models, bridging both the temporal and semantic gaps. In detail, we propose a temporal prompt generator to offer temporally-aware prompts for different tasks. These prompts stand out for their minimalistic design, relying solely on the tuning of the prompt generator with very little supervision data. To cater to varying computational resource demands, we propose an extended ``pre-train, prompt-based fine-tune'' paradigm, offering greater flexibility. Through extensive experiments, the TIGPrompt demonstrates the SOTA performance and remarkable efficiency advantages.",
      "Understanding the 3D structures of protein multimers is crucial, as they play a vital role in regulating various cellular processes. It has been empirically confirmed that the multimer structure prediction~(MSP) can be well handled in a step-wise assembly fashion using provided dimer structures and predicted protein-protein interactions~(PPIs). However, due to the biological gap in the formation of dimers and larger multimers, directly applying PPI prediction techniques can often cause a \\textit{poor generalization} to the MSP task. To address this challenge, we aim to extend the PPI knowledge to multimers of different scales~(i.e., chain numbers). Specifically, we propose \\textbf{\\textsc{PromptMSP}}, a pre-training and \\textbf{Prompt} tuning framework for \\textbf{M}ultimer \\textbf{S}tructure \\textbf{P}rediction. First, we tailor the source and target tasks for effective PPI knowledge learning and efficient inference, respectively. We design PPI-inspired prompt learning to narrow the gaps of two task formats and generalize the PPI knowledge to multimers of different scales. We provide a meta-learning strategy to learn a reliable initialization of the prompt model, enabling our prompting framework to effectively adapt to limited data for large-scale multimers. Empirically, we achieve both significant accuracy (RMSD and TM-Score) and efficiency improvements compared to advanced MSP models. The code, data and checkpoints are released at \\url{https://github.com/zqgao22/PromptMSP}.",
      "Predicting drug-drug interaction adverse events, so-called DDI events, is increasingly valuable as it facilitates the study of mechanisms underlying drug use or adverse reactions. Existing models often neglect the distinctive characteristics of individual event classes when integrating multi-source features, which contributes to systematic unfairness when dealing with highly imbalanced event samples. Moreover, the limited capacity of these models to abstract the unique attributes of each event subclass considerably hampers their application in predicting rare drug-drug interaction events with a limited sample size. Reducing dataset bias and abstracting event subclass characteristics are two unresolved challenges. Recently, prompt tuning with frozen pre-trained graph models, namely\"pre-train, prompt, fine-tune\"strategy, has demonstrated impressive performance in few-shot tasks. Motivated by this, we propose an advanced method as a solution to address these aforementioned challenges. Specifically, our proposed approach entails a hierarchical pre-training task that aims to capture crucial aspects of drug molecular structure and intermolecular interactions while effectively mitigating implicit dataset bias within the node embeddings. Furthermore, we construct a prototypical graph by strategically sampling data from distinct event types and design subgraph prompts utilizing pre-trained node features. Through comprehensive benchmark experiments, we validate the efficacy of our subgraph prompts in accurately representing event classes and achieve exemplary results in both overall and subclass prediction tasks.",
      "This paper is an extended abstract of our original work published in KDD23, where we won the best research paper award. The paper introduces a novel approach to bridging the gap between pre-trained graph models and the diverse tasks they\u2019re applied to, inspired by the success of prompt learning in NLP. Recognizing the challenge of aligning pre-trained models with varied graph tasks (node level, edge level, and graph level), which can lead to negative transfer and poor performance, we propose a multi-task prompting method for graphs. This method involves unifying graph and language prompt formats, enabling NLP\u2019s prompting strategies to be adapted for graph tasks. By analyzing the task space of graph applications, we reformulate problems to fit graph-level tasks and apply meta-learning to improve prompt initialization for multiple tasks. Experiments show our method\u2019s effectiveness in enhancing model performance across different graph tasks. Beyond the original work, in this extended abstract, we further discuss the graph prompt from a bigger picture and provide some of the latest work toward this area.",
      "In recent years, graph prompting has emerged as a promising research direction, enabling the learning of additional tokens or subgraphs appended to the original graphs without requiring retraining of pre-trained graph models across various applications. This novel paradigm, shifting from the traditional pretraining and finetuning to pretraining and prompting has shown significant empirical success in simulating graph data operations, with applications ranging from recommendation systems to biological networks and graph transferring. However, despite its potential, the theoretical underpinnings of graph prompting remain underexplored, raising critical questions about its fundamental effectiveness. The lack of rigorous theoretical proof of why and how much it works is more like a dark cloud over the graph prompt area to go further. To fill this gap, this paper introduces a theoretical framework that rigorously analyzes graph prompting from a data operation perspective. Our contributions are threefold: First, we provide a formal guarantee theorem, demonstrating graph prompts capacity to approximate graph transformation operators, effectively linking upstream and downstream tasks. Second, we derive upper bounds on the error of these data operations by graph prompts for a single graph and extend this discussion to batches of graphs, which are common in graph model training. Third, we analyze the distribution of data operation errors, extending our theoretical findings from linear graph models (e.g., GCN) to non-linear graph models (e.g., GAT). Extensive experiments support our theoretical results and confirm the practical implications of these guarantees.",
      "Graph plays a significant role in representing and analyzing complex relationships in real-world applications such as citation networks, social networks, and biological data. Recently, Large Language Models (LLMs), which have achieved tremendous success in various domains, have also been leveraged in graph-related tasks to surpass traditional Graph Neural Networks (GNNs) based methods and yield state-of-the-art performance. In this survey, we first present a comprehensive review and analysis of existing methods that integrate LLMs with graphs. First of all, we propose a new taxonomy, which organizes existing methods into three categories based on the role (i.e., enhancer, predictor, and alignment component) played by LLMs in graph-related tasks. Then we systematically survey the representative methods along the three categories of the taxonomy. Finally, we discuss the remaining limitations of existing studies and highlight promising avenues for future research. The relevant papers are summarized and will be consistently updated at: https://github.com/yhLeeee/Awesome-LLMs-in-Graph-tasks.",
      "Artificial General Intelligence (AGI) has revolutionized numerous fields, yet its integration with graph data, a cornerstone in our interconnected world, remains nascent. This paper presents a pioneering survey on the emerging domain of graph prompts in AGI, addressing key challenges and opportunities in harnessing graph data for AGI applications. Despite substantial advancements in AGI across natural language processing and computer vision, the application to graph data is relatively underexplored. This survey critically evaluates the current landscape of AGI in handling graph data, highlighting the distinct challenges in cross-modality, cross-domain, and cross-task applications specific to graphs. Our work is the first to propose a unified framework for understanding graph prompt learning, offering clarity on prompt tokens, token structures, and insertion patterns in the graph domain. We delve into the intrinsic properties of graph prompts, exploring their flexibility, expressiveness, and interplay with existing graph models. A comprehensive taxonomy categorizes over 100 works in this field, aligning them with pre-training tasks across node-level, edge-level, and graph-level objectives. Additionally, we present, ProG, a Python library, and an accompanying website, to support and advance research in graph prompting. The survey culminates in a discussion of current challenges and future directions, offering a roadmap for research in graph prompting within AGI. Through this comprehensive analysis, we aim to catalyze further exploration and practical applications of AGI in graph data, underlining its potential to reshape AGI fields and beyond. ProG and the website can be accessed by \\url{https://github.com/WxxShirley/Awesome-Graph-Prompt}, and \\url{https://github.com/sheldonresearch/ProG}, respectively.",
      "Scoring systems are commonly seen for platforms in the era of Big Data. From credit scoring systems in financial services to membership scores in E-commerce shopping platforms, platform managers use such systems to guide users towards the encouraged activity pattern, and manage resources more effectively and efficiently. To establish such scoring systems, several \u201cempirical criteria\u201d are first determined, followed by a dedicated top-down design for each score factor, which usually requires enormous effort to adjust and tune the scoring function in the new application scenario. What's worse, many fresh projects usually have no ground truth or any experience to evaluate a reasonable scoring system, making the designing even harder. To reduce the effort of manual adjustment of the scoring function in every new scoring system, we innovatively study the scoring system from the preset empirical criteria without any ground truth and propose a novel framework to improve the system from scratch. In this paper, we propose a \u201ccounter-empirical attacking\u201d mechanism that can generate \u201cattacking\u201d behavior traces and try to break the empirical rules of the scoring system. Then an adversarial \u201cenhancer\u201d is applied to evaluate the scoring system and find the improvement strategy. By training the adversarial learning problem, a proper scoring function can be learned to be robust to the attacking activity traces that are trying to violate the empirical criteria. Extensive experiments have been conducted on two scoring systems, including a shared computing resource platform and a financial credit system. The experimental results have validated the effectiveness of our proposed framework."
    ],
    "domain": [
      "Graph Neural Network",
      "Large Language Models",
      "Multi-domain Learning",
      "Drug Interaction Prediction"
    ],
    "is_leader_candidate": true,
    "is_member_candidate": true,
    "is_reviewer_candidate": true,
    "is_chair_candidate": true
  },
  "096e2ae0-1b8f-438c-a792-7b4da5a4b521": {
    "pk": "096e2ae0-1b8f-438c-a792-7b4da5a4b521",
    "name": "Kostadin Cvejoski",
    "bio": "I am a researcher dedicated to advancing the understanding and application of dynamical systems, particularly through the lens of ordinary differential equations (ODEs). My recent work focuses on developing innovative machine learning frameworks for zero-shot inference and time series imputation, leveraging neural networks to extract meaningful insights from noisy data. I have introduced foundational inference models (FIM) that outperform traditional methods by enabling zero-shot learning across various dynamical systems, demonstrating the power of pretrained models in diverse applications.\n\nIn addition to my work on ODEs, I explore the intersection of language models and temporal dynamics, addressing challenges in natural language processing and recommender systems. My research includes developing dynamic topic models that account for the evolution of topics over time, as well as enhancing the performance of large pretrained language models in the face of distribution shifts.\n\nI am also passionate about knowledge-augmented machine learning, where I integrate expert knowledge into model training to improve generalization and robustness, especially in scenarios with limited data. My work spans various domains, from environmental modeling to password generation, and I strive to create methodologies that are not only effective but also interpretable and accessible.\n\nThrough my research, I aim to bridge the gap between theoretical foundations and practical applications, contributing to the development of robust, scalable, and insightful machine learning models that can tackle real-world challenges.",
    "collaborators": [
      "Rams\u00e9s J. S\u00e1nchez",
      "C. Ojeda",
      "C. Bauckhage",
      "B. Georgiev",
      "Patrick Seifner",
      "Laura von Rueden",
      "Sebastian Houben",
      "D. Biesner",
      "R. Sifa",
      "J. Sch\u00fccker",
      "Jannis Schuecker",
      "Antonia Korner",
      "N. Piatkowski",
      "Julian Wormann",
      "Daniel Bogdoll",
      "Etienne Buhrle",
      "Hang Chen",
      "E. F. Chuo",
      "L. V. Elst",
      "Tobias Glei\u00dfner",
      "Philip Gottschall",
      "Stefan Griesche",
      "Christian Hellert",
      "Christian Hesels",
      "Tim Joseph",
      "Niklas Keil",
      "J. Kelsch",
      "Hendrik Konigshof",
      "E. Kraft",
      "Leonie Kreuser",
      "Kevin Krone",
      "T. Latka",
      "Denny Mattern",
      "Stefan Matthes",
      "Mohsin Munir",
      "Moritz Nekolla",
      "A. Paschke",
      "Maximilian Pintz",
      "Tianming Qiu",
      "Faraz Qureishi",
      "Syed Tahseen Raza Rizvi",
      "J. Reichardt",
      "Stefan Rudolph",
      "A. Sagel",
      "G. Schunk",
      "Hao Shen",
      "Hendrik Stapelbroek",
      "V. Stehr",
      "G. Srinivas",
      "Anh Tuan Tran",
      "A.K. Vivekanandan",
      "Y. Wang",
      "Florian Wasserrab",
      "Tino Werner",
      "Christian Wirth",
      "Stefan Zwicklbauer",
      "L. Conrads",
      "Pascal Welke",
      "Anne-Katrin Mahlein",
      "Viktoriya Olari",
      "\u00d8yvind Eide",
      "Erik Krupicka"
    ],
    "pub_titles": [
      "Foundational Inference Models for Dynamical Systems",
      "Foundational Inference Models for Dynamical Systems",
      "Neural Dynamic Focused Topic Model",
      "The future is different: Large pre-trained language models fail in prediction tasks",
      "Informed Pre-Training on Prior Knowledge",
      "Knowledge Augmented Machine Learning with Applications in Autonomous Driving: A Survey",
      "Hidden Schema Networks",
      "Combining Variational Autoencoders and Transformer Language Models for Improved Password Generation",
      "Switching Dynamical Systems with Deep Neural Networks",
      "Combining expert knowledge and neural networks to model environmental stresses in agriculture",
      "Introduction to Machine Learning with Robots and Playful Learning",
      "Dynamic Review-based Recommenders",
      "Learning Deep Generative Models for Queuing Systems",
      "Auto Encoding Explanatory Examples with Stochastic Paths",
      "Generative Deep Learning Techniques for Password Generation",
      "Recurrent Point Review Models"
    ],
    "pub_abstracts": [
      "Ordinary differential equations (ODEs) underlie dynamical systems which serve as models for a vast number of natural and social phenomena. Yet inferring the ODE that best describes a set of noisy observations on one such phenomenon can be remarkably challenging, and the models available to achieve it tend to be highly specialized and complex too. In this work we propose a novel supervised learning framework for zero-shot inference of ODEs from noisy data. We first generate large datasets of one-dimensional ODEs, by sampling distributions over the space of initial conditions, and the space of vector fields defining them. We then learn neural maps between noisy observations on the solutions of these equations, and their corresponding initial condition and vector fields. The resulting models, which we call foundational inference models (FIM), can be (i) copied and matched along the time dimension to increase their resolution; and (ii) copied and composed to build inference models of any dimensionality, without the need of any finetuning. We use FIM to model both ground-truth dynamical systems of different dimensionalities and empirical time series data in a zero-shot fashion , and outperform state-of-the-art models which are fine-tuned to these systems. Our (pretrained) FIMs are available online.",
      "Dynamical systems governed by ordinary differential equations (ODEs) serve as models for a vast number of natural and social phenomena. In this work, we offer a fresh perspective on the classical problem of imputing missing time series data, whose underlying dynamics are assumed to be determined by ODEs. Specifically, we revisit ideas from amortized inference and neural operators, and propose a novel supervised learning framework for zero-shot time series imputation, through parametric functions satisfying some (hidden) ODEs. Our proposal consists of two components. First, a broad probability distribution over the space of ODE solutions, observation times and noise mechanisms, with which we generate a large, synthetic dataset of (hidden) ODE solutions, along with their noisy and sparse observations. Second, a neural recognition model that is trained offline, to map the generated time series onto the spaces of initial conditions and time derivatives of the (hidden) ODE solutions, which we then integrate to impute the missing data. We empirically demonstrate that one and the same (pretrained) recognition model can perform zero-shot imputation across 63 distinct time series with missing values, each sampled from widely different dynamical systems. Likewise, we demonstrate that it can perform zero-shot imputation of missing high-dimensional data in 10 vastly different settings, spanning human motion, air quality, traffic and electricity studies, as well as Navier-Stokes simulations -- without requiring any fine-tuning. What is more, our proposal often outperforms state-of-the-art methods, which are trained on the target datasets. Our pretrained model will be available online soon.",
      "Topic models and all their variants analyse text by learning meaningful representations through word co-occurrences. As pointed out by previous work, such models implicitly assume that the probability of a topic to be active and its proportion within each document are positively correlated. This correlation can be strongly detrimental in the case of documents created over time, simply because recent documents are likely better described by new and hence rare topics. In this work we leverage recent advances in neural variational inference and present an alternative neural approach to the dynamic Focused Topic Model. Indeed, we develop a neural model for topic evolution which exploits sequences of Bernoulli random variables in order to track the appearances of topics, thereby decoupling their activities from their proportions. We evaluate our model on three different datasets (the UN general debates, the collection of NeurIPS papers, and the ACL Anthology dataset) and show that it (i) outperforms state-of-the-art topic models in generalization tasks and (ii) performs comparably to them on prediction tasks, while employing roughly the same number of parameters, and converging about two times faster.",
      "Large pre-trained language models (LPLM) have shown spectacular success when fine-tuned on downstream supervised tasks. Yet, it is known that their performance can drastically drop when there is a distribution shift between the data used during training and that used at inference time. In this paper we focus on data distributions that naturally change over time and introduce four new REDDIT datasets, namely the WALLSTREETBETS, ASKSCIENCE, THE DONALD, and POLITICS sub-reddits. First, we empirically demonstrate that LPLM can display average performance drops of about 88% (in the best case!) when predicting the popularity of future posts from sub-reddits whose topic distribution changes with time. We then introduce a simple methodology that leverages neural variational dynamic topic models and attention mechanisms to infer temporal language model representations for regression tasks. Our models display performance drops of only about 40% in the worst cases (2% in the best ones) when predicting the popularity of future posts, while using only about 7% of the total number of parameters of LPLM and providing interpretable representations that offer insight into real-world events, like the GameStop short squeeze of 2021",
      "When training data is scarce, the incorporation of additional prior knowledge can assist the learning process. While it is common to initialize neural networks with weights that have been pre-trained on other large data sets, pre-training on more concise forms of knowledge has rather been overlooked. In this paper, we propose a novel informed machine learning approach and suggest to pre-train on prior knowledge. Formal knowledge representations, e.g. graphs or equations, are first transformed into a small and condensed data set of knowledge prototypes. We show that informed pre-training on such knowledge prototypes (i) speeds up the learning processes, (ii) improves generalization capabilities in the regime where not enough training data is available, and (iii) increases model robustness. Analyzing which parts of the model are affected most by the prototypes reveals that improvements come from deeper layers that typically represent high-level features. This confirms that informed pre-training can indeed transfer semantic knowledge. This is a novel effect, which shows that knowledge-based pre-training has additional and complementary strengths to existing approaches.",
      "The availability of representative datasets is an essential prerequisite for many successful artificial intelligence and machine learning models. However, in real life applications these models often encounter scenarios that are inadequately represented in the data used for training. There are various reasons for the absence of sufficient data, ranging from time and cost constraints to ethical considerations. As a consequence, the reliable usage of these models, especially in safety-critical applications, is still a tremendous challenge. Leveraging additional, already existing sources of knowledge is key to overcome the limitations of purely data-driven approaches. Knowledge augmented machine learning approaches offer the possibility of compensating for deficiencies, errors, or ambiguities in the data, thus increasing the generalization capability of the applied models. Even more, predictions that conform with knowledge are crucial for making trustworthy and safe decisions even in underrepresented scenarios. This work provides an overview of existing techniques and methods in the literature that combine data-driven models with existing knowledge. The identified approaches are structured according to the categories knowledge integration, extraction and conformity. In particular, we address the application of the presented methods in the field of autonomous driving.",
      "Large, pretrained language models infer powerful representations that encode rich semantic and syntactic content, albeit implicitly. In this work we introduce a novel neural language model that enforces, via inductive biases, explicit relational structures which allow for compositionality onto the output representations of pretrained language models. Specifically, the model encodes sentences into sequences of symbols (composed representations), which correspond to the nodes visited by biased random walkers on a global latent graph, and infers the posterior distribution of the latter. We first demonstrate that the model is able to uncover ground-truth graphs from artificially generated datasets of random token sequences. Next, we leverage pretrained BERT and GPT-2 language models as encoder and decoder, respectively, to infer networks of symbols (schemata) from natural language datasets. Our experiments show that (i) the inferred symbols can be interpreted as encoding different aspects of language, as e.g. topics or sentiments, and that (ii) GPT-2-like models can effectively be conditioned on symbolic representations. Finally, we explore training autoregressive, random walk \u201creasoning\u201d models on schema networks inferred from commonsense knowledge databases, and using the sampled paths to enhance the performance of pretrained language models on commonsense If-Then reasoning tasks.",
      "Password generation techniques have recently been explored by leveraging deep-learning natural language processing (NLP) algorithms. Previous work has raised the state of the art for password guessing algorithms significantly, by approaching the problem using either variational autoencoders with CNN-based encoder and decoder architectures or transformer-based architectures (namely GPT2) for text generation. In this work we aim to combine both paradigms, introducing a novel architecture that leverages the expressive power of transformers with the natural sampling approach to text generation of variational autoencoders. We show how our architecture generates state-of-the-art results in password matching performance across multiple benchmark datasets.",
      "The problem of uncovering different dynamical regimes is of pivotal importance in time series analysis. Switching dynamical systems provide a solution for modeling physical phenomena whose time series data exhibit different dynamical modes. In this work we propose a novel variational RNN model for switching dynamics allowing for both non-Markovian and nonlinear dynamical behavior between and within dynamic modes. Attention mechanisms are provided to inform the switching distribution. We evaluate our model on synthetic and empirical datasets of diverse nature and successfully uncover different dynamical regimes and predict the switching dynamics.",
      "In this work we combine representation learning capabilities of neural network with agricultural knowledge from experts to model environmental heat and drought stresses. We first design deterministic expert models which serve as a benchmark and inform the design of flexible neural-network architectures. Finally, a sensitivity analysis of the latter allows a clustering of hybrids into susceptible and resistant ones.",
      "Inspired by explanations of machine learning concepts in children\u2019s books, we developed an approach to introduce supervised, unsupervised, and reinforcement learning using a block-based programming language in combination with the benefits of educational robotics. Instead of using blocks as high-end APIs to access AI cloud services or to reproduce the machine learning algorithms, we use them as a means to put the student \u201cin the algorithm\u2019s shoes.\u201d We adapt the training of neural networks, Q-learning, and k-means algorithms to a design and format suitable for children and equip the students with hands-on tools for playful experimentation. The children learn about direct supervision by modifying the weights in the neural networks and immediately observing the effects on the simulated robot. Following the ideas of constructionism, they experience how the algorithms and underlying machine learning concepts work in practice. We conducted and evaluated this approach with students in primary, middle, and high school. All the age groups perceived the topics to be very easy to moderately hard to grasp. Younger students experienced direct supervision as challenging, whereas they found Q-learning and k-means algorithms much more accessible. Most high-school students could cope with all the topics without particular difficulties.",
      "Just as user preferences change with time, item reviews also reflect those same preference changes. In a nutshell, if one is to sequentially incorporate review content knowledge into recommender systems, one is naturally led to dynamical models of text. In the present work we leverage the known power of reviews to enhance rating predictions in a way that (i) respects the causality of review generation and (ii) includes, in a bidirectional fashion, the ability of ratings to inform language review models and vice-versa, language representations that help predict ratings end-to-end. Moreover, our representations are time-interval aware and thus yield a continuous-time representation of the dynamics. We provide experiments on real-world datasets and show that our methodology is able to outperform several state-of-the-art models. Source code for all models can be found at [1].",
      "Modern society is heavily dependent on large scale client-server systems with applications ranging from Internet and Communication Services to sophisticated logistics and deployment of goods. To maintain and improve such a system, a careful study of client and server dynamics is needed \u2013 e.g. response/service times, aver-age number of clients at given times, etc. To this end, one traditionally relies, within the queuing theory formalism,on parametric analysis and explicit distribution forms.However, parametric forms limit the model\u2019s expressiveness and could struggle on extensively large datasets. We propose a novel data-driven approach towards queuing systems: the Deep Generative Service Times. Our methodology delivers a flexible and scalable model for service and response times. We leverage the representation capabilities of Recurrent Marked Point Processes for the temporal dynamics of clients, as well as Wasserstein Generative Adversarial Network techniques, to learn deep generative models which are able to represent complex conditional service time distributions. We provide extensive experimental analysis on both empirical and synthetic datasets, showing the effectiveness of the proposed models",
      "In this paper we ask for the main factors that determine a classifier\u2019s decision making process and uncover such factors by studying latent codes produced by auto-encoding frameworks. To deliver an explanation of a classifier\u2019s behaviour, we propose a method that provides series of examples highlighting semantic differences between the classifier\u2019s decisions. These examples are generated through interpolations in latent space. We introduce and formalize the notion of a semantic stochastic path, as a suitable stochastic process defined in feature (data) space via latent code interpolations. We then introduce the concept of semantic Lagrangians as a way to incorporate the desired classifier\u2019s behaviour and find that the solution of the associated variational problem allows for highlighting differences in the classifier decision. Very importantly, within our framework the classifier is used as a black-box, and only its evaluation is required.",
      "Password guessing approaches via deep learning have recently been investigated with significant breakthroughs in their ability to generate novel, realistic password candidates. In the present work we study a broad collection of deep learning and probabilistic based models in the light of password guessing: attention-based deep neural networks, autoencoding mechanisms and generative adversarial networks. We provide novel generative deep-learning models in terms of variational autoencoders exhibiting state-of-art sampling performance, yielding additional latent-space features such as interpolations and targeted sampling. Lastly, we perform a thorough empirical analysis in a unified controlled framework over well-known datasets (RockYou, LinkedIn, Youku, Zomato, Pwnd). Our results not only identify the most promising schemes driven by deep neural networks, but also illustrate the strengths of each approach in terms of generation variability and sample uniqueness.",
      "Deep neural network models represent the state-of-the-art methodologies for natural language processing. Here we build on top of these methodologies to incorporate temporal information and model how review data changes with time. Specifically, we use the dynamic representations of recurrent point process models, which encode the history of how business or service reviews are received in time, to generate instantaneous language models with improved prediction capabilities. Simultaneously, our methodologies enhance the predictive power of our point process models by incorporating summarized review content representations. We provide recurrent network and temporal convolution solutions for modeling the review content. We deploy our methodologies in the context of recommender systems, effectively characterizing the change in preference and taste of users as time evolves. Source code is available at [1]."
    ],
    "domain": [
      "Dynamical Systems",
      "Machine Learning",
      "Natural Language Processing",
      "Time Series Analysis"
    ],
    "is_leader_candidate": true,
    "is_member_candidate": true,
    "is_reviewer_candidate": true,
    "is_chair_candidate": true
  },
  "f16fb819-8800-4795-927a-04b39f4fa535": {
    "pk": "f16fb819-8800-4795-927a-04b39f4fa535",
    "name": "Patrick Seifner",
    "bio": "I am a researcher dedicated to the intersection of dynamical systems and machine learning, particularly focusing on ordinary differential equations (ODEs) and their applications in modeling complex phenomena. My recent work has led to the development of foundational inference models (FIMs) that enable zero-shot inference of ODEs from noisy observations. This innovative framework allows for the generation of large datasets of one-dimensional ODEs, facilitating the learning of neural maps that can accurately predict initial conditions and vector fields without the need for fine-tuning.\n\nIn addition to ODE inference, I have explored the challenge of imputing missing time series data governed by ODEs. By leveraging concepts from amortized inference and neural operators, I proposed a novel approach that demonstrates remarkable zero-shot imputation capabilities across diverse datasets, outperforming state-of-the-art methods.\n\nMy research also extends to Markov jump processes, where I introduced a variational inference algorithm that utilizes neural ODEs for efficient and effective inference. This work highlights my commitment to developing scalable and robust methodologies that can be applied across various domains, from human motion analysis to molecular dynamics simulations.\n\nOverall, my goal is to bridge the gap between theoretical dynamical systems and practical machine learning applications, providing tools that enhance our understanding and prediction of complex systems.",
    "collaborators": [
      "Rams\u00e9s J. S\u00e1nchez",
      "K. Cvejoski",
      "Antonia Korner"
    ],
    "pub_titles": [
      "Foundational Inference Models for Dynamical Systems",
      "Foundational Inference Models for Dynamical Systems",
      "Neural Markov Jump Processes"
    ],
    "pub_abstracts": [
      "Ordinary differential equations (ODEs) underlie dynamical systems which serve as models for a vast number of natural and social phenomena. Yet inferring the ODE that best describes a set of noisy observations on one such phenomenon can be remarkably challenging, and the models available to achieve it tend to be highly specialized and complex too. In this work we propose a novel supervised learning framework for zero-shot inference of ODEs from noisy data. We first generate large datasets of one-dimensional ODEs, by sampling distributions over the space of initial conditions, and the space of vector fields defining them. We then learn neural maps between noisy observations on the solutions of these equations, and their corresponding initial condition and vector fields. The resulting models, which we call foundational inference models (FIM), can be (i) copied and matched along the time dimension to increase their resolution; and (ii) copied and composed to build inference models of any dimensionality, without the need of any finetuning. We use FIM to model both ground-truth dynamical systems of different dimensionalities and empirical time series data in a zero-shot fashion , and outperform state-of-the-art models which are fine-tuned to these systems. Our (pretrained) FIMs are available online.",
      "Dynamical systems governed by ordinary differential equations (ODEs) serve as models for a vast number of natural and social phenomena. In this work, we offer a fresh perspective on the classical problem of imputing missing time series data, whose underlying dynamics are assumed to be determined by ODEs. Specifically, we revisit ideas from amortized inference and neural operators, and propose a novel supervised learning framework for zero-shot time series imputation, through parametric functions satisfying some (hidden) ODEs. Our proposal consists of two components. First, a broad probability distribution over the space of ODE solutions, observation times and noise mechanisms, with which we generate a large, synthetic dataset of (hidden) ODE solutions, along with their noisy and sparse observations. Second, a neural recognition model that is trained offline, to map the generated time series onto the spaces of initial conditions and time derivatives of the (hidden) ODE solutions, which we then integrate to impute the missing data. We empirically demonstrate that one and the same (pretrained) recognition model can perform zero-shot imputation across 63 distinct time series with missing values, each sampled from widely different dynamical systems. Likewise, we demonstrate that it can perform zero-shot imputation of missing high-dimensional data in 10 vastly different settings, spanning human motion, air quality, traffic and electricity studies, as well as Navier-Stokes simulations -- without requiring any fine-tuning. What is more, our proposal often outperforms state-of-the-art methods, which are trained on the target datasets. Our pretrained model will be available online soon.",
      "Markov jump processes are continuous-time stochastic processes with a wide range of applications in both natural and social sciences. Despite their widespread use, inference in these models is highly non-trivial and typically proceeds via either Monte Carlo or expectation-maximization methods. In this work we introduce an alternative, variational inference algorithm for Markov jump processes which relies on neural ordinary differential equations, and is trainable via back-propagation. Our methodology learns neural, continuous-time representations of the observed data, that are used to approximate the initial distribution and time-dependent transition probability rates of the posterior Markov jump process. The time-independent rates of the prior process are in contrast trained akin to generative adversarial networks. We test our approach on synthetic data sampled from ground-truth Markov jump processes, experimental switching ion channel data and molecular dynamics simulations. Source code to reproduce our experiments is available online."
    ],
    "domain": [
      "Dynamical Systems",
      "Neural Networks",
      "Variational Inference",
      "Time Series Analysis"
    ],
    "is_leader_candidate": true,
    "is_member_candidate": true,
    "is_reviewer_candidate": true,
    "is_chair_candidate": true
  },
  "a42f1038-0e6d-465a-b5c1-66ef7558bcd6": {
    "pk": "a42f1038-0e6d-465a-b5c1-66ef7558bcd6",
    "name": "Cesar Ojeda",
    "bio": "I am a researcher deeply engaged in the intersection of machine learning, time series analysis, and natural language processing. My work primarily focuses on developing innovative models that capture the dynamics of data over time, particularly in the context of topic modeling, recommender systems, and service systems. \n\nIn my recent publications, I have introduced novel approaches such as a neural model for dynamic topic evolution that decouples topic activity from their proportions, significantly improving generalization tasks. I have also explored the application of Schr\u00f6dinger bridges for generative data modeling, enhancing the understanding of complex data distributions. My research on large pre-trained language models has revealed their vulnerabilities to distribution shifts, leading to the development of methodologies that leverage neural variational dynamic topic models for more robust predictions.\n\nI am particularly interested in the dynamics of user preferences and how they evolve, which has driven my work on recommender systems that integrate review content with rating predictions in a causally aware manner. Additionally, I have contributed to the field of queuing theory by proposing deep generative models that effectively capture client-server dynamics without the constraints of traditional parametric forms.\n\nThrough my research, I aim to provide interpretable and scalable solutions that not only advance theoretical understanding but also have practical implications across various domains, from social media analysis to service system optimization. My work is characterized by a commitment to leveraging advanced statistical techniques and deep learning methodologies to address real-world challenges.",
    "collaborators": [
      "Rams\u00e9s J. S\u00e1nchez",
      "K. Cvejoski",
      "C. Bauckhage",
      "B. Georgiev",
      "M. Opper",
      "W. Palma",
      "S. Eyheramendy",
      "F. Elorrieta",
      "J. Sch\u00fccker",
      "Ludwig Winkler",
      "Noa Malem-Shinitski",
      "L. Conrads",
      "Pascal Welke",
      "Jannis Schuecker",
      "Kostadin Cvejosky"
    ],
    "pub_titles": [
      "Neural Dynamic Focused Topic Model",
      "A Score-Based Approach for Training Schr\u00f6dinger Bridges for Data Modelling",
      "The future is different: Large pre-trained language models fail in prediction tasks",
      "Stochastic Control for Bayesian Neural Network Training",
      "A Novel First-Order Autoregressive Moving Average Model to Analyze Discrete-Time Series Irregularly Observed",
      "Hidden Schema Networks",
      "Nonlinear Hawkes Process with Gaussian Process Self Effects",
      "A novel bivariate autoregressive model for predicting and forecasting irregularly observed time series",
      "An irregularly spaced first-order moving average model",
      "Switching Dynamical Systems with Deep Neural Networks",
      "Flexible Temporal Point Processes Modeling with Nonlinear Hawkes Processes with Gaussian Processes Excitations and Inhibitions",
      "Dynamic Review-based Recommenders",
      "Learning Deep Generative Models for Queuing Systems",
      "Auto Encoding Explanatory Examples with Stochastic Paths",
      "Recurrent Point Review Models",
      "Recurrent Adversarial Service Times"
    ],
    "pub_abstracts": [
      "Topic models and all their variants analyse text by learning meaningful representations through word co-occurrences. As pointed out by previous work, such models implicitly assume that the probability of a topic to be active and its proportion within each document are positively correlated. This correlation can be strongly detrimental in the case of documents created over time, simply because recent documents are likely better described by new and hence rare topics. In this work we leverage recent advances in neural variational inference and present an alternative neural approach to the dynamic Focused Topic Model. Indeed, we develop a neural model for topic evolution which exploits sequences of Bernoulli random variables in order to track the appearances of topics, thereby decoupling their activities from their proportions. We evaluate our model on three different datasets (the UN general debates, the collection of NeurIPS papers, and the ACL Anthology dataset) and show that it (i) outperforms state-of-the-art topic models in generalization tasks and (ii) performs comparably to them on prediction tasks, while employing roughly the same number of parameters, and converging about two times faster.",
      "A Schr\u00f6dinger bridge is a stochastic process connecting two given probability distributions over time. It has been recently applied as an approach for generative data modelling. The computational training of such bridges requires the repeated estimation of the drift function for a time-reversed stochastic process using samples generated by the corresponding forward process. We introduce a modified score- function-based method for computing such reverse drifts, which can be efficiently implemented by a feed-forward neural network. We applied our approach to artificial datasets with increasing complexity. Finally, we evaluated its performance on genetic data, where Schr\u00f6dinger bridges can be used to model the time evolution of single-cell RNA measurements.",
      "Large pre-trained language models (LPLM) have shown spectacular success when fine-tuned on downstream supervised tasks. Yet, it is known that their performance can drastically drop when there is a distribution shift between the data used during training and that used at inference time. In this paper we focus on data distributions that naturally change over time and introduce four new REDDIT datasets, namely the WALLSTREETBETS, ASKSCIENCE, THE DONALD, and POLITICS sub-reddits. First, we empirically demonstrate that LPLM can display average performance drops of about 88% (in the best case!) when predicting the popularity of future posts from sub-reddits whose topic distribution changes with time. We then introduce a simple methodology that leverages neural variational dynamic topic models and attention mechanisms to infer temporal language model representations for regression tasks. Our models display performance drops of only about 40% in the worst cases (2% in the best ones) when predicting the popularity of future posts, while using only about 7% of the total number of parameters of LPLM and providing interpretable representations that offer insight into real-world events, like the GameStop short squeeze of 2021",
      "In this paper, we propose to leverage the Bayesian uncertainty information encoded in parameter distributions to inform the learning procedure for Bayesian models. We derive a first principle stochastic differential equation for the training dynamics of the mean and uncertainty parameter in the variational distributions. On the basis of the derived Bayesian stochastic differential equation, we apply the methodology of stochastic optimal control on the variational parameters to obtain individually controlled learning rates. We show that the resulting optimizer, StochControlSGD, is significantly more robust to large learning rates and can adaptively and individually control the learning rates of the variational parameters. The evolution of the control suggests separate and distinct dynamical behaviours in the training regimes for the mean and uncertainty parameters in Bayesian neural networks.",
      "A novel first-order autoregressive moving average model for analyzing discrete-time series observed at irregularly spaced times is introduced. Under Gaussianity, it is established that the model is strictly stationary and ergodic. In the general case, it is shown that the model is weakly stationary. The lowest dimension of the state-space representation is given along with the one-step linear predictors and their mean squared errors. The maximum likelihood estimation procedure is discussed, and their finite-sample behavior is assessed through Monte Carlo experiments. These experiments show that bias, root mean squared error, and coefficient of variation are smaller when the length of the series increases. Further, the method provides good estimations for the standard errors, even with relatively small sample sizes. Also, the irregularly spaced times seem to increase the estimation variability. The application of the proposed model is made through two real-life examples. The first is concerned with medical data, whereas the second describes an astronomical data set analysis.",
      "Large, pretrained language models infer powerful representations that encode rich semantic and syntactic content, albeit implicitly. In this work we introduce a novel neural language model that enforces, via inductive biases, explicit relational structures which allow for compositionality onto the output representations of pretrained language models. Specifically, the model encodes sentences into sequences of symbols (composed representations), which correspond to the nodes visited by biased random walkers on a global latent graph, and infers the posterior distribution of the latter. We first demonstrate that the model is able to uncover ground-truth graphs from artificially generated datasets of random token sequences. Next, we leverage pretrained BERT and GPT-2 language models as encoder and decoder, respectively, to infer networks of symbols (schemata) from natural language datasets. Our experiments show that (i) the inferred symbols can be interpreted as encoding different aspects of language, as e.g. topics or sentiments, and that (ii) GPT-2-like models can effectively be conditioned on symbolic representations. Finally, we explore training autoregressive, random walk \u201creasoning\u201d models on schema networks inferred from commonsense knowledge databases, and using the sampled paths to enhance the performance of pretrained language models on commonsense If-Then reasoning tasks.",
      "Traditionally, Hawkes processes are used to model time--continuous point processes with history dependence. Here we propose an extended model where the self--effects are of both excitatory and inhibitory type and follow a Gaussian Process. Whereas previous work either relies on a less flexible parameterization of the model, or requires a large amount of data, our formulation allows for both a flexible model and learning when data are scarce. We continue the line of work of Bayesian inference for Hawkes processes, and our approach dispenses with the necessity of estimating a branching structure for the posterior, as we perform inference on an aggregated sum of Gaussian Processes. Efficient approximate Bayesian inference is achieved via data augmentation, and we describe a mean--field variational inference approach to learn the model parameters. To demonstrate the flexibility of the model we apply our methodology on data from three different domains and compare it to previously reported results.",
      "In several disciplines it is common to find time series measured at irregular observational times. In particular, in astronomy there are a large number of surveys that gather information over irregular time gaps and in more than one passband. Some examples are Pan-STARRS, ZTF and also the LSST. However, current commonly used time series models that estimate the time dependency in astronomical light curves consider the information of each band separately (e.g, CIAR, IAR and CARMA models) disregarding the dependency that might exist between different passbands. In this paper we propose a novel bivariate model for irregularly sampled time series, called the bivariate irregular autoregressive (BIAR) model. The BIAR model assumes an autoregressive structure on each time series, it is stationary, and it allows to estimate the autocorrelation, the cross-correlation and the contemporary correlation between two unequally spaced time series. We implemented the BIAR model on light curves, in the g and r bands, obtained from the ZTF alerts processed by the ALeRCE broker. We show that if the light curves of the two bands are highly correlated, the model has more accurate forecast and prediction using the bivariate model than a similar method that uses only univariate information. Further, the estimated parameters of the BIAR are useful to characterize Long Period Variable Stars and to distinguish between classes of stochastic objects, providing promising features that can be used for classification purposes.",
      "A novel first-order moving-average model for analyzing time series observed at irregularly spaced intervals is introduced. Two definitions are presented, which are equivalent under Gaussianity. The first one relies on normally distributed data and the specification of second-order moments. The second definition provided is more flexible in the sense that it allows for considering other distributional assumptions. The statistical properties are investigated along with the one-step linear predictors and their mean squared errors. It is established that the process is strictly stationary under normality and weakly stationary in the general case. Maximum likelihood and bootstrap estimation procedures are discussed and the finite-sample behavior of these estimates is assessed through Monte Carlo experiments. In these simulations, both methods perform well in terms of estimation bias and standard errors, even with relatively small sample sizes. Moreover, we show that for non-Gaussian data, for t-Student and Generalized errors distributions, the parameters of the model can be estimated precisely by maximum likelihood. The proposed IMA model is compared to the continuous autoregressive moving average (CARMA) models, exhibiting good performance. Finally, the practical application and usefulness of the proposed model are illustrated with two real-life data examples.",
      "The problem of uncovering different dynamical regimes is of pivotal importance in time series analysis. Switching dynamical systems provide a solution for modeling physical phenomena whose time series data exhibit different dynamical modes. In this work we propose a novel variational RNN model for switching dynamics allowing for both non-Markovian and nonlinear dynamical behavior between and within dynamic modes. Attention mechanisms are provided to inform the switching distribution. We evaluate our model on synthetic and empirical datasets of diverse nature and successfully uncover different dynamical regimes and predict the switching dynamics.",
      "We propose an extended Hawkes process model where the self\u2013effects are of both excitatory and inhibitory type and follow a Gaussian Process. Whereas previous work either relies on a less \ufb02ex-ible parameterization of the model, or requires a large amount of data, our formulation allows for both a \ufb02exible model and learning when data are scarce. Ef\ufb01cient approximate Bayesian inference is achieved via data augmentation, and we describe a mean\u2013\ufb01eld variational inference approach to learn the model parameters. To demonstrate the \ufb02exibility of the model we apply our methodology on data from two different domains and compare it to previously reported results.",
      "Just as user preferences change with time, item reviews also reflect those same preference changes. In a nutshell, if one is to sequentially incorporate review content knowledge into recommender systems, one is naturally led to dynamical models of text. In the present work we leverage the known power of reviews to enhance rating predictions in a way that (i) respects the causality of review generation and (ii) includes, in a bidirectional fashion, the ability of ratings to inform language review models and vice-versa, language representations that help predict ratings end-to-end. Moreover, our representations are time-interval aware and thus yield a continuous-time representation of the dynamics. We provide experiments on real-world datasets and show that our methodology is able to outperform several state-of-the-art models. Source code for all models can be found at [1].",
      "Modern society is heavily dependent on large scale client-server systems with applications ranging from Internet and Communication Services to sophisticated logistics and deployment of goods. To maintain and improve such a system, a careful study of client and server dynamics is needed \u2013 e.g. response/service times, aver-age number of clients at given times, etc. To this end, one traditionally relies, within the queuing theory formalism,on parametric analysis and explicit distribution forms.However, parametric forms limit the model\u2019s expressiveness and could struggle on extensively large datasets. We propose a novel data-driven approach towards queuing systems: the Deep Generative Service Times. Our methodology delivers a flexible and scalable model for service and response times. We leverage the representation capabilities of Recurrent Marked Point Processes for the temporal dynamics of clients, as well as Wasserstein Generative Adversarial Network techniques, to learn deep generative models which are able to represent complex conditional service time distributions. We provide extensive experimental analysis on both empirical and synthetic datasets, showing the effectiveness of the proposed models",
      "In this paper we ask for the main factors that determine a classifier\u2019s decision making process and uncover such factors by studying latent codes produced by auto-encoding frameworks. To deliver an explanation of a classifier\u2019s behaviour, we propose a method that provides series of examples highlighting semantic differences between the classifier\u2019s decisions. These examples are generated through interpolations in latent space. We introduce and formalize the notion of a semantic stochastic path, as a suitable stochastic process defined in feature (data) space via latent code interpolations. We then introduce the concept of semantic Lagrangians as a way to incorporate the desired classifier\u2019s behaviour and find that the solution of the associated variational problem allows for highlighting differences in the classifier decision. Very importantly, within our framework the classifier is used as a black-box, and only its evaluation is required.",
      "Deep neural network models represent the state-of-the-art methodologies for natural language processing. Here we build on top of these methodologies to incorporate temporal information and model how review data changes with time. Specifically, we use the dynamic representations of recurrent point process models, which encode the history of how business or service reviews are received in time, to generate instantaneous language models with improved prediction capabilities. Simultaneously, our methodologies enhance the predictive power of our point process models by incorporating summarized review content representations. We provide recurrent network and temporal convolution solutions for modeling the review content. We deploy our methodologies in the context of recommender systems, effectively characterizing the change in preference and taste of users as time evolves. Source code is available at [1].",
      "Service system dynamics occur at the interplay between customer behaviour and a service provider's response. This kind of dynamics can effectively be modeled within the framework of queuing theory where customers' arrivals are described by point process models. However, these approaches are limited by parametric assumptions as to, for example, inter-event time distributions. In this paper, we address these limitations and propose a novel, deep neural network solution to the queuing problem. Our solution combines a recurrent neural network that models the arrival process with a recurrent generative adversarial network which models the service time distribution. We evaluate our methodology on various empirical datasets ranging from internet services (Blockchain, GitHub, Stackoverflow) to mobility service systems (New York taxi cab)."
    ],
    "domain": [
      "Natural Language Processing",
      "Time Series Analysis",
      "Bayesian Inference",
      "Deep Learning"
    ],
    "is_leader_candidate": true,
    "is_member_candidate": true,
    "is_reviewer_candidate": true,
    "is_chair_candidate": true
  },
  "e73be071-9a9a-4d65-bd99-fe97646ea967": {
    "pk": "e73be071-9a9a-4d65-bd99-fe97646ea967",
    "name": "Ramses J. Sanchez",
    "bio": "I am a researcher dedicated to exploring the intersection of dynamical systems and machine learning, particularly in the context of time series data. My recent work focuses on the challenge of imputing missing values in time series governed by ordinary differential equations (ODEs). By leveraging concepts from amortized inference and neural operators, I have developed a novel supervised learning framework for zero-shot time series imputation. \n\nThis framework is built on two key components: a broad probability distribution that generates synthetic datasets of ODE solutions, and a neural recognition model that maps these time series to the initial conditions and derivatives of the underlying ODEs. Remarkably, this pretrained model can perform zero-shot imputation across a diverse range of 63 distinct time series, spanning various domains such as human motion, air quality, and traffic studies, without the need for fine-tuning. \n\nMy approach not only demonstrates the versatility of the model but also consistently outperforms state-of-the-art methods that require training on specific datasets. I am excited about the potential applications of this work and look forward to making my pretrained model publicly available to further advance research in this area.",
    "collaborators": [
      "Patrick Seifner",
      "K. Cvejoski",
      "Antonia Korner"
    ],
    "pub_titles": [
      "Foundational Inference Models for Dynamical Systems"
    ],
    "pub_abstracts": [
      "Dynamical systems governed by ordinary differential equations (ODEs) serve as models for a vast number of natural and social phenomena. In this work, we offer a fresh perspective on the classical problem of imputing missing time series data, whose underlying dynamics are assumed to be determined by ODEs. Specifically, we revisit ideas from amortized inference and neural operators, and propose a novel supervised learning framework for zero-shot time series imputation, through parametric functions satisfying some (hidden) ODEs. Our proposal consists of two components. First, a broad probability distribution over the space of ODE solutions, observation times and noise mechanisms, with which we generate a large, synthetic dataset of (hidden) ODE solutions, along with their noisy and sparse observations. Second, a neural recognition model that is trained offline, to map the generated time series onto the spaces of initial conditions and time derivatives of the (hidden) ODE solutions, which we then integrate to impute the missing data. We empirically demonstrate that one and the same (pretrained) recognition model can perform zero-shot imputation across 63 distinct time series with missing values, each sampled from widely different dynamical systems. Likewise, we demonstrate that it can perform zero-shot imputation of missing high-dimensional data in 10 vastly different settings, spanning human motion, air quality, traffic and electricity studies, as well as Navier-Stokes simulations -- without requiring any fine-tuning. What is more, our proposal often outperforms state-of-the-art methods, which are trained on the target datasets. Our pretrained model will be available online soon."
    ],
    "domain": [
      "Time Series Analysis",
      "Ordinary Differential Equations",
      "Machine Learning",
      "Neural Networks"
    ],
    "is_leader_candidate": true,
    "is_member_candidate": true,
    "is_reviewer_candidate": true,
    "is_chair_candidate": true
  },
  "41951516-8b8f-4c84-ace8-01fb5e960185": {
    "pk": "41951516-8b8f-4c84-ace8-01fb5e960185",
    "name": "Yuan He",
    "bio": "I am a researcher dedicated to the intersection of language models and ontological knowledge representation. My recent work focuses on leveraging pre-trained language models (PLMs) and large language models (LLMs) to enhance the integration of new concepts into ontologies, a critical task in fields like bioinformatics and the Semantic Web. I have developed a systematic approach that includes edge search, formation, and selection, utilizing advanced techniques such as contrastive learning and fine-tuning with models like BERT and GPT.\n\nIn addition to my work on concept insertion, I have conducted comprehensive surveys on ontology embeddings and knowledge graph-aware learning, analyzing over 90 papers to provide insights into the challenges and future directions in these fields. My contributions also include the development of DeepOnto, a Python package that bridges deep learning and ontology engineering, facilitating tasks like ontology alignment and completion.\n\nI am particularly interested in the capabilities of LLMs for ontology alignment, where my findings suggest that with careful prompt design, these models can outperform traditional systems. My research aims to address the limitations of existing methods, particularly in low-resource settings, by integrating auxiliary information from knowledge graphs to improve model performance. Through my work, I strive to advance the understanding and application of language models in knowledge representation, ultimately contributing to more effective and intelligent information systems.",
    "collaborators": [
      "Jiaoyan Chen",
      "Ian Horrocks",
      "Hang Dong",
      "Yuxia Geng",
      "Zhuo Chen",
      "Jeff Z. Pan",
      "Wen Zhang",
      "Ernesto Jim\u00e9nez-Ruiz",
      "Alsayed Algergawy",
      "Patrice Buche",
      "L. J. Castro",
      "Omaima Fallatah",
      "Daniel Faria",
      "I. Fundulaki",
      "S. Hertling",
      "I. Horrocks",
      "Martin Huschka",
      "Liliana Ibanescu",
      "Naouel Karam",
      "Patrick Lambrix",
      "Huanyu Li",
      "Ying Li",
      "Engy Nasr",
      "Heiko Paulheim",
      "Catia Pesquita",
      "Tzanina Saveta",
      "P. Shvaiko",
      "C. Trojahn",
      "Mingfang Wu",
      "Beyza Yaman",
      "Lu Zhou",
      "Hua-zeng Chen",
      "Denvar Antonyrajah",
      "Yongsheng Gao",
      "Olga Mashkova",
      "Fernando Zhapa-Camacho",
      "R. Hoehndorf",
      "Carlo Allocca",
      "Taehun Kim",
      "B. Sapkota",
      "Yinan Liu",
      "Mina Abd",
      "Nikooie Pour",
      "Adrien Coulet",
      "J. Cufi",
      "Sarika Jain",
      "Ernesto Jim\u00b4enez-Ruiz",
      "Pierre Monnin",
      "Guilherme Sousa",
      "Jana Vatascinov\u00e1",
      "Ond\u0159ej Zamazal",
      "A. Hadian",
      "Mina Abd Nikooie Pour",
      "E. Jim\u00e9nez-Ruiz",
      "Amir Laadhar",
      "Franck Michel",
      "Chantelle Verhey",
      "Ondrej Sv\u00e1b-Zamazal",
      "Ryan Brate",
      "Minh-Hoang Dang",
      "Fabian Hoppe",
      "Albert Mero\u00f1o-Pe\u00f1uela",
      "Vijay Sadashivaiah",
      "Huajun Chen",
      "Shay B. Cohen"
    ],
    "pub_titles": [
      "A Language Model based Framework for New Concept Placement in Ontologies",
      "Ontology Embedding: A Survey of Methods, Applications and Resources",
      "Language Model Analysis for Ontology Subsumption Inference",
      "DeepOnto: A Python Package for Ontology Engineering with Deep Learning",
      "Exploring Large Language Models for Ontology Alignment",
      "Ontology Enrichment from Texts: A Biomedical Dataset for Concept Discovery and Placement",
      "Reveal the Unknown: Out-of-Knowledge-Base Mention Discovery with Entity Linking",
      "Results of the Ontology Alignment Evaluation Initiative 2023",
      "Machine Learning-Friendly Biomedical Datasets for Equivalence and Subsumption Ontology Matching",
      "Results of the Ontology Alignment Evaluation Initiative 2022",
      "Improving Language Model Predictions via Prompts Enriched with Knowledge Graphs",
      "Low-resource Learning with Knowledge Graphs: A Comprehensive Survey",
      "BERTMap: A BERT-based Ontology Alignment System",
      "Zero-Shot and Few-Shot Learning With Knowledge Graphs: A Comprehensive Survey",
      "Low-resource Learning with Knowledge Graphs: A Comprehensive Survey",
      "Biomedical ontology alignment with BERT",
      "English-to-Chinese Transliteration with Phonetic Auxiliary Task"
    ],
    "pub_abstracts": [
      "We investigate the task of inserting new concepts extracted from texts into an ontology using language models. We explore an approach with three steps: edge search which is to find a set of candidate locations to insert (i.e., subsumptions between concepts), edge formation and enrichment which leverages the ontological structure to produce and enhance the edge candidates, and edge selection which eventually locates the edge to be placed into. In all steps, we propose to leverage neural methods, where we apply embedding-based methods and contrastive learning with Pre-trained Language Models (PLMs) such as BERT for edge search, and adapt a BERT fine-tuning-based multi-label Edge-Cross-encoder, and Large Language Models (LLMs) such as GPT series, FLAN-T5, and Llama 2, for edge selection. We evaluate the methods on recent datasets created using the SNOMED CT ontology and the MedMentions entity linking benchmark. The best settings in our framework use fine-tuned PLM for search and a multi-label Cross-encoder for selection. Zero-shot prompting of LLMs is still not adequate for the task, and we propose explainable instruction tuning of LLMs for improved performance. Our study shows the advantages of PLMs and highlights the encouraging performance of LLMs that motivates future studies.",
      "Ontologies are widely used for representing domain knowledge and meta data, playing an increasingly important role in Information Systems, the Semantic Web, Bioinformatics and many other domains. However, logical reasoning that ontologies can directly support are quite limited in learning, approximation and prediction. One straightforward solution is to integrate statistical analysis and machine learning. To this end, automatically learning vector representation for knowledge of an ontology i.e., ontology embedding has been widely investigated in recent years. Numerous papers have been published on ontology embedding, but a lack of systematic reviews hinders researchers from gaining a comprehensive understanding of this field. To bridge this gap, we write this survey paper, which first introduces different kinds of semantics of ontologies, and formally defines ontology embedding from the perspectives of both mathematics and machine learning, as well as its property of faithfulness. Based on this, it systematically categorises and analyses a relatively complete set of over 80 papers, according to the ontologies and semantics that they aim at, and their technical solutions including geometric modeling, sequence modeling and graph propagation. This survey also introduces the applications of ontology embedding in ontology engineering, machine learning augmentation and life sciences, presents a new library mOWL, and discusses the challenges and future directions.",
      "Investigating whether pre-trained language models (LMs) can function as knowledge bases (KBs) has raised wide research interests recently. However, existing works focus on simple, triple-based, relational KBs, but omit more sophisticated, logic-based, conceptualised KBs such as OWL ontologies. To investigate an LM's knowledge of ontologies, we propose OntoLAMA, a set of inference-based probing tasks and datasets from ontology subsumption axioms involving both atomic and complex concepts. We conduct extensive experiments on ontologies of different domains and scales, and our results demonstrate that LMs encode relatively less background knowledge of Subsumption Inference (SI) than traditional Natural Language Inference (NLI) but can improve on SI significantly when a small number of samples are given. We will open-source our code and datasets.",
      "Integrating deep learning techniques, particularly language models (LMs), with knowledge representation techniques like ontologies has raised widespread attention, urging the need of a platform that supports both paradigms. Although packages such as OWL API and Jena offer robust support for basic ontology processing features, they lack the capability to transform various types of information within ontologies into formats suitable for downstream deep learning-based applications. Moreover, widely-used ontology APIs are primarily Java-based while deep learning frameworks like PyTorch and Tensorflow are mainly for Python programming. To address the needs, we present DeepOnto, a Python package designed for ontology engineering with deep learning. The package encompasses a core ontology processing module founded on the widely-recognised and reliable OWL API, encapsulating its fundamental features in a more \u201cPythonic\u201d manner and extending its capabilities to incorporate other essential components including reasoning, verbalisation, normalisation, taxonomy, projection, and more. Building on this module, DeepOnto offers a suite of tools, resources, and algorithms that support various ontology engineering tasks, such as ontology alignment and completion, by harnessing deep learning methods, primarily pre-trained LMs. In this paper, we also demonstrate the practical utility of DeepOnto through two use-cases: the Digital Health Coaching in Samsung Research UK and the Bio-ML track of the Ontology Alignment Evaluation Initiative (OAEI).",
      "This work investigates the applicability of recent generative Large Language Models (LLMs), such as the GPT series and Flan-T5, to ontology alignment for identifying concept equivalence mappings across ontologies. To test the zero-shot performance of Flan-T5-XXL and GPT-3.5-turbo, we leverage challenging subsets from two equivalence matching datasets of the OAEI Bio-ML track, taking into account concept labels and structural contexts. Preliminary findings suggest that LLMs have the potential to outperform existing ontology alignment systems like BERTMap, given careful framework and prompt design.",
      "Mentions of new concepts appear regularly in texts and require automated approaches to harvest and place them into Knowledge Bases (KB), e.g., ontologies and taxonomies. Existing datasets suffer from three issues, (i) mostly assuming that a new concept is pre-discovered and cannot support out-of-KB mention discovery; (ii) only using the concept label as the input along with the KB and thus lacking the contexts of a concept label; and (iii) mostly focusing on concept placement w.r.t a taxonomy of atomic concepts, instead of complex concepts, i.e., with logical operators. To address these issues, we propose a new benchmark, adapting MedMentions dataset (PubMed abstracts) with SNOMED CT versions in 2014 and 2017 under the Diseases sub-category and the broader categories of Clinical finding, Procedure, and Pharmaceutical / biologic product. We provide usage on the evaluation with the dataset for out-of-KB mention discovery and concept placement, adapting recent Large Language Model based methods.",
      "Discovering entity mentions that are out of a Knowledge Base (KB) from texts plays a critical role in KB maintenance, but has not yet been fully explored. The current methods are mostly limited to the simple threshold-based approach and feature-based classification, and the datasets for evaluation are relatively rare. We propose BLINKout, a new BERT-based Entity Linking (EL) method which can identify mentions that do not have corresponding KB entities by matching them to a special NIL entity. To better utilize BERT, we propose new techniques including NIL entity representation and classification, with synonym enhancement. We also apply KB Pruning and Versioning strategies to automatically construct out-of-KB datasets from common in-KB EL datasets. Results on five datasets of clinical notes, biomedical publications, and Wikipedia articles in various domains show the advantages of BLINKout over existing methods to identify out-of-KB mentions for the medical ontologies, UMLS, SNOMED CT, and the general KB, WikiData.",
      "The Ontology Alignment Evaluation Initiative (OAEI) aims at comparing ontology matching systems on precisely defined test cases. These test cases can be based on ontologies of different levels of complexity and use different evaluation modalities. The OAEI 2023 campaign offered 15 tracks and was attended by 16 participants. This paper is an overall presentation of that campaign.",
      "Ontology Matching (OM) plays an important role in many domains such as bioinformatics and the Semantic Web, and its research is becoming increasingly popular, especially with the application of machine learning (ML) techniques. Although the Ontology Alignment Evaluation Initiative (OAEI) represents an impressive effort for the systematic evaluation of OM systems, it still suffers from several limitations including limited evaluation of subsumption mappings, suboptimal reference mappings, and limited support for the evaluation of ML-based systems. To tackle these limitations, we introduce five new biomedical OM tasks involving ontologies extracted from Mondo and UMLS. Each task includes both equivalence and subsumption matching; the quality of reference mappings is ensured by human curation, ontology pruning, etc.; and a comprehensive evaluation framework is proposed to measure OM performance from various perspectives for both ML-based and non-ML-based OM systems. We report evaluation results for OM systems of different types to demonstrate the usage of these resources, all of which are publicly available as part of the new BioML track at OAEI 2022.",
      "The Ontology Alignment Evaluation Initiative (OAEI) aims at comparing ontology matching systems on precisely defined test cases. These test cases can be based on ontologies of different levels of complexity and use different evaluation modalities. The OAEI 2022 campaign offered 14 tracks and was attended by 18 participants. This paper is an overall presentation of that campaign.",
      "Despite advances in deep learning and knowledge graphs (KGs), using language models for natural language understanding and question answering remains a challenging task. Pre-trained language models (PLMs) have shown to be able to leverage contextual information, to complete cloze prompts, next sentence completion and question answering tasks in various domains. Unlike structured data querying in e.g. KGs, mapping an input question to data that may or may not be stored by the language model is not a simple task. Recent studies have highlighted the improvements that can be made to the quality of information retrieved from PLMs by adding auxiliary data to otherwise naive prompts. In this paper, we explore the effects of enriching prompts with additional contextual information leveraged from the Wikidata KG on language model performance. Specifically, we compare the performance of naive vs. KG-engineered cloze prompts for entity genre classification in the movie domain. Selecting a broad range of commonly available Wikidata properties, we show that enrichment of cloze-style prompts with Wikidata information can result in a significantly higher recall for the investigated BERT and RoBERTa large PLMs. However, it is also apparent that the optimum level of data enrichment differs between models.",
      "\u2014Machine learning methods especially deep neural networks have achieved great success but many of them often rely on a number of labeled samples for training. In real-world applications, we often need to address sample shortage due to e.g., dynamic contexts with emerging prediction targets and costly sample annotation. Therefore, low-resource learning, which aims to learn robust prediction models with limited resources (especially training samples), is now being widely investigated. Among all the low-resource learning studies, many prefer to utilize some auxiliary information in the form of Knowledge Graph (KG), which is becoming more and more popular for knowledge representation, to reduce the reliance on labeled samples. In this survey, we very comprehensively reviewed over 90 papers about KG-aware research for two major low-resource learning settings \u2014 zero-shot learning (ZSL) where new classes for prediction have never appeared in training, and few-shot learning (FSL) where new classes for prediction have only a small number of labeled samples that are available. We \ufb01rst introduced the KGs used in ZSL and FSL studies as well as the existing and potential KG construction solutions, and then systematically categorized and summarized KG-aware ZSL and FSL methods, dividing them into different paradigms such as the mapping-based, the data augmentation, the propagation- based and the optimization-based. We next presented different applications, including not only KG augmented prediction tasks in Computer Vision and Natural Language Processing (e.g., image classi\ufb01cation, visual question answering, text classi\ufb01cation and knowledge extraction), but also tasks for KG curation (e.g., inductive KG completion), and some typical evaluation resources for each task. We eventually discussed some challenges and future directions on aspects such as new learning and reasoning paradigms, and the construction of high quality KGs.",
      "Ontology alignment (a.k.a ontology matching (OM)) plays a critical role in knowledge integration. Owing to the success of machine learning in many domains, it has been applied in OM. However, the existing methods, which often adopt ad-hoc feature engineering or non-contextual word embeddings, have not yet outperformed rule-based systems especially in an unsupervised setting. In this paper, we propose a novel OM system named BERTMap which can support both unsupervised and semi-supervised settings. It first predicts mappings using a classifier based on fine-tuning the contextual embedding model BERT on text semantics corpora extracted from ontologies, and then refines the mappings through extension and repair by utilizing the ontology structure and logic. Our evaluation with three alignment tasks on biomedical ontologies demonstrates that BERTMap can often perform better than the leading OM systems LogMap and AML.",
      "Machine learning (ML), especially deep neural networks, has achieved great success, but many of them often rely on a number of labeled samples for supervision. As sufficient labeled training data are not always ready due to, e.g., continuously emerging prediction targets and costly sample annotation in real-world applications, ML with sample shortage is now being widely investigated. Among all these studies, many prefer to utilize auxiliary information including those in the form of knowledge graph (KG) to reduce the reliance on labeled samples. In this survey, we have comprehensively reviewed over 90 articles about KG-aware research for two major sample shortage settings\u2014zero-shot learning (ZSL) where some classes to be predicted have no labeled samples and few-shot learning (FSL) where some classes to be predicted have only a small number of labeled samples that are available. We first introduce KGs used in ZSL and FSL as well as their construction methods and then systematically categorize and summarize KG-aware ZSL and FSL methods, dividing them into different paradigms, such as the mapping-based, the data augmentation, the propagation-based, and the optimization-based. We next present different applications, including not only KG augmented prediction tasks such as image classification, question answering, text classification, and knowledge extraction but also KG completion tasks and some typical evaluation resources for each task. We eventually discuss some challenges and open problems from different perspectives.",
      "Machine learning methods especially deep neural networks have achieved great success but many of them often rely on a number of labeled samples for training. In real-world applications, we often need to address sample shortage due to e.g., dynamic contexts with emerging prediction targets and costly sample annotation. Therefore, low-resource learning, which aims to learn robust prediction models with no enough resources (especially training samples), is now being widely investigated. Among all the low-resource learning studies, many prefer to utilize some auxiliary information in the form of Knowledge Graph (KG), which is becoming more and more popular for knowledge representation, to reduce the reliance on labeled samples. In this survey, we very comprehensively reviewed over 90 papers about KG-aware research for two major low-resource learning settings \u2014 zero-shot learning (ZSL) where new classes for prediction have never appeared in training, and few-shot learning (FSL) where new classes for prediction have only a small number of labeled samples that are available. We first introduced the KGs used in ZSL and FSL studies as well as the existing and potential KG construction solutions, and then systematically categorized and summarized KG-aware ZSL and FSL methods, dividing them into different paradigms such as the mapping-based, the data augmentation, the propagation-based and the optimization-based. We next presented different applications, including not only KG augmented prediction tasks in Computer Vision and Natural Language Processing (e.g., image classification, visual question answering, text classification and knowledge extraction), but also tasks for KG curation (e.g., inductive KG completion), and some typical evaluation resources for each task. We eventually discussed some challenges and future directions on aspects such as new learning and reasoning paradigms",
      ". Existing machine learning-based ontology alignment systems often adopt complicated feature engineering or traditional non-contextual word embeddings. However, they are often outrun by the rule-based systems despite the model complexity. This paper proposes a novel ontology alignment system based on a contextual embedding model named BERT, aiming to su\ufb03ciently utilize the text semantics implied by ontologies. Our results on two biomedical alignment tasks demonstrate that, despite using the to-be-aligned classes alone as the input, our system outperforms the leading systems",
      "Approaching named entities transliteration as a Neural Machine Translation (NMT) problem is common practice. While many have applied various NMT techniques to enhance machine transliteration models, few focus on the linguistic features particular to the relevant languages. In this paper, we investigate the effect of incorporating phonetic features for English-to-Chinese transliteration under the multi-task learning (MTL) setting\u2014where we define a phonetic auxiliary task aimed to improve the generalization performance of the main transliteration task. In addition to our system, we also release a new English-to-Chinese dataset and propose a novel evaluation metric which considers multiple possible transliterations given a source name. Our results show that the multi-task model achieves similar performance as the previous state of the art with a model of a much smaller size."
    ],
    "domain": [
      "Ontology",
      "Knowledge Graph",
      "Natural Language Processing",
      "Machine Learning"
    ],
    "is_leader_candidate": true,
    "is_member_candidate": true,
    "is_reviewer_candidate": true,
    "is_chair_candidate": true
  },
  "0c9f84ec-b56e-47b0-97e2-7565feeda4f4": {
    "pk": "0c9f84ec-b56e-47b0-97e2-7565feeda4f4",
    "name": "Zhangdie Yuan",
    "bio": "I am a researcher dedicated to advancing multilingual task-oriented dialogue (ToD) systems and enhancing the capabilities of language models. My recent work includes the development of DIALIGHT, a comprehensive toolkit that facilitates the evaluation and comparison of ToD systems, leveraging both fine-tuning of pretrained language models (PLMs) and the innovative zero-shot and in-context learning capabilities of large language models (LLMs). Through this toolkit, I aim to lower entry barriers in the field and provide valuable resources for researchers.\n\nIn my exploration of language models, I introduced PRobELM, a benchmark designed to assess the ability of models to discern plausible scenarios, bridging the gap between factual accuracy and plausibility. My empirical analyses reveal significant performance disparities in multilingual ToD systems, driven by factors such as task nature, model architecture, and data availability. I also proposed a novel zero-shot method for automated fact-checking that leverages semantic triples and external knowledge graphs, demonstrating its effectiveness across various datasets.\n\nMy research emphasizes the importance of understanding the limitations of PLMs, particularly in their reasoning capabilities. I have conducted controlled experiments that highlight the inadequacies of PLMs in generalizing learned logic rules and the challenges they face in retaining knowledge during fine-tuning. Overall, my work seeks to push the boundaries of multilingual NLP and improve the robustness of language technologies across diverse languages and tasks.",
    "collaborators": [
      "Songbo Hu",
      "Andreas Vlachos",
      "Anna Korhonen",
      "Ivan Vuli'c",
      "Xiaobin Wang",
      "Chenxi Whitehouse",
      "Eric Chamoun",
      "Rami Aly",
      "Han Zhou",
      "Milan Gritta",
      "Guchun Zhang",
      "Ignacio Iacobacci",
      "Ivan Vulic",
      "A. Korhonen",
      "Zaiqiao Meng",
      "N. Ousidhoum"
    ],
    "pub_titles": [
      "DIALIGHT: Lightweight Multilingual Development and Evaluation of Task-Oriented Dialogue Systems with Large Language Models",
      "PRobELM: Plausibility Ranking Evaluation for Language Models",
      "A Systematic Study of Performance Disparities in Multilingual Task-Oriented Dialogue Systems",
      "Zero-Shot Fact-Checking with Semantic Triples and Knowledge Graphs",
      "Can Pretrained Language Models (Yet) Reason Deductively?",
      "Varifocal Question Generation for Fact-checking"
    ],
    "pub_abstracts": [
      "We present DIALIGHT, a toolkit for developing and evaluating multilingual Task-Oriented Dialogue (ToD) systems which facilitates systematic evaluations and comparisons between ToD systems using fine-tuning of Pretrained Language Models (PLMs) and those utilising the zero-shot and in-context learning capabilities of Large Language Models (LLMs). In addition to automatic evaluation, this toolkit features (i) a secure, user-friendly web interface for fine-grained human evaluation at both local utterance level and global dialogue level, and (ii) a microservice-based backend, improving efficiency and scalability. Our evaluations reveal that while PLM fine-tuning leads to higher accuracy and coherence, LLM-based systems excel in producing diverse and likeable responses. However, we also identify significant challenges of LLMs in adherence to task-specific instructions and generating outputs in multiple languages, highlighting areas for future research. We hope this open-sourced toolkit will serve as a valuable resource for researchers aiming to develop and properly evaluate multilingual ToD systems and will lower, currently still high, entry barriers in the field.",
      "This paper introduces PRobELM (Plausibility Ranking Evaluation for Language Models), a benchmark designed to assess language models' ability to discern more plausible from less plausible scenarios through their parametric knowledge. While benchmarks such as TruthfulQA emphasise factual accuracy or truthfulness, and others such as COPA explore plausible scenarios without explicitly incorporating world knowledge, PRobELM seeks to bridge this gap by evaluating models' capabilities to prioritise plausible scenarios that leverage world knowledge over less plausible alternatives. This design allows us to assess the potential of language models for downstream use cases such as literature-based discovery where the focus is on identifying information that is likely but not yet known. Our benchmark is constructed from a dataset curated from Wikidata edit histories, tailored to align the temporal bounds of the training data for the evaluated models. PRobELM facilitates the evaluation of language models across multiple prompting types, including statement, text completion, and question-answering. Experiments with 10 models of various sizes and architectures on the relationship between model scales, training recency, and plausibility performance, reveal that factual accuracy does not directly correlate with plausibility performance and that up-to-date training data enhances plausibility assessment across different model architectures.",
      "Achieving robust language technologies that can perform well across the world's many languages is a central goal of multilingual NLP. In this work, we take stock of and empirically analyse task performance disparities that exist between multilingual task-oriented dialogue (ToD) systems. We first define new quantitative measures of absolute and relative equivalence in system performance, capturing disparities across languages and within individual languages. Through a series of controlled experiments, we demonstrate that performance disparities depend on a number of factors: the nature of the ToD task at hand, the underlying pretrained language model, the target language, and the amount of ToD annotated data. We empirically prove the existence of the adaptation and intrinsic biases in current ToD systems: e.g., ToD systems trained for Arabic or Turkish using annotated ToD data fully parallel to English ToD data still exhibit diminished ToD task performance. Beyond providing a series of insights into the performance disparities of ToD systems in different languages, our analyses offer practical tips on how to approach ToD data collection and system development for new languages.",
      "Despite progress in automated fact-checking, most systems require a significant amount of labeled training data, which is expensive. In this paper, we propose a novel zero-shot method, which instead of operating directly on the claim and evidence sentences, decomposes them into semantic triples augmented using external knowledge graphs, and uses large language models trained for natural language inference. This allows it to generalize to adversarial datasets and domains that supervised models require specific training data for. Our empirical results show that our approach outperforms previous zero-shot approaches on FEVER, FEVER-Symmetric, FEVER 2.0, and Climate-FEVER, while being comparable or better than supervised models on the adversarial and the out-of-domain datasets.",
      "Acquiring factual knowledge with Pretrained Language Models (PLMs) has attracted increasing attention, showing promising performance in many knowledge-intensive tasks. Their good performance has led the community to believe that the models do possess a modicum of reasoning competence rather than merely memorising the knowledge. In this paper, we conduct a comprehensive evaluation of the learnable deductive (also known as explicit) reasoning capability of PLMs. Through a series of controlled experiments, we posit two main findings. 1) PLMs inadequately generalise learned logic rules and perform inconsistently against simple adversarial surface form edits. 2) While the deductive reasoning fine-tuning of PLMs does improve their performance on reasoning over unseen knowledge facts, it results in catastrophically forgetting the previously learnt knowledge. Our main results suggest that PLMs cannot yet perform reliable deductive reasoning, demonstrating the importance of controlled examinations and probing of PLMs\u2019 deductive reasoning abilities; we reach beyond (misleading) task performance, revealing that PLMs are still far from robust reasoning capabilities, even for simple deductive tasks.",
      "Fact-checking requires retrieving evidence related to a claim under investigation. The task can be formulated as question generation based on a claim, followed by question answering.However, recent question generation approaches assume that the answer is known and typically contained in a passage given as input,whereas such passages are what is being sought when verifying a claim.In this paper, we present Varifocal, a method that generates questions based on different focal points within a given claim, i.e. different spans of the claim and its metadata, such as its source and date.Our method outperforms previous work on a fact-checking question generation dataset on a wide range of automatic evaluation metrics.These results are corroborated by our manual evaluation, which indicates that our method generates more relevant and informative questions.We further demonstrate the potential of focal points in generating sets of clarification questions for product descriptions."
    ],
    "domain": [
      "Natural Language Processing",
      "Dialogue Systems",
      "Fact-Checking",
      "Multilingual NLP"
    ],
    "is_leader_candidate": true,
    "is_member_candidate": true,
    "is_reviewer_candidate": true,
    "is_chair_candidate": true
  },
  "ed7ba1e2-09cf-4b35-9301-dcb91d7d2c2a": {
    "pk": "ed7ba1e2-09cf-4b35-9301-dcb91d7d2c2a",
    "name": "Jiaoyan Chen",
    "bio": "I am a researcher deeply engaged in the intersection of knowledge graphs (KGs), ontology representation, and machine learning. My recent work has focused on enhancing the robustness of knowledge graph embedding (KGE) methods against adversarial attacks, particularly through the development of rule-based strategies that effectively assess and mitigate vulnerabilities in KGE systems. I have also explored innovative approaches for integrating language models into ontology management, specifically in inserting new concepts into ontologies, leveraging advanced techniques like contrastive learning and fine-tuning with pre-trained language models.\n\nMy contributions extend to systematically reviewing the field of ontology embedding, where I have categorized over 80 papers to provide a comprehensive understanding of the various methodologies and their applications. I have developed Box 2 EL, a novel method for representing ontologies that preserves their logical structure while achieving state-of-the-art results in various predictive tasks.\n\nAdditionally, I have investigated the application of generative large language models for ontology alignment, demonstrating their potential to outperform traditional systems. My work also encompasses a broader vision for the role of graph-based technologies in life sciences, emphasizing their importance in knowledge discovery and artificial intelligence applications.\n\nThrough my research, I aim to bridge the gap between theoretical advancements and practical applications, ultimately contributing to the development of more robust and efficient systems for managing and interpreting complex relational data.",
    "collaborators": [
      "Yuan He",
      "Hang Dong",
      "Ian Horrocks",
      "Catia Pesquita",
      "Yuxia Geng",
      "Pierre Monnin",
      "Alsayed Algergawy",
      "Patrice Buche",
      "L. J. Castro",
      "Omaima Fallatah",
      "Daniel Faria",
      "I. Fundulaki",
      "S. Hertling",
      "I. Horrocks",
      "Martin Huschka",
      "Liliana Ibanescu",
      "Naouel Karam",
      "Patrick Lambrix",
      "Huanyu Li",
      "Ying Li",
      "Engy Nasr",
      "Heiko Paulheim",
      "Tzanina Saveta",
      "P. Shvaiko",
      "C. Trojahn",
      "Mingfang Wu",
      "Beyza Yaman",
      "Lu Zhou",
      "Tianzhe Zhao",
      "Yanchi Ru",
      "Qika Lin",
      "Jun Liu",
      "Yongsheng Gao",
      "Olga Mashkova",
      "Fernando Zhapa-Camacho",
      "R. Hoehndorf",
      "Mathias Jackermeier",
      "Janna Hastings",
      "Ernesto Jim\u00e9nez-Ruiz",
      "Vanessa Lopez",
      "Petr vSkoda",
      "Valentina A. M. Tamma",
      "Mina Abd",
      "Nikooie Pour",
      "Adrien Coulet",
      "J. Cufi",
      "Sarika Jain",
      "Ernesto Jim\u00b4enez-Ruiz",
      "Guilherme Sousa",
      "Jana Vatascinov\u00e1",
      "Ond\u0159ej Zamazal",
      "Mina Abd Nikooie Pour",
      "E. Jim\u00e9nez-Ruiz",
      "Amir Laadhar",
      "Franck Michel",
      "Chantelle Verhey",
      "Ondrej Sv\u00e1b-Zamazal",
      "Zhuo Chen",
      "Jeff Z. Pan",
      "Wen Zhang",
      "Hua-zeng Chen"
    ],
    "pub_titles": [
      "Untargeted Adversarial Attack on Knowledge Graph Embeddings",
      "A Language Model based Framework for New Concept Placement in Ontologies",
      "Ontology Embedding: A Survey of Methods, Applications and Resources",
      "Box2EL: Concept and Role Box Embeddings for the Description Logic EL++",
      "Knowledge Graphs for the Life Sciences: Recent Developments, Challenges and Opportunities",
      "Exploring Large Language Models for Ontology Alignment",
      "Results of the Ontology Alignment Evaluation Initiative 2023",
      "Results of the Ontology Alignment Evaluation Initiative 2022",
      "Low-resource Learning with Knowledge Graphs: A Comprehensive Survey"
    ],
    "pub_abstracts": [
      "Knowledge graph embedding (KGE) methods have achieved great success in handling various knowledge graph (KG) downstream tasks. However, KGE methods may learn biased representations on low-quality KGs that are prevalent in the real world. Some recent studies propose adversarial attacks to investigate the vulnerabilities of KGE methods, but their attackers are target-oriented with the KGE method and the target triples to predict are given in advance, which lacks practicability. In this work, we explore untargeted attacks with the aim of reducing the global performances of KGE methods over a set of unknown test triples and conducting systematic analyses on KGE robustness. Considering logic rules can effectively summarize the global structure of a KG, we develop rule-based attack strategies to enhance the attack efficiency. In particular,we consider adversarial deletion which learns rules, applying the rules to score triple importance and delete important triples, and adversarial addition which corrupts the learned rules and applies them for negative triples as perturbations. Extensive experiments on two datasets over three representative classes of KGE methods demonstrate the effectiveness of our proposed untargeted attacks in diminishing the link prediction results. And we also find that different KGE methods exhibit different robustness to untargeted attacks. For example, the robustness of methods engaged with graph neural networks and logic rules depends on the density of the graph. But rule-based methods like NCRL are easily affected by adversarial addition attacks to capture negative rules",
      "We investigate the task of inserting new concepts extracted from texts into an ontology using language models. We explore an approach with three steps: edge search which is to find a set of candidate locations to insert (i.e., subsumptions between concepts), edge formation and enrichment which leverages the ontological structure to produce and enhance the edge candidates, and edge selection which eventually locates the edge to be placed into. In all steps, we propose to leverage neural methods, where we apply embedding-based methods and contrastive learning with Pre-trained Language Models (PLMs) such as BERT for edge search, and adapt a BERT fine-tuning-based multi-label Edge-Cross-encoder, and Large Language Models (LLMs) such as GPT series, FLAN-T5, and Llama 2, for edge selection. We evaluate the methods on recent datasets created using the SNOMED CT ontology and the MedMentions entity linking benchmark. The best settings in our framework use fine-tuned PLM for search and a multi-label Cross-encoder for selection. Zero-shot prompting of LLMs is still not adequate for the task, and we propose explainable instruction tuning of LLMs for improved performance. Our study shows the advantages of PLMs and highlights the encouraging performance of LLMs that motivates future studies.",
      "Ontologies are widely used for representing domain knowledge and meta data, playing an increasingly important role in Information Systems, the Semantic Web, Bioinformatics and many other domains. However, logical reasoning that ontologies can directly support are quite limited in learning, approximation and prediction. One straightforward solution is to integrate statistical analysis and machine learning. To this end, automatically learning vector representation for knowledge of an ontology i.e., ontology embedding has been widely investigated in recent years. Numerous papers have been published on ontology embedding, but a lack of systematic reviews hinders researchers from gaining a comprehensive understanding of this field. To bridge this gap, we write this survey paper, which first introduces different kinds of semantics of ontologies, and formally defines ontology embedding from the perspectives of both mathematics and machine learning, as well as its property of faithfulness. Based on this, it systematically categorises and analyses a relatively complete set of over 80 papers, according to the ontologies and semantics that they aim at, and their technical solutions including geometric modeling, sequence modeling and graph propagation. This survey also introduces the applications of ontology embedding in ontology engineering, machine learning augmentation and life sciences, presents a new library mOWL, and discusses the challenges and future directions.",
      "Representation learning in the form of semantic embeddings has been successfully applied to a variety of tasks in natural language processing and knowledge graphs. Recently, there has been growing interest in developing similar methods for learning embeddings of entire ontologies. We propose Box 2 EL , a novel method for representation learning of ontologies in the Description Logic EL ++ , which represents both concepts and roles as boxes (i.e. axis-aligned hyperrectangles), such that the logical structure of the ontology is preserved. We theoretically prove the soundness of our model and conduct an extensive empirical evaluation, in which we achieve state-of-the-art results in subsumption prediction, link prediction, and deductive reasoning. As part of our evaluation, we introduce a novel benchmark for evaluating EL ++ embedding models on predicting subsumptions involving both atomic and complex concepts.",
      "The term life sciences refers to the disciplines that study living organisms and life processes, and include chemistry, biology, medicine, and a range of other related disciplines. Research efforts in life sciences are heavily data-driven, as they produce and consume vast amounts of scientific data, much of which is intrinsically relational and graph-structured. The volume of data and the complexity of scientific concepts and relations referred to therein promote the application of advanced knowledge-driven technologies for managing and interpreting data, with the ultimate aim to advance scientific discovery. In this survey and position paper, we discuss recent developments and advances in the use of graph-based technologies in life sciences and set out a vision for how these technologies will impact these fields into the future. We focus on three broad topics: the construction and management of Knowledge Graphs (KGs), the use of KGs and associated technologies in the discovery of new knowledge, and the use of KGs in artificial intelligence applications to support explanations (explainable AI). We select a few exemplary use cases for each topic, discuss the challenges and open research questions within these topics, and conclude with a perspective and outlook that summarizes the overarching challenges and their potential solutions as a guide for future research.",
      "This work investigates the applicability of recent generative Large Language Models (LLMs), such as the GPT series and Flan-T5, to ontology alignment for identifying concept equivalence mappings across ontologies. To test the zero-shot performance of Flan-T5-XXL and GPT-3.5-turbo, we leverage challenging subsets from two equivalence matching datasets of the OAEI Bio-ML track, taking into account concept labels and structural contexts. Preliminary findings suggest that LLMs have the potential to outperform existing ontology alignment systems like BERTMap, given careful framework and prompt design.",
      "The Ontology Alignment Evaluation Initiative (OAEI) aims at comparing ontology matching systems on precisely defined test cases. These test cases can be based on ontologies of different levels of complexity and use different evaluation modalities. The OAEI 2023 campaign offered 15 tracks and was attended by 16 participants. This paper is an overall presentation of that campaign.",
      "The Ontology Alignment Evaluation Initiative (OAEI) aims at comparing ontology matching systems on precisely defined test cases. These test cases can be based on ontologies of different levels of complexity and use different evaluation modalities. The OAEI 2022 campaign offered 14 tracks and was attended by 18 participants. This paper is an overall presentation of that campaign.",
      "\u2014Machine learning methods especially deep neural networks have achieved great success but many of them often rely on a number of labeled samples for training. In real-world applications, we often need to address sample shortage due to e.g., dynamic contexts with emerging prediction targets and costly sample annotation. Therefore, low-resource learning, which aims to learn robust prediction models with limited resources (especially training samples), is now being widely investigated. Among all the low-resource learning studies, many prefer to utilize some auxiliary information in the form of Knowledge Graph (KG), which is becoming more and more popular for knowledge representation, to reduce the reliance on labeled samples. In this survey, we very comprehensively reviewed over 90 papers about KG-aware research for two major low-resource learning settings \u2014 zero-shot learning (ZSL) where new classes for prediction have never appeared in training, and few-shot learning (FSL) where new classes for prediction have only a small number of labeled samples that are available. We \ufb01rst introduced the KGs used in ZSL and FSL studies as well as the existing and potential KG construction solutions, and then systematically categorized and summarized KG-aware ZSL and FSL methods, dividing them into different paradigms such as the mapping-based, the data augmentation, the propagation- based and the optimization-based. We next presented different applications, including not only KG augmented prediction tasks in Computer Vision and Natural Language Processing (e.g., image classi\ufb01cation, visual question answering, text classi\ufb01cation and knowledge extraction), but also tasks for KG curation (e.g., inductive KG completion), and some typical evaluation resources for each task. We eventually discussed some challenges and future directions on aspects such as new learning and reasoning paradigms, and the construction of high quality KGs."
    ],
    "domain": [
      "Knowledge Graph",
      "Ontology",
      "Machine Learning",
      "Natural Language Processing"
    ],
    "is_leader_candidate": true,
    "is_member_candidate": true,
    "is_reviewer_candidate": true,
    "is_chair_candidate": true
  },
  "3596417a-999c-4fd5-9612-8c0cdf8db5a7": {
    "pk": "3596417a-999c-4fd5-9612-8c0cdf8db5a7",
    "name": "Ian Horrocks",
    "bio": "I am a researcher dedicated to enhancing the fields of ontology engineering, knowledge representation, and low-resource learning through innovative methodologies. My recent work has focused on enriching taxonomies and ontologies by leveraging existing structures to identify and insert implicit concepts, exemplified by my development of the ICON system. This modular approach integrates techniques from entity retrieval and text summarization, achieving state-of-the-art performance in taxonomy completion.\n\nI have also explored the integration of language models in ontology management, proposing a three-step method for inserting new concepts into ontologies that utilizes advanced neural techniques. My systematic review of ontology embedding has provided a comprehensive understanding of the field, categorizing over 80 papers and introducing the mOWL library to facilitate future research.\n\nIn addition, I have developed Box2EL, a novel representation learning method for ontologies that preserves logical structures while achieving impressive results in subsumption and link prediction tasks. My work on optimizing materialization for Datalog reasoning has led to significant improvements in memory efficiency, demonstrating my commitment to practical solutions in complex reasoning scenarios.\n\nFurthermore, I have conducted an extensive survey on knowledge graph-aware low-resource learning, analyzing over 90 studies to highlight the potential of knowledge graphs in zero-shot and few-shot learning settings. My research aims to bridge the gap between theoretical advancements and real-world applications, driving innovation in knowledge representation and machine learning.",
    "collaborators": [
      "Jiaoyan Chen",
      "Yuan He",
      "Hang Dong",
      "Jingchuan Shi",
      "Zhe Wu",
      "Yongsheng Gao",
      "Olga Mashkova",
      "Fernando Zhapa-Camacho",
      "R. Hoehndorf",
      "Mathias Jackermeier",
      "Xinyue Zhang",
      "Pan Hu",
      "Yavor Nenov",
      "Yuxia Geng",
      "Zhuo Chen",
      "Jeff Z. Pan",
      "Wen Zhang",
      "Hua-zeng Chen"
    ],
    "pub_titles": [
      "Taxonomy Completion via Implicit Concept Insertion",
      "A Language Model based Framework for New Concept Placement in Ontologies",
      "Ontology Embedding: A Survey of Methods, Applications and Resources",
      "Box2EL: Concept and Role Box Embeddings for the Description Logic EL++",
      "Optimised Storage for Datalog Reasoning",
      "Low-resource Learning with Knowledge Graphs: A Comprehensive Survey"
    ],
    "pub_abstracts": [
      "\\beginabstract High quality taxonomies play a critical role in various domains such as e-commerce, web search and ontology engineering. While there has been extensive work on expanding taxonomies from externally mined data, there has been less attention paid to enriching taxonomies by exploiting existing concepts and structure within the taxonomy. In this work, we show the usefulness of this kind of enrichment, and explore its viability with a new taxonomy completion system ICON (I mplicit CON cept Insertion). ICON generates new concepts by identifying implicit concepts based on the existing concept structure, generating names for such concepts and inserting them in appropriate positions within the taxonomy. ICON integrates techniques from entity retrieval, text summary, and subsumption prediction; this modular architecture offers high flexibility while achieving state-of-the-art performance. We have evaluated ICON on two e-commerce taxonomies, and the results show that it offers significant advantages over strong baselines including recent taxonomy completion models and the large language model, ChatGPT.",
      "We investigate the task of inserting new concepts extracted from texts into an ontology using language models. We explore an approach with three steps: edge search which is to find a set of candidate locations to insert (i.e., subsumptions between concepts), edge formation and enrichment which leverages the ontological structure to produce and enhance the edge candidates, and edge selection which eventually locates the edge to be placed into. In all steps, we propose to leverage neural methods, where we apply embedding-based methods and contrastive learning with Pre-trained Language Models (PLMs) such as BERT for edge search, and adapt a BERT fine-tuning-based multi-label Edge-Cross-encoder, and Large Language Models (LLMs) such as GPT series, FLAN-T5, and Llama 2, for edge selection. We evaluate the methods on recent datasets created using the SNOMED CT ontology and the MedMentions entity linking benchmark. The best settings in our framework use fine-tuned PLM for search and a multi-label Cross-encoder for selection. Zero-shot prompting of LLMs is still not adequate for the task, and we propose explainable instruction tuning of LLMs for improved performance. Our study shows the advantages of PLMs and highlights the encouraging performance of LLMs that motivates future studies.",
      "Ontologies are widely used for representing domain knowledge and meta data, playing an increasingly important role in Information Systems, the Semantic Web, Bioinformatics and many other domains. However, logical reasoning that ontologies can directly support are quite limited in learning, approximation and prediction. One straightforward solution is to integrate statistical analysis and machine learning. To this end, automatically learning vector representation for knowledge of an ontology i.e., ontology embedding has been widely investigated in recent years. Numerous papers have been published on ontology embedding, but a lack of systematic reviews hinders researchers from gaining a comprehensive understanding of this field. To bridge this gap, we write this survey paper, which first introduces different kinds of semantics of ontologies, and formally defines ontology embedding from the perspectives of both mathematics and machine learning, as well as its property of faithfulness. Based on this, it systematically categorises and analyses a relatively complete set of over 80 papers, according to the ontologies and semantics that they aim at, and their technical solutions including geometric modeling, sequence modeling and graph propagation. This survey also introduces the applications of ontology embedding in ontology engineering, machine learning augmentation and life sciences, presents a new library mOWL, and discusses the challenges and future directions.",
      "Representation learning in the form of semantic embeddings has been successfully applied to a variety of tasks in natural language processing and knowledge graphs. Recently, there has been growing interest in developing similar methods for learning embeddings of entire ontologies. We propose Box 2 EL , a novel method for representation learning of ontologies in the Description Logic EL ++ , which represents both concepts and roles as boxes (i.e. axis-aligned hyperrectangles), such that the logical structure of the ontology is preserved. We theoretically prove the soundness of our model and conduct an extensive empirical evaluation, in which we achieve state-of-the-art results in subsumption prediction, link prediction, and deductive reasoning. As part of our evaluation, we introduce a novel benchmark for evaluating EL ++ embedding models on predicting subsumptions involving both atomic and complex concepts.",
      "Materialisation facilitates Datalog reasoning by precomputing all consequences of the facts and the rules so that queries can be directly answered over the materialised facts. However, storing all materialised facts may be infeasible in practice, especially when the rules are complex and the given set of facts is large. We observe that for certain combinations of rules, there exist data structures that compactly represent the reasoning result and can be efficiently queried when necessary. In this paper, we present a general framework that allows for the integration of such optimised storage schemes with standard materialisation algorithms. Moreover, we devise optimised storage schemes targeting at transitive rules and union rules, two types of (combination of) rules that commonly occur in practice. Our experimental evaluation shows that our approach significantly improves memory consumption, sometimes by orders of magnitude, while remaining competitive in terms of query answering time.",
      "\u2014Machine learning methods especially deep neural networks have achieved great success but many of them often rely on a number of labeled samples for training. In real-world applications, we often need to address sample shortage due to e.g., dynamic contexts with emerging prediction targets and costly sample annotation. Therefore, low-resource learning, which aims to learn robust prediction models with limited resources (especially training samples), is now being widely investigated. Among all the low-resource learning studies, many prefer to utilize some auxiliary information in the form of Knowledge Graph (KG), which is becoming more and more popular for knowledge representation, to reduce the reliance on labeled samples. In this survey, we very comprehensively reviewed over 90 papers about KG-aware research for two major low-resource learning settings \u2014 zero-shot learning (ZSL) where new classes for prediction have never appeared in training, and few-shot learning (FSL) where new classes for prediction have only a small number of labeled samples that are available. We \ufb01rst introduced the KGs used in ZSL and FSL studies as well as the existing and potential KG construction solutions, and then systematically categorized and summarized KG-aware ZSL and FSL methods, dividing them into different paradigms such as the mapping-based, the data augmentation, the propagation- based and the optimization-based. We next presented different applications, including not only KG augmented prediction tasks in Computer Vision and Natural Language Processing (e.g., image classi\ufb01cation, visual question answering, text classi\ufb01cation and knowledge extraction), but also tasks for KG curation (e.g., inductive KG completion), and some typical evaluation resources for each task. We eventually discussed some challenges and future directions on aspects such as new learning and reasoning paradigms, and the construction of high quality KGs."
    ],
    "domain": [
      "Ontology Engineering",
      "Knowledge Graph",
      "Low-Resource Learning",
      "Natural Language Processing"
    ],
    "is_leader_candidate": true,
    "is_member_candidate": true,
    "is_reviewer_candidate": true,
    "is_chair_candidate": true
  }
}