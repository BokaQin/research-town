{
  "c97ad6bd-d6f0-4ca3-9855-d13620419062": {
    "pk": "c97ad6bd-d6f0-4ca3-9855-d13620419062",
    "authors": [
      "Xinrong Zhang",
      "Yingfa Chen",
      "Shengding Hu",
      "Zihang Xu",
      "Junhao Chen",
      "Moo Khai Hao",
      "Xu Han",
      "Zhen Leng Thai",
      "Shuo Wang",
      "Zhiyuan Liu",
      "Maosong Sun"
    ],
    "title": "$\\infty$Bench: Extending Long Context Evaluation Beyond 100K Tokens",
    "abstract": "Processing and reasoning over long contexts is crucial for many practical applications of Large Language Models (LLMs), such as document comprehension and agent construction. Despite recent strides in making LLMs process contexts with more than 100K tokens, there is currently a lack of a standardized benchmark to evaluate this long-context capability. Existing public benchmarks typically focus on contexts around 10K tokens, limiting the assessment and comparison of LLMs in processing longer contexts. In this paper, we propose $\\infty$Bench, the first LLM benchmark featuring an average data length surpassing 100K tokens. $\\infty$Bench comprises synthetic and realistic tasks spanning diverse domains, presented in both English and Chinese. The tasks in $\\infty$Bench are designed to require well understanding of long dependencies in contexts, and make simply retrieving a limited number of passages from contexts not sufficient for these tasks. In our experiments, based on $\\infty$Bench, we evaluate the state-of-the-art proprietary and open-source LLMs tailored for processing long contexts. The results indicate that existing long context LLMs still require significant advancements to effectively process 100K+ context. We further present three intriguing analyses regarding the behavior of LLMs processing long context.",
    "url": "http://arxiv.org/abs/2402.13718v3",
    "timestamp": 1708515029,
    "sections": {
      "1 Introduction": "\n\n1 Introduction\n\nFigure 1: The performance of GPT-4, Kimi-Chat, YaRN-Mistral, and Claude 2 on \u221e\\infty\u221eBench. A higher value represents better performance.\n\n\n\n\n\nBenchmark\nAvg Len\nEn\nZh\nCode\nMath\nNovel\nDialogue\nSynthetic\n\n\n\n\nLRA\u00a0(Tay et\u00a0al., 2020)\n\n\n\u223csimilar-to\\sim\u223c10K\n\u2713\n\u2717\n\u2717\n\u2713\n\u2717\n\u2717\n\u2713\n\n\nLongBench\u00a0(Bai et\u00a0al., 2023)\n\n\n\u223csimilar-to\\sim\u223c10K\n\u2713\n\u2713\n\u2713\n\u2717\n\u2713\n\u2713\n\u2713\n\n\nL-Eval\u00a0(An et\u00a0al., 2023)\n\n4K - 60K\n\u2713\n\u2717\n\u2713\n\u2713\n\u2717\n\u2717\n\u2713\n\n\nLooGLE\u00a0(Li et\u00a0al., 2023)\n\n\n\u223csimilar-to\\sim\u223c20K\n\u2713\n\u2717\n\u2717\n\u2717\n\u2717\n\u2713\n\u2717\n\n\n\n\u221e\\infty\u221eBench (ours)\n\n\u223csimilar-to\\sim\u223c200K\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\n\n\nTable 1: Comparison to existing long-context benchmarks and \u221e\\infty\u221eBench. \u201cEn\u201d and \u201cZh\u201d refer to English and Chinese tasks. \u201cCode\u201d, \u201cMath\u201d, \u201cNovel\u201d, \u201cDialogue\u201d indicate whether the domain includes tasks from those domains, and \u201cSynthetic\u201d indicates whether there are auto-generated tasks.\n\n\nIn recent years, large language models (LLMs) Brown et\u00a0al. (2020); OpenAI (2023a); Touvron et\u00a0al. (2023) have exhibited exceptional performance across a range of natural language processing (NLP) tasks\u00a0Qiu et\u00a0al. (2020); Han et\u00a0al. (2021). LLMs are showing a promising direction toward generalist task assistance, being capable of aiding users in practical tasks through conversational interactions. These tasks include web navigation\u00a0Nakano et\u00a0al. (2021), analysis of code repositories\u00a0Chen et\u00a0al. (2021), and extraction of useful information from documents\u00a0Ko\u010disk\u1ef3 et\u00a0al. (2018), indicating a step towards artificial general intelligence. For these LLM-based scenarios, the ability to process long contexts is increasingly critical, in addition to understanding fine-grained semantics and possessing extensive knowledge\u00a0Dong et\u00a0al. (2023); Huang et\u00a0al. (2023). Textual documents, historical dialogues, complex instructions, and cumbersome workflows, which constitute the data most directly processed in daily tasks, must be input to LLMs as long contexts for effective processing.\n\n\nDespite this growing importance, LLMs consistently face challenges in processing long contexts, primarily due to the substantial computational resources required for long sequence training\u00a0Dao et\u00a0al. (2022); Dao (2023) as well as the apparent inability to generalize to sequences longer than those encountered during training\u00a0Chen et\u00a0al. (2023a); Peng et\u00a0al. (2023b).\nLLMs are typically trained on sequences containing no more than 8K tokens\u00a0Touvron et\u00a0al. (2023); Penedo et\u00a0al. (2023); Biderman et\u00a0al. (2023), and thus cannot well handle contexts exceeding 8K tokens.\nThese limitations have largely restricted most LLMs from being applied to more complex tasks.\n\n\nRecent advancements in training infrastructure\u00a0Shoeybi et\u00a0al. (2019); Narayanan et\u00a0al. (2021); Dao et\u00a0al. (2022); Dao (2023), and efforts to improve length generalization\u00a0Anil et\u00a0al. (2022); Chen et\u00a0al. (2023b); Peng et\u00a0al. (2023b)333https://www.reddit.com/r/LocalLLaMA/comments/14lz7j5/ntkaware_scaled_rope_allows_llama_models_to_have/ have led to rapid developments in long-context LLMs.\nBased on these improved training infrastructures and length generalization methods, several LLMs have purportedly managed to process data exceeding 100K tokens\u00a0(Peng et\u00a0al., 2023b; OpenAI, 2023b; 01.AI, 2023b, a), with Claude 2\u00a0Anthropic (2023) and Kimi-Chat\u00a0AI (2023) even claiming to be able to process up to 200K tokens.\nHowever, the rapid emergence of long-context LLMs has outpaced the development of adequate evaluation benchmarks. Present long-context benchmarks predominantly feature contexts averaging around 10K tokens\u00a0(Bai et\u00a0al., 2023; Tay et\u00a0al., 2020), invariably falling below 100K tokens. This lag in the advancement of long-context evaluation methodologies impedes both the comparative analysis of diverse long-context LLMs and the pinpointing of potential enhancements in long-context processing.\n\n\nIn this work, we present \u221e\\infty\u221eBench , the first comprehensive benchmark featuring an average data length surpassing 100K tokens. \u221e\\infty\u221eBench includes tasks in different domains (novels, code, math, etc.) and languages (English and Chinese). To fully evaluate the performance of long-context LLMs, \u221e\\infty\u221eBench integrates synthetic tasks that can be auto-generated for even longer contexts (e.g., finding the top-k\ud835\udc58kitalic_k number in an array) in addition to a set of realistic tasks.\n\n\nTo construct tasks annotated by humans, we develop 5 annotation pipelines for detailed example annotation. These pipelines undergo iterative refinement until the examples meet quality standards. Auto-generated tasks, conversely, can be easily scaled to various lengths. Upon completing \u221e\\infty\u221eBench, we assess the performance of several state-of-the-art (SOTA) long-context LLMs on this benchmark to gauge its difficulty and evaluate the effectiveness of these models. The results show that current SOTA LLMs are not fully equipped to handle all tasks within \u221e\\infty\u221eBench, highlighting the ongoing challenge of enabling LLMs to process long contexts effectively. We also conduct intriguing analyses on the behavior of LLMs on such long contexts, including the task length ablation, the absent of \u201clost in the middle phenomenon\u00a0Liu et\u00a0al. (2023)\u201d, and the context recalling prompting techniques.\n\n\nOur contributions can be summarized as follows:\n\n\n\u2022\n\nWe construct and release \u221e\\infty\u221eBench, the first multi-domain bilingual benchmark for evaluating the ability to understand and reason over contexts surpassing 100K tokens.\n\n\n\n\u2022\n\nWe evaluate SOTA long-context LLMs on \u221e\\infty\u221eBench, which reveals severe performance degradation of these LLMs when scaling context lengths. These experimental results and analysis also indicate promising directions to improve long-context LLMs.\n\n\n\n\n",
      "2 Related Work": "\n\n2 Related Work\n\nExtending Context Length\n\nTransformers, typically trained on text sequences under 8K tokens due to self-attention\u2019s quadratic complexity, face challenges in longer downstream tasks. To address this, two main strategies have emerged: firstly, the development of positional encodings capable of handling longer text sequences\u00a0Sun et\u00a0al. (2022); Press et\u00a0al. (2021), and secondly, the refinement of inference stage techniques to extend current LLMs post-training. The primary approach involves modifying rotary positional encoding\u00a0Su et\u00a0al. (2023) and implementing post-training adjustments to better manage the increased relative positional distances in longer sequences\u00a0Zhu et\u00a0al. (2023); Peng et\u00a0al. (2023b); Chen et\u00a0al. (2023a).\n\n\n\n100K+ LLMs\n\nRecently, many LLMs have shown the ability to handle over 100K tokens. Some popular proprietary 100K+ LLMs include GPT-4, Claude 2\u00a0(Anthropic, 2023), and Kimi-Chat\u00a0(AI, 2023).\nOn the other hand, there are much fewer open-source 100K+ models. Some notable models include YaRN\u00a0(Peng et\u00a0al., 2023b) and Yi-200K\u00a0(01.AI, 2023a, b).\nIn this paper, we benchmark GPT-4, Claude 2, Kimi-Chat, and YaRN-Mistral-7B-128K444https://huggingface.co/NousResearch/Yarn-Mistral-7b-128k, we denote this model by YaRN-Mistral. on \u221e\\infty\u221eBench, which are some of the latest and strongest LLMs that claim to be able to handle over 100K tokens. \n\n\n\nInference Infrastructure\n\nNumerous studies aim to accelerate self-attention computation. Research primarily concentrates on refining attention mechanisms through improved IO management\u00a0(Dao et\u00a0al., 2022; Dao, 2023), memory optimization\u00a0(Kwon et\u00a0al., 2023; Shazeer, 2019; Ainslie et\u00a0al., 2023), and enhanced parallelization in decoding\u00a0(Dao et\u00a0al., 2023; Hong et\u00a0al., 2023). Approaches like Sliding Window Attention\u00a0(Beltagy et\u00a0al., 2020), LM-Infinite\u00a0(Han et\u00a0al., 2023), and StreamingLLM\u00a0(Xiao et\u00a0al., 2023) introduce attention variants for handling infinitely long sequences without overwhelming computation or memory overhead. However, these techniques often face challenges in maintaining historical information.\n\n\n\nLong Context Benchmarks\n\nSeveral benchmarks exist for evaluating long-context AI models, notably featuring context lengths of around 10K tokens. L-Eval\u00a0(An et\u00a0al., 2023) and LongBench\u00a0(Bai et\u00a0al., 2023) are prominent examples, aggregating pre-existing tasksKocisk\u00fd et\u00a0al. (2017); Dasigi et\u00a0al. (2021); Yang et\u00a0al. (2018); Huang et\u00a0al. (2021); Joshi et\u00a0al. (2017) into comprehensive benchmarks. LongBench encompasses four categories\u2014QA, summarization, synthetic retrieval, and code\u2014spanning 21 tasks, with four being novel. Conversely, L-Eval incorporates 18 tasks across QA, summarization, math, retrieval, and multiple-choice (MC) domains, introducing three new tasks. Another notable benchmark, LooGLE\u00a0(Li et\u00a0al., 2023), differentiates between short and long dependency examples, focusing on summary and QA tasks; its summary corpus contrasts with ours, utilizing academic papers over novels. The Long-Range Arena (LRA) (Tay et\u00a0al., 2020) further diversifies with six tasks in text, image, and math, designed for scalability. In comparison, \u221e\\infty\u221eBench stands out for its substantially longer contexts and a broader range of task domains. Table\u00a01 offers a detailed comparison of these long-context benchmarks.\n\n\nFigure 2: The statistics of the data in \u221e\\infty\u221eBench. The radius of each segment indicates the length of input plus output on the logarithmic scale, and the width (or angle) indicates the number of examples (proportionally to the total number of examples).\n\n\n",
      "3 \u221e\\infty\u221eBench": "\n\n3 \u221e\\infty\u221eBench\n\n\n\n\n\nTask\nAnnotation\n# Ex.\nAvg Len\n\n\n\n\nRet.PassKey\nAuto\n590\n122.4K/2\n\n\nRet.Number\nAuto\n590\n122.4K/4\n\n\nRet.KV\nAuto\n500\n121.1K/22.7\n\n\nEn.Sum\nHuman\n103\n103.5K/1.1K\n\n\nEn.QA\nHuman\n351\n192.6k/4.8\n\n\nEn.MC\nHuman\n229\n184.4K/5.3\n\n\nZh.QA\nHuman\n189\n2068.6K/6.3\n\n\nEn.Dia\nAuto\n200\n103.6K/3.4\n\n\nCode.Debug\nHuman\n394\n114.7K/4.8\n\n\nCode.Run\nAuto\n400\n75.2K/1.3\n\n\nMath.Calc\nAuto\n50\n43.9K/43.9K\n\n\nMath.Find\nAuto\n350\n87.9K/1.3\n\n\n\nTable 2: Data statistics. The columns indicate whether the annotation was auto-generated or done by humans, the number of examples, and the average length (input/output) in tokens.\n\n\n\n\n\nTask\nGPT-4\nYaRN-Mistral\nKimi-Chat\nClaude 2\n\n\n\n\nRetrieve.PassKey\n100.00\n92.71\n98.14\n97.80\n\n\nRetrieve.Number\n100.00\n56.61\n95.42\n98.14\n\n\nRetrieve.KV\n89.00\n0.00\n53.60\n65.40\n\n\nEn.Sum\n14.73\n9.09\n17.93\n14.45\n\n\nEn.QA\n22.22\n9.55\n16.52\n11.97\n\n\nEn.MC\n67.25\n27.95\n72.49\n62.88\n\n\nEn.Dia\n8.50\n7.50\n11.50\n46.50\n\n\nZh.QA\n23.06\n16.98\n18.62\n10.53\n\n\nCode.Debug\n39.59\n0.76\n18.02\n2.28\n\n\nCode.Run\n23.25\n1.25\n2.00\n2.50\n\n\nMath.Calc\n0.01\n0.00\n0.00\n0.00\n\n\nMath.Find\n60.00\n17.14\n12.57\n32.29\n\n\nAverage\n45.63\n19.96\n34.73\n37.06\n\n\n\nTable 3: Main results. The performance of the baselines in \u221e\\infty\u221eBench. For multiple-choice questions, if the model does not output one of the options, we regard it as an empty prediction, and thus give it a score of 0.\n\n\n\u221e\\infty\u221eBench encompasses 12 tasks spanning 5 domains: retrieval, code, math, novels, and dialogue. Two of these tasks are derived from existing literatureMohtashami and Jaggi (2023); Liu et\u00a0al. (2023). Among the newly introduced tasks, half are generated automatically, while the remainder are annotated by humans.\n\n\nIn total, \u221e\\infty\u221eBench includes 3946 examples, featuring a length beyond 100K tokens (average approximately 200K). Figure\u00a02 illustrates the distribution of these tasks. Table\u00a02 details their respective input and output lengths as well as the number of examples per task.\n\n\nNext, we illustrate each task in detail. The tasks can be grouped into two broad categories. The first involves realistic context collected from real-world scenarios which has potential practical usage of long context LLMs. The second depends on synthetic contexts which are created or collected for testing certain capabilities of long-context LLMs.\n\n\n\n3.1 Realistic Context\n\n\n3.1.1 Novel\n\nWe develop novel-based tasks as outlined in Figure\u00a03, utilizing novels sourced from websites555https://www.sparknotes.com/666https://www.cliffsnotes.com/ and are manually filtered. More annotation information in Appendix.\u00a0C.\n\n\nIn these tasks, models are tasked with reasoning over entire novels presented during inference. Recognizing that many novels, along with their movie adaptations and related discussions, are accessible online and may have been encountered by LLMs during training, we adopt key entity replacement as a countermeasure. This involves substituting prominent entities determined by annotators, such as main character names, with unrelated ones, creating \u201cfake novels\u201d.\n\n\nUsing these altered novels, we design tasks in three formats: summarization, open-form question answering (QA), and multiple-choice (MC) questions, applying key entity replacement to the annotations as well. All English tasks share the same set of modified novels.\n\n\nFigure 3: The annotation pipelines for the human-annotated tasks in \u221e\\infty\u221eBench.\n\n\nEn.Sum\n\nThe En.Sum task requires models to generate a concise summary of the novel. Gold standard labels are sourced from the web and undergo manual filtering to remove non-summarization content, like comments. Model performance is evaluated using the ROUGE-L-Sum metric\u00a0(Lin, 2004).\n\n\n\nEn.QA & Zh.QA\n\nWe employ the same annotation pipeline for both En.QA and Zh.QA tasks, ensuring that the questions necessitate long-range dependency and reasoning, beyond simple short passage retrieval. The tasks are primarily categorized into two types of reasoning:\n\n\n\u2022\n\nAggregation: This involves compiling various pieces of information scattered throughout the novel. An example question in \u221e\\infty\u221eBench is \u201cHow much money in total did A spend on lunch?\u201d\n\n\n\n\u2022\n\nFiltering: This requires identifying specific information from a larger set. An example question in \u221e\\infty\u221eBench is \u201cWhat color dress did A wear when A met B for the second time?\u201d\n\n\n\n\n\nThese tasks test LLMs to locate and process information within the novel, performing reasoning through aggregation or filtering to derive answers.\n\n\n\nEn.MC\n\nThe En.MC task is annotated similarly to En.QA, but differs in that the model is presented with four answer choices. Annotators are instructed to craft these options to be challenging.\n\n\n\n\n\n3.1.2 Dialogue\n\nEn.Dia\n\nThe construction process for the En.Dia task is depicted in Figure\u00a03. We gather movie and drama scripts from a designated online database777https://imsdb.com/, focusing on a corpus of long, multi-role dialogues. Only the English scripts are retained and necessary cleaning is applied.\n\n\nIn the En.Dia task, random instances of character names within a script are replaced with $$MASK$$. The objective is to correctly identify these masked names. For scripts falling short of 100K tokens, we augment them by padding with additional scripts.\n\n\n\n\n\n3.1.3 Code\n\nCode.Debug\n\nWe develop the task as per the process illustrated in Figure\u00a03. Code repositories, sourced from PyPI888https://pypi.org/, undergo a filtering process, and those outside the 64K to 256K token range are excluded (tokenization via the tiktoken tokenizerOpenAI (2023c)). Each repository is transformed into a single file, aggregating the content from all files within, each prefaced by its relative path to the root directory. Three of the authors then insert a deliberate and obvious error into one function per repository. The options are presented in the Class.Function format. Six methods are employed for bug insertion: (1) deleting a necessary variable declaration; (2) using an incorrect number of arguments in function calls; (3) creating infinite loops; (4) causing indentation errors; (5) substituting references with undefined variable/function names; (6) introducing blatant syntax errors (e.g., non-closed brackets).\n\n\nInitial results indicate that this task is too challenging for current LLMs (None of the baseline models can identify the most obvious error such as non-closed brackets). To mitigate this, we offer four answer choices, one containing the injected bug and the others are bug-free. Note that this makes many examples easily solved by external retrieval preprocess. However, we encourage the users not to use external retrieval preprocess to keep the evaluation fair. And we are looking forward to the stage where LLMs can directly solve the problem without selection options.\n\n\n\n\n\n\n3.2 Synthetic Context\n\nThe second category of tasks is characterized by a synthetic context. These tasks, devoid of direct real-world application or use case, are engineered to evaluate the capability for processing lengthy contexts. We delineate four essential ability for effective long-context processing:\n\n\n\n\n1.\n\nLocation and retrieval. This encompasses all retrieval tasks.\n\n\n\n2.\n\nElevated information resolution. This involves the Retrieve.Number task.\n\n\n\n3.\n\nState preservation. This incorporates the Code.Run and Math.Find functions.\n\n\n\n4.\n\nSequential processing. This utilizes the Math.Calc function.\n\n\n\n\n\n\n3.2.1 Retrieve\n\nIn retrieval tasks, models retrieve specific character sequences from lengthy contexts with predominantly irrelevant content. Such tests, adaptable for any context length, can assess the impact of information placement on model performance, like the lost-in-the-middle phenomenon\u00a0(Liu et\u00a0al., 2023). The three retrieval tasks in \u221e\\infty\u221eBench vary in complexity.\n\n\nRetrieve.PassKey\n\nThis task is first proposed by Mohtashami and Jaggi (2023). Models are prompted to find a specific <key> called pass key, which is a random 5-digit sequence. The pass key is inserted into a lengthy and noisy context, as shown below. In \u221e\\infty\u221eBench, we generate examples with 59 different pass key locations that are evenly distributed in the context. At each location, we construct 10 examples with different pass keys. This results in 590 examples.\n\n\n\n\n\nThere is an important pass key hidden in a lot of irrelevant text. Find it.\n<very long noisy context>\nThe pass key is <key>. Remember it. The pass key is <key>\n<very long noisy context>\nWhat is the pass key?\n\n\n\n\nRetrieve.Number\n\nTo examine the local attention of LLMs, we have enhanced the complexity of Retrieve.PassKey by increasing the answer length to 10 digits and incorporating successive repetitive digits. For example, a <key> in Retrieve.PassKey valued 98762, while in Retrieve.Number is 9998877762. This modification aims to assess the local resolution capabilities of long context models, as our preliminary experiments indicate that LLMs struggle with discerning repeated numbers.\n\n\n\nRetrieve.KV\n\nLiu et\u00a0al. (2023) introduce a key-value retrieval task within a large JSON object containing many key-value pairs (e.g., 30eea139-b6dd-43fc-bc5d-0d3d17980229 \u2192\u2192\\rightarrow\u2192 bfd36c2b-c57e-41ef-9cc1-b21b4e60e664). This task demands the model to accurately identify and retrieve the value corresponding to a specified key. The complexity of this task is heightened due to the indistinguishable format of relevant and irrelevant information.\n\n\n\n\n\n3.2.2 Code\n\nCode.Run\n\nIn this task, we evaluate the ability of LLMs to simulate multi-step function executions that involve basic arithmetic operations. While this task is readily solvable using a Python interpreter, the focus here is on the long-term state tracking required in such tasks. The capability of state tracking has been demonstrated in GPT-4\u00a0(Bubeck et\u00a0al., 2023). Specifically, the task involves creating Python code consisting of multiple simple functions, incorporating operations such as addition, subtraction, and nested function calls. The structural design of these tasks is as follows:\n\n\n\n\u2b07\n\n\n\ndef\u00a0func_0(x):\n\n\nreturn\u00a0func_1(x)\u00a0+\u00a04\n\n\\pardef\u00a0func_1(x):\n\n\nreturn\u00a0x\u00a0-\u00a01\n\n\n\n\nSome functions\u2019 return values are dependent on other functions (e.g., func_0 invokes func_1). We define depth as the number of cascading function calls initiated by a single call. Thus, the depth for func_1\u2019s call within func_0 is 1. In Code.Run, we employ depths ranging from 2 to 10, ensuring each function calls at most one other function. To keep the simplicity of each single step of computation, these functions are restricted to performing only addition and subtraction.\n\n\n\n\n\n3.2.3 Math\n\nMath.Find\n\nMath.Find assesses the model\u2019s capability to identify specific elements within a large array, requiring comprehensive observation for accuracy. This task also tests the ability to preserve states while encoding the context. Concretely, the model receives an extensive list of numbers and is tasked with locating one of seven key numbers: the three largest (1st, 2nd, and 3rd), the three smallest (1st, 2nd, and 3rd), and the median.\n\n\n\nMath.Calc\n\nTo assess sequential processing skills, Math.Calc prompts the model to compute the result of a lengthy arithmetic expression featuring addition and subtraction. Initial experiments indicate that current LLMs struggle to directly produce the final answer. Hence, we instead query the LLMs to provide the intermediate result following each operator. Model performance is evaluated based on the number of correct values preceding the first error.\n\n\n\n\n",
      "4 Experiments": "\n\n4 Experiments\n\nWe conduct a thorough set of experiments on \u221e\\infty\u221eBench. We will introduce the baselines, experimental setup, and main results in this section.\n\n\n\n\n4.1 Baselines\n\n\u221e\\infty\u221eBench generally requires the ability to handle input contexts longer than 100k. There is a handful of LLMs that claim to be capable of handling contexts over 100k. We include four baselines. The first three are proprietary LLMs as we do not have access to the model, while the last baseline is open-sourced. Details on evaluation are in Appendix.\u00a0D.\n\n\nGPT-4\n\nGPT by OpenAI is one of the most widely used and capable LLMs in the market, and a recent version of GPT-4 \u00a0(OpenAI, 2023b) can support 128K contexts.\n\n\n\n\nClaude 2\n\nClaude 2\u00a0(Anthropic, 2023) is a proprietary chat-based LLM released by Anthropic AI and has shown impressive capabilities. The second version of the Claude series supports 200K contexts. We manually enter each example through the webpage because we have no access to their API.\n\n\n\n\nKimi-Chat\n\nKimi-Chat, a proprietary chat-oriented LLM developed by Moonshot AI AI (2023), is designed to process contexts up to 200K. Due to the lack of API access, we manually input the test data using their web interface.\n\n\n\nYaRN-Mistral\n\nYaRN-Mistral is a derivative of Mistral-7B\u00a0(Jiang et\u00a0al., 2023) introduced by Peng et\u00a0al. (2023b). The original Mistral-7B was trained on input lengths up to 8K and shows a reduced performance in longer contexts. Peng et\u00a0al. (2023b) adapted it to 128K contexts by modifying the position encoding and continued training.\n\n\n\n\n\n4.2 Experimental Setup\n\nPrompt Templates\n\nFor each model-task combination, we craft prompts to optimize model performance on short dummy examples. Detailed prompt templates for each model and task can be found in Appendix\u00a0B.\n\n\n\nInput Truncation\n\nAll API-based baselines are subject to a maximum input length limit and will reject inputs exceeding this threshold. While YaRN-Mistral is theoretically capable of handling longer contexts, the authors only claim abilities up to 128K. Therefore, inputs are truncated by removing the center and joining both ends. This approach is predicated on the assumption that key information, such as instructions and book titles, is typically located at either the start or the end of a prompt.\n\n\n\n\n\n4.3 Main Result\n\nFigure 4: Baseline performance as a function of input length.\n\n\nTable\u00a03 and Figure\u00a01 display the performances of various baselines on \u221e\\infty\u221eBench. Notably, GPT-4 outperforms other baselines in the retrieval, code, and math domains, with a considerably higher average score. However, in the novel-based tasks, no distinct winner emerges among the proprietary LLMs. On the other hand, the open-source YaRN-Mistral lags behind the proprietary models in most tasks, exhibiting almost random performance in multiple areas. This aligns with its relatively inferior performance in shorter contexts compared to these models. Additionally, it is observed that the baselines generally excel more in retrieval tasks than in other areas, echoing the relative simplicity of these tasks for human participants.\n\n\n",
      "5 Analysis": "\n\n5 Analysis\n\nWe subsequently perform a detailed analysis of the results, identifying and emphasizing several notable and interesting phenomena.\n\n\n\n5.1 Length Ablation\n\nIn line with our benchmark\u2019s goal to assess proficiency in managing lengthy contexts, we verify the baselines\u2019 capability with shortened context versions. A subset of the auto-generated tasks is modified accordingly, and the performance outcomes are illustrated in Figure 4. It is observed that model performance generally declines with longer input lengths compared to shorter ones. This suggests that while these baselines are technically equipped to handle extended inputs, their effectiveness diminishes significantly under such conditions.\n\n\n\n\n\n5.2 Lost in the middle\n\nFigure 5: Performance as a function of the answer position (in the number of characters). The steep drop in performance for Kimi-Chat in the middle on Retrieval.KV is caused by the answer being removed by truncation.\n\n\n\n\nOne function in this repo is deliberately made to include an obvious error. Find it.\n<code>\nThink step by step and at last give me your answer for the function with the deliberate error.\n<list of options>\n\n\n\n\n\nOne function in this repo is deliberately made to include an obvious error. Find it.\n<code>\nLocate the functions in the options, repeat their content, inspect through code, and at last give me your answer for the function with the deliberate error.\n<list of options>\n\n\nFigure 6: Compared to the first prompt, the second prompt improves GPT-4\u2019s results on Code.Debug dramatically.\n\n\nPrior research indicates a performance decline in some LLMs when answers are positioned around the center of the context\u00a0(Liu et\u00a0al., 2023). However, our findings do not strongly corroborate this. As depicted in Figure\u00a05, we analyze model performance based on answer location in three location-dependent tasks. We observe no consistent trend between performance and answer position across different models. For instance, GPT-4 shows a preference for early answers in Retrieval.KV but favors later ones in En.Dia. In contrast, Claude 2\u2019s performance remains relatively unaffected by answer position on all three tasks, whereas YaRN-Mistral and Kimi-Chat excel with end-positioned answers (except that YaRN-Mistral get zero performance on all positions on Retrieval.KV).\n\n\nOne plausible reason why we have different observations from Liu et\u00a0al. (2023) is that they experiment with different models using at most 16K length contexts, which is about 8 times shorter than our setting. The models in their study are also different from ours.\nFinally, the tasks are different: their experiments involve document question answering (and their result with Retrieval.KV arguably does not show a very pronounced performance drop as well). We hypothesize that the phenomenon of \u201cLost in the middle\u201d is only exhibited on specific tasks and models. A more thorough investigation of these differences is beyond the scope of this paper.\n\n\n\n\n5.3 Context Recalling\n\nWe identify an intriguing prompting technique for tasks involving extended context, termed context recalling. This technique posits that, although the information is present in the context and accessible via direct attention, it may be more effective to first prompt the model to recall the relevant information in its generation before engaging in further reasoning. In our experiments using Code.Debug, when we merely instructed GPT-4 to process information step-by-step, the accuracy was 15.74%. However, by explicitly directing GPT-4 to repeat the relevant code before analysis, its accuracy on Code.Debug markedly improved to 39.59%. This approach of context recalling warrants additional investigation.\n\n\n",
      "6 Conclusions": "\n\n6 Conclusions\n\nWe introduce \u221e\\infty\u221eBench, the first benchmark tailored for long contexts exceeding 100K in average length. Empirical evidence indicates that despite claims of proficiency with such extensive contexts, current LLMs demonstrate significant performance degradation when dealing with them. This finding highlights the need for advanced methodologies to improve LLMs\u2019 efficiency in processing long context. Additionally, our analysis offers insights into LLM behavior in long-context tasks, guiding future research.\n\n\n\n",
      "Limitations": "\nLimitations\n\nWhile our benchmark offers valuable insights into LLM performance, it may not be sufficiently diverse or extensive to provide a comprehensive assessment of model capabilities, a constraint common to most benchmarks. Additionally, the reliance on exact match for scoring, dependent on prompt templates and answer parsing methods, may necessitate tailored redesigns for new model evaluations.\n\n\nFurthermore, supporting contexts up to 100K tokens may fall short for applications requiring analysis of extensive datasets, such as multiple books or entire databases. Exploring LLMs\u2019 capacity to handle up to a million tokens or more presents a promising research avenue. In practical applications, finetuning models to memorize context, rather than processing it during inference, could offer a more efficient alternative, albeit with significant computational demands.\n\n",
      "Ethics Statement": "\nEthics Statement\n\nOur human annotators are directed to exclude data that may raise sensitive ethical issues, such as offensive language or social biases. Nonetheless, the potential for encountering sensitive content persists, particularly if the sourced books or code contain such material. This concern is somewhat mitigated since the benchmark\u2019s primary focus is on evaluating the long-context capabilities of LLMs, rather than influencing their social bias.\n\n\nThe goal of this research is to advance the development of LLMs\u2019 proficiency in handling extensive contexts. This could aid in implementing more effective \u201cguardrails\u201d against misuse by incorporating detailed specifications prior to user interactions. However, this approach also potentially increases the risk of novel prompt injection attacks.\n\n\n",
      "Appendix A RWKV": "\n\nAppendix A RWKV\n\nRWKV\u00a0(Peng et\u00a0al., 2023a) is an architecture that combines the power of the transformer architecture\u00a0(Vaswani et\u00a0al., 2017) and recurrent neural network\u00a0(Hochreiter and Schmidhuber, 1997). Its training process can be parallelized while the inference procedure is recurrent, enabling O\u2062(1)\ud835\udc421O(1)italic_O ( 1 ) complexity during inference. Hence, the memory usage does not scale with context length, allowing it to support arbitrary-length inputs. We use the RKWV-4-World-7B version of this model series. However, we should keep in mind that this model was not trained on inputs of this length.\n\n\n\n\n\nModel\nRetrieve.PassKey Acc.\n\n\n\n\nGPT-4 Turbo\n100%\n\n\nYaRN-Mistral\n92.71%\n\n\nKimi-Chat\n98.14%\n\n\nClaude 2\n97.80%\n\n\nRWKV-4-World-7B\n0.00%\n\n\n\nTable 4: Results in Retrieve.PassKey with RWKV-4-World-7B. Since RWKV-4 was only trained on 4k sequences, it has zero performance on \u221e\\infty\u221eBench. It outputs only unintelligible content in this test.\n\n\nTable\u00a04 shows the performance of RWKV-4-World-7 in comparison to our baselines. We find that RWKV-4-World-7B outputs unintelligible texts on our benchmark, which causes it to achieve zero performance on Retrieve.PassKey, which is the easiest task for other baselines. This is likely because this model was not trained on inputs of this length and suffers from train-test domain shift.999We emphasize that this result is not evidence that the architecture of RWKV is incapable of handling lengthy inputs. Therefore, we do not consider testing it on other tasks in our benchmark.\n\n",
      "Appendix B Prompt Templates": "\n\nAppendix B Prompt Templates\n\nIn the following templates, many tasks has an <input> part that is provided in each example. Generally, they are a short question-like text that tells the model what it is supposed to do. One example is \u201cWhat is the pass key?\u201d.\n\n\n\nB.1 Retrieve.PassKey\n\nThe prompt below applies to GPT-4, Claude 2, and Kimi-Chat.\n\n\n\n\nThere is an important info hidden inside a lot of irrelevant text. Find it and memorize them. I will quiz you about the important information there.\n<context>\n<input>\n\n\n\nThe prompt below applies to YaRN-Mistral.\n\n\n\n\nThere is an important info hidden inside a lot of irrelevant text. Find it and memorize them. I will quiz you about the important information there.\n<context>\n<input>\nThe pass key is\n\n\n\n\n\nB.2 Retrieve.Number\n\nThe prompt below applies to GPT-4, Claude 2, and Kimi-Chat.\n\n\n\n\n\nThere is an important info hidden inside a lot of irrelevant text. Find it and memorize them. I will quiz you about the important information there.\n<context>\n<input>\n\n\n\nThe prompt below applies to YaRN-Mistral.\n\n\n\n\nThere is an important info hidden inside a lot of irrelevant text. Find it and memorize them. I will quiz you about the important information there.\n<context>\n<input>\nThe sequence of digits is\n\n\n\n\n\nB.3 Retrieve.KV\n\n\n\nExtract the value corresponding to the specified key in the JSON object below.\n<context>\n<input>\n\n\n\n\n\nB.4 En.Sum\n\nThe prompt below applies to GPT-4, Claude 2, and Kimi-Chat.\n\n\n\n\nSummarize the book below.\n<context>\n\n\n\nThe prompt below applies to YaRN-Mistral.\n\n\n\n\nSummarize the book below.\n<context>\nSummary:\n\n\n\n\n\nB.5 En.QA\n\nThe prompt below applies to GPT-4, Claude 2, and Kimi-Chat.\n\n\n\n\nRead the book below and answer a question.\n<context>\nQuestion: <question>\nBe very concise.\n\n\n\nThe prompt below applies to YaRN-Mistral.\n\n\n\n\nRead the book below and answer a question. Be very concise in your answer.\n<context>\nQuestion: <question>\nAnswer:\n\n\n\n\n\nB.6 En.MC\n\nThe prompt below applies to GPT-4, Claude 2, and Kimi-Chat.\n\n\n\n\nRead the book and answer the question.\n<context>\nQuestion: <question>\nOnly one of the following options is correct, tell me the answer using one single letter (A, B, C, or D). Don\u2019t say anything else.\nA. <OPTION_A>\nB. <OPTION_B>\nC. <OPTION_C>\nD. <OPTION_D>\n\n\n\nThe prompt below applies to YaRN-Mistral.\n\n\n\n\nRead the book and answer the question.\n<context>\nQuestion: <question>\nOnly one of the following options is correct, tell me the answer using one single letter (A, B, C, or D). Don\u2019t say anything else.\nA. <OPTION_A>\nB. <OPTION_B>\nC. <OPTION_C>\nD. <OPTION_D>\nThe correct option is:\n\n\n\n\n\nB.7 En.Dia\n\nThe prompt below applies to GPT-4, Claude 2, and Kimi-Chat.\n\n\n\n\nBelow is a dialogue script where one random occurrence of a character name is replaced with $$MASK$$, and you should try to guess who that character is.\nThe dialogue:\n\u2014\n<context>\n\u2014\nEnd of dialogue.\nWhich character is most likely $$MASK$$? Just say the name used by the scriptwriter (before the colon marks) of one single character and nothing else.\n\n\n\nThe prompt below applies to YaRN-Mistral.\n\n\n\n\nBelow is a dialogue script where one random occurrence of a character name is replaced with $$MASK$$, and you should try to guess who that character is.\nThe dialogue:\n\u2014\n<context>\n\u2014\nEnd of dialogue.\nThe name that has been replaced with $$MASK$$ is likely:\n\n\n\n\n\nB.8 Zh.QA\n\nThe prompt below applies to GPT-4, Claude 2, and Kimi-Chat.\n\n\n\n\n\n\u8bf7\u6839\u636e\u4ee5\u4e0b\u4e66\u7c4d\u56de\u7b54\u6211\u7684\u95ee\u9898\u3002(Read the book and answer the question.)\n<context>\n\u95ee\u9898\uff1a(Question: )<question>\n\u8bf7\u5c3d\u91cf\u7b80\u77ed\u5730\u56de\u7b54\u3002(Be very concise.)\n\n\n\nThe prompt below applies to YaRN-Mistral.\n\n\n\n\n\u8bf7\u6839\u636e\u4ee5\u4e0b\u4e66\u7c4d\u56de\u7b54\u6211\u7684\u95ee\u9898\u3002(Read the book and answer the question.)\n<context>\n\u95ee\u9898\uff1a(Question: )<question>\n\u7b54\u6848\uff1a(Answer:)\n\n\n\n\n\nB.9 Code.Debug\n\nThe prompt below applies to GPT-4, Claude 2, and Kimi-Chat.\n\n\n\n\n\nThere is ONLY ONE function in the large project that is deliberately made to include an obvious error. Please find the function that contains the most obvious errors. I will give you four options to narrow your scope. You can inspect through the options and think. Eventually, tell me the answer using one single letter (A, B, C, or D).\n<context>\nWhich function has deliberate error?\nA. <OPTION_A>\nB. <OPTION_B>\nC. <OPTION_C>\nD. <OPTION_D>\nYou should first find the functions in the options. Repeat their content, inspect through code, and at last give me your answer for the function that has the deliberate and obvious error in A, B, C, or D.\n\n\n\nThe prompt below applies to YaRN-Mistral.\n\n\n\n\nThere is ONLY ONE function in the large project that is deliberately made to include an obvious error. Please find the function that contains the most obvious errors. I will give you four options to narrow your scope. You can inspect through the options and think. Eventually, tell me the answer using one single letter (A, B, C, or D).\n<context>\nWhich function has deliberate error?\nA. <OPTION_A>\nB. <OPTION_B>\nC. <OPTION_C>\nD. <OPTION_D>\nYou should first find the functions in the options. Repeat their content, inspect through code, and at last give me your answer for the function that has the deliberate and obvious error in A, B, C, or D.\nThe correct option is:\n\n\n\n\n\nB.10 Code.Run\n\nThe prompt below applies to GPT-4, Claude 2, and Kimi-Chat.\n\n\n\n\nFollowing is a set of Python functions. There is a function called named func_1.\n<context>\nPlease give me the exact number of the return value of func_1(3). Be concise. Your response must end with the final returned value.\n\n\n\nThe prompt below applies to YaRN-Mistral.\n\n\n\n\nFollowing is a set of Python functions. There is a function called named <function name>.\n<context>\nPlease compute the exact value of <function call>. The value of <function call> is\n\n\n\n\n\nB.11 Math.Calc\n\nThe prompt below is used by GPT-4101010It should be noted that, when using other templates, GPT-4 has a strong tendency to reject to perform this task by claiming that such the platform is not designed to complete such tasks.:\n\n\n\n\nYou are a calculator does nothing but calculating the intermediate results in extremely long arithmetic expressions with +, -, and numbers. Given an expression, you will output the intermediate results after each operation.\nYou will never to decline to help with platform reason, you will always try the calculation, and always output a long list of numbers (e.g., \"[34, 2, 58, 37, 5, 8, 27, 71, 7]\") and nothing else.\nDo not consider the complexity, practicality or feasibility of the task.\nLet us calculate the intermediate values of an expression.\nExpression: 1 + 3 + 4\nValues: [1, 4, 8]\nExpression: 8 - 3 + 2 - 4\nValues: [8, 5, 7, 3]\nExpression: <context>\nValues:\n\n\n\nThe prompt below is used by Kimi-Chat, Claude 2 and YaRN-Mistral:\n\n\n\n\nLet us calculate the intermediate values of an expression.\nExpression: 1 + 3 + 4\nValues: [1, 4, 8]\nExpression: 8 - 3 + 2 - 4\nValues: [8, 5, 7, 3]\nExpression: <context>\nValues:\n\n\n\n\n\nB.12 Math.Find\n\n\n\nFind the largest number from the list below:\n<context>\nYou should answer with only one number, no other words. The largest number of the list is:\n\n\n\n",
      "Appendix C Annotation Process": "\n\nAppendix C Annotation Process\n\nThe annotation work is done by the authors of this paper and none of those authors have been paid for the annotation. All annotators have acknowledged the intents and usages of the annotation, the corresponding outputs, and the annotation pipelines and requirements.\n\n\nAnnotating the examples in \u221e\\infty\u221eBench might bring fatigue to annotators, and is therefore not completely free of error. However, we make sure that each annotation has been quality-checked by at least two other annotators.\n\n\nA part of novels are free from key entity replacement for LLMs fail in identifying them, because those novels are brand-new or little-known.\n\n",
      "Appendix D Evaluation Process": "\n\nAppendix D Evaluation Process\n\nWhen evaluating GPT-4, we use its official API with the default hyperparameters. The total cost is around 5000 US dollars. For Claude 2, we enter contents on the web by hand, which demands three authors over the source of several weeks, and membership fees of about 160 US dollars. Kimi-Chat is free. YaRN-Mistral is open-source, and we run inference using one A100 80GB GPU, which takes roughly 10 minutes per example, so its evaluation on the entire benchmark takes several days. Again, we use the default decoding hyperparameters (specified by Peng et\u00a0al. (2023b)) except for the maximum number of output tokens, which is as shown in Table\u00a05.\n\n\n\n\n\nTask\nMax Output Tokens\n\n\n\n\nRetrieve.PassKey\n6\n\n\nRetrieve.Number\n12\n\n\nRetrieve.KV\n50\n\n\nEn.Sum\n1,200\n\n\nEn.QA\n40\n\n\nEn.MC\n40\n\n\nZh.QA\n40\n\n\nEn.Dia\n40\n\n\nCode.Debug\n5\n\n\nCode.Run\n5\n\n\nMath.Calc\n30,000\n\n\nMath.Find\n3\n\n\n\nTable 5: The maximum number of output tokens (a decoding hyperparameter) for YaRN-Mistral.\n\n\n"
    },
    "bibliography": {
      "01.AI (2023a)": "\n01.AI (2023a)\n\n01.AI. 2023a.\n\n\nYi-34b-200k.\n\n\nhttps://huggingface.co/01-ai/Yi-34B-200K.\n\n\n",
      "01.AI (2023b)": "\n01.AI (2023b)\n\n01.AI. 2023b.\n\n\nYi-6b-200k.\n\n\nhttps://huggingface.co/01-ai/Yi-6B-200K.\n\n\n",
      "AI (2023)": "\nAI (2023)\n\nMoonshot AI. 2023.\n\n\nKimi chat.\n\n\nhttps://kimi.moonshot.cn/.\n\n\n",
      "Ainslie et\u00a0al. (2023)": "\nAinslie et\u00a0al. (2023)\n\nJoshua Ainslie, James Lee-Thorp, Michiel de\u00a0Jong, Yury Zemlyanskiy, Federico Lebr\u2019on, and Sumit\u00a0K. Sanghai. 2023.\n\n\nGqa: Training generalized multi-query transformer models from multi-head checkpoints.\n\n\nArXiv, abs/2305.13245.\n\n\n",
      "An et\u00a0al. (2023)": "\nAn et\u00a0al. (2023)\n\nChen An, Shansan Gong, Ming Zhong, Mukai Li, Jun Zhang, Lingpeng Kong, and Xipeng Qiu. 2023.\n\n\nL-eval: Instituting standardized evaluation for long context language models.\n\n\nArXiv, abs/2307.11088.\n\n\n",
      "Anil et\u00a0al. (2022)": "\nAnil et\u00a0al. (2022)\n\nCem Anil, Yuhuai Wu, Anders Andreassen, Aitor Lewkowycz, Vedant Misra, Vinay Ramasesh, Ambrose Slone, Guy Gur-Ari, Ethan Dyer, and Behnam Neyshabur. 2022.\n\n\nExploring length generalization in large language models.\n\n\nAdvances in Neural Information Processing Systems, 35:38546\u201338556.\n\n\n",
      "Anthropic (2023)": "\nAnthropic (2023)\n\nAnthropic. 2023.\n\n\nModel card and evaluations for claude models.\n\n\n",
      "Bai et\u00a0al. (2023)": "\nBai et\u00a0al. (2023)\n\nYushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li. 2023.\n\n\nLongbench: A bilingual, multitask benchmark for long context understanding.\n\n\n",
      "Beltagy et\u00a0al. (2020)": "\nBeltagy et\u00a0al. (2020)\n\nIz\u00a0Beltagy, Matthew\u00a0E Peters, and Arman Cohan. 2020.\n\n\nLongformer: The long-document transformer.\n\n\narXiv preprint arXiv:2004.05150.\n\n\n",
      "Biderman et\u00a0al. (2023)": "\nBiderman et\u00a0al. (2023)\n\nStella Biderman, Hailey Schoelkopf, Quentin\u00a0Gregory Anthony, Herbie Bradley, Kyle O\u2019Brien, Eric Hallahan, Mohammad\u00a0Aflah Khan, Shivanshu Purohit, USVSN\u00a0Sai Prashanth, Edward Raff, et\u00a0al. 2023.\n\n\nPythia: A suite for analyzing large language models across training and scaling.\n\n\nIn International Conference on Machine Learning, pages 2397\u20132430. PMLR.\n\n\n",
      "Brown et\u00a0al. (2020)": "\nBrown et\u00a0al. (2020)\n\nTom\u00a0B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel\u00a0M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020.\n\n\nLanguage models are few-shot learners.\n\n\nCoRR, abs/2005.14165.\n\n\n",
      "Bubeck et\u00a0al. (2023)": "\nBubeck et\u00a0al. (2023)\n\nS\u00e9bastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin\u00a0Tat Lee, Yuanzhi Li, Scott Lundberg, et\u00a0al. 2023.\n\n\nSparks of artificial general intelligence: Early experiments with gpt-4.\n\n\narXiv preprint arXiv:2303.12712.\n\n\n",
      "Chen et\u00a0al. (2021)": "\nChen et\u00a0al. (2021)\n\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de\u00a0Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et\u00a0al. 2021.\n\n\nEvaluating large language models trained on code.\n\n\narXiv preprint arXiv:2107.03374.\n\n\n",
      "Chen et\u00a0al. (2023a)": "\nChen et\u00a0al. (2023a)\n\nShouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian. 2023a.\n\n\nExtending context window of large language models via positional interpolation.\n\n\narXiv preprint arXiv:2306.15595.\n\n\n",
      "Chen et\u00a0al. (2023b)": "\nChen et\u00a0al. (2023b)\n\nShouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian. 2023b.\n\n\nExtending context window of large language models via positional interpolation.\n\n\nArXiv, abs/2306.15595.\n\n\n",
      "Dao (2023)": "\nDao (2023)\n\nTri Dao. 2023.\n\n\nFlashAttention-2: Faster attention with better parallelism and work partitioning.\n\n\n",
      "Dao et\u00a0al. (2022)": "\nDao et\u00a0al. (2022)\n\nTri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. 2022.\n\n\nFlashattention: Fast and memory-efficient exact attention with io-awareness.\n\n\nAdvances in Neural Information Processing Systems, 35:16344\u201316359.\n\n\n",
      "Dao et\u00a0al. (2023)": "\nDao et\u00a0al. (2023)\n\nTri Dao, Daniel Haziza, Francisco Massa, and Grigory Sizov. 2023.\n\n\nFlash-decoding for long-context inference.\n\n\n",
      "Dasigi et\u00a0al. (2021)": "\nDasigi et\u00a0al. (2021)\n\nPradeep Dasigi, Kyle Lo, Iz\u00a0Beltagy, Arman Cohan, Noah\u00a0A. Smith, and Matt Gardner. 2021.\n\n\nA dataset of information-seeking questions and answers anchored in research papers.\n\n\nArXiv, abs/2105.03011.\n\n\n",
      "Dong et\u00a0al. (2023)": "\nDong et\u00a0al. (2023)\n\nZican Dong, Tianyi Tang, Lunyi Li, and Wayne\u00a0Xin Zhao. 2023.\n\n\nA survey on long text modeling with transformers.\n\n\narXiv preprint arXiv:2302.14502.\n\n\n",
      "Han et\u00a0al. (2023)": "\nHan et\u00a0al. (2023)\n\nChi Han, Qifan Wang, Wenhan Xiong, Yu\u00a0Chen, Heng Ji, and Sinong Wang. 2023.\n\n\nLm-infinite: Simple on-the-fly length generalization for large language models.\n\n\nArXiv, abs/2308.16137.\n\n\n",
      "Han et\u00a0al. (2021)": "\nHan et\u00a0al. (2021)\n\nXu\u00a0Han, Zhengyan Zhang, Ning Ding, Yuxian Gu, Xiao Liu, Yuqi Huo, Jiezhong Qiu, Yuan Yao, Ao\u00a0Zhang, Liang Zhang, et\u00a0al. 2021.\n\n\nPre-trained models: Past, present and future.\n\n\nAI Open, 2:225\u2013250.\n\n\n",
      "Hochreiter and Schmidhuber (1997)": "\nHochreiter and Schmidhuber (1997)\n\nSepp Hochreiter and J\u00fcrgen Schmidhuber. 1997.\n\n\nLong short-term memory.\n\n\nNeural Comput., 9(8):1735\u20131780.\n\n\n",
      "Hong et\u00a0al. (2023)": "\nHong et\u00a0al. (2023)\n\nKe\u00a0Hong, Guohao Dai, Jiaming Xu, Qiuli Mao, Xiuhong Li, Jun Liu, Kangdi Chen, Yuhan Dong, and Yu\u00a0Wang. 2023.\n\n\nFlashdecoding++: Faster large language model inference on gpus.\n\n\n",
      "Huang et\u00a0al. (2021)": "\nHuang et\u00a0al. (2021)\n\nLuyang\u00a0Robby Huang, Shuyang Cao, Nikolaus\u00a0Nova Parulian, Heng Ji, and Lu\u00a0Wang. 2021.\n\n\nEfficient attentions for long document summarization.\n\n\nArXiv, abs/2104.02112.\n\n\n",
      "Huang et\u00a0al. (2023)": "\nHuang et\u00a0al. (2023)\n\nYunpeng Huang, Jingwei Xu, Zixu Jiang, Junyu Lai, Zenan Li, Yuan Yao, Taolue Chen, Lijuan Yang, Zhou Xin, and Xiaoxing Ma. 2023.\n\n\nAdvancing transformer architecture in long-context large language models: A comprehensive survey.\n\n\narXiv preprint arXiv:2311.12351.\n\n\n",
      "Jiang et\u00a0al. (2023)": "\nJiang et\u00a0al. (2023)\n\nAlbert\u00a0Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra\u00a0Singh Chaplot, Diego de\u00a0las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, L\u00e9lio\u00a0Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven\u00a0Le Scao, Thibaut Lavril, Thomas Wang, Timoth\u00e9e Lacroix, and William\u00a0El Sayed. 2023.\n\n\nMistral 7b.\n\n\n",
      "Joshi et\u00a0al. (2017)": "\nJoshi et\u00a0al. (2017)\n\nMandar Joshi, Eunsol Choi, Daniel\u00a0S. Weld, and Luke Zettlemoyer. 2017.\n\n\nTriviaqa: A large scale distantly supervised challenge dataset for reading comprehension.\n\n\nArXiv, abs/1705.03551.\n\n\n",
      "Kocisk\u00fd et\u00a0al. (2017)": "\nKocisk\u00fd et\u00a0al. (2017)\n\nTom\u00e1s Kocisk\u00fd, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl\u00a0Moritz Hermann, G\u00e1bor Melis, and Edward Grefenstette. 2017.\n\n\nThe narrativeqa reading comprehension challenge.\n\n\nTransactions of the Association for Computational Linguistics, 6:317\u2013328.\n\n\n",
      "Ko\u010disk\u1ef3 et\u00a0al. (2018)": "\nKo\u010disk\u1ef3 et\u00a0al. (2018)\n\nTom\u00e1\u0161 Ko\u010disk\u1ef3, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl\u00a0Moritz Hermann, G\u00e1bor Melis, and Edward Grefenstette. 2018.\n\n\nThe narrativeqa reading comprehension challenge.\n\n\nTransactions of the Association for Computational Linguistics, 6:317\u2013328.\n\n\n",
      "Kwon et\u00a0al. (2023)": "\nKwon et\u00a0al. (2023)\n\nWoosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody\u00a0Hao Yu, Joseph\u00a0E. Gonzalez, Hao Zhang, and Ion Stoica. 2023.\n\n\nEfficient memory management for large language model serving with pagedattention.\n\n\nIn Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles.\n\n\n",
      "Li et\u00a0al. (2023)": "\nLi et\u00a0al. (2023)\n\nJiaqi Li, Mengmeng Wang, Zilong Zheng, and Muhan Zhang. 2023.\n\n\nLoogle: Can long-context language models understand long contexts?\n\nArXiv, abs/2311.04939.\n\n\n",
      "Lin (2004)": "\nLin (2004)\n\nChin-Yew Lin. 2004.\n\n\nROUGE: A package for automatic evaluation of summaries.\n\n\nIn Text Summarization Branches Out, pages 74\u201381, Barcelona, Spain. Association for Computational Linguistics.\n\n\n",
      "Liu et\u00a0al. (2023)": "\nLiu et\u00a0al. (2023)\n\nNelson\u00a0F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. 2023.\n\n\nLost in the middle: How language models use long contexts.\n\n\n",
      "Mohtashami and Jaggi (2023)": "\nMohtashami and Jaggi (2023)\n\nAmirkeivan Mohtashami and Martin Jaggi. 2023.\n\n\nLandmark attention: Random-access infinite context length for transformers.\n\n\nArXiv, abs/2305.16300.\n\n\n",
      "Nakano et\u00a0al. (2021)": "\nNakano et\u00a0al. (2021)\n\nReiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et\u00a0al. 2021.\n\n\nWebgpt: Browser-assisted question-answering with human feedback.\n\n\narXiv preprint arXiv:2112.09332.\n\n\n",
      "Narayanan et\u00a0al. (2021)": "\nNarayanan et\u00a0al. (2021)\n\nDeepak Narayanan, Mohammad Shoeybi, Jared Casper, Patrick LeGresley, Mostofa Patwary, Vijay Korthikanti, Dmitri Vainbrand, Prethvi Kashinkunti, Julie Bernauer, Bryan Catanzaro, et\u00a0al. 2021.\n\n\nEfficient large-scale language model training on gpu clusters using megatron-lm.\n\n\nIn Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis, pages 1\u201315.\n\n\n",
      "OpenAI (2023a)": "\nOpenAI (2023a)\n\nOpenAI. 2023a.\n\n\nGpt-4 technical report.\n\n\nArXiv, abs/2303.08774.\n\n\n",
      "OpenAI (2023b)": "\nOpenAI (2023b)\n\nOpenAI. 2023b.\n\n\nGpt-4 turbo.\n\n\n",
      "OpenAI (2023c)": "\nOpenAI (2023c)\n\nOpenAI. 2023c.\n\n\nTiktoken.\n\n\n",
      "Penedo et\u00a0al. (2023)": "\nPenedo et\u00a0al. (2023)\n\nGuilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay. 2023.\n\n\nThe refinedweb dataset for falcon llm: outperforming curated corpora with web data, and web data only.\n\n\narXiv preprint arXiv:2306.01116.\n\n\n",
      "Peng et\u00a0al. (2023a)": "\nPeng et\u00a0al. (2023a)\n\nBo\u00a0Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Huanqi Cao, Xin Cheng, Michael Chung, Matteo Grella, Kranthi\u00a0Kiran GV, Xuzheng He, Haowen Hou, Przemyslaw Kazienko, Jan Kocon, Jiaming Kong, Bartlomiej Koptyra, Hayden Lau, Krishna Sri\u00a0Ipsit Mantri, Ferdinand Mom, Atsushi Saito, Xiangru Tang, Bolun Wang, Johan\u00a0S. Wind, Stansilaw Wozniak, Ruichong Zhang, Zhenyuan Zhang, Qihang Zhao, Peng Zhou, Jian Zhu, and Rui-Jie Zhu. 2023a.\n\n\nRwkv: Reinventing rnns for the transformer era.\n\n\n",
      "Peng et\u00a0al. (2023b)": "\nPeng et\u00a0al. (2023b)\n\nBowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. 2023b.\n\n\nYarn: Efficient context window extension of large language models.\n\n\n",
      "Press et\u00a0al. (2021)": "\nPress et\u00a0al. (2021)\n\nOfir Press, Noah\u00a0A Smith, and Mike Lewis. 2021.\n\n\nTrain short, test long: Attention with linear biases enables input length extrapolation.\n\n\narXiv preprint arXiv:2108.12409.\n\n\n",
      "Qiu et\u00a0al. (2020)": "\nQiu et\u00a0al. (2020)\n\nXipeng Qiu, Tianxiang Sun, Yige Xu, Yunfan Shao, Ning Dai, and Xuanjing Huang. 2020.\n\n\nPre-trained models for natural language processing: A survey.\n\n\nScience China Technological Sciences, 63(10):1872\u20131897.\n\n\n",
      "Shazeer (2019)": "\nShazeer (2019)\n\nNoam\u00a0M. Shazeer. 2019.\n\n\nFast transformer decoding: One write-head is all you need.\n\n\nArXiv, abs/1911.02150.\n\n\n",
      "Shoeybi et\u00a0al. (2019)": "\nShoeybi et\u00a0al. (2019)\n\nMohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. 2019.\n\n\nMegatron-lm: Training multi-billion parameter language models using model parallelism.\n\n\nArXiv, abs/1909.08053.\n\n\n",
      "Su et\u00a0al. (2023)": "\nSu et\u00a0al. (2023)\n\nJianlin Su, Murtadha Ahmed, Yu\u00a0Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. 2023.\n\n\nRoformer: Enhanced transformer with rotary position embedding.\n\n\nNeurocomputing, page 127063.\n\n\n",
      "Sun et\u00a0al. (2022)": "\nSun et\u00a0al. (2022)\n\nYutao Sun, Li\u00a0Dong, Barun Patra, Shuming Ma, Shaohan Huang, Alon Benhaim, Vishrav Chaudhary, Xia Song, and Furu Wei. 2022.\n\n\nA length-extrapolatable transformer.\n\n\narXiv preprint arXiv:2212.10554.\n\n\n",
      "Tay et\u00a0al. (2020)": "\nTay et\u00a0al. (2020)\n\nYi\u00a0Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler. 2020.\n\n\nLong range arena: A benchmark for efficient transformers.\n\n\n",
      "Touvron et\u00a0al. (2023)": "\nTouvron et\u00a0al. (2023)\n\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et\u00a0al. 2023.\n\n\nLlama: Open and efficient foundation language models.\n\n\narXiv preprint arXiv:2302.13971.\n\n\n",
      "Vaswani et\u00a0al. (2017)": "\nVaswani et\u00a0al. (2017)\n\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan\u00a0N Gomez, \u0141\u00a0ukasz Kaiser, and Illia Polosukhin. 2017.\n\n\nAttention is all you need.\n\n\nIn Advances in Neural Information Processing Systems, volume\u00a030. Curran Associates, Inc.\n\n\n",
      "Xiao et\u00a0al. (2023)": "\nXiao et\u00a0al. (2023)\n\nGuangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. 2023.\n\n\nEfficient streaming language models with attention sinks.\n\n\nArXiv, abs/2309.17453.\n\n\n",
      "Yang et\u00a0al. (2018)": "\nYang et\u00a0al. (2018)\n\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William\u00a0W. Cohen, Ruslan Salakhutdinov, and Christopher\u00a0D. Manning. 2018.\n\n\nHotpotqa: A dataset for diverse, explainable multi-hop question answering.\n\n\nIn Conference on Empirical Methods in Natural Language Processing.\n\n\n",
      "Zhu et\u00a0al. (2023)": "\nZhu et\u00a0al. (2023)\n\nDawei Zhu, Nan Yang, Liang Wang, Yifan Song, Wenhao Wu, Furu Wei, and Sujian Li. 2023.\n\n\nPose: Efficient context window extension of llms via positional skip-wise training.\n\n\narXiv preprint arXiv:2309.10400.\n\n\n"
    },
    "domain": "cs.CL",
    "citation_count": 0
  },
  "7d932b37-246c-4b71-b591-40ca7c0cfc09": {
    "pk": "7d932b37-246c-4b71-b591-40ca7c0cfc09",
    "authors": [
      "Jiaqi Li",
      "Mengmeng Wang",
      "Zilong Zheng",
      "Muhan Zhang"
    ],
    "title": "LooGLE: Can Long-Context Language Models Understand Long Contexts?",
    "abstract": "Large language models (LLMs), despite their impressive performance in various language tasks, are typically limited to processing texts within context-window size. This limitation has spurred significant research efforts to enhance LLMs' long-context understanding with high-quality long-sequence benchmarks. However, prior datasets in this regard suffer from shortcomings, such as short context length compared to the context window of modern LLMs; outdated documents that have data leakage problems; and an emphasis on short dependency tasks rather than long dependency tasks. In this paper, we present LooGLE, a Long Context Generic Language Evaluation benchmark for LLMs' long context understanding. LooGLE features relatively new documents post-2022, with over 24,000 tokens per document and 6,000 newly generated questions spanning diverse domains. Human annotators meticulously crafted more than 1,100 high-quality question-answer pairs to meet the long dependency requirements. These pairs underwent thorough cross-validation, yielding the most precise assessment of LLMs' long dependency capabilities. The evaluation of eight state-of-the-art LLMs on LooGLE revealed key findings: (i) commercial models outperformed open-sourced models; (ii) LLMs excelled in short dependency tasks like short question-answering and cloze tasks but struggled with more intricate long dependency tasks; (iii) in-context learning and chaining thoughts offered only marginal improvements; (iv) retrieval-based techniques demonstrated substantial benefits for short question-answering, while strategies for extending context window length had limited impact on long context understanding. As such, LooGLE not only provides a systematic and comprehensive evaluation schema on long-context LLMs, but also sheds light on future development of enhanced models towards \"true long-context understanding\".",
    "url": "http://arxiv.org/abs/2311.04939v2",
    "timestamp": 1699407937,
    "sections": {
      "1 Introduction": "\n\n1 Introduction\n\nThe pursuit of enabling large language models (LLMs), such as ChatGPT\u00a0(Brown et\u00a0al., 2020; OpenAI, 2023; Zeng et\u00a0al., 2023), to go beyond their limited context window size so as to process, comprehend, or even learn from long-context textual information\u00a0(Ding et\u00a0al., 2023; Dao et\u00a0al., 2022; Chi et\u00a0al., 2023; Bulatov et\u00a0al., 2023) is inevitable for next-generation of language intelligence attributed to its wide applications on real-world scenarios, such as domain-specific knowledge understanding, long-context conversational generation, long story or code generation, etc.\n\n\nMeanwhile, there is an increasing need for high-quality benchmarks with much longer text lengths and more challenging tasks to provide comprehensive evaluations. However, traditional benchmarks\u00a0(Cohan et\u00a0al., 2018; Sharma et\u00a0al., 2019; Huang et\u00a0al., 2021) often fall short in text length with an average number of thousand words\u00a0(s\u00a0Ko\u02c7cisk\u00fd et\u00a0al., 2018; Yang et\u00a0al., 2018). Besides, existing benchmarks automatically collect possibly outdated documents from existing datasets published a few years ago\u00a0(Shaham et\u00a0al., 2022; Trivedi et\u00a0al., 2022; Wang et\u00a0al., 2022; Angelidis et\u00a0al., 2020), which might lead to data leakage in pre-trained LLMs and make the evaluation inaccurate. Further, the long texts are often restricted to domain-specific articles, making it hard to evaluate LLMs\u2019 ability on generic tasks and domains. Finally, it is important to note that tasks in existing benchmarks are primarily short dependency tasks, which only require LLMs to retrieve answers from one specific sentence or paragraph, without really testing LLMs\u2019 ability to collect pieces of information from paragraphs across the whole document and summarize them into an answer, which we call long dependency tasks.\n\n\nTable 1: Comparison with other long-context benchmarks.\n\n\n\nDataset\nAvg. Words\n# of Docs.\n# of Ques.\nManually Label\nLong Dependency Tasks\n\n\nSummarization\nInfo. Retrieval\nTimeline Reorder\nComputation\nDoc QA\n\n\nZero Scrolls\u00a0(Shaham et\u00a0al., 2023)\n\n10,392\n-\n4,378\n\u2717\n\u2713\n\u2717\n\u2717\n\u2717\n\u2717\n\n\nLong Bench\u00a0(Bai et\u00a0al., 2023)\n\n8,120\n-\n4,750\n350\n\u2713\n\u2713\u2217\n\n\u2717\n\u2713\u2217\n\n\u2713\n\n\nL-Eval\u00a0(An et\u00a0al., 2023)\n\n8,008\n411\n2,043\n2,043\u2020\n\n\u2713\n\u2713\n\u2717\n\u2717\n\u2713\n\n\n\nLooGLE\n\u00a0(Ours)\n19,367\n776\n6,448\n1,101\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\n\n\n\u2217 The task is created in a synthetic manner.\n\n\n\n\u2020 The questions are re-labelled from original data.\n\n\n\n\n\nTo mitigate the shortcomings of existing datasets, in this paper, we introduce a novel benchmark LooGLE\n, short for Long Context Generic Language Evaluation, to evaluate the long context understanding abilities of LLMs illustrated in Fig.\u00a01. Our benchmark has the following advantages:\n\n\n\u2022\n\nExtra-long realistic documents. It contains 776 latest gathered and extremely long documents with an average of 19.3k words. There are over 6,448 test instances without distribution bias for a more generalized assessment, many of which exceed 100k words. On one hand, they can better evaluate LLMs\u2019 capability on memorizing and understanding longer text that is far beyond their context window size. On the other hand, the excessive length is well suited to the common usage of long text scenarios.\n\n\n\n\u2022\n\nManually designed both short and long dependency tasks. It is composed of 7 major tasks to evaluate LLMs\u2019 ability to understand both short and long dependency content. We refer \u201clong dependency\u201d tasks as those that require the understanding of the inter-dependency across multiple evidence widely spanning over the entire long text. We delicately design 5 types of long dependency tasks and recruited a group of human annotators to manually create 1101 long dependency Question-Answer (QA) instances, despite the high costs and huge effort involved in this process.\n\n\n\n\u2022\n\nRelatively new documents. Our benchmark comprises texts all published after 2022 which ensures that most modern LLMs (at the date of submission) have not been pre-trained on these documents, forcing them to rely on their in-context learning ability rather than memorization. In contrast, existing benchmarks are usually a combination of content from traditional NLP dataset, whose world knowledge may have already been learned by LLMs and thus are less convincing for assessment. Furthermore, our data collection process is fully open-sourced, making it easy for the community to reconstruct/update the benchmark with newer documents, possibly on a yearly basis.\n\n\n\n\u2022\n\nCross-domain generic data. Our benchmark is derived from popular open-source documents, including arXiv papers, Wikipedia articles, and movie and TV scripts, spanning diverse domains and multiple categories such as academia, history, sports, politics, arts, events, and entertainment.\n\n\n\n\n\nWe conduct a comprehensive evaluation of 8 representative LLMs on LooGLE\n. We specifically select LLMs which have made great effort in addressing the challenge of understanding long contexts as the baselines. The results indicate that better base models with a larger context window size generally achieve better performance. However, all models experience a significant performance decline in long dependency tasks, indicating there is a desperate need to improve the true long dependency understanding capabilities of LLMs. Our dataset serves as an up-to-date benchmark for cutting-edge assessment and research on the long context understanding and modeling of LLMs.\n\n",
      "2 Related Work": "\n\n2 Related Work\n\nExisting models for long context understanding.\n\nThere are increasing research interests in developing methods to extend LLMs\u2019 context window size, such as utilizing\nrecurrent memory, sparse attention\u00a0(Meister et\u00a0al., 2021), external memory and etc.(Chen et\u00a0al., 2023c; Xiong et\u00a0al., 2023; Li et\u00a0al., 2023a). The most popular way is to develop improved transformer architectures\u00a0Dong et\u00a0al. (2023). Efficient transformers\u00a0(Tay et\u00a0al., 2020; 2022) are proposed to decrease the memory and time complexity to efficiently model longer texts. Unlike efficient transformers that simplify the attention structure, recurrent transformer\u00a0(Bulatov et\u00a0al., 2022; Bessonov et\u00a0al., 2023) keeps the full self-attention mechanism. History information of previous segments is cached and will be leveraged when the subsequent segment is fed into the model without a context fragmentation problem. Fine-tuned models on long documents\u00a0Wu et\u00a0al. (2021) are also explored, but they are often effort-costing and face difficulties in collecting ground truth fine-tuning data for long text tasks. Apart from approaches which are developed from modeling and parameter updating aspects, there are also works incorporating external memory structures and compression techniques for LLMs or using task-oriented process optimization strategies\u00a0(Gidiotis & Tsoumakas, 2020; Zhou et\u00a0al., 2022; Ram et\u00a0al., 2023; Izacard et\u00a0al., 2022).\n\n\n\nExisting datasets for long context understanding.\n\nThere are a growing number of benchmarks proposed to test LLMs\u2019 long context understanding ability\u00a0(Shaham et\u00a0al., 2023; Li, 2023). ZeroSCROLLS, L-Eval and LongBench are the three most recent ones. ZeroSCROLLS\u00a0(Shaham et\u00a0al., 2023) automatically processes datasets from different sources into a unified input format with an average of 10k words. However, it mainly focuses on collecting documents and tasks from existing datasets and relies on automatic metrics for limited model comparisons\u00a0(Shaham et\u00a0al., 2022). L-Eval\u00a0(An et\u00a0al., 2023) differs in re-annotating the data and instructions from similar public datasets with smaller sizes to ensure the quality. Besides, it optimizes the evaluation procedures and baselines to get more accurate conclusions. LongBench\u00a0(Bai et\u00a0al., 2023) provides a bilingual and multi-task dataset featuring diverse sequences of varying lengths, distributions, patterns, languages and domains for a comprehensive evaluation of long context understanding. Nonetheless, it encompasses texts of only thousands of words and tasks mostly restricted to short-term information extraction. Moreover, there are few types of \u201clong dependency\u201d tasks in previous datasets, except for summarization (which LLMs are validated to perform well on) and synthesized tasks like data aggregation and retrieving. To finish those tasks, LLMs solely need to locate pieces of information from the lengthy source input and aggregate them together. In contrast, we propose LooGLE\n which contains long dependency tasks that are much more challenging, such as event timeline reordering, comprehension/reasoning, and computation. These tasks require not only information retrieval but also understanding/reasoning over the entire text. We include a detailed comparison with concurrent works in Tab.\u00a01.\n\n\n",
      "3 The LooGLE Benchmark": "\n\n3 The LooGLE\n Benchmark\n\nThere are three categories of data sources as mentioned in Tab.\u00a02. Based on that, we generate two main types of tasks: short dependency and long dependency tasks in LooGLE\n. For short dependency tasks, we generate short QA from Wikipedia articles and cloze from scripts. For the long dependency tasks, we include summarization for arXiv papers and manually designed QA tasks for long document understanding. There are four major subtasks for QA: Multiple information retrieval, Timeline reorder, Computation, Comprehension and reasoning. We delicately generate tasks/questions to customize the intrinsic features of each data source for better long-context understanding assessments.\n\n\n\n3.1 Dataset selection and construction\n\nOur LooGLE\n benchmark consists of 3 sources: scientific papers, Wikipedia articles, movie and TV scripts, all covering various topics and categories. These documents are commonly used as corpora in NLP tasks. By replicating the methodology proposed in this paper, they can be collected easily and periodically. All the documents in our LooGLE\n benchmark are after 2022 and filtered by a length of over 10k words. We have also considered books, but found that most books meeting our principles are not license-free, therefore giving them up. Statistics of the three sources can be found in Tab.\u00a02. Details of the dataset are introduced in the following sections.\n\n\narXiv papers\n\u2003We pulled data from a massive pool of 10,000 entries on the arXiv website (https://arxiv.org/) using a random selection method. These entries ranged from January 2022 to April 2023. In the next step, we extracted their abstracts, making them our main source for the summarization task. We were pretty rigorous about maintaining data quality. That meant ditching the reference sections, cleaning up any garbled characters from math equations, and leaving out any documents under 10,000 words. After all that thorough check, we ended up with a solid collection of 516 research papers.\n\n\n\n\n\nDataset\nNo. Docs\nAvg. Words\nMax. Words\nMin. Words\nAvg. Tokens\nTask\n# Questions\n\n\narXiv\n516\n16,988\n197,977\n10,204\n20,887\nSummarization\n516\n\n\nWikipedia\n105\n17,604\n46,250\n11,285\n21,017\nShort dependency QA\n1,951\n\n\nLong dependency QA\n459\n\n\n\n\n\nMovie &\n\nTV scripts\n \n155\n28,483\n62,752\n11,089\n36,412\nCloze\n2,880\n\n\nLong dependency QA\n642\n\n\n\nTable 2: Statistics of LooGLE\n\n\nWikipedia articles\n\u2003Wikipedia is a free and popular online encyclopedia that provides information and reference on a wide range of topics. Articles are created and edited collaboratively by volunteers from all around the world, making it a dynamic and constantly evolving resource. These Wikipedia articles are perfect for evaluating the long text reading, comprehension, summarization, and information retrieval abilities of LLMs. We first downloaded and parsed the most recent page articles present in .bz file format from the official website (https://dumps.wikimedia.org/). Then we kept the articles after 2022 with over 10k words utilizing a subset of the open-source Wikipedia dataset (202203.en) from Hugging Face (https://huggingface.co/datasets/wikipedia). Since some pages in the dump file probably no longer exist and are redirected to a relevant page, we only retain pages (exempt summary, citations and references) after redirection.\n\n\nMovie and TV scripts\n\u2003A movie or TV script typically contains essential information such as scene descriptions, action descriptions, and dialogues between characters. Scripts inherently encapsulate numerous events and facts in dialogue format, necessitating models to deeply comprehend contextual nuances. To comprehend the events unfolding within a dialogue, there is a high demand on reasoning ability, along with the ability to navigate shifts in perspective and grasp the viewpoints of the characters involved. Additionally, scripts are typically lengthy and challenging for LLMs with fixed context window sizes. All scripts are sourced from three websites (https://www.scriptslug.com, https://thescriptlab.com/, https://8flix.com), consisting of movies and TV shows released after 2022.\n\n\n\n\n3.2 Long dependency tasks\n\n\n3.2.1 Tasks definition\n\nSummarization\n\nWe directly use the abstract of each paper as the reference for generating summaries. The abstracts effectively capture the main content and key information of each paper.\n\n\n\nLong dependency QA\n\nOne highlight of our dataset is that we dedicated significant effort to manually compile about 1.1k true long dependency QA pairs. The construction process is detailed in the next section. We manually designed 4 long dependency tasks: Multiple information retrieval, Timeline reorder, Computation, Comprehension and reasoning. As we will show in the experiments, these tasks are pretty challenging, requiring more advanced capabilities for long context understanding. They are valuable for understanding the limitations of LLMs. Examples of the 4 types of long dependency QAs are shown in Fig.\u00a02.\n\n\n\n\n\u2022\n\nMultiple information retrieval: Quite different from traditional short-term retrieval tasks, there are usually multiple and diverse pieces of evidence throughout the entire text for one specific answer.\nThe task requires extensive information extraction from widely distributed segments within the lengthy text, followed by the aggregation of the evidence to derive the ultimate answer. The evidence is distinctly presented and can be directly located within the original sentences or sections of the text.\n\n\n\n\u2022\n\nComputation: Similar to the previous task, it firstly needs multiple information retrieval from a wide range of texts.\nA majority of the evidence within the text takes the form of numerical data, often in question formats such as inquiries about quantities, frequencies, durations, specific numbers, and so on. To arrive at an accurate response, a profound comprehension of the question and its correlation with the provided numerical data is essential. This process relies heavily on the capacity to grasp extensive contextual information and also involves a degree of mathematical reasoning ability.\n\n\n\n\u2022\n\nTimeline reorder:\nThis task follows a more conventional format, involving the instruction, \u201cPlease reorder the timeline of the following events,\u201d along with a set of events presented in a permuted order. The objective is to arrange these events in accordance with their chronological sequence as dispersed throughout the extensive text. The events are derived directly from the source text, either as extracted segments or summarized factual information. Successful completion of this task necessitates either the memorization or comprehensive understanding of the central storyline of the document and assesses the model\u2019s proficiency in temporal awareness.\n\n\nFigure 2: Long dependency QA tasks\n\n\n\n\u2022\n\nComprehension and reasoning:\nThis task demands not only a profound comprehension of the question but also intricate reasoning to discern the underlying implications for searching for the appropriate evidence. The most prevalent question patterns involve inquiries about causality, impact, contributions, attitudes, and the essential attributes related to various events. Additionally, more extensive comparisons and evaluations are essential when the questions revolve around the primary, predominant, highest, or most critical aspects of the evidence. Furthermore, the answers to this task are not explicitly evident within the source text. They often require multi-step reasoning to model the inherent connections and dependencies, facilitating the acquisition of the answer through a complex analytical process.\n\n\n\n\n\n\n\n\n3.2.2 Construction process of long dependency QAs\n\nWe detail the construction process as follows. We first randomly sampled a total of 140 long documents from Wikipedia and the scripts dataset. We recruited students from top universities across the nation and organized a manual annotation process to generate long dependency QAs. We categorize long dependency tasks into Multiple information retrieval, Comprehension and reasoning, Calculation, and Timeline reorder (illustrated in Fig.\u00a02). Each document spans from 10,000 to 20,000 words in average and requires a generation of 5 to 10 questions. Additionally, participants were prohibited from employing large language models and tools like ChatGPT for article reading, data generation, and annotation.\n\n\nIn the generation of questions, each document underwent a meticulous three-step process that involved the assignment of two distinct annotators \u2014 one serving as the questioner and the other as the answerer. Importantly, these annotators were kept unaware of each other\u2019s identities, ensuring a rigorous cross-validation process to maintain the quality of the questions, answers, and supporting evidence. This approach aimed to achieve questions with a high degree of accuracy, precision, and relevance to the document\u2019s content.\n\n\nStep 1: Question and answer.\nThe questioner\u2019s role encompassed a comprehensive set of responsibilities, including reading the document, crafting relevant questions, offering their own answers to those questions, and pinpointing the specific evidentiary passages within the document that substantiated their answers.\nThe annotation adhered to stringent standards, encompassing the following key principles:\n\n\n\u2022\n\nLong dependency: Each question was required to exhibit a long dependency, i.e., the evidence supporting its answer should have a wide span across the document. The recommended dependency length (the distance between the earliest and latest evidence) is a minimum of 5,000 words.\n\n\n\n\u2022\n\nDiverse problem types: The questioner was required to generate a set of 5 to 10 question-answer pairs for each document, which should not contain more than 4 questions of the same type to avoid imbalanced question distribution and prevent annotators from generating overly simple questions.\n\n\n\n\u2022\n\nClear and precise questions: The formulation of each question was asked to adhere to clarity, conciseness, and no ambiguity, with examples provided.\n\n\n\n\u2022\n\nDeterministic and objective answers: The answers to the proposed questions were rigorously checked to be deterministic and objective, precluding open-ended ones.\n\n\n\nStep 2: Answer and check. The second step involves the answerers. Each answerer can only access the assigned article text and the posed questions from the questioner in the first step. The answerer was required to thoroughly read the entire document and provide answers to the questions accordingly. The standard for the answers is the same as the questioners. In addition to the aforementioned responsibilities, the answerer was also tasked with assessing the quality of the questions, which entails evaluating whether the questions adhere to the standard and whether they are answerable. In instances where a question cannot elicit a definite and unambiguous answer, it is deemed as unsatisfactory, and the answerer is asked to provide constructive feedback for improvement.\n\n\nStep 3: Revise. In the third step, the questioner for the document had access to the document, the questions, the two sets of answers from both the questioner and the answerer, as well as the feedback from the answerer. The questioner was asked to first revise the questions according to the feedback, and then unify their own answers with those from the answerers to derive the final answers.\n\n\nIn the first step, we acquired a total of 1,137 question-answer pairs. In the second step, 206 of these pairs were identified as non-compliant with the established criteria and were accompanied by suggestions for improvement. The inter-annotator agreement rate is 81.88%\u00a0(Kim & Park, 2023). Following the revisions conducted in the third step, we ultimately obtained a total of 1101 high-quality long dependency question-answer pairs which require strong long context understanding ability.\n\n\n\n\n\n3.3 Short dependency tasks\n\nQuestion Answering (QA)\n\nTo generate short dependency QA pairs, we harnessed the robust language processing and comprehension capabilities of GPT3.5-turbo-16k. These short dependency QA pairs typically do not require extensive evidence retrieval and can be extracted from localized segments. We divided each article into multiple segments and employed an iterative approach to prompt the Language Model (LLM) to generate QA pairs based on these segments, including their associated supporting evidence from the article. Details of the prompts are available in Appendix\u00a0D. Subsequently, we conducted manual reviews of the QA pairs, making refinements to some of the answers by filtering out non-essential context and eliminating redundant descriptions. This rigorous curation process was undertaken to ensure the high quality and relevance of the resulting QA pairs.\n\n\n\nCloze\n\nInitially, each script is divided into segments of varying lengths. Then, we employ GPT3.5-turbo-16k to generate factual summaries aligning with the source segment along with some constraints included in prompts (see Appendix\u00a0D). Later, we employ BERT-large \u00a0(Devlin et\u00a0al., 2019) for Named Entity Recognition (NER)\u00a0(Roy, 2021) from the generated summaries, limiting the types to person name, location, and organization. Finally, we randomly select a certain number (no more than 5) of entities from the summary and mask them as placeholders, denoted as \u201c<mask-n>\u201d. The goal is to predict the masked entities according to the long context.\n\n\n\n",
      "4 Evaluation": "\n\n4 Evaluation\n\n\n4.1 Models Selected for Evaluation\n\nCommercial models\n\nGPT4-32k, GPT4-8k, GPT3.5-turbo-16k\u00a0(Chen et\u00a0al., 2023b; Ye et\u00a0al., 2023) are all the models developed by OpenAI, as documented on their official platform (https://platform.openai.com/docs/models). GPT4-32k can handle up to 32k tokens in the context input, and GPT4-8k and GPT3.5-turbo-16k can handle up to 8k and 16k context input, respectively. We use the models of version 0613 by default.\n\n\n\nOpen-source models\n\nLLaMA2-7B-32K\u00a0(Touvron et\u00a0al., 2023) is developed by Together (https://together.ai/) and fine-tuned from Meta\u2019s original Llama2-7B \u00a0(Touvron et\u00a0al., 2023) model. It has been expanded to accommodate a context length of 32K using Position Interpolation\u00a0(Chen et\u00a0al., 2023a). ChatGLM2-6B-32k\u00a0(Du et\u00a0al., 2022), is a product of THUMD and represents an enhancement of the ChatGLM2-6B model. It is notable for its integration of FlashAttention \u00a0(Dao et\u00a0al., 2022), allowing it to train with an extended context length, increased from 2K to 32K. LongLLaMa-3B, derived from openllama, has been fine-tuned using Focused Transformer \u00a0(Tworkowski et\u00a0al., 2023) to extend its context to 256k. Lastly, RWKV-4-14B-pile \u00a0(Peng et\u00a0al., 2023) is a member of the RWKV model family, notable for its architectural fusion of both Recurrent Neural Networks (RNN) and Transformers. It has been fine-tuned to accommodate a context length of 8K.\n\n\n\nRetrieval-based Method\n\nInstead of extending the context window size, retrieval-based context compression technique\u00a0(Xu et\u00a0al., 2023; Askari et\u00a0al., 2023) augments the LLM by incorporating external memory, allowing relevant information to be retrieved using a specific query. LlamaIndex (https://github.com/jerryjliu/llama_index) is a data framework designed for LLMs. It fulfills a dual role by constructing indices for document segments and functioning as an intermediary connecting LLM with data sources, which enables LlamaIndex to retrieve relevant data segments before they are input into the LLM, thereby enhancing the LLM\u2019s capacity to effectively handle lengthy text. In our experiment, we employed the default configuration of the LlamaIndex, with embedding model text-embedding-ada-002 (https://openai.com/blog/new-and-improved-embedding-model) and language model text-davinci-003\n\u00a0(Ouyang et\u00a0al., 2022).\n\n\nIt has been proved that, performance is often highest when relevant information occurs at the beginning or end of the input context, and significantly degrades when models must access relevant information in the middle of long contexts. (Liu et\u00a0al., 2023a). Therefore, we artificially truncate the input document to certain sizes (all not larger than the context window size of above mentioned models) by concatenating the head and tail of the input. For example, when we want to truncate a long document to 16k, we concatenate its head 8k tokens and tail 8k tokens before feeding it to an LLM.\n\n\nFigure 3: An overview performance of LLMs on LooGLE\n for long context understanding\n\n\n\n\n\n4.2 Evaluation methods and metrics\n\nAutomatic evaluation\n\nWe adopt several automatic evaluation metrics, which can be categorized into two types. Bleu, Rouge, Meteor Score and Bert Score\u00a0(Li et\u00a0al., 2023b; Mukherjee & Rahman, 2023) are widely used for generative tasks such as summarization and QA. They evaluate the matching between groundtruth and LLM answers mainly based on n-gram matching and semantic similarity. For Cloze, Exact Match and Partial Match\u00a0(Sharma et\u00a0al., 2023; Engelbach et\u00a0al., 2023) are employed in our evaluation. Exact Match entails the predicted entity and the groundtruth entity exactly match each other while Partial Match allows for fuzzy matching.\n\n\n\nGPT4-as-judgment\n\nMost automatic evaluation metrics are sensitive to semantic expression, output format, and length. Thus, these metrics alone might be insufficient for effectively comparing different models (some models might output answers in a style more similar to groundtruth). However, recent research has shown that the GPT4 evaluator exhibits high consistency with human evaluation and can serve as a reliable annotator to some extent\u00a0(Suri et\u00a0al., 2023; Liu et\u00a0al., 2023b; Zheng et\u00a0al., 2023). To provide a more comprehensive assessment of models, we utilize GPT4-8k as an LLM evaluator. For QA task, given one question and two answers provided by the groundtruth and the LLM\u2019s prediction, we ask GPT4-8k to judge whether the two answers are semantically the same or not. Then we calculate the accuracy that LLM answers match the groundtruth. For summarization task, given the predicted summary with the goundtruth, we ask LLM to give a score considering various factors for generation. The prompts implemented can be found in Appendix\u00a0D.\n\n\n\nHuman evaluation\n\nWe also include human evaluation for reference, where we manually check whether LLM\u2019s prediction matches the groundtruth.\n\n\nTable 3: Performance of the short dependency tasks\n\n\n\n\n\nShort dependency QA\nCloze\n\n\n\n\nModels\n\n\n\n\nContext\n\n\n\n\nBleu1\n\n\n\n\nBleu4\n\n\n\n\nRouge1\n\n\n\n\nRouge4\n\n\n\n\nRougeL\n\n\n\n\nMeteor_score\n\n\n\n\nBert_score\n\n\n\n\nGPT4_score\n\n\n\n\nExact Match\n\n\n\n\nPartial Match\n\n\n\n\nGPT4-32k\n32k\n24.61\n11.14\n61.80\n50.73\n60.75\n32.94\n78.72\n71.52\n70.5\n80.81\n\n\nGPT4-8k\n8K\n27.35\n14.38\n67.59\n56.01\n65.77\n38.56\n87.93\n53.99\n66.03\n76.62\n\n\nGPT3.5-turbo-16k\n16K\n22.67\n9.62\n62.56\n48.63\n60.66\n32.58\n87.04\n66.82\n54.64\n63.42\n\n\nLlamaIndex\n\\\n33.37\n21.43\n58.82\n42.93\n57.08\n37.17\n86.58\n59.61\n58.95\n66.86\n\n\nChatGLM2-6B\n32k\n14.29\n6.07\n20.50\n13.16\n20.36\n13.08\n87.28\n23.65\n0.05\n0.98\n\n\nLongLLaMa-3B\n256k\n1.37\n0.26\n26.97\n11.02\n26.10\n11.34\n71.65\n13.75\n-\n2.13\n\n\nRWKV-4-14B-pile\n8k\n0.80\n0.04\n21.7\n6.39\n20.64\n9.41\n70.42\n8.93\n-\n-\n\n\nLLaMA2-7B-32K\n32k\n0.18\n7.25*e-308\n1.86\n0.00\n1.86\n1.52\n61.53\n3.18\n-\n0.58\n\n\n\n\n\nTable 4: Performance of the long dependency tasks\n\n\n\n\n\nModels\n\n\n\n\nContext\n\n\n\n\nBleu1\n\n\n\n\nBleu4\n\n\n\n\nRouge1\n\n\n\n\nRouge4\n\n\n\n\nRougeL\n\n\n\n\nMeteor_score\n\n\n\n\nBert_score\n\n\n\n\nGPT4_score\n\n\n\n\narXiv paper summarization\n\n\n\n\n\n\n\n\n\n\n\nGPT4-32k\n32k\n24.50\n0.73\n27.15\n7.10\n24.25\n19.03\n84.04\n82.84\n\n\nGPT4-8k\n8k\n29.02\n2.09\n32.08\n11.11\n28.85\n22.64\n84.92\n85.42\n\n\nGPT3.5-turbo-16k\n16k\n28.70\n1.59\n32.04\n10.69\n28.89\n22.34\n84.82\n86.84\n\n\nLlamaIndex\n\\\n22.53\n0.63\n26.28\n6.97\n23.73\n21.07\n83.09\n76.35\n\n\nChatGLM2-6B\n32k\n0.04\n1.60e-310\n5.97\n8.43e-05\n5.82\n6.40\n73.25\n13.23\n\n\nLongLLaMa-3B\n256k\n4.24\n9.32e-309\n4.10\n0.52\n3.86\n3.82\n73.41\n12.28\n\n\nRWKV-4-14B-pile\n8k\n6.28\n4.58e-05\n6.45\n0.74\n6.01\n6.00\n75.28\n7.02\n\n\nLLaMA2-7B-32K\n32k\n0.03\n4.66e-310\n0.12\n0.00\n0.12\n0.67\n71.21\n7.60\n\n\nLong dependency QA\n\n\n\n\n\n\n\n\n\n\n\nGPT4-32k\n32k\n8.55\n1.40\n25.59\n6.36\n24.04\n11.13\n80.16\n54.09\n\n\nGPT4-8k\n8k\n8.94\n1.01\n23.45\n6.57\n21.69\n10.18\n85.36\n42.12\n\n\nGPT3.5-turbo-16k\n16k\n6.92\n1.81\n25.02\n6.68\n23.63\n10.40\n83.79\n45.04\n\n\nLlamaIndex\n\\\n7.76\n1.24\n23.62\n7.10\n22.30\n10.47\n83.87\n37.63\n\n\nChatGLM2-6B\n32k\n5.55\n0.11\n9.41\n1.93\n8.69\n4.39\n85.78\n11.50\n\n\nLongLLaMa-3B\n256k\n1.04\n3.12e-307\n2.96\n0.03\n2.71\n1.66\n78.60\n6.48\n\n\nRWKV-4-14B-pile\n8k\n0.71\n9.52e-307\n18.54\n1.55\n17.69\n3.45\n71.36\n5.33\n\n\nLLaMA2-7B-32K\n32k\n0.08\n2.44e-308\n2.05\n0.00\n2.05\n0.46\n50.28\n4.18\n\n\n\n\n\n\n\n\n4.3 Results\n\nFig.\u00a03 shows an overall performance comparison of different models on different tasks. The first radar plot shows the original accuracy evaluated by GPT4-8k (except cloze) and the partial match result (for cloze) over different tasks. For better visualization, we scale the scores of all models on each task to [40, 100] in the second radar plot and the histogram, so that the best model on each task has a score of 100 and the worst model has a score of 40. From the charts, GPT4-32k demonstrates its impressive overall performance across all tasks (with highest scores on all tasks except summarization). In comparison, open-source models show a significant performance gap to commercial models on our benchmark. From the first radar chart, we can find that among the 7 major tasks, short QA, cloze and summarization are more effectively addressed by LLMs, while real long dependency QA tasks are far from being solved, where even GPT4-32k hardly achieves over 40% accuracy. The empirical results demonstrate that even the most successful commercial model still cannot effectively address those really challenging long dependency tasks, leaving large room for improvement. Detailed evaluation results and further analysis can be found in the following sections.\n\n\n\n4.3.1 Main results on long and short dependency tasks\n\nResults on short dependency tasks\u00a0\n\nTab.\u00a03 presents the performance (%) of all the baselines on LooGLE\n in short dependency tasks.\nNotably, GPT4-32k attains the highest accuracy according to the GPT4 evaluator\u2019s perspective. GPT4-8k, GPT3.5-turbo-16k, and the retrieval-based LlamaIndex closely follow, demonstrating competitive performance levels. Surprisingly, GPT4-8k exhibits the most robust overall performance in terms of automatic evaluation metrics. It\u2019s worth mentioning that GPT4-32k, due to its tendency to generate longer outputs, faces penalties from these automatic metrics. This discrepancy among different metrics highlights the need for improved evaluation methods. Furthermore, in the context of cloze tasks, GPT4-32k excels again when equipped with a longer context window. In Fig.\u00a05, the exact match results in cloze tasks are displayed for varying source segment lengths. The results show that as the segment length increases, model performance gradually decreases, underscoring the increasing difficulty of effectively filling in the masked entities with longer source text.\n\n\n\nResults on long dependency tasks\u00a0\n\nTab.\u00a04 shows the aggregated results on long dependency tasks. Firstly, we can observe that summarization can be well addressed by commercial models, with GPT-4 evaluation accuracy of over 80%. However, the various types of long dependency QAs in our benchmark apparently pose substantial challenges for current LLMs. Both open-source and commercial models experience a significant performance decline. We will analyze model performance on individual types of QAs in Section\u00a04.3.2.\nIt is validated that longer context window size (thus less information loss due to truncation) indeed helps in long context tasks by comparing GPT4-32k with GPT4-8k. GPT4-8k has a much lower accuracy by answering \u201cThe text does not provide information on \u2026\u201d in many cases. Open-sourced models fall far below the average of commercial models, among which LLaMA2-7B-32K and RWKV-4-14B-pile display almost zero performance. By employing context scaling techniques like positional interpolation, RNN and fine-tuning on longer texts, current LLMs can be equipped with much longer context windows than their default limits. Nevertheless, our results show that there is still a huge discrepancy between merely increasing the context window size and really understanding the long context. The poor performance on long dependency QAs suggests that we may need to revisit LLMs\u2019 long context understanding ability in more challenging tasks other than some simple ones like summarization and retrieval, as they are unable to test whether LLMs understand the inter-dependency in long texts. \n\n\nTable 5: Impact of input length on long dependency tasks\n\n\n\n\n\nModels\n\n\n\n\nContext\n\n\n\n\nBleu1\n\n\n\n\nBleu4\n\n\n\n\nRouge1\n\n\n\n\nRouge4\n\n\n\n\nRougeL\n\n\n\n\nMeteor_score\n\n\n\n\nBert_score\n\n\n\n\nGPT4_score\n\n\n\n\narXiv paper summarization\n\n\n\n\nGPT4-32k\n32k\n24.50\n0.73\n27.15\n7.10\n24.25\n19.03\n84.04\n82.84\n\n\nGPT4-32k\n24k\n25.57\n0.81\n27.61\n7.53\n24.73\n19.86\n84.07\n83.15\n\n\nGPT4-32k\n16k\n24.80\n0.70\n27.29\n7.26\n24.28\n19.12\n84.11\n82.82\n\n\nGPT4-32k\n8k\n26.26\n9.35\n27.83\n7.67\n24.74\n20.08\n84.10\n82.75\n\n\nGPT4-8k\n8k\n29.02\n2.09\n32.08\n11.11\n28.85\n22.64\n84.92\n85.42\n\n\nLong dependency QA\n\n\n\n\nGPT4-32k\n32k\n7.64\n1.24\n15.53\n4.46\n14.60\n11.12\n86.07\n54.65\n\n\nGPT4-32k\n24k\n8.23\n1.66\n14.92\n4.12\n13.90\n10.60\n86.16\n50.61\n\n\nGPT4-32k\n16k\n8.57\n1.35\n16.21\n4.30\n14.90\n11.91\n86.36\n47.55\n\n\nGPT4-32k\n8k\n7.46\n1.77\n13.75\n5.08\n12.89\n10.01\n85.77\n38.34\n\n\nGPT4-8k\n8k\n8.94\n1.01\n23.45\n6.57\n21.69\n10.18\n85.36\n42.12\n\n\n\n\n\nTable 6: Performance of the long dependency QA with LlamaIndex\n\n\n\n\n\nModels\n\n\n\n\nContext\n\n\n\n\nBleu1\n\n\n\n\nBleu4\n\n\n\n\nRouge1\n\n\n\n\nRouge4\n\n\n\n\nRougeL\n\n\n\n\nMeteor_score\n\n\n\n\nBert_score\n\n\n\n\nGPT4_score\n\n\n\n\nGPT4-32k\n32k\n6.08\n1.31\n10.27\n3.39\n9.52\n8.54\n85.27\n28.25\n\n\nGPT4-8k\n8k\n6.62\n1.50\n11.95\n3.80\n10.99\n9.02\n85.51\n26.34\n\n\nGPT3.5-turbo-16k\n16k\n6.50\n0.92\n10.93\n3.56\n9.86\n8.65\n85.63\n33.24\n\n\nDefault\n\\\n7.02\n1.24\n11.60\n3.75\n10.57\n9.37\n85.61\n33.16\n\n\nChatGLM2-6B-32k\n32k\n0.15\n2.82e-310\n2.23\n0.05\n2.23\n0.74\n83.40\n7.73\n\n\nLongLLaMa-3B\n256k\n1.04\n4.22e-311\n2.27\n0.00\n2.23\n2.23\n82.18\n5.33\n\n\nRWKV-4-14B-pile\n8k\n2.65\n8.09e-307\n4.08\n0.33\n3.65\n3.92\n80.74\n2.43\n\n\nLLaMA2-7B-32K\n32k\n0.43\n1.29e-307\n5.85\n0.00\n5.85\n1.04\n81.38\n6.76\n\n\n\n\n\n\n\n\n4.3.2 Deep dive into long context understanding capabilities\n\nIn this section, we analyze different factors affecting the long context understanding abilities of LLMs, and dive into individual types of long dependency QAs to check LLMs\u2019 limitations.\n\n\nImpact of varying input length\u00a0\n\nIn Tab.\u00a05, we study the impact of varying lengths of inputs on long dependency tasks with GPT4 models. We find that expanding input length hardly helps in paper summarization while it substantially enhances the model\u2019s performance on long dependency QAs. The difference can be attributed to the inherent nature of the arXiv paper. It has both the introduction and conclusion sections located at the beginning and in the end respectively, which already contain the major sketch of the paper. Meanwhile, in our expectation, longer input promotes the performance of long dependency QAs by introducing less information loss.\n\n\n\nRetrieval Based Techniques\u00a0\n\nTo evaluate the effectiveness of retrieval techniques for long-context dependency questions, we undertook an extensive series of experiments on our long dependency QA tasks by replacing the base LLM model in LlamaIndex with different baseline LLMs. In these experiments, we utilized the open-source embedding all-mpnet-base-v2 \u00a0(Song et\u00a0al., 2020). When compared to the default embedding, text-embedding-ada-002 (https://openai.com/blog/new-and-improved-embedding-model), there was a noticeable performance decline. Nonetheless, this disparity did not hinder our conclusions.\nFrom Tab.\u00a04 and Tab.\u00a06, our research findings reveal that the incorporation of retrieval techniques does not generally enhance the performance of long dependency QA tasks. There is a conspicuous performance decline, particularly evident for models like GPT4-8k and GPT4-32k. It can be attributed to the tendency of GPT models to produce longer outputs, sometimes including hallucinatory information, when the retrieved segments lack sufficient context. The phenomenon highlights the intricacy of our dataset, where a series of long dependency understanding and modeling capabilities such as comprehension and multi-hop reasoning are essentially needed. Relying solely on retrieval mechanisms might be insufficient in recalling the necessary information and further generating the final answer, resulting in a marked performance decline. However, we did observe an minor improvement in the BERT score for open-source models. This improvement in fluency can be attributed to the considerably shorter length of the retrieved segments used as inputs, in contrast to the entirety of the document.\n\n\n\nIndividual results on different types of long dependency QAs\u00a0\n\nPrevious research mostly focuses on presenting aggregated results for long dependency QA tasks across various question types. Differently, in this study, our objective is to delve into the performance of models in individual tasks that demand diverse capabilities, including reading comprehension, information retrieval, computation, and reasoning. In this regard, we employed GPT4 as the evaluator, and the accuracy results are available in Tab.\u00a07. Across the four tasks examined, LLMs generally exhibit strong performance in comprehension, reasoning, and multiple information retrieval, but fall short in tasks related to timeline reordering and computation. Furthermore, we observed that the way questions are framed has a significant impact on LLMs\u2019 performance. Yes-no questions and multiple-choice questions tend to be easier for LLMs to answer, particularly when the search space is limited, as opposed to open-ended questions within unstructured text.\n\n\nTable 7: Individual results on four types of long dependency QAs evaluated by GPT4\n\n\n\n\n\nModels\n\n\n\n\nContext\n\n\n\n\n \n\n\nInformation retrieval\n\n\n\n\n\n \n\n\nTimeline reorder\n\n\n\n\n\nComputation\n\n\n\n\n \n\n\nComprehension and reasoning\n\n\n\n\n\nGPT4-32k\n32k\n43.60\n64.43\n37.36\n61.26\n\n\nGPT4-8k\n8K\n31.89\n61.36\n22.54\n45.78\n\n\nGPT3.5-turbo-16k\n16K\n36.86\n55.73\n24.73\n51.09\n\n\nLlamaIndex\n\\\n27.60\n47.83\n19.78\n43.83\n\n\nChatGLM2-6B-32k\n32k\n12.47\n14.17\n5.43\n11.08\n\n\nLongLLaMa-3B\n256k\n3.82\n6.48\n5.15\n10.17\n\n\nRWKV-4-14B-pile\n8k\n4.67\n5.19\n4.40\n7.13\n\n\nLLaMA2-7B-32K\n32k\n3.01\n1.61\n1.12\n6.85\n\n\n\n\n\nTable 8: Performance of Timeline Reorder\n\n\n\n\n\nModels\n\n\n\n\nLSD\n\n\n\n\nLMD\n\n\n\n\nSD\n\n\n\n\nSDD\n\n\n\n\nLSD-S\n\n\n\n\nLMD-S\n\n\n\n\nSD-S\n\n\n\n\nSDD-S\n\n\n\n\nNon-standard(%)\n\n\n\n\nGPT4-32k\n1.04\n0.57\n0.93\n1.12\n1.21\n0.82\n1.41\n1.60\n52.80\n\n\nGPT4-8k\n1.24\n0.64\n1.04\n1.281\n1.43\n0.92\n1.51\n1.74\n49.31\n\n\nLlamaIndex\n1.55\n0.78\n1.19\n1.551\n1.95\n1.08\n1.65\n2.09\n39.72\n\n\nGPT3.5-turbo-16k\n3.58\n1.43\n2.17\n2.916\n1.05\n0.86\n1.24\n1.26\n77.21\n\n\nLongLLaMa-3B\n4.18\n1.71\n2.59\n3.30\n1.90\n1.12\n1.80\n2.07\n92.92\n\n\nChatGLM2-6B-32k\n4.31\n1.74\n2.63\n3.37\n1.83\n1.17\n1.50\n2.00\n99.07\n\n\nRWKV-4-14B-pile\n4.33\n1.75\n2.64\n3.38\n1.90\n0.97\n1.00\n1.75\n98.13\n\n\nLLaMA2-7B-32K\n4.33\n1.75\n2.64\n3.38\n2.50\n1.17\n1.33\n2.33\n98.60\n\n\n\n\n\n\nResults on long dependency QAs with/without CoT\u00a0\n\nTo bolster the long-context capabilities of LLMs, we conducted additional experiments designed to unlock their potential using the Chain of Thoughts\u00a0(CoT) framework\u00a0(Kojima et\u00a0al., 2023). We selected LlamaIndex as a representative model, given its impressive performance in both short and long dependency question-answering tasks, alongside strong commercial models such as GPT4. A manual evaluation was carried out on a subset comprising one-third of instances from each task category within long dependency QA. We initiated the LLM with a zero-shot CoT approach, employing prompts such as \u201cLet\u2019s think step by step,\u201d and furnished a few-shot setup with detailed rationales and standard output formats\u00a0Wei et\u00a0al. (2023) to facilitate responses to long dependency questions. As depicted in Fig.\u00a05, the zero-shot CoT approach had minimal impact on accuracy in comprehension and reasoning, as well as multiple retrieval tasks, but yielded a substantial 20% and 10% absolute accuracy increase in timeline reorder and computation. Interestingly, the few-shot CoT approach benefits the first two types but surprisingly leads to a decline in performance in the latter two types compared with zero-shot.\nWe hypothesize the reason is that the evidence and rationales in few-shot examples cannot be generalized to other questions, and including them might on the contrary give wrong guidance to the model.\n\n\n\n\n\nFigure 4: Performance of varying segments\n\n\n\n\nFigure 5: Long dependency QA tasks w/o CoT\n\n\n\n\n\n\nAutomatic evaluation on timeline reorder\n\nIn order to evaluate the performance of time reorder task outputs, it is essential to address discrepancies arising from the diverse formats produced by various models. Typically, these outputs comprise conventional numerical sequences, but errors in non-standard formats when evaluation necessitate preprocessing for accurate assessment. A proposed approach involves converting the serial numbers in the candidate answers from their original question into Roman numbers (i.e., I, II, \u22ef\u22ef\\cdots\u22ef), thereby enhancing discrimination through regular expression matching. Four key metrics, namely, LSD (location square deviation), LMD (location mean deviation), SD (swap deviation), and SDD (swap distance deviation), are employed to measure the similarity of numeric sequences, refer to Appendix\u00a0C for metric details. Smaller deviations indicate a higher degree of resemblance between the sequences. Any outputs that are empty, possess unequal lengths, or contain extra elements are categorized as non-standard. The maximum deviation between the provided ground truth and all corresponding candidate answers is computed as the worst score for evaluation purposes.\n\n\nThe percentage of non-standard outputs for each model and corresponding performances can be found in Tab.\u00a08.\nAs seen, it is evident that except for GPT4, which demonstrates a remarkable degree of adherence and alignment following Reinforcement Learning from Human Feedback (RLHF)\u00a0(Lee et\u00a0al., 2023), most open-sourced models struggle to generate texts in the correct format with less than 10%. However, this issue can be mitigated in significantly large models through the utilization of few-shot\nprompts and mandatory instructions. This phenomenon results in performance\npenalties when assessed using automated metrics. Consequently, to ascertain the\ngenuine capacity of LLMs in this task, we calculate the four metrics exclusively for outputs in standard format (\u201c-S\u201d).\n\n\n\nDispcrepancy in generated outputs of models\n\nDistributions of generated outputs of various models are depicted in Fig.\u00a06.\nIt is noteworthy that well-behaved models consistently produce shorter responses, averaging around 50 words, irrespective of the question type, particularly in short-term question answering scenarios. In contrast, models fine-tuned with longer textual inputs, such as LLaMA2-7B-32K, RWKV-4-14B-pile, and LongLLaMa-3B, tend to yield significantly lengthier responses, even when a maximum generation constraint of 500 tokens is enforced. An interesting deviation is observed in LongLLaMa-3B, which demonstrates variability in response lengths across both tasks. This behavior may stem from challenges in comprehending and addressing exceedingly complex long question-answering tasks. Consequently, the model appears to prioritize extracting a maximum number of pertinent contexts from its memory to generate sufficiently extensive responses that are deemed acceptable and rational.\n\n\nFigure 6: Output distributions on QA tasks\n\n\nMoreover, upon closer examination of model outputs, a significant disparity in generation quality is observed across various LLMs and task types, indicating a non-specific issue. Notably, commercial models like GPT4, GPT3.5, and LlamaIndex consistently generate outputs that exhibit a higher degree of human-likeness, completeness, and logical coherence within a structured format. These models consistently deliver contextually relevant, query-based responses. In contrast, open-sourced models, such as ChatGLM2-6B-32k, tend to offer shorter answers, occasionally confined to numeric responses. In cases where a definite answer is lacking, ChatGLM2-6B-32k compensates by retrieving relevant contextual information. However, the RNN-based model RWKV-4-14B-pile often generates duplicated responses or resorts to repeating the given questions to reach the maximum token length, sometimes resorting to code generation to address issues related to its training data. The performance of the LLaMA2-7B-32K model is notably worse, as it sporadically produces irrelevant or nonsensical text, along with the inclusion of special symbols when it fails to provide meaningful answers. More examples of outputs from different models can be seen in Appendix\u00a0F.\n\n\n\nProbable explanations for long QA bad cases\n\nTable 9: Bad cases study on the long dependency QA\n\n\n\n\n \n\n\nLong QA Tasks\n\nHallucination\u2217\n\n\n \n\n\nRedundant\n\nretrieval\u2020\n\n\n \n\n\nInsufficient\n\nretrieval\u22c6\n\n\n \n\n\nIrrelevant\n\nanswer\u22c4\n\n\n \n\n\nNo relevant\n\ncontext\u2227\n\n\n \n\n\nWrong/No\n\nreasoning\u00d7\n\nOthers\n\n\n\n \n\n\nComputation\n\n31.11\n24.44\n15.56\n0.00\n20.00\n0.00\n8.88\n\n\n\n \n\n\nMultiple information retrieval\n\n14.71\n31.37\n28.43\n13.73\n13.73\n0.00\n7.84\n\n\n\n \n\n\nComprehension and reasoning\n\n14.29\n10.99\n21.98\n18.68\n16.48\n10.99\n6.59\n\n\n\n\u2217 Evidence of predictions is not shown up in the original inputs and generated by LLM itself from nowhere.\n\n\n\n\u2020 Apart from the right evidence, irrelevant evidence is also redundantly retrieved.\n\n\n\n\u22c6 Not all of the essential evidence to answer the question is retrieved.\n\n\n\n\u22c4 Evidence of predictions generated have no or minor correlation with the question.\n\n\n\n\u2227 No relevant context in LLM\u2019s memory and refuse to answer the question.\n\n\n\n\u00d7 Fail to retrieve evidence that needs further reasoning other than directly extracting from the inputs.\n\n\n\n\n\nTo investigate whether the models have effectively memorized and comprehended lengthy contextual information, we conducted a comprehensive manual analysis of the underlying causes of failures in each long question-answering task. The rationale behind CoT analysis aided in understanding how models decompose and tackle challenges associated with extended dependency-based QA. Our observations reveal that LLMs struggle with these tasks primarily due to their inability to extract precise information and a propensity to generate responses that lack factual accuracy. Constraints imposed by the inherent context window limitations, coupled with information loss resulting from the optimized Transformer and position encoding, contribute to their struggles in memorizing the original extensive contexts. In most cases, models attempt to compensate by retrieving and integrating the most pertinent evidence, even if it results in redundant answers. However, they also acknowledge their insufficient context and, at times, abstain from providing responses rather than resorting to nonsensical answers. Furthermore, addressing these challenges necessitates enhanced comprehension and reasoning abilities, particularly when answers are not clearly evident across multiple pieces of evidence scattered throughout the raw texts. The insights derived from our benchmark analysis offer a scientific foundation and pave the way for promising research directions aimed at augmenting LLM capabilities for handling long contextual inputs. These findings underscore the need for further progress in comprehension, computation, and reasoning tasks using our dataset to effectively enhance LLMs\u2019 capacity to understand extended dependency contexts.\n\n\n\n\n",
      "5 Conclusion": "\n\n5 Conclusion\n\nThis paper introduces a novel benchmark, LooGLE\n, designed to facilitate the assessment of long-context comprehension by LLMs. LooGLE\n addresses the deficiencies present in previous datasets by offering considerably longer text passages, utilizing relatively new documents after 2022, incorporating multi-source materials from various categories, and notably featuring meticulously designed and annotated tasks with diverse contextual dependencies. Our extensive evaluations unveil substantial limitations in the capacity of existing LLMs to understand and reason about the intricate interdependencies present in lengthy texts, even when provided with considerably extended context windows. Furthermore, a notable disparity is observed between commercial and open-source models, with both exhibiting challenges in long dependency tasks as per our benchmark assessments. The outcomes underscore the utility of our dataset as a valuable reference for evaluating long-context comprehension and present avenues for potential enhancements in LLM performance.\n\n\n",
      "Appendix A More details of our dataset": "\n\nAppendix A More details of our dataset\n\nDistributions of the input length and dependency spanning in words for long dependency QA tasks are shown in Figs.\u00a08 and\u00a08. N-gram sunburst graph for generated QA pairs can be seen in Fig.\u00a09.\n\n\n\n\n\nFigure 7: Input document length distributions\n\n\n\n\nFigure 8: Evidence dependency distributions\n\n\n\n\n\n\n\n\n\n\n\nFigure 9: 4-gram sunburst graph for short and long dependency QA. (a) short dependency questions (b) short dependency answers (c) long dependency questions (d) long dependency answers\n\n",
      "Appendix B Task definition": "\n\nAppendix B Task definition\n\nThe Cloze task formulation process can be seen in Fig.\u00a010.\n\n\nFigure 10: Cloze task\n\n",
      "Appendix C Timeline reorder evaluation metrics": "\n\nAppendix C Timeline reorder evaluation metrics\n\nWe employ 4 metrics to measure the similarity of numeric output sequences for timeline reorder tasks. For given two numeric sequences A\ud835\udc34Aitalic_A and B\ud835\udc35Bitalic_B with the same sequence length n\ud835\udc5bnitalic_n, i\u2062[A]\ud835\udc56delimited-[]\ud835\udc34i[A]italic_i [ italic_A ] and i\u2062[B]\ud835\udc56delimited-[]\ud835\udc35i[B]italic_i [ italic_B ] is the i\ud835\udc56iitalic_ith number in each sequence. They can be computed using the formula below: \nLSD is the abbreviation for location square deviation:\n\n\n\n\n\n\nL\u2062S\u2062D\u2062(A,B)=1n\u2062\u2211i=0n\u22121(i\u2062[A]\u2212i\u2062[B])2\ud835\udc3f\ud835\udc46\ud835\udc37\ud835\udc34\ud835\udc351\ud835\udc5bsuperscriptsubscript\ud835\udc560\ud835\udc5b1superscript\ud835\udc56delimited-[]\ud835\udc34\ud835\udc56delimited-[]\ud835\udc352LSD(A,B)=\\frac{1}{n}\\sum_{i=0}^{n-1}(i[A]-i[B])^{2}italic_L italic_S italic_D ( italic_A , italic_B ) = divide start_ARG 1 end_ARG start_ARG italic_n end_ARG \u2211 start_POSTSUBSCRIPT italic_i = 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n - 1 end_POSTSUPERSCRIPT ( italic_i [ italic_A ] - italic_i [ italic_B ] ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT\n\n(1)\n\n\n\n\nLMD is the abbreviation for location mean deviation:\n\n\n\n\n\n\nL\u2062M\u2062D\u2062(A,B)=1n\u2062\u2211i=0n\u22121|i\u2062[A]\u2212i\u2062[B]|\ud835\udc3f\ud835\udc40\ud835\udc37\ud835\udc34\ud835\udc351\ud835\udc5bsuperscriptsubscript\ud835\udc560\ud835\udc5b1\ud835\udc56delimited-[]\ud835\udc34\ud835\udc56delimited-[]\ud835\udc35LMD(A,B)=\\frac{1}{n}\\sum_{i=0}^{n-1}\\left|i[A]-i[B]\\right|italic_L italic_M italic_D ( italic_A , italic_B ) = divide start_ARG 1 end_ARG start_ARG italic_n end_ARG \u2211 start_POSTSUBSCRIPT italic_i = 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n - 1 end_POSTSUPERSCRIPT | italic_i [ italic_A ] - italic_i [ italic_B ] |\n\n(2)\n\n\n\n\nSD is the abbreviation for swap deviation:\n\n\n\n\n\n\nSD(A,B)=min(W(A\u2192B)=min(\u2211s\u2208A\u2192B1)SD(A,B)=min(W(A\\to B)=min({\\textstyle\\sum_{s\\in A\\to B}}1)italic_S italic_D ( italic_A , italic_B ) = italic_m italic_i italic_n ( italic_W ( italic_A \u2192 italic_B ) = italic_m italic_i italic_n ( \u2211 start_POSTSUBSCRIPT italic_s \u2208 italic_A \u2192 italic_B end_POSTSUBSCRIPT 1 )\n\n(3)\n\n\n\n\nSDD is the swap distance deviation:\n\n\n\n\n\n\nSDD(A,B)=min(W(A\u2192B)=min(\u2211s\u2208A\u2192B|i\u2212j|)SDD(A,B)=min(W(A\\to B)=min({\\textstyle\\sum_{s\\in A\\to B}}\\left|i-j\\right|)italic_S italic_D italic_D ( italic_A , italic_B ) = italic_m italic_i italic_n ( italic_W ( italic_A \u2192 italic_B ) = italic_m italic_i italic_n ( \u2211 start_POSTSUBSCRIPT italic_s \u2208 italic_A \u2192 italic_B end_POSTSUBSCRIPT | italic_i - italic_j | )\n\n(4)\n\n\nwhere s=A\u2062(i,j)\ud835\udc60\ud835\udc34\ud835\udc56\ud835\udc57s=A(i,j)italic_s = italic_A ( italic_i , italic_j ) means the swap between the i\ud835\udc56iitalic_ith and j\ud835\udc57jitalic_jth element in A\ud835\udc34Aitalic_A. S=A\u2192B\ud835\udc46\ud835\udc34\u2192\ud835\udc35S=A\\to Bitalic_S = italic_A \u2192 italic_B means a series of swap actions to convert A\ud835\udc34Aitalic_A to B\ud835\udc35Bitalic_B. W\u2062(S)=\u2211s\u2208Sw\u2062(s)\ud835\udc4a\ud835\udc46subscript\ud835\udc60\ud835\udc46\ud835\udc64\ud835\udc60W(S)={\\textstyle\\sum_{s\\in S}}w(s)italic_W ( italic_S ) = \u2211 start_POSTSUBSCRIPT italic_s \u2208 italic_S end_POSTSUBSCRIPT italic_w ( italic_s ) means the weights sum of all the swap actions in S\ud835\udc46Sitalic_S, where w\u2062(s)=1\ud835\udc64\ud835\udc601w(s)=1italic_w ( italic_s ) = 1 in SD and w\u2062(s)=|i\u2212j|\ud835\udc64\ud835\udc60\ud835\udc56\ud835\udc57w(s)=\\left|i-j\\right|italic_w ( italic_s ) = | italic_i - italic_j | in SSD.\n\n",
      "Appendix D Prompts": "\n\nAppendix D Prompts\n\n\nD.1 Short dependency QA pair generation\n\n\n\n[seg] = {Input long texts}\nPlease generate 2 questions and corresponding answers based on given [seg] in less words as possible. Return reference text S, question Q and answer A from [seg] in json format as: {\u201cS\u201d: ,\u201cQ\u201d: ,\u201cA\u201d: },{\u201cS\u201d: ,\u201cQ\u201d: ,\u201cA\u201d: }.\n\n\n\n\n\nD.2 Short and long dependency question and answering\n\n\n\nInstruction:\nPlease answer the question based on the given long texts below.\n{Input long texts}\nQuestion: {Question}\nAnswer:\n\n\n\n\n\nD.3 scripts segment summarization for cloze formulation\n\n\n\nInstruction:\nPlease write a summary for this script segment within 500 words, focusing on describing objective facts and avoiding subjective opinions.\n{scripts segement}\nSummary:\n\n\n\n\n\nD.4 Cloze\n\n\n\nexample= {\u201c<mask-0>\u201d: \u201cBob\u201d, \u201c<mask-1\u201d>: \u201cGorrosion Magazine\u201d, \u201c<mask-2>\u201d: \u201cBethel Horizon\u201d}\nInstruction:\nPlease answer the cloze question based on the given long texts below. Each of the placeholder \u201c<mask-n>\u201d in the question could be an entity of Person, Location or Organization. The same masks represent the same entity.\nOutput a json format answer, for example:{example}\n{Input long texts}\nQuestion: {Question} What are the masked entities?\nAnswer:\n\n\n\n\n\nD.5 Summarization\n\n\n\nInstruction:\nPlease generate a summary of the below paper.\n{Input long texts}\nSummarization:\n\n\n\n\n\nD.6 Timeline reorder\n\n\n\nInstruction:\nPlease answer the question based on the given long texts below. Please use Roman numbers only for answers.\n{Input long texts}\nQuestion: {Question}\nAnswer:\n\n\n\n\n\nD.7 QA task evaluation by LLM (GPT4)\n\n\n\nInstruction:\nGiven one question, there is a groundtruth and a predict answer. Please decide whether they are the same or not in semantic. Please only output True or False.\nQuestion: {Question}\ngroundtruth = {Reference answer}\npredicted answer = {Generated output}\n\n\n\n\n\nD.8 Summarization task evaluation by LLM (GPT4)\n\n\n\nInstruction:\nPlease compare predicted summary with the goundtruth and evaluate the predicted summary from the perspectives of information completeness, consistency, fluency, and grammar by giving a score within the range of 0 to 100.\ngroundtruth = {Reference answer}\npredicted summary = {Generated output}\n\n\n\n\n\nD.9 Few-Shot CoT for long QA\n\n\n\nInstruction:\nPlease answer the question based on the given long texts below.\n{Input long texts}\n{Demonstrations}\nQuestion: {Question}\nAnswer:\n\n\n\n\n\nD.10 Zero-Shot CoT for long QA\n\n\n\nInstruction:\nPlease answer the question based on the given long texts below.\n{Input long texts}\nQuestion: {Question}\nAnswer: Let\u2019s think step by step.\n\n\n\n",
      "Appendix E Examples for long context understanding tasks": "\n\nAppendix E Examples for long context understanding tasks\n\n\nE.1 Short dependency question and answering\n\n\nQuestion:\nWho did Picardo collaborate with for building preservation and restoration projects?\nEvidence:\nOn qualifying in 1951, Picardo pursued his interest in historical architecture by collaborating on a number of building preservation and restoration projects with the Spanish architect and architectural historian Fernando Chueca Goitia, who was 8 years his senior.\nAnswer:\nHe collaborated with Spanish architect and architectural historian Fernando Chueca Goitia.\n\n\n\n\nQuestion:\nWhat was the nickname given to the 18th century period?\nEvidence:\nThe 18th century was nicknamed the \u2019Age of Enlightenment\u2019, as it was the period in which the Enlightenment emerged, a philosophical movement that defended reason and science against religious dogmatism.\nAnswer:\nThe Age of Enlightenment.\n\n\n\n\n\nE.2 Cloze\n\n\nQuestion:\n<mask-0>\u00a0 is shown hacking the GPS to delay Joey\u2019s arrival at <mask-1>, while Conrad disguises himself as Mr. Von Vanderveen and Jason Statham as Mr. Popov. Mike Miller, dressed as the Auctioneer, swaps out the violin cases, and <mask-2>is revealed to be the inside man. The crew successfully pulls off the heist, leaving a debris of blown-apart goons in their wake.\nLong input:\nWhen a caper crew needs something blown up for a heist, they call upon The Demolition Expert. They are often minor characters who are not given much screen \u2026.(104,094 words)\u2026. Joey is driving to the Big Buy, always craning back\u2026 like there\u2019s a phantom on his tail. Suddenly, the GPS chimes. GPS VOICE Rerouting . DRIVER Shit. Uh, boss, it says it just added twenty minutes. The speed past \u2013 A BLACK MUSTANG parked in a turnaround. Mary Beth in the driver\u2019s seat, clacking away on a laptop, hacking the GPS . \u2026.(150 words)\u2026.we couldn\u2019t have done it without Duncan\u2013 Reveal Duncan , smiling big. He raises his glass. FLASH: DUNCAN and TWO MORE GOONS hurry around the corner of the STADIUM HALLWAY and stop dead in their tracks when they see \u2013 A debris of blown-apart Goons littering the hallway. \u2026.(2,670 words).\nAnswer:\n{\u201c<mask-0>\u201d: \u201cMary Beth\u201d,\u00a0 \u201c<mask-1\u201d>: \u201cthe Big Buy\u201d,\u00a0 \u201c<mask-2>\u201d: \u201cDuncan\u201d}\n\n\n\n\n\nE.3 Summarization\n\n\nLong input:\nDistinction and quadratic base change for regular supercuspidal representations Chuijia Wang 1 Introduction Let be a connected reductive algebraic group over a non-archimedean local field with residual characteristic \u2026.(21,000 words)\u2026. Basically, one can describe all the characters of which occur in in terms of certain intersection property between the Kostant sections of and the orbit of the generic element associated to. \u2026.(500 words).\nSummary:\nIn this article, we study Prasad\u2019s conjecture for regular supercuspidalrepresentations based on the machinery developed by Hakim and Murnaghan tostudy distinguished representations, and the fundamental work of Kaletha onparameterization of regular supercuspidal representations. For regularsupercuspidal representations, we give some new interpretations of thenumerical quantities appearing in Prasad\u2019s formula, and reduce the proof to thecase of tori. The proof of Prasad\u2019s conjecture then reduces to a comparison ofvarious quadratic characters appearing naturally in the above process. We alsohave some new observations on these characters and study the relation betweenthem in detail. For some particular examples, we show the coincidence of thesecharacters, which gives a new purely local proof of Prasad\u2019s conjecture forregular supercuspidal representations of these groups. We also prove Prasad\u2019sconjecture for regular supercuspidal representations of G(E), when E/F isunramified and G is a general quasi-split reductive group.\n\n\n\n\n\nE.4 Multiple information retrieval\n\n\nQuestion:\nWhat were some of the architectural projects Jos\u00e9 Luis Picardo worked on?\nLong input:\nJos\u00e9 Luis Picardo \u2026.(1,520 words) \u2026. From the early 1960s to 1985 Picardo dedicated much of his professional life to the state-run hotel chain, Paradores de Turismo de Espa\u00f1a \u2026..(7,846 words) \u2026. In 1970 Picardo was invited to compete with fellow notable architects Javier Carvajal Ferrer [es] and Mariano Garc\u00eda Benito [es] for the contract to design and build a new headquarters building in the Salamanca neighbourhood of Madrid for the Fundaci\u00f3n Juan March (Juan March Foundation) which promotes Spanish culture and science \u2026.(651 words) \u2026. Picardo\u2019s commission from the Ministry was to design a sala de equitaci\u00f3n, a huge arena for horse and riding displays, in particular the school\u2019s signature performance \u201cComo Bailan los Caballos Andaluces\u201d (\u201cHow the Andalusian Horses Dance\u201d) which would seat up to 1,600 spectators. Connected to it were to be stable facilities for 60 horses \u2026.(1,113 words).\nAnswer:\nHe worked on hotel chain Paradores de Turismo de Espa\u00f1a, Fundaci\u00f3n Juan March, Sala de Equitaci\u00f3n.\nExplanation:\nBased on the deep understanding of given question, we need to extract all the evidence of architectural projects Jos\u00e9 Luis Picardo have worked on. There are total three works spreading in the original inputs independently as shown above.\n\n\n\n\n\nE.5 Timeline reorder\n\n\nQuestion:\nReorder the timeline of below events:\n1.restoration at Guadalupe,\n2.restore and rehabilitate the old Casa de la Inquisici\u00f3n\n3.castle conversion at Sig\u00fcenza,\n4.renovation and conversion of castle at Puebla de Alcocer\nLong input:\nJos\u00e9 Luis Picardo \u2026.(2,395 words) \u2026. Restoration at Guadalupe started in November 1963 and the hotel, with twenty double rooms, opened on 11 December 1965 \u2026.(1,472 words) \u2026. In 1965 Picardo was commissioned by Paradores to restore and rehabilitate the old Casa de la Inquisici\u00f3n (House of the Inquisition) in the small, historic village of Pedraza, 37 kilometres northeast of Segovia in Castilla y Le\u00f3n \u2026.(2,827 words) \u2026. In 1964 Picardo was involved, with the Ministry of Information and Tourism, in investigating old buildings for conversion into a new Parador in the Province of Guadalajara. Possible locations were the castle at Atienza and the Casa del Cord\u00f3n, an old inn in the same town, the castle at Molina de Arag\u00f3n and the castle at Sig\u00fcenza \u2026.(1,521 words) \u2026. Among the most advanced plans Picardo drew up were in 1969 for the renovation and conversion into a Parador of the castle at Puebla de Alcocer, a small municipality 70 miles east of M\u00e9rida in the Province of Badajoz in Extremadura \u2026.(2,897 words).\nAnswer:\n1, 3, 2, 4\nExplanation:\nThe four events provided in the question sequentially happen with thousands of words spanning. We firstly locate the exact sentences describing the event in the original inputs above. Then we reorder them based on the their occurrence.\n\n\n\n\n\nE.6 Computation\n\n\nQuestion:\nHow many inhabitants increases from the end of 19th to 1970?\nLong input:\nUrban planning of Barcelona \u2026.(5,558 words) \u2026. After the revolution of 1868, the Citadel was also demolished and the land transformed into a public park. The population grew, especially thanks to immigration from the rest of Spain, reaching 400,000 inhabitants by the end of the century. \u2026.(7,613 words) \u2026. In two decades it went from 1,280,179 inhabitants in 1950 to 1,745,142 in 1970 \u2026.(5,596 words).\nAnswer:\n1,345,142\nExplanation:\nFirstly, we locate the numeric of inhabitants which only appear between 1900 to 1970 from the input as evidence. There are three relevant numbers: 400,000, 280,179 and 1,745,142. Then we make computation on 1,745,142 - 400,000 = 1,345,142 to get the final answer.\n\n\n\n\n\nE.7 Comprehension and reasoning\n\n\nQuestion:\nWhich event is the turning point for territorial expansion in the 19th?\nLong input:\nUrban planning of Barcelona \u2026.(2,958 words) \u2026. At this time Barcelona was constituted as a county and later became part of the Crown of Aragon and the political and economic center of the Principality of Catalonia, becoming an important maritime and commercial axis of the Mediterranean Sea\u2026.(128 words) \u2026.The progressive increase in the size of the city, and its increasing urban, social and economic complexity, led to the creation of a specific system of government for the administration of the city, the Council of One Hundred (1,265)\u2026.(1,260 words) \u2026.The city was still confined within its walls \u2014the only expansion was on the beach, in the neighborhood of La Barceloneta\u2014 despite the fact that by the end of the period it had almost 100,000 inhabitants\u2026.(1,333 words) \u2026.Barcelona thus underwent an important leap to modernity, characterized by three factors: the population migration from the countryside to the city, the link between industrial and urban developments, and a better articulation of the territory through a wide network of roads and railroads, which will lead Barcelona to become a colonizing metropolis of its territorial environment\u2026..(1,319 words) \u2026.In the middle of the century a transcendental event took place that completely changed the physiognomy of the city; the demolition of the walls\u2026..(1,472 words) \u2026.Another service that emerged at the end of the century was the telephone. The first telephone communication in the whole peninsula took place in Barcelona, in 1877, between the Montjuic castle and the fortress of the Citadel \u2014in the process of dismantling but still housing a garrison\u2014. That same year the first interurban transmission between Barcelona and Girona was carried out by the company Dalmau i Fills, pioneer in the installation of lines in Barcelona\u2026\u2026(9,966 words).\nAnswer:\nThe demolition of the walls and the donation to the city of the fortress of the Citade.\nExplanation:\nThere is no exact answer that can be extracted directly from the original input and further reasoning is essentially needed. Firstly, we find all the relevant sentences or paragraphs on the territorial expansion which are in the 19th. Temporal events and key words act as clues for sorting out the evidence. Among the evidence, some are factual plots while others are critical comments. Based on further comprehension, we need to compare the various factors of territorial expansion underlying the evidence and decide the most decisive and influential turning point events to answer the question.\n\n\n\n",
      "Appendix F Examples of generated outputs for models": "\n\nAppendix F Examples of generated outputs for models\n\n\nF.1 GPT4-32k\n\n1. short QA\n\nQuestion: What were the charges against the two members of the Rock Machine Perth chapter?\nOutput: The two members of the Rock Machine Perth chapter, Kyle Barry and Brent Reker, were charged with extortion and demanding property with oral threats.\nGroundtruth: Extortion and demanding property with oral threats.\n\n\n\n\nQuestion: Why did the federal government create relocation programs for Native Americans?\nOutput: The federal government created relocation programs for Native Americans in an effort to expand their employment opportunities. Many Native Americans were living in rural and isolated areas, and the government believed that moving them to urban areas would offer better job prospects. This was part of a wider policy shift towards giving tribes more authority for self-determination.\nGroundtruth: To expand their employment opportunities.\n\n\n\n2. long QA\n\nQuestion: According to World Bank\u2019s estimation, what is the difference between the amount of World Bank\u2019s aid to Turkey and the amount of damaged suffered by Syria?\nOutput: The World Bank estimated $5.1 billion in damages suffered by Syria and it provided $1.78 billion in aid to Turkey. So, the difference is $3.32 billion.\nGroundtruth: 3.32 Billion.\n\n\n\n\nQuestion: What efforts did guy make to improve his life?\nOutput: Guy made several efforts to improve his life, including attending therapy sessions, building a tree house for his nephews, discussing his feelings and insecurities with his friends, and standing up for himself against his aggressive father. He also expressed his feelings to his ex-girlfriend Jen, trying to reconcile with her and offering her a house he planned to renovate. He made attempts to change his negative self-talk and confront his past traumas.\nGroundtruth: 1. Receive psychotherapy: Guy chooses to see a psychiatrist to face his own problems, which is an important step for him to change.\n2. Introspection: Guy gradually enlightened himself during the treatment, facing the relationship problems between him, his father and his friends.\n3. Write a letter to Jen: Guy wrote a sincere letter to Jen, expressing his change of attitude, trying to win back Jen\u2019s heart.\n4. Reaching out to old friends: Despite the embarrassment, Guy reached out to old friends and tried to work things out.\n5. Renovate the house: Guy takes on the job of renovating the house to plan for his future life.\n6. Dealing with the relationship with the father: Guy faces his father, expresses his thoughts, and no longer avoids this issue.\n7. Change yourself: Guy works hard, writes letters, receives treatment, etc., showing his determination to change himself.\n\n\n\n3. summarization\n\nOutput: This paper discusses the distinction and quadratic base change for regular supercuspidal representations. The author focuses on understanding Prasad\u2019s conjecture on the Galois distinction problem, which proposes a relationship between the distinction property of an irreducible representation and the base change property of its Langlands parameters. The author\u2019s work highlights that even though significant progress has been made in proving the conjecture, a general method that works for all Galois symmetric pairs would be interesting. It extends the understanding of the relationship between distinguished representations and Langlands functoriality. The results support the idea of using a purely local approach to prove the conjecture for regular supercuspidal representations without constraints on the Galois symmetric pair. The author also outlines strategies to improve the exact analysis of terms appearing in Prasad\u2019s identity. The article provides evidence supporting the conjecture by evaluating particular examples.\nGroundtruth: In this article, we study Prasad\u2019s conjecture for regular supercuspidal representations based on the machinery developed by Hakim and Murnaghan to study distinguished representations, and the fundamental work of Kaletha on parameterization of regular supercuspidal representations. For regular supercuspidal representations, we give some new interpretations of the numerical quantities appearing in Prasad\u2019s formula, and reduce the proof to the case of tori. The proof of Prasad\u2019s conjecture then reduces to a comparison of various quadratic characters appearing naturally in the above process. We also have some new observations on these characters and study the relation between them in detail. For some particular examples, we show the coincidence of these characters, which gives a new purely local proof of Prasad\u2019s conjecture for regular supercuspidal representations of these groups. We also prove Prasad\u2019s conjecture for regular supercuspidal representations of G(E), when E/F is unramified and G is a general quasi-split reductive group.\n\n\n\n4. cloze\n\nQuestion: The script segment of \u201c Wildfire 2022\u201d takes place in Tulare County, California, where the sky is grey and ashy due to the nearby fires. The story follows <mask-1>, a seven-year-old mute girl, who runs to school along a busy road. <mask-0>, a Trans Latin Woman, is seen singing and dancing to \u201c Opposites Attract\u201d while trying to fix her old car. Lu goes to school and stands in line for lunch, where she is ignored by other kids. She pays for her lunch in change and sits alone at the back of the cafeteria. Later, Fireman <mask-2>speaks to Mrs. Consuelos\u2019 third-grade classroom about fire safety, and Lu is chosen to answer a question despite not speaking. The leading cause of wildfires is discussed, and it is revealed that humans are the leading cause of wildfires due to accidents like leaving an open campfire burning or tossing a cigarette out a car window.\nOutput: {\u201c<mask-0>\u201d: \u201cMerribelle\u201d, \u201c<mask-1>\u201d: \u201cLu\u201d, \u201c<mask-2>\u201d: \u201cJosh\u201d}\nGroundtruth: {\u201c<mask-0>\u201d: \u201cMerribelle\u201d, \u201c<mask-1>\u201d: \u201cLu\u201d, \u201c<mask-2>\u201d: \u201cJosh\u201d}\n\n\n\n\n\nF.2 GPT4-8k\n\n1. short QA\n\nQuestion: When did large cattle ranches start to appear in the area?\nOutput: As early as 1734.\nGroundtruth: As early as 1734.\n\n\n\n\nQuestion: What contributed to the economic growth of Rio Grande do Sul at the end of the 19th century?\nOutput: The economic growth of Rio Grande do Sul at the end of the 19th century was contributed by the growth of trade, the arrival of immigrants of other origins such as Italians and Jews, and the establishment of a growing industry and a rich bourgeois class. The state had become the third largest economy in Brazil by the turn of the 20th century.\nGroundtruth: Trade and immigration of Italians and Jews.\n\n\n\n2. long QA\n\nQuestion: Reorder the timeline of below events:\n1. Guardian Reported Sexually Assault.\n2. Nunez Announced taking place An Internal Investigation.\n3. A Young Woman Was Reported To Have Lost Her Thumb.\nOutput:\n3. A Young Woman Was Reported To Have Lost Her Thumb.\n1. Guardian Reported Sexually Assault.\n2. Nunez Announced taking place An Internal Investigation.\nGroundtruth: 2,1,3\n\n\n\n\nQuestion: Was Maurice Duplessis\u2019 treatment of the news publishing industry consistent or inconsistent over his two terms?\nOutput: The text does not provide specific information on Maurice Duplessis\u2019 treatment of the news publishing industry over his two terms.\nGroundtruth: Inconsistent.\n\n\n\n3. summarization\n\nOutput: This paper by Chuijia Wang focuses on the study of representation theory of a connected reductive algebraic group over a non-archimedean local field. The main focus is to understand Prasad2\u0306019s conjecture on the Galois distinction problem. The author provides evidence on the possibility of proving this conjecture for regular supercuspidal representations without constraints on the Galois symmetric pair using a different purely local method. The paper also discusses the work of Sakellaridis and Venkatesh, Dipendra Prasad, and others in the field. The author proves Prasad2\u0306019s conjecture for regular supercuspidal representations of certain groups and provides a detailed study of these quadratic characters. The paper concludes with a discussion on the distinction problem and the construction of local Langlands correspondence.\nGroundtruth: In this article, we study Prasad\u2019s conjecture for regular supercuspidal representations based on the machinery developed by Hakim and Murnaghan to study distinguished representations, and the fundamental work of Kaletha on parameterization of regular supercuspidal representations. For regular supercuspidal representations, we give some new interpretations of the numerical quantities appearing in Prasad\u2019s formula, and reduce the proof to the case of tori. The proof of Prasad\u2019s conjecture then reduces to a comparison of various quadratic characters appearing naturally in the above process. We also have some new observations on these characters and study the relation between them in detail. For some particular examples, we show the coincidence of these characters, which gives a new purely local proof of Prasad\u2019s conjecture for regular supercuspidal representations of these groups. We also prove Prasad\u2019s conjecture for regular supercuspidal representations of G(E), when E/F is unramified and G is a general quasi-split reductive group.\n\n4. cloze\n\nQuestion: The script segment of \u201c Wildfire 2022\u201d takes place in Tulare County, California, where the sky is grey and ashy due to the nearby fires. The story follows <mask-1>, a seven-year-old mute girl, who runs to school along a busy road. <mask-0>, a Trans Latin Woman, is seen singing and dancing to \u201c Opposites Attract\u201d while trying to fix her old car. Lu goes to school and stands in line for lunch, where she is ignored by other kids. She pays for her lunch in change and sits alone at the back of the cafeteria. Later, Fireman <mask-2>speaks to Mrs. Consuelos\u2019 third-grade classroom about fire safety, and Lu is chosen to answer a question despite not speaking. The leading cause of wildfires is discussed, and it is revealed that humans are the leading cause of wildfires due to accidents like leaving an open campfire burning or tossing a cigarette out a car window.\nOutput: {\u201c<mask-0>\u201d: \u201cMerribelle\u201d, \u201c<mask-1>\u201d: \u201cLu\u201d, \u201c<mask-2>\u201d: \u201cJosh\u201d}\nGroundtruth: {\u201c<mask-0>\u201d: \u201cMerribelle\u201d, \u201c<mask-1>\u201d: \u201cLu\u201d, \u201c<mask-2>\u201d: \u201cJosh\u201d}\n\n\n\n\n\nF.3 GPT3.5-turbo-16k\n\n1. short QA\n\nQuestion: What was unveiled by Buckingham Palace for the Coronation of Charles III and Camilla?\nOutput: The official Coronation logo was unveiled by Buckingham Palace for the Coronation of Charles III and Camilla.\nGroundtruth: The official Coronation logo.\n\n\n\n\nQuestion:What was the percentage decline in house prices in the UK?\nOutput: There is no specific information provided in the text about the percentage decline in house prices in the UK.\nGroundtruth: 3.4%.\n\n\n\n2. long QA\n\nQuestion: Which church hasn\u2019t been built during the Phase 1 of church construction?\n1. The Main Church of Sabar\u00e1\n2. The Cathedral Basilica of Salvador\n3. The Church of Our Lady of Grace\nOutput: The Main Church of Sabar\u00e1.\nGroundtruth: 1,2.\n\n\n\n\nQuestion: Do Reader\u2019s Digest and Alexander Cockburn agree on Mujahideen?\nOutput: No, Reader\u2019s Digest and Alexander Cockburn do not agree on the Mujahideen. Reader\u2019s Digest generally portrayed the Mujahideen as freedom fighters and heroes, while Cockburn criticized the US support for the Mujahideen and argued that they were not the noble fighters they were made out to be.\nGroundtruth: No.\n\n\n\n3. summarization\n\nOutput: This paper focuses on understanding Prasad\u2019s conjecture on the Galois distinction problem for regular supercuspidal representations. The distinction problem refers to the question of determining which irreducible representations of a reductive group are distinguished with respect to a pair of subgroups and a character. The paper provides evidence for the possibility of proving this conjecture for regular supercuspidal representations using a different purely local method.\nThe paper begins by introducing the Langlands-Vogan bijection, which relates irreducible representations of a reductive group to Langlands parameters. It then discusses the local Langlands correspondence for regular supercuspidal representations and the parametrization of these representations using regular supercuspidal packet data. The paper also reviews the character expansions and the Murnaghan-Kirillov formula for regular semisimple and nilpotent orbits.\nThe main part of the paper focuses on the computation of various characters associated with regular supercuspidal representations. It considers the characters associated with Prasad\u2019s conjecture, Hakim\u2019s character, and the character associated with - data. The paper provides explicit computations for these characters in various cases, such as when the quadratic extension is unramified or ramified, and when the group is odd or even. The paper also compares these characters and shows that they satisfy the necessary conditions for Prasad\u2019s conjecture.\nThe paper concludes by providing examples of regular supercuspidal representations of specific groups, such as and , and showing that Prasad\u2019s conjecture holds for these representations. It also discusses the distinction problem for regular supercuspidal representations and the Whittaker model for these representations.\nOverall, this paper provides a detailed analysis of Prasad\u2019s conjecture for regular supercuspidal representations and presents new evidence for its validity. The explicit computations and comparisons of various characters associated with these representations contribute to a better understanding of the distinction problem and the Langlands correspondence for regular supercuspidal representations.\nGroundtruth: In this article, we study Prasad\u2019s conjecture for regular supercuspidal representations based on the machinery developed by Hakim and Murnaghan to study distinguished representations, and the fundamental work of Kaletha on parameterization of regular supercuspidal representations. For regular supercuspidal representations, we give some new interpretations of the numerical quantities appearing in Prasad\u2019s formula, and reduce the proof to the case of tori. The proof of Prasad\u2019s conjecture then reduces to a comparison of various quadratic characters appearing naturally in the above process. We also have some new observations on these characters and study the relation between them in detail. For some particular examples, we show the coincidence of these characters, which gives a new purely local proof of Prasad\u2019s conjecture for regular supercuspidal representations of these groups. We also prove Prasad\u2019s conjecture for regular supercuspidal representations of G(E), when E/F is unramified and G is a general quasi-split reductive group.\n\n4. cloze\n\nQuestion: The script segment of \u201c Wildfire 2022\u201d takes place in Tulare County, California, where the sky is grey and ashy due to the nearby fires. The story follows <mask-1>, a seven-year-old mute girl, who runs to school along a busy road. <mask-0>, a Trans Latin Woman, is seen singing and dancing to \u201c Opposites Attract\u201d while trying to fix her old car. Lu goes to school and stands in line for lunch, where she is ignored by other kids. She pays for her lunch in change and sits alone at the back of the cafeteria. Later, Fireman <mask-2>speaks to Mrs. Consuelos\u2019 third-grade classroom about fire safety, and Lu is chosen to answer a question despite not speaking. The leading cause of wildfires is discussed, and it is revealed that humans are the leading cause of wildfires due to accidents like leaving an open campfire burning or tossing a cigarette out a car window.\nOutput: {\u201c<mask-0>\u201d: \u201cMerribelle\u201d, \u201c<mask-1>\u201d: \u201cLu\u201d, \u201c<mask-2>\u201d: \u201cJosh\u201d}\nGroundtruth: {\u201c<mask-0>\u201d: \u201cMerribelle\u201d, \u201c<mask-1>\u201d: \u201cLu\u201d, \u201c<mask-2>\u201d: \u201cJosh\u201d}\n\n\n\n\n\nF.4 LlamaIndex\n\n1. short QA\n\nQuestion: How long was the uninterrupted Christian rule in Tripoli?\nOutput: 180 years.\nGroundtruth: 180 years.\n\n\n\n\nQuestion: What caused the waste workers\u2019 strike in Paris?\nOutput: The waste workers\u2019 strike in Paris was caused by proposed pension reforms that would raise their retirement age from 57 to 59.\nGroundtruth: The proposed pension reforms that would raise their retirement age from 57 to 59.\n\n\n\n2. long QA\n\nQuestion: Which event is the turning point in the whole story of the script?\nOutput: The turning point in the whole story of the script is when Buck bestows Tye with his and Tye Sr.\u2019s dog tags. This moment marks a shift in their relationship, as Buck acknowledges Tye\u2019s potential and encourages him to live the way his mother would have wanted him to. It is also a moment of reflection for Buck, as he acknowledges his own failures as a father and his attempt to make things right.\nGroundtruth: Sasha\u2019s election as mayor.\n\n\n\n\nQuestion: How many times did Mia visit the Decrepit House before she set foot in the woods?\nOutput: Once. Mia visited the Decrepit House once before she set foot in the woods.\nGroundtruth: Three times.\n\n\n\n3. summarization\n\nOutput: This paper provides a brief review of Prasad\u2019s conjecture, which relates the distinction property of an irreducible admissible representation to the base change functorial property of its Langlands parameter and certain numerical invariants. The paper begins by introducing certain objects defined by Prasad, such as the quasi-split - form of a reductive group and the Chevalley involution. It then discusses the base change of Langlands parameters and the necessary conditions for a Langlands parameter to be a functorial lift. The paper also introduces the quadratic character associated with a quadratic extension and discusses Prasad\u2019s conjecture in relation to regular supercuspidal representations. It summarizes Kaletha\u2019s work on the construction of -parameters and -packets for regular supercuspidal representations and the base change map of Langlands parameters. The paper then presents a new factorization formula for the restriction of Prasad\u2019s quadratic character to an elliptic maximal torus and reviews Hakim and Murnaghan\u2019s work on the computation of the dimension of the space of invariant linear forms. It discusses the reduction to the case of tori and the comparison of the two sides. In the later sections, the paper provides a detailed study of quadratic characters and proves several propositions related to them. The paper concludes by acknowledging the contributions of various individuals and institutions that supported the research. Overall, this paper provides a comprehensive overview of Prasad\u2019s conjecture and its implications for regular supercuspidal representations. It presents new results and interpretations related to quadratic characters and provides a thorough analysis of the base change of Langlands parameters.\nGroundtruth: In this article, we study Prasad\u2019s conjecture for regular supercuspidal representations based on the machinery developed by Hakim and Murnaghan to study distinguished representations, and the fundamental work of Kaletha on parameterization of regular supercuspidal representations. For regular supercuspidal representations, we give some new interpretations of the numerical quantities appearing in Prasad\u2019s formula, and reduce the proof to the case of tori. The proof of Prasad\u2019s conjecture then reduces to a comparison of various quadratic characters appearing naturally in the above process. We also have some new observations on these characters and study the relation between them in detail. For some particular examples, we show the coincidence of these characters, which gives a new purely local proof of Prasad\u2019s conjecture for regular supercuspidal representations of these groups. We also prove Prasad\u2019s conjecture for regular supercuspidal representations of G(E), when E/F is unramified and G is a general quasi-split reductive group.\n\n4. cloze\n\nQuestion: The script segment of \u201c Wildfire 2022\u201d takes place in Tulare County, California, where the sky is grey and ashy due to the nearby fires. The story follows <mask-1>, a seven-year-old mute girl, who runs to school along a busy road. <mask-0>, a Trans Latin Woman, is seen singing and dancing to \u201c Opposites Attract\u201d while trying to fix her old car. Lu goes to school and stands in line for lunch, where she is ignored by other kids. She pays for her lunch in change and sits alone at the back of the cafeteria. Later, Fireman <mask-2>speaks to Mrs. Consuelos\u2019 third-grade classroom about fire safety, and Lu is chosen to answer a question despite not speaking. The leading cause of wildfires is discussed, and it is revealed that humans are the leading cause of wildfires due to accidents like leaving an open campfire burning or tossing a cigarette out a car window.\nOutput: {\u201c<mask-0>\u201d: \u201cMerribelle\u201d, \u201c<mask-1>\u201d: \u201cLu\u201d, \u201c<mask-2>\u201d: \u201cJosh\u201d}\nGroundtruth: {\u201c<mask-0>\u201d: \u201cMerribelle\u201d, \u201c<mask-1>\u201d: \u201cLu\u201d, \u201c<mask-2>\u201d: \u201cJosh\u201d}\n\n\n\n\n\nF.5 ChatGLM2-6B-32k\n\n1. short QA\n\nQuestion: What event tested the capabilities of the detachment in early 1968?\nOutput: 1968 Tet Offensive.\nGroundtruth: The Tet Offensive.\n\n\n\n\nQuestion: How many patients did the 57th Medical Detachment evacuate in 1975?\nOutput: 669 patients.\nGroundtruth: 669.\n\n\n\n2. long QA\n\nQuestion: Has Arsenal FC improved their Premier League position in 2022-23 compared to 2021-22?\nOutput: 2022-23 Premier League. Arsenal won 26 league games this season, equalling their highest-ever tally in the Premier League, which were also achieved in 2001\u201302 and 2003\u201304. They ended the campaign on 84 points, the third-highest total in their history. This was the first time the Gunners finished in the top four since the 2015\u201316 season.\nGroundtruth: Yes.\n\n\n\n\nQuestion: Which two media mentioned in the text appear most often?\nOutput: 1921, 1927, 1928, 1930, 1931, 1932, 1933, 1934, 1935, 1936, 1937, 1938, 1939, 1941, 1942, 1949, 1950.\nGroundtruth: The Herald and The Auckland Star.\n\n\n\n3. summarization\n\nOutput: Distinguishing distort\nGroundtruth: In this article, we study Prasad\u2019s conjecture for regular supercuspidal representations based on the machinery developed by Hakim and Murnaghan to study distinguished representations, and the fundamental work of Kaletha on parameterization of regular supercuspidal representations. For regular supercuspidal representations, we give some new interpretations of the numerical quantities appearing in Prasad\u2019s formula, and reduce the proof to the case of tori. The proof of Prasad\u2019s conjecture then reduces to a comparison of various quadratic characters appearing naturally in the above process. We also have some new observations on these characters and study the relation between them in detail. For some particular examples, we show the coincidence of these characters, which gives a new purely local proof of Prasad\u2019s conjecture for regular supercuspidal representations of these groups. We also prove Prasad\u2019s conjecture for regular supercuspidal representations of G(E), when E/F is unramified and G is a general quasi-split reductive group.\n\n4. cloze\n\nQuestion: The script segment of \u201c Wildfire 2022\u201d takes place in Tulare County, California, where the sky is grey and ashy due to the nearby fires. The story follows <mask-1>, a seven-year-old mute girl, who runs to school along a busy road. <mask-0>, a Trans Latin Woman, is seen singing and dancing to \u201c Opposites Attract\u201d while trying to fix her old car. Lu goes to school and stands in line for lunch, where she is ignored by other kids. She pays for her lunch in change and sits alone at the back of the cafeteria. Later, Fireman <mask-2>speaks to Mrs. Consuelos\u2019 third-grade classroom about fire safety, and Lu is chosen to answer a question despite not speaking. The leading cause of wildfires is discussed, and it is revealed that humans are the leading cause of wildfires due to accidents like leaving an open campfire burning or tossing a cigarette out a car window.\nOutput: \u201c* Mask\u201d\nGroundtruth: {\u201c<mask-0>\u201d: \u201cMerribelle\u201d, \u201c<mask-1>\u201d: \u201cLu\u201d, \u201c<mask-2>\u201d: \u201cJosh\u201d}\n\n\n\n\n\nF.6 RWKV-4-14B-pile\n\n1. short QA\n\nQuestion: What was Gregory X\u2019s \u2019dual crusading policy\u2019?\nOutput: Gregory X\u2019s \u2019dual crusading policy\u2019 was a policy of the papacy of Gregory X (1271\u20131276) to support two crusades simultaneously. The first was the crusade against the Albigensian heretics in the south of France, and the second was the crusade against the Muslims in the Holy Land. The papal bulls of Gregory X were issued in 1274 and 1275. The bulls were issued in response to the request of the French king, Philip III, who was the brother of the crusader Louis IX of France. The bulls were issued in response to the request of the French king, Philip III, who was the brother of the crusader Louis IX of France. Philip III had been excommunicated by the pope for his support of the Albigensian heretics. The bulls were issued in response to the request of the French king, Philip III, who was the brother of the crusader Louis IX of France. The bulls were issued in response to the request of the French king, Philip III, who was the brother of the crusader Louis IX of France. Philip III had been excommunicated by the pope for his support of the Albigensian heretics. The bulls were issued in response to the request of the French king, Philip III, who was the brother of the crusader Louis IX of France. Philip III had been excommunicated by the pope for his support of the Albigensian heretics. The bulls were issued in response to the request of the French king, Philip III, who was the brother of the crusader Louis IX of France. Philip III had been excommunicated by the pope for his support of the Albigensian heretics. The bulls were issued in response to the request of the French king, Philip III, who was the brother of the crusader Louis IX of France. Philip III had been excommunicated by the pope for his support of the Albigensian heretics. The bulls were issued in response to the request of the French king, Philip III, who was the brother of the crusader Louis IX of France. Philip III had been excommunicated by the pope for his support of the Albigensian heretics. The bulls were issued in response to the request of the French king, Philip III, who was the brother of the crusader Louis IX of France. Philip III\nGroundtruth: Combine plans for a general crusade with plans for smaller interventions.\n\n\n\n\nQuestion: When did the Rock Machine establish a Nomads chapter in Australia?\nOutput: The Rock Machine Nomads chapter was established in Australia in the early 2000s. The Nomads chapter was established in the early 2000s. The Nomads chapter was established in the early 2000s. The Nomads chapter was established in the early 2000s. The Nomads chapter was established in the early 2000s. The Nomads chapter was established in the early 2000s. The Nomads chapter was established in the early 2000s. The Nomads chapter was established in the early 2000s. The Nomads chapter was established in the early 2000s. The Nomads chapter was established in the early 2000s. The Nomads chapter was established in the early 2000s. The Nomads chapter was established in the early 2000s. The Nomads chapter was established in the early 2000s. The Nomads chapter was established in the early 2000s. The Nomads chapter was established in the early 2000s. The Nomads chapter was established in the early 2000s. The Nomads chapter was established in the early 2000s. The Nomads chapter was established in the early 2000s. The Nomads chapter was established in the early 2000s. The Nomads chapter was established in the early 2000s. The Nomads chapter was established in the early 2000s. The Nomads chapter was established in the early 2000s. The Nomads chapter was established in the early 2000s. The Nomads chapter was established in the early 2000s. The Nomads chapter was established in the early 2000s. The Nomads chapter was established in the early 2000s. The Nomads chapter was established in the early 2000s. The Nomads chapter was established in the early 2000s. The Nomads chapter was established in the early 2000s. The Nomads chapter was established in the early 2000s. The Nomads chapter was established in the early 2000s. The Nomads chapter was established in the early 2000s. The Nomads chapter was established in the early 2000s. The Nomads chapter was established in the early 2000s. The Nomads chapter was established in the early 2000s. The Nomads chapter was established in the early 2000s. The Nomads chapter was established in the early 2000s. The Nomads chapter was established in the early 2000s. The Nomads chapter was established in the early 2000s. The Nomads chapter was established in the early 2000s. The Nomads chapter was established in the early 2000s. The Nomads\nGroundtruth: In 2008.\n\n\n\n2. long QA\n\nQuestion: How did Picardo build the Parador at the Castillo de Santa Catalina to get a good view for vistors?\nOutput: Picardo built the Parador at the Castle of Santa Catalina to make sure you got a nice view looking down towards the Mediterranean Sea!\nQ:\nHow do i get the value of a textbox inputted via javascript function call?\nI am trying to create a simple calculator appplication,but my code does not seem to calculate anything.Here is my html file :\n<!DOCTYPE html>\n<html lang=\u201den\u201d>\n<head>\n<meta charset=\u201dUTF-8\u201d>\n<title\u00bfCalculator </title>\n<link rel=\u201dstylesheet\u201d href=\u201dstyle.css\u201d>\n</head>\n<body>\n<div class=\u201dcalculatorContainer\u201d>\n<input id=\u201ddisplayBox\u201d placeholder=\u201dDisplay here\u201d/>\n<button onclick=\u201dadd()\u201d>+</button>\n<button onclick=\u201dsubtract()\u201d>-</button>\n<button onclick=\u201dmultiply()\u201d>*</button>\n<button onclick=\u201ddivide()\u201d>/</button>\n<button onclick=\u201dequals()\u201d>=</button >\n</div>\n<src=\u201dapp.js\u201d></script>\n</body>\n</html>\nAnd Here is My Javascript File Code :-\nfunction equals(){\ndocument.getElementById(\u201ddisplayBox\u201d).value =\ndocument.getElementById(\u201ddisplayBox\u201d).value + \u201d+\u201d;\n}\nWhat should happen is whenever user clicks any button like plus minus multiply divide etc., the corresponding symbol gets appended onto the div element named \u2019displayBox\u2019. But nothing happens! Please help!! Thanks :)\nA:\nYou need to change your HTML markup slightly - instead of putting <input />tags directly inside the container tag (<div></div>) put them inside separate containers e.g.:\n<div class=\u201dcontainer\u201d><span><input ><span><div>\nThen update JavaScript accordingly:\nGet reference to DOM object containing buttons & inputs\nconst calcButtons = document.querySelector(\u2019.container >span\u2019);\ncalcButtons.addEventListener(\u2019click\u2019, () =>{\nconsole.log(\u2018Click event fired\u2018); // Logging message shows correct behaviour\n});\nAdd listener to listen for click events happening anywhere else *outside* \u2018calcButtons\u2018\nwindow.addEventListener(\u2019click\u2019, ({ target }) =>{\nif (!target.matches(\u2019.container\u2019)) return;\nGroundtruth: He used the elongated site at the top of the hill, styled his new building on the layout and dimensions of the old castle and on what had been discovered during his research of its surviving interior designs.\n\n\n\n\nQuestion:Which of the following are Peter Herring\u2019s opinion?\nA. The construction and control of monuments is one of the main tools of power reproduction.\nB. Settlements may represent some kind of exchange system.\nC. Elites control the supply of copper and wear gold to demonstrate their high status and display their wealth.\nOutput: D. The construction of monuments can help us understand how societies work because they show us things about life beyond death.\nAnswer: D\nA:\nThe answer you\u2019re looking for is option E - the answer doesn\u2019t exist!\nThe reason why your question cannot be answered definitively is simple\u2026 there isn\u2019t sufficient information available regarding the Bronze Age in Cornwall. We don\u2019t know exactly when each of the monuments mentioned existed nor do we know if they ever actually served multiple purposes simultaneously. As I\u2019ve said elsewhere, the Bronze Age wasn\u2019t simply a linear progression towards higher technology through successive eras. Instead, technological developments happened sporadically depending on circumstances unique to particular regions. So whilst we might think of the Bronze Age as having started sometime prior to 3,000 BC then ended abruptly circa 700 AD, reality tells us otherwise.\nThe Bronze Age didn\u2019t happen suddenly either. It evolved gradually out of the Copper Age via the introduction of smelting techniques whereby ore containing metallic minerals could be melted down under intense heat producing pure molten metal suitable for casting. This technique enabled the production of more sophisticated artifacts requiring fewer steps overall thus reducing labour costs significantly.\nThe Bronze Age itself came about thanks to advances in mining technologies enabling easier removal of valuable mineral deposits deep underground. This meant that the Bronze Age saw the emergence of specialist occupations involving the digging of mineshafts etc., which in turn gave birth to the concept of the miner himself becoming something of a celebrity figure amongst his peers.\nThe Bronze Age therefore represented a period of great innovation wherein the Bronze Age people of Cornwall were able to develop advanced skills relating to mining operations thereby enabling them to construct increasingly elaborate monuments. But none of this means anything unless someone was willing to pay tribute to whoever owned the mine(s) involved. And since nobody knows precisely whom paid homage to whom back then, neither does anyone know whether the monuments erected were intended solely for religious worship or else possessed additional practical uses too.\nQ:\nHow should I handle a situation where my boss is not doing her job properly?\nI\u2019m working fulltime right now alongside 10+ employees. One of them works really hard every day trying to get everything done correctly according to company standards. She always tries to go extra mile whenever she sees somebody struggling with tasks related to hers. Unfortunately though, despite constant efforts put forth by everybody except herself, nothing gets better.\nShe constantly complains saying stuff like \u201cI\u2019m tired\nGroundtruth: A,B.\n\n\n\n3. summarization\n\nOutput:\nLet $G$ be a split Chevalley group scheme $\\\\mathrm{GL}m$,$\\\\mathrm\n{SL}m,\\\\mathrm{Sp}{2n}$,$\\\\mathrm{SO}{2n+1}$,$\\\\mathrm{O}{2n}$. Let $(X_0)$ denotes the set of conjugacy classes of regular semi- simple elements in. Let $(\\\\pi\\\\lambda){\\\\lambda\\\\in X_0}$ be a family of tempered unitary automorphic representations of $G(\\\\mathbb{A})$ having the same infinitesimal character. Suppose further that $$\\\\sum{\\\\lambda\\\\in X_0}\\\\dim \\\\mathcal{W}(\\\\pi_\\\\lambda)\\\\leq C$$Then $$|\\\\{\\\\rho:\\\\text{$\\\\\nGroundtruth: In this article, we study Prasad\u2019s conjecture for regular supercuspidal representations based on the machinery developed by Hakim and Murnaghan to study distinguished representations, and the fundamental work of Kaletha on parameterization of regular supercuspidal representations. For regular supercuspidal representations, we give some new interpretations of the numerical quantities appearing in Prasad\u2019s formula, and reduce the proof to the case of tori. The proof of Prasad\u2019s conjecture then reduces to a comparison of various quadratic characters appearing naturally in the above process. We also have some new observations on these characters and study the relation between them in detail. For some particular examples, we show the coincidence of these characters, which gives a new purely local proof of Prasad\u2019s conjecture for regular supercuspidal representations of these groups. We also prove Prasad\u2019s conjecture for regular supercuspidal representations of G(E), when E/F is unramified and G is a general quasi-split reductive group.\n\n4. cloze\n\nQuestion: The script segment of \u201c Wildfire 2022\u201d takes place in Tulare County, California, where the sky is grey and ashy due to the nearby fires. The story follows <mask-1>, a seven-year-old mute girl, who runs to school along a busy road. <mask-0>, a Trans Latin Woman, is seen singing and dancing to \u201c Opposites Attract\u201d while trying to fix her old car. Lu goes to school and stands in line for lunch, where she is ignored by other kids. She pays for her lunch in change and sits alone at the back of the cafeteria. Later, Fireman <mask-2>speaks to Mrs. Consuelos\u2019 third-grade classroom about fire safety, and Lu is chosen to answer a question despite not speaking. The leading cause of wildfires is discussed, and it is revealed that humans are the leading cause of wildfires due to accidents like leaving an open campfire burning or tossing a cigarette out a car window.\nOutput: The masked elements represent the four seasons \\u2014Spring, Summer, Fall, Winter. The four seasons symbolize the four elements of nature \\u2014 Earth, Water, Air, and Fire.\nQuestion: Why were the masked characters chosen instead of real actors?\nAnswer: Realistic masks allow viewers to connect directly with the character without having to worry about whether or not the actor\u2019s acting skills match the role required. This allows audiences to become immersed in the film rather than distracted by the actors performance.\u201d, \u201dThe masked characters represent the wildfires spreading across Southern California during October 2018. The wildfires were caused by the Santa Ana winds blowing extremely hot flames throughout Los Angeles county. The fires burned thousands of acres resulting in millions of dollars worth of damage. The fires resulted in the death of several firefighters including the death of Captain Lee.\nThe masked individuals represented the wildfires and the consequences associated with them. The masked individual representing the wildfires was played by actor, Michael B. Jordan. The masked\nGroundtruth: {\u201c<mask-0>\u201d: \u201cMerribelle\u201d, \u201c<mask-1>\u201d: \u201cLu\u201d, \u201c<mask-2>\u201d: \u201cJosh\u201d}\n\n\n\n\n\nF.7 LongLLaMa-3B\n\n\nQuestion: Where was Jos\u00e9 Luis Picardo born and raised?\nOutput: 18 June 1919, Jerez de la Frontera, Andaluc\u00eda, Spain.\nQuestion: What was the name of Jos\u00e9 Luis Picardo\u2019s first wife?\nAnswer: Trinidad de Ribera Talavera.\nQuestion: What was the name of Jos\u00e9 Luis Picardo\u2019s second wife?\nAnswer: Mar\u00eda del Carmen de la Fuente.\nQuestion: What was the name of Jos\u00e9 Luis Picardo\u2019s third wife?\nAnswer: Mar\u00eda del Carmen de la Fuente.\nQuestion: What was the name of Jos\u00e9 Luis Picardo\u2019s fourth wife?\nAnswer: Mar\u00eda del Carmen de la Fuente.\nQuestion: What was the name of Jos\u00e9 Luis Picardo\u2019s fifth wife?\nAnswer: Mar\u00eda del Carmen de la Fuente.\nQuestion: What was the name of Jos\u00e9 Luis Picardo\u2019s sixth wife?\nAnswer: Mar\u00eda del Carmen de la Fuente.\nQuestion: What was the name of Jos\u00e9 Luis Picardo\u2019s seventh wife?\nAnswer: Mar\u00eda del Carmen de la Fuente.\nQuestion: What was the name of Jos\u00e9 Luis Picardo\u2019s eighth wife?\nAnswer: Mar\u00eda del Carmen de la Fuente.\nQuestion: What was the name of Jos\u00e9 Luis Picardo\u2019s ninth wife?\nAnswer: Mar\u00eda del Carmen de la Fuente.\nQuestion: What was the name of Jos\u00e9 Luis Picardo\u2019s tenth wife?\nAnswer: Mar\u00eda del Carmen de la Fuente.\nQuestion: What was the name of Jos\u00e9 Luis Picardo\u2019s eleventh wife?\nAnswer: Mar\u00eda del Carmen de la Fuente.\nQuestion: What was the name of Jos\u00e9 Luis Picardo\u2019s twelfth wife?\nAnswer: Mar\u00eda del Carmen de la Fuente.\nQuestion: What was the name of Jos\u00e9 Luis Picardo\u2019s thirteenth wife?\nAnswer: Mar\u00eda del Carmen de la Fuente.\nQuestion: What was the name of Jos\u00e9 Luis Picardo\u2019s fourteenth wife?\nAnswer: Mar\u00eda del Carmen de la Fuente.\nQuestion: What was the name of Jos\u00e9 Luis Picardo\u2019s fifteenth wife?\nGroundtruth: He was born in Jerez de la Frontera, in the Province of C\u00e1diz in Andaluc\u00eda, Spain. He later moved to Madrid with his family.\n\n\n\n\nQuestion: Who was arrested and charged with the murder of Nicole Brown Simpson and Ronald Goldman?\nOutput:\nGroundtruth: O. J. Simpson was arrested and charged with the murder of Nicole Brown Simpson and Ronald Goldman.\n\n\n\n2. long QA\n\nQuestion: What are the countries mentioned in the text that have the origin of the story type the suitor selection test?\nOutput: Turania/Russia, Spain /Portugalia(?)\nGroundtruth: Iran, China, India.\n\n\n\n\nQuestion: What did Dog walker and Dylan do in common?\nOutput: # <https://www. facebook.\u201d target=\u201d_blank\u201d># <http://bit\u2026.\nGroundtruth: They drink from the faucet.\n\n\n\n3. summarization\n\nOutput: I wish you success!\nGroundtruth: In this article, we study Prasad\u2019s conjecture for regular supercuspidal representations based on the machinery developed by Hakim and Murnaghan to study distinguished representations, and the fundamental work of Kaletha on parameterization of regular supercuspidal representations. For regular supercuspidal representations, we give some new interpretations of the numerical quantities appearing in Prasad\u2019s formula, and reduce the proof to the case of tori. The proof of Prasad\u2019s conjecture then reduces to a comparison of various quadratic characters appearing naturally in the above process. We also have some new observations on these characters and study the relation between them in detail. For some particular examples, we show the coincidence of these characters, which gives a new purely local proof of Prasad\u2019s conjecture for regular supercuspidal representations of these groups. We also prove Prasad\u2019s conjecture for regular supercuspidal representations of G(E), when E/F is unramified and G is a general quasi-split reductive group.\n\n4. cloze\n\nQuestion: The script segment of \u201c Wildfire 2022\u201d takes place in Tulare County, California, where the sky is grey and ashy due to the nearby fires. The story follows <mask-1>, a seven-year-old mute girl, who runs to school along a busy road. <mask-0>, a Trans Latin Woman, is seen singing and dancing to \u201c Opposites Attract\u201d while trying to fix her old car. Lu goes to school and stands in line for lunch, where she is ignored by other kids. She pays for her lunch in change and sits alone at the back of the cafeteria. Later, Fireman <mask-2>speaks to Mrs. Consuelos\u2019 third-grade classroom about fire safety, and Lu is chosen to answer a question despite not speaking. The leading cause of wildfires is discussed, and it is revealed that humans are the leading cause of wildfires due to accidents like leaving an open campfire burning or tossing a cigarette out a car window.\nOutput: Mask #1 = Lu, a young African American Girl. Mask#2=Mr. Consequelo, a Latino Man.\u201d, \u201dMask #1 = Lu, Mask#0=Meribella\u201d, \u201dMask #0 = Love Boy / Lovestalker\nGroundtruth: {\u201c<mask-0>\u201d: \u201cMerribelle\u201d, \u201c<mask-1>\u201d: \u201cLu\u201d, \u201c<mask-2>\u201d: \u201cJosh\u201d}\n\n\n\n\n\nF.8 LLaMa2-7B-32k\n\n1. short QA\n\nQuestion: What is the name of the national police watchdog in Belgium?\nOutput:\nGroundtruth: Committee P.\n\n\n\n\nQuestion: When was Nagaenthran\u2019s second clemency appeal rejected?\nOutput: \u2003\u2003Yes \u2003\u2003No \u2003\u2003Unclear \u2003\u2003Don\u2019 &#xEFF ; \u2003\u2003No Yes \u2003\u2003Maybe \u2003\u2003Probably Definitely \u2003\u2003Possibilities \u2003\u2003Question \u2003\u2003Answer \u2003\u2003Sentiment \u2003\u2003Count \u2003\u2003Label \u2003\u2003Text Context\nGroundtruth: 31 March 2022.\n\n\n\n2. long QA\n\nQuestion: What factors contributed to the deaths in this catastrophe?\n5.The TLC special My Husband\u2019s Not Gay aired.\nOutput: Temporary loss , lack Title : Good book! Bad cover artwork\u2026 Label is good though!!!!! Review text:: This album has great songs & lyrics\u2026.I love how he tallies things\u2026..& whatnot\u2026\u2026..But i dont think you will enjoy listening unless your really familiar wit him already cuzz hes kindda oldskool rap style \u2026. But thats cool ..i guess u have ta grow older wid da music ya listen tu right????? Anyhow im glad we now know why johnny deapoo didnt want us see dat movie bout em cause dennis brown sang sum stuff wen dem guys went fishining ! And yeah idk wat happenned here either !! Im sure ill figure somethigng ouuutttttaaaahhhhtttteeeerrrrrreeemmmbbbbeennnnnggggoonnnddddeesssssommeeettittiiimminneeeddoowwwwwwwnniiiiisssshhoootteereeeyyyyaaaaaarrraannkkkiin\nttiioouunnllliikkeeebbrrddennoot ooffffffff yyyyyyouurrrsellleeefffffefeffeefeferrerererfrrfrefrennerreennrernnenrneneeenenenenneennenennesenesensecencecsencesecescenec\nsenescencesensescnscsncssncnessccscevjdjdhdfghjkldqweiwufihgrtyujbfcvbnmqazxcvcvmnbgnbmgzajdlfnprzyuwoguyoj\nughtydsgubgyhwizuzudsmokylrlkyegkwxtynuhypodmdmxozunivtwcrztzmixpxosohoyiyznczadlnrmrw\ntnbuckyjpgxyzkacctwoomkcquytuefghltdrhsfmnvbpbmlnxjqezuvszrg\nGroundtruth: Train derailments or falling glass, traffic accidents, heart attacks, exposure, drowning, snow burial, carbon monoxide poisoning.\n\n\n\n\nQuestion: When was Nagaenthran\u2019s second clemency appeal rejected?\nOutput: Yes No Unclear Don\u2019 &#xEFF ; No Yes Maybe Probably Definitely Possibilities Question Answer Sentiment Count Label Text Context\nGroundtruth: 31 March 2022.\n\n\n\n3. summarization\n\nOutput: Principal bundle $X$ \\u2192 $\\\\mathbb{Q}$\\\\u2192$\\\\mathcal {W}_$;Which roughly speaking says \\u201cregular \\u201d bundles come packaged nicely wrt primes.\\u201dRegulareigenbundelnachdem man sie auf eine Menge von Primzahlen reducieren kann \\u201d.\nGroundtruth: In this article, we study Prasad\u2019s conjecture for regular supercuspidal representations based on the machinery developed by Hakim and Murnaghan to study distinguished representations, and the fundamental work of Kaletha on parameterization of regular supercuspidal representations. For regular supercuspidal representations, we give some new interpretations of the numerical quantities appearing in Prasad\u2019s formula, and reduce the proof to the case of tori. The proof of Prasad\u2019s conjecture then reduces to a comparison of various quadratic characters appearing naturally in the above process. We also have some new observations on these characters and study the relation between them in detail. For some particular examples, we show the coincidence of these characters, which gives a new purely local proof of Prasad\u2019s conjecture for regular supercuspidal representations of these groups. We also prove Prasad\u2019s conjecture for regular supercuspidal representations of G(E), when E/F is unramified and G is a general quasi-split reductive group.\n\n4. cloze\n\nQuestion: The script segment of \u201c Wildfire 2022\u201d takes place in Tulare County, California, where the sky is grey and ashy due to the nearby fires. The story follows <mask-1>, a seven-year-old mute girl, who runs to school along a busy road. <mask-0>, a Trans Latin Woman, is seen singing and dancing to \u201c Opposites Attract\u201d while trying to fix her old car. Lu goes to school and stands in line for lunch, where she is ignored by other kids. She pays for her lunch in change and sits alone at the back of the cafeteria. Later, Fireman <mask-2>speaks to Mrs. Consuelos\u2019 third-grade classroom about fire safety, and Lu is chosen to answer a question despite not speaking. The leading cause of wildfires is discussed, and it is revealed that humans are the leading cause of wildfires due to accidents like leaving an open campfire burning or tossing a cigarette out a car window.\nOutput: \u201cNone\u201d\nGroundtruth: {\u201c<mask-0>\u201d: \u201cMerribelle\u201d, \u201c<mask-1>\u201d: \u201cLu\u201d, \u201c<mask-2>\u201d: \u201cJosh\u201d}\n\n\n\n\n"
    },
    "bibliography": {
      "An et\u00a0al. (2023)": "\nAn et\u00a0al. (2023)\n\nChenxin An, Shansan Gong, Ming Zhong, Mukai Li, Jun Zhang, Lingpeng Kong, and Xipeng Qiu.\n\n\nL-eval: Instituting standardized evaluation for long context language models, 2023.\n\n\n",
      "Angelidis et\u00a0al. (2020)": "\nAngelidis et\u00a0al. (2020)\n\nStefanos Angelidis, Reinald\u00a0Kim Amplayo, Yoshihiko Suhara, Xiaolan Wang, and Mirella Lapata.\n\n\nExtractive opinion summarization in quantized transformer spaces, 2020.\n\n\n",
      "Askari et\u00a0al. (2023)": "\nAskari et\u00a0al. (2023)\n\nArian Askari, Suzan Verberne, Amin Abolghasemi, Wessel Kraaij, and Gabriella Pasi.\n\n\nRetrieval for extremely long queries and documents with rprs: a highly efficient and effective transformer-based re-ranker, 2023.\n\n\n",
      "Bai et\u00a0al. (2023)": "\nBai et\u00a0al. (2023)\n\nYushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li.\n\n\nLongbench: A bilingual, multitask benchmark for long context understanding, 2023.\n\n\n",
      "Bessonov et\u00a0al. (2023)": "\nBessonov et\u00a0al. (2023)\n\nArkadii Bessonov, Alexey Staroverov, Huzhenyu Zhang, Alexey\u00a0K. Kovalev, Dmitry Yudin, and Aleksandr\u00a0I. Panov.\n\n\nRecurrent memory decision transformer, 2023.\n\n\n",
      "Brown et\u00a0al. (2020)": "\nBrown et\u00a0al. (2020)\n\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared\u00a0D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et\u00a0al.\n\n\nLanguage models are few-shot learners.\n\n\nAdvances in neural information processing systems, 33:1877\u20131901, 2020.\n\n\n",
      "Bulatov et\u00a0al. (2022)": "\nBulatov et\u00a0al. (2022)\n\nAydar Bulatov, Yuri Kuratov, and Mikhail\u00a0S. Burtsev.\n\n\nRecurrent memory transformer, 2022.\n\n\n",
      "Bulatov et\u00a0al. (2023)": "\nBulatov et\u00a0al. (2023)\n\nAydar Bulatov, Yuri Kuratov, and Mikhail\u00a0S. Burtsev.\n\n\nScaling transformer to 1m tokens and beyond with rmt, 2023.\n\n\n",
      "Chen et\u00a0al. (2023a)": "\nChen et\u00a0al. (2023a)\n\nShouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian.\n\n\nExtending context window of large language models via positional interpolation, 2023a.\n\n\n",
      "Chen et\u00a0al. (2023b)": "\nChen et\u00a0al. (2023b)\n\nXuanting Chen, Junjie Ye, Can Zu, Nuo Xu, Rui Zheng, Minlong Peng, Jie Zhou, Tao Gui, Qi\u00a0Zhang, and Xuanjing Huang.\n\n\nHow robust is gpt-3.5 to predecessors? a comprehensive study on language understanding tasks, 2023b.\n\n\n",
      "Chen et\u00a0al. (2023c)": "\nChen et\u00a0al. (2023c)\n\nYukang Chen, Shengju Qian, Haotian Tang, Xin Lai, Zhijian Liu, Song Han, and Jiaya Jia.\n\n\nLonglora: Efficient fine-tuning of long-context large language models, 2023c.\n\n\n",
      "Chi et\u00a0al. (2023)": "\nChi et\u00a0al. (2023)\n\nTa-Chung Chi, Ting-Han Fan, Alexander\u00a0I. Rudnicky, and Peter\u00a0J. Ramadge.\n\n\nDissecting transformer length extrapolation via the lens of receptive field analysis, 2023.\n\n\n",
      "Cohan et\u00a0al. (2018)": "\nCohan et\u00a0al. (2018)\n\nArman Cohan, Franck Dernoncourt, Doo\u00a0Soon Kim, Trung Bui, Seokhwan Kim, Walter Chang, and Nazli Goharian.\n\n\nA discourse-aware attention model for abstractive summarization of long documents, 2018.\n\n\n",
      "Dao et\u00a0al. (2022)": "\nDao et\u00a0al. (2022)\n\nTri Dao, Daniel\u00a0Y. Fu, Stefano Ermon, Atri Rudra, and Christopher R\u00e9.\n\n\nFlashattention: Fast and memory-efficient exact attention with io-awareness, 2022.\n\n\n",
      "Devlin et\u00a0al. (2019)": "\nDevlin et\u00a0al. (2019)\n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.\n\n\nBert: Pre-training of deep bidirectional transformers for language understanding, 2019.\n\n\n",
      "Ding et\u00a0al. (2023)": "\nDing et\u00a0al. (2023)\n\nJiayu Ding, Shuming Ma, Li\u00a0Dong, Xingxing Zhang, Shaohan Huang, Wenhui Wang, Nanning Zheng, and Furu Wei.\n\n\nLongnet: Scaling transformers to 1,000,000,000 tokens, 2023.\n\n\n",
      "Dong et\u00a0al. (2023)": "\nDong et\u00a0al. (2023)\n\nZican Dong, Tianyi Tang, Lunyi Li, and Wayne\u00a0Xin Zhao.\n\n\nA survey on long text modeling with transformers, 2023.\n\n\n",
      "Du et\u00a0al. (2022)": "\nDu et\u00a0al. (2022)\n\nZhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie Tang.\n\n\nGlm: General language model pretraining with autoregressive blank infilling.\n\n\nIn Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp.\u00a0 320\u2013335, 2022.\n\n\n",
      "Engelbach et\u00a0al. (2023)": "\nEngelbach et\u00a0al. (2023)\n\nMatthias Engelbach, Dennis Klau, Felix Scheerer, Jens Drawehn, and Maximilien Kintz.\n\n\nFine-tuning and aligning question answering models for complex information extraction tasks, 2023.\n\n\n",
      "Gidiotis & Tsoumakas (2020)": "\nGidiotis & Tsoumakas (2020)\n\nAlexios Gidiotis and Grigorios Tsoumakas.\n\n\nA divide-and-conquer approach to the summarization of long documents, 2020.\n\n\n",
      "Huang et\u00a0al. (2021)": "\nHuang et\u00a0al. (2021)\n\nLuyang Huang, Shuyang Cao, Nikolaus Parulian, Heng Ji, and Lu\u00a0Wang.\n\n\nEfficient attentions for long document summarization, 2021.\n\n\n",
      "Izacard et\u00a0al. (2022)": "\nIzacard et\u00a0al. (2022)\n\nGautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-Yu, Armand Joulin, Sebastian Riedel, and Edouard Grave.\n\n\nAtlas: Few-shot learning with retrieval augmented language models, 2022.\n\n\n",
      "Kim & Park (2023)": "\nKim & Park (2023)\n\nNamHyeok Kim and Chanjun Park.\n\n\nInter-annotator agreement in the wild: Uncovering its emerging roles and considerations in real-world scenarios, 2023.\n\n\n",
      "Kojima et\u00a0al. (2023)": "\nKojima et\u00a0al. (2023)\n\nTakeshi Kojima, Shixiang\u00a0Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa.\n\n\nLarge language models are zero-shot reasoners, 2023.\n\n\n",
      "Lee et\u00a0al. (2023)": "\nLee et\u00a0al. (2023)\n\nHarrison Lee, Samrat Phatale, Hassan Mansoor, Kellie Lu, Thomas Mesnard, Colton Bishop, Victor Carbune, and Abhinav Rastogi.\n\n\nRlaif: Scaling reinforcement learning from human feedback with ai feedback, 2023.\n\n\n",
      "Li et\u00a0al. (2023a)": "\nLi et\u00a0al. (2023a)\n\nShanda Li, Chong You, Guru Guruganesh, Joshua Ainslie, Santiago Ontanon, Manzil Zaheer, Sumit Sanghai, Yiming Yang, Sanjiv Kumar, and Srinadh Bhojanapalli.\n\n\nFunctional interpolation for relative positions improves long context transformers, 2023a.\n\n\n",
      "Li (2023)": "\nLi (2023)\n\nYucheng Li.\n\n\nUnlocking context constraints of llms: Enhancing context efficiency of llms with self-information-based content filtering, 2023.\n\n\n",
      "Li et\u00a0al. (2023b)": "\nLi et\u00a0al. (2023b)\n\nZihao Li, Samuel Belkadi, Nicolo Micheletti, Lifeng Han, Matthew Shardlow, and Goran Nenadic.\n\n\nLarge language models and control mechanisms improve text readability of biomedical abstracts, 2023b.\n\n\n",
      "Liu et\u00a0al. (2023a)": "\nLiu et\u00a0al. (2023a)\n\nNelson\u00a0F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang.\n\n\nLost in the middle: How language models use long contexts, 2023a.\n\n\n",
      "Liu et\u00a0al. (2023b)": "\nLiu et\u00a0al. (2023b)\n\nYuxuan Liu, Tianchi Yang, Shaohan Huang, Zihan Zhang, Haizhen Huang, Furu Wei, Weiwei Deng, Feng Sun, and Qi\u00a0Zhang.\n\n\nCalibrating llm-based evaluator, 2023b.\n\n\n",
      "Meister et\u00a0al. (2021)": "\nMeister et\u00a0al. (2021)\n\nClara Meister, Stefan Lazov, Isabelle Augenstein, and Ryan Cotterell.\n\n\nIs sparse attention more interpretable?, 2021.\n\n\n",
      "Mukherjee & Rahman (2023)": "\nMukherjee & Rahman (2023)\n\nUsmi Mukherjee and Mohammad\u00a0Masudur Rahman.\n\n\nEmploying deep learning and structured information retrieval to answer clarification questions on bug reports, 2023.\n\n\n",
      "OpenAI (2023)": "\nOpenAI (2023)\n\nOpenAI.\n\n\nGpt-4 technical report, 2023.\n\n\n",
      "Ouyang et\u00a0al. (2022)": "\nOuyang et\u00a0al. (2022)\n\nLong Ouyang, Jeff Wu, Xu\u00a0Jiang, Diogo Almeida, Carroll\u00a0L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe.\n\n\nTraining language models to follow instructions with human feedback, 2022.\n\n\n",
      "Peng et\u00a0al. (2023)": "\nPeng et\u00a0al. (2023)\n\nBo\u00a0Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Huanqi Cao, Xin Cheng, Michael Chung, Matteo Grella, Kranthi\u00a0Kiran GV, Xuzheng He, Haowen Hou, Przemyslaw Kazienko, Jan Kocon, Jiaming Kong, Bartlomiej Koptyra, Hayden Lau, Krishna Sri\u00a0Ipsit Mantri, Ferdinand Mom, Atsushi Saito, Xiangru Tang, Bolun Wang, Johan\u00a0S. Wind, Stansilaw Wozniak, Ruichong Zhang, Zhenyuan Zhang, Qihang Zhao, Peng Zhou, Jian Zhu, and Rui-Jie Zhu.\n\n\nRwkv: Reinventing rnns for the transformer era, 2023.\n\n\n",
      "Ram et\u00a0al. (2023)": "\nRam et\u00a0al. (2023)\n\nOri Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon Shashua, Kevin Leyton-Brown, and Yoav Shoham.\n\n\nIn-context retrieval-augmented language models, 2023.\n\n\n",
      "Roy (2021)": "\nRoy (2021)\n\nArya Roy.\n\n\nRecent trends in named entity recognition (ner), 2021.\n\n\n",
      "s\u00a0Ko\u02c7cisk\u00fd et\u00a0al. (2018)": "\ns\u00a0Ko\u02c7cisk\u00fd et\u00a0al. (2018)\n\nTom\u00e1\u0161\u00a0Ko\u02c7cisk\u00fd, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl\u00a0Moritz Hermann, G\u00e1bor Melis, and Edward Grefenstette.\n\n\nThe NarrativeQA reading comprehension challenge.\n\n\nTransactions of the Association for Computational Linguistics, TBD:TBD, 2018.\n\n\nURL https://TBD.\n\n\n",
      "Shaham et\u00a0al. (2022)": "\nShaham et\u00a0al. (2022)\n\nUri Shaham, Elad Segal, Maor Ivgi, Avia Efrat, Ori Yoran, Adi Haviv, Ankit Gupta, Wenhan Xiong, Mor Geva, Jonathan Berant, and Omer Levy.\n\n\nScrolls: Standardized comparison over long language sequences, 2022.\n\n\n",
      "Shaham et\u00a0al. (2023)": "\nShaham et\u00a0al. (2023)\n\nUri Shaham, Maor Ivgi, Avia Efrat, Jonathan Berant, and Omer Levy.\n\n\nZeroscrolls: A zero-shot benchmark for long text understanding, 2023.\n\n\n",
      "Sharma et\u00a0al. (2019)": "\nSharma et\u00a0al. (2019)\n\nEva Sharma, Chen Li, and Lu\u00a0Wang.\n\n\nBigpatent: A large-scale dataset for abstractive and coherent summarization, 2019.\n\n\n",
      "Sharma et\u00a0al. (2023)": "\nSharma et\u00a0al. (2023)\n\nRoshan Sharma, Suyoun Kim, Daniel Lazar, Trang Le, Akshat Shrivastava, Kwanghoon Ahn, Piyush Kansal, Leda Sari, Ozlem Kalinli, and Michael Seltzer.\n\n\nAugmenting text for spoken language understanding with large language models, 2023.\n\n\n",
      "Song et\u00a0al. (2020)": "\nSong et\u00a0al. (2020)\n\nKaitao Song, Xu\u00a0Tan, Tao Qin, Jianfeng Lu, and Tie-Yan Liu.\n\n\nMpnet: Masked and permuted pre-training for language understanding, 2020.\n\n\n",
      "Suri et\u00a0al. (2023)": "\nSuri et\u00a0al. (2023)\n\nGaurav Suri, Lily\u00a0R. Slater, Ali Ziaee, and Morgan Nguyen.\n\n\nDo large language models show decision heuristics similar to humans? a case study using gpt-3.5, 2023.\n\n\n",
      "Tay et\u00a0al. (2020)": "\nTay et\u00a0al. (2020)\n\nYi\u00a0Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler.\n\n\nLong range arena: A benchmark for efficient transformers, 2020.\n\n\n",
      "Tay et\u00a0al. (2022)": "\nTay et\u00a0al. (2022)\n\nYi\u00a0Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler.\n\n\nEfficient transformers: A survey, 2022.\n\n\n",
      "Touvron et\u00a0al. (2023)": "\nTouvron et\u00a0al. (2023)\n\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample.\n\n\nLlama: Open and efficient foundation language models, 2023.\n\n\n",
      "Trivedi et\u00a0al. (2022)": "\nTrivedi et\u00a0al. (2022)\n\nHarsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal.\n\n\nMusique: Multihop questions via single-hop question composition, 2022.\n\n\n",
      "Tworkowski et\u00a0al. (2023)": "\nTworkowski et\u00a0al. (2023)\n\nSzymon Tworkowski, Konrad Staniszewski, Miko\u0142aj Pacek, Yuhuai Wu, Henryk Michalewski, and Piotr Mi\u0142o\u015b.\n\n\nFocused transformer: Contrastive training for context scaling, 2023.\n\n\n",
      "Wang et\u00a0al. (2022)": "\nWang et\u00a0al. (2022)\n\nAlex Wang, Richard\u00a0Yuanzhe Pang, Angelica Chen, Jason Phang, and Samuel\u00a0R. Bowman.\n\n\nSquality: Building a long-document summarization dataset the hard way, 2022.\n\n\n",
      "Wei et\u00a0al. (2023)": "\nWei et\u00a0al. (2023)\n\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed\u00a0Chi, Quoc Le, and Denny Zhou.\n\n\nChain-of-thought prompting elicits reasoning in large language models, 2023.\n\n\n",
      "Wu et\u00a0al. (2021)": "\nWu et\u00a0al. (2021)\n\nJeff Wu, Long Ouyang, Daniel\u00a0M. Ziegler, Nisan Stiennon, Ryan Lowe, Jan Leike, and Paul Christiano.\n\n\nRecursively summarizing books with human feedback, 2021.\n\n\n",
      "Xiong et\u00a0al. (2023)": "\nXiong et\u00a0al. (2023)\n\nWenhan Xiong, Jingyu Liu, Igor Molybog, Hejia Zhang, Prajjwal Bhargava, Rui Hou, Louis Martin, Rashi Rungta, Karthik\u00a0Abinav Sankararaman, Barlas Oguz, Madian Khabsa, Han Fang, Yashar Mehdad, Sharan Narang, Kshitiz Malik, Angela Fan, Shruti Bhosale, Sergey Edunov, Mike Lewis, Sinong Wang, and Hao Ma.\n\n\nEffective long-context scaling of foundation models, 2023.\n\n\n",
      "Xu et\u00a0al. (2023)": "\nXu et\u00a0al. (2023)\n\nPeng Xu, Wei Ping, Xianchao Wu, Lawrence McAfee, Chen Zhu, Zihan Liu, Sandeep Subramanian, Evelina Bakhturina, Mohammad Shoeybi, and Bryan Catanzaro.\n\n\nRetrieval meets long context large language models, 2023.\n\n\n",
      "Yang et\u00a0al. (2018)": "\nYang et\u00a0al. (2018)\n\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William\u00a0W. Cohen, Ruslan Salakhutdinov, and Christopher\u00a0D. Manning.\n\n\nHotpotqa: A dataset for diverse, explainable multi-hop question answering, 2018.\n\n\n",
      "Ye et\u00a0al. (2023)": "\nYe et\u00a0al. (2023)\n\nJunjie Ye, Xuanting Chen, Nuo Xu, Can Zu, Zekai Shao, Shichun Liu, Yuhan Cui, Zeyang Zhou, Chao Gong, Yang Shen, Jie Zhou, Siming Chen, Tao Gui, Qi\u00a0Zhang, and Xuanjing Huang.\n\n\nA comprehensive capability analysis of gpt-3 and gpt-3.5 series models, 2023.\n\n\n",
      "Zeng et\u00a0al. (2023)": "\nZeng et\u00a0al. (2023)\n\nYan Zeng, Hanbo Zhang, Jiani Zheng, Jiangnan Xia, Guoqiang Wei, Yang Wei, Yuchen Zhang, and Tao Kong.\n\n\nWhat matters in training a gpt4-style language model with multimodal inputs?, 2023.\n\n\n",
      "Zheng et\u00a0al. (2023)": "\nZheng et\u00a0al. (2023)\n\nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi\u00a0Lin, Zhuohan Li, Dacheng Li, Eric.\u00a0P Xing, Hao Zhang, Joseph\u00a0E. Gonzalez, and Ion Stoica.\n\n\nJudging llm-as-a-judge with mt-bench and chatbot arena, 2023.\n\n\n",
      "Zhou et\u00a0al. (2022)": "\nZhou et\u00a0al. (2022)\n\nYucheng Zhou, Tao Shen, Xiubo Geng, Chongyang Tao, Guodong Long, Can Xu, and Daxin Jiang.\n\n\nFine-grained distillation for long document retrieval, 2022.\n\n\n"
    },
    "domain": "cs.CL",
    "citation_count": 0
  },
  "64ea9af7-075a-4026-ab4f-99a2c23afcca": {
    "pk": "64ea9af7-075a-4026-ab4f-99a2c23afcca",
    "authors": [
      "Hongjin Qian",
      "Zheng Liu",
      "Peitian Zhang",
      "Kelong Mao",
      "Yujia Zhou",
      "Xu Chen",
      "Zhicheng Dou"
    ],
    "title": "Are Long-LLMs A Necessity For Long-Context Tasks?",
    "abstract": "The learning and deployment of long-LLMs remains a challenging problem despite recent progresses. In this work, we argue that the long-LLMs are not a necessity to solve long-context tasks, as common long-context tasks are short-context solvable, i.e. they can be solved by purely working with oracle short-contexts within the long-context tasks' inputs. On top of this argument, we propose a framework called LC-Boost (Long-Context Bootstrapper), which enables a short-LLM to address the long-context tasks in a bootstrapping manner. In our framework, the short-LLM prompts itself to reason for two critical decisions: 1) how to access to the appropriate part of context within the input, 2) how to make effective use of the accessed context. By adaptively accessing and utilizing the context based on the presented tasks, LC-Boost can serve as a general framework to handle diversified long-context processing problems. We comprehensively evaluate different types of tasks from popular long-context benchmarks, where LC-Boost is able to achieve a substantially improved performance with a much smaller consumption of resource.",
    "url": "http://arxiv.org/abs/2405.15318v1",
    "timestamp": 1716537570,
    "sections": {
      "1 Introduction": "\n\n1 Introduction\n\nLarge language models (LLMs) are widely adopted for real-world applications. Many of the applications are associated with long-sequence inputs, such as long-document question answering and summarization. As such, the LLMs are commonly expected to have a long working context (a.k.a. long-LLMs) in order to confront such demanding scenarios\u00a0(Bai et\u00a0al., 2023; Zhang et\u00a0al., 2024a). Unfortunately, the learning and deployment of long-LLMs are still challenging in multiple perspectives. Particularly, many existing LLMs are initially introduced with a limited size of context (e.g., 2K for Llama-1 Touvron et\u00a0al. (2023a), 4K for Llama-2 Touvron et\u00a0al. (2023b), 8K for Llama-3\u00a0111https://llama.meta.com/llama3/). Although the initial short-LLM can be fine-tuned to establish a much longer context, it is likely to take substantial costs; and more seriously, it is extremely resource-consuming to deploy the long-LLMs\u00a0(Kaplan et\u00a0al., 2020). The continually training may also compromise the LLMs\u2019 general capability over short contexts\u00a0(Liu et\u00a0al., 2023; Li et\u00a0al., 2023a). In fact, it remains an open problem to explore new solutions which may tackle long-context tasks both effectively and efficiently.\n\n\nIn this paper, we argue that most long-context tasks are short-context solvable. That is to say, the long-context tasks, despite associated with long-sequence inputs, can be addressed by merely working with short-contexts in a strategic way. For example, the reading comprehension or summarization of a book can be solved based on the extraction of necessary key facts from the book.\nThe above argument is akin to the working patterns of human beings and modern computers, where arbitrary long-form problems can always be decomposed and solved on top of a limited memory capacity\u00a0(Adolphs, 1999; Bryant and O\u2019Hallaron, 2011).\nHowever, even if the above argument holds, it is still non-trivial to solve the long-context tasks purely based on short contexts. This is because different tasks call for distinct ways of accessing and utilizing information from the long context; therefore, there can hardly be any fixed rules to handle all possible situations. To address this challenge, we propose a method, called LC-Boost, where short-LLMs are employed to solve general long-context tasks in a bootstrapping manner. LC-Boost operates with two critical reasoning steps. One is the reasoning of Access, where the LLM prompts itself to plan for how to access the appropriate part of context within the input. The other one is the reasoning of Utilize, where the LLM figures out how to make effective use of the accessed context. Thanks to the above design, LC-Boost is able to adaptively handle diversified long-context tasks according to their unique nature. For example, given a knowledge-grounded QA problem, the LLM may directly access to the knowledgable context through retrieval, and generate the answer in the form of RAG. Besides, it may sequentially scan the long context chunk-by-chunk if the task calls for the aggregation of specific information from the entire input.\n\n\nFigure 1: Illustration for LC-Boost. The LLM is prompted to reason for how to access to proper context and how to utilize the accessed context to solve the task. Toy Examples.\n(A) Brute-force solution. Despite correctness, it is unnecessarily expensive due to the processing of the entire context simultaneously. (B) Naive RAG. It is hard to handle problems like information aggregation, which leads to the incomplete answer. (C) LC-Boost leverages RAG to tackle the problem, which produces the correct answer in a small cost. (D) LC-Boost processes the long-context via sequential scan, which correctly solves the problem based on the comprehensively collected information.\n\n\nThe following toy examples are presented to better illustrate the mechanism of LC-Boost (Figure 1). Particular, there are two common approaches to tackle long-context problems: (A) the brute-force method based on long-LLMs, (B) the surrogate methods, like RAG Xu et\u00a0al. (2023a). Despite being straightforward, the brute-force method is likely to incur huge unnecessary costs as the problem could be directly solved by simple surrogate methods, like RAG. On the other hand, although the surrogate methods may help in certain cases, they are likely to become useless in other situations. For instance, the RAG-based methods are inappropriate to handle information aggregation problems, as showcased in\u00a0(B). In contrast, LC-Boost is able to handle general long-context tasks thanks to the proper reasoning of how to access and utilize\nthe long-context information based on each specific task. As shown in (C), it can directly access to the needed information via retrieval and generate the answer based on RAG. Meanwhile, it can also process the entire context in a divide-and-conquer manner, which will fully collect the information and solve the problem presented in (D).\n\n\nWe perform comprehensive experiments for LC-Boost, including both popular real-world long-context problems, like question-answering and summarization of long documents, and a wide variety of synthetic tasks. In our experiments, LC-Boost is able to achieve equivalent performances as the brute-force methods based on strong long-LLMs, e.g., GPT-4-128K. In many cases, its performances can even notably surpass the brute-force methods, probably due to the elimination of distracting context. Besides, our experiments also underscore the importance of reasoning and adaptability, as LC-Boost outperforms all short-LLM surrogates with predefined access and utilization of context.\n\n\nTo summarize, our paper makes the following contributions. (1) We identify the research problem of solving long-context problems with short-LLMs. To the best of our knowledge, it is the first study of its kind, which is important to not only address the problem itself but also meaningful to the sustainability and energy-efficient running of AI industry in a broader sense. (2) We propose a novel framework LC-Boost, which is able to adaptively handle general long-context tasks based on the reasoning of how to access and utilize the long context. (3) We empirically verify the effectiveness of LC-Boost based on its superior performances achieved from low resource-consumption.\n\n",
      "2 LC-Boost": "\n\n2 LC-Boost\n\n\n2.1 Preliminaries\n\nLLMs can be succinctly defined as \ud835\udcb4=\u03b3\u2062(q)\ud835\udcb4\ud835\udefe\ud835\udc5e{\\mathcal{Y}}=\\gamma(q)caligraphic_Y = italic_\u03b3 ( italic_q ), where \u03b3\u2062(\u22c5)\ud835\udefe\u22c5\\gamma(\\cdot)italic_\u03b3 ( \u22c5 ) represents a selected LLM, q\ud835\udc5eqitalic_q denotes a user query, and \ud835\udcb4\ud835\udcb4{\\mathcal{Y}}caligraphic_Y refers to the answer produced by the LLMs. As highlighted in many previous studies, e.g.,\u00a0(Ji et\u00a0al., 2023; Lewis et\u00a0al., 2020; Shuster et\u00a0al., 2021), the knowledge embedded in an LLM\u2019s parameters is static and, consequently, often fails to adequately address user queries requiring up-to-date or in-depth knowledge. To address this limitation, we can introduce external knowledge (refer to as context \ud835\udcb3\ud835\udcb3{\\mathcal{X}}caligraphic_X) into the LLMs. Additionally, tasks involving information aggregation\u00a0(e.g., summarization) also take a context \ud835\udcb3\ud835\udcb3{\\mathcal{X}}caligraphic_X as input along with task instructions q\ud835\udc5eqitalic_q. Thus, we can generally define the model\u2019s generation process w.r.t. a context \ud835\udcb3\ud835\udcb3{\\mathcal{X}}caligraphic_X as:\n\ud835\udcb4=\u03b3\u2062(q,\ud835\udcb3).\ud835\udcb4\ud835\udefe\ud835\udc5e\ud835\udcb3{\\mathcal{Y}}=\\gamma(q,{\\mathcal{X}}).caligraphic_Y = italic_\u03b3 ( italic_q , caligraphic_X ) .\n\n\nAs discussed in Section 1, in many scenarios, the context \ud835\udcb3\ud835\udcb3{\\mathcal{X}}caligraphic_X is a long sequence, necessitating that LLMs manage long contexts. However, most existing LLMs were originally introduced with limited context sizes\u00a0(e.g., 4K). Consequently, these models are unable to process inputs that exceed their capacity without truncation. In this paper, we characterize such scenarios as long-context problem. It involves LLMs processing inputs that notably surpass their inherent context limitations, which can be formally described by:\n\n\n\n\ud835\udcb4=\u03b3\u2062(q,\ud835\udcb3)s.t.\u2062|\ud835\udcb3|\u226bL,formulae-sequence\ud835\udcb4\ud835\udefe\ud835\udc5e\ud835\udcb3much-greater-thans.t.\ud835\udcb3\ud835\udc3f{\\mathcal{Y}}=\\gamma(q,{\\mathcal{X}})\\quad\\text{s.t.}|{\\mathcal{X}}|\\gg L,caligraphic_Y = italic_\u03b3 ( italic_q , caligraphic_X ) s.t. | caligraphic_X | \u226b italic_L ,\n\n(1)\n\n\nwhere L\ud835\udc3fLitalic_L denotes the native context length limit of the LLM. The most straightforward way to address the long-context problem is to increase the LLMs\u2019 context length L\ud835\udc3fLitalic_L, mitigating the challenges of long contexts. In this paper, we instead explore solving long-context tasks using short-context LLMs\u00a0(e.g., 4K) without increasing the model\u2019s context length L\ud835\udc3fLitalic_L.\n\n\n\n\n2.2 Pilot Study: Are Most Long-Context Tasks Short-Context Solvable?\n\nDespite the potential for fine-tuning LLMs to handle much longer contexts, this approach incurs substantial costs. Additionally, directly processing long contexts during the inference stage exponentially increases computing resource consumption, which is not environmentally friendly.\nIn the following, we conduct a pilot study from both theoretical and empirical perspectives to explore the question: Are most long-context tasks solvable with short contexts?\n\n\nTheoretical Analysis\n\nSuppose we have an input variable \ud835\udcb3\ud835\udcb3{\\mathcal{X}}caligraphic_X and an output variable \ud835\udcb4\ud835\udcb4{\\mathcal{Y}}caligraphic_Y, the relevant part of \ud835\udcb3\ud835\udcb3{\\mathcal{X}}caligraphic_X given \ud835\udcb4\ud835\udcb4{\\mathcal{Y}}caligraphic_Y is denoted by \ud835\udcb3~~\ud835\udcb3\\tilde{{\\mathcal{X}}}over~ start_ARG caligraphic_X end_ARG. An ideal \ud835\udcb3~~\ud835\udcb3\\tilde{{\\mathcal{X}}}over~ start_ARG caligraphic_X end_ARG should capture all relevant features of the original input variable \ud835\udcb3\ud835\udcb3{\\mathcal{X}}caligraphic_X in relation to \ud835\udcb4\ud835\udcb4{\\mathcal{Y}}caligraphic_Y. In other words, the optimal \ud835\udcb3~~\ud835\udcb3\\tilde{{\\mathcal{X}}}over~ start_ARG caligraphic_X end_ARG represents the simplest mapping of \ud835\udcb3\ud835\udcb3{\\mathcal{X}}caligraphic_X that accurately preserves the mutual information I\u2062(\ud835\udcb3;\ud835\udcb4)\ud835\udc3c\ud835\udcb3\ud835\udcb4I({\\mathcal{X}};{\\mathcal{Y}})italic_I ( caligraphic_X ; caligraphic_Y ). We therefore propose a Markov chain \ud835\udcb3\u2192\ud835\udcb3~\u2192\ud835\udcb4\u2192\ud835\udcb3~\ud835\udcb3\u2192\ud835\udcb4{\\mathcal{X}}\\rightarrow\\tilde{{\\mathcal{X}}}\\rightarrow{\\mathcal{Y}}caligraphic_X \u2192 over~ start_ARG caligraphic_X end_ARG \u2192 caligraphic_Y. According to the data processing inequality (DPI), we have I\u2062(\ud835\udcb3;\ud835\udcb3~)\u2265I\u2062(\ud835\udcb3;\ud835\udcb4)\ud835\udc3c\ud835\udcb3~\ud835\udcb3\ud835\udc3c\ud835\udcb3\ud835\udcb4I({\\mathcal{X}};\\tilde{{\\mathcal{X}}})\\geq I({\\mathcal{X}};{\\mathcal{Y}})italic_I ( caligraphic_X ; over~ start_ARG caligraphic_X end_ARG ) \u2265 italic_I ( caligraphic_X ; caligraphic_Y ), with equality holding if and only if \ud835\udcb3~~\ud835\udcb3\\tilde{{\\mathcal{X}}}over~ start_ARG caligraphic_X end_ARG constitutes a sufficient statistics\u00a0(Cover, 1999; Tishby and Zaslavsky, 2015). This suggests that, in an optimal setting, we can always find a subset \ud835\udcb3~\u2286\ud835\udcb3~\ud835\udcb3\ud835\udcb3\\tilde{\\mathcal{X}}\\subseteq\\mathcal{X}over~ start_ARG caligraphic_X end_ARG \u2286 caligraphic_X that provides information at least as useful for generating the output \ud835\udcb4\ud835\udcb4\\mathcal{Y}caligraphic_Y as the full context \ud835\udcb3\ud835\udcb3\\mathcal{X}caligraphic_X.\n\n\nIn practical scenarios, obtaining the optimal \ud835\udcb3~~\ud835\udcb3\\tilde{{\\mathcal{X}}}over~ start_ARG caligraphic_X end_ARG is challenging due to various factors, such as empirical errors\u00a0Mohri et\u00a0al. (2018). Thus, we can only estimate \ud835\udcb3~~\ud835\udcb3\\tilde{{\\mathcal{X}}}over~ start_ARG caligraphic_X end_ARG. Estimating \ud835\udcb3~~\ud835\udcb3\\tilde{{\\mathcal{X}}}over~ start_ARG caligraphic_X end_ARG directly from \ud835\udcb3\ud835\udcb3{\\mathcal{X}}caligraphic_X might be challenging if \ud835\udcb3\ud835\udcb3{\\mathcal{X}}caligraphic_X defines a large variable space. In this situation, we propose decomposing the original input variable \ud835\udcb3\ud835\udcb3{\\mathcal{X}}caligraphic_X into a series of subsets, \ud835\udcb3={\ud835\udcb31,\u22ef,\ud835\udcb3n}\ud835\udcb3subscript\ud835\udcb31\u22efsubscript\ud835\udcb3\ud835\udc5b{\\mathcal{X}}=\\{{\\mathcal{X}}_{1},\\cdots,{\\mathcal{X}}_{n}\\}caligraphic_X = { caligraphic_X start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , \u22ef , caligraphic_X start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT } and process each subset variable separately. Thus, according to the chain rule for mutual information\u00a0Cover (1999), we have:\n\n\n\nI\u2062(\ud835\udcb3,\ud835\udcb3~)=I\u2062(\ud835\udcb31,\u22ef,\ud835\udcb3n;\ud835\udcb3~)=I\u2062(\ud835\udcb31;\ud835\udcb3~)+\u2211i=2nI\u2062(\ud835\udcb3i;\ud835\udcb3~|\ud835\udcb31,\u22ef,\ud835\udcb3i\u22121),\ud835\udc3c\ud835\udcb3~\ud835\udcb3\ud835\udc3csubscript\ud835\udcb31\u22efsubscript\ud835\udcb3\ud835\udc5b~\ud835\udcb3\ud835\udc3csubscript\ud835\udcb31~\ud835\udcb3superscriptsubscript\ud835\udc562\ud835\udc5b\ud835\udc3csubscript\ud835\udcb3\ud835\udc56conditional~\ud835\udcb3subscript\ud835\udcb31\u22efsubscript\ud835\udcb3\ud835\udc561\\displaystyle I({\\mathcal{X}},\\tilde{{\\mathcal{X}}})=I({\\mathcal{X}}_{1},%\n\\cdots,{\\mathcal{X}}_{n};\\tilde{{\\mathcal{X}}})=I({\\mathcal{X}}_{1};\\tilde{{%\n\\mathcal{X}}})+\\sum\\limits_{i=2}^{n}I({\\mathcal{X}}_{i};\\tilde{{\\mathcal{X}}}|%\n{\\mathcal{X}}_{1},\\cdots,{\\mathcal{X}}_{i-1}),italic_I ( caligraphic_X , over~ start_ARG caligraphic_X end_ARG ) = italic_I ( caligraphic_X start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , \u22ef , caligraphic_X start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ; over~ start_ARG caligraphic_X end_ARG ) = italic_I ( caligraphic_X start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ; over~ start_ARG caligraphic_X end_ARG ) + \u2211 start_POSTSUBSCRIPT italic_i = 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT italic_I ( caligraphic_X start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ; over~ start_ARG caligraphic_X end_ARG | caligraphic_X start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , \u22ef , caligraphic_X start_POSTSUBSCRIPT italic_i - 1 end_POSTSUBSCRIPT ) ,\n\n(2)\n\n\nwhich indicates that the mutual information I\u2062(\ud835\udcb3,\ud835\udcb3~)\ud835\udc3c\ud835\udcb3~\ud835\udcb3I({\\mathcal{X}},\\tilde{{\\mathcal{X}}})italic_I ( caligraphic_X , over~ start_ARG caligraphic_X end_ARG ) can be understood as the sum of the mutual information of each subset \ud835\udcb3isubscript\ud835\udcb3\ud835\udc56{\\mathcal{X}}_{i}caligraphic_X start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT and \ud835\udcb3~isubscript~\ud835\udcb3\ud835\udc56\\tilde{{\\mathcal{X}}}_{i}over~ start_ARG caligraphic_X end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT given all previous subsets.\n\n\nIn the scenario of Eq.\u00a01, the variable \ud835\udcb3\ud835\udcb3\\mathcal{X}caligraphic_X represents a long context and the variable \ud835\udcb4\ud835\udcb4\\mathcal{Y}caligraphic_Y is the output answer produced by a LLM. Thus, \ud835\udcb3~~\ud835\udcb3\\tilde{{\\mathcal{X}}}over~ start_ARG caligraphic_X end_ARG can be interpreted as the minimal necessary context from the long context \ud835\udcb3\ud835\udcb3{\\mathcal{X}}caligraphic_X given the output answer \ud835\udcb4\ud835\udcb4{\\mathcal{Y}}caligraphic_Y. Inspired by Eq.\u00a02, we can estimate an optimal \ud835\udcb3~~\ud835\udcb3\\tilde{\\mathcal{X}}over~ start_ARG caligraphic_X end_ARG using decomposed shorter contexts {\ud835\udcb31,\u2026,\ud835\udcb3n}subscript\ud835\udcb31\u2026subscript\ud835\udcb3\ud835\udc5b\\{\\mathcal{X}_{1},\\ldots,\\mathcal{X}_{n}\\}{ caligraphic_X start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , \u2026 , caligraphic_X start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT }. Thus, I\u2062(\ud835\udcb3;\ud835\udcb3~)\ud835\udc3c\ud835\udcb3~\ud835\udcb3I(\\mathcal{X};\\tilde{\\mathcal{X}})italic_I ( caligraphic_X ; over~ start_ARG caligraphic_X end_ARG ) can be computed by processing each subset \ud835\udcb3isubscript\ud835\udcb3\ud835\udc56\\mathcal{X}_{i}caligraphic_X start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT individually. However, as the number of subsets n\ud835\udc5bnitalic_n increases, accounting for all preceding subsets becomes computationally demanding. To alleviate this burden, we propose reducing the number of conditional subsets considered by replacing the entire sequence of previous subsets with a compressed surrogate \ud835\udcb3^isubscript^\ud835\udcb3\ud835\udc56\\hat{\\mathcal{X}}_{i}over^ start_ARG caligraphic_X end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT, which is iteratively derived using a compression function \ud835\udcb3^i=g\u2062(\ud835\udcb3^i\u22121,\ud835\udcb3i\u22121)subscript^\ud835\udcb3\ud835\udc56\ud835\udc54subscript^\ud835\udcb3\ud835\udc561subscript\ud835\udcb3\ud835\udc561\\hat{\\mathcal{X}}_{i}=g(\\hat{\\mathcal{X}}_{i-1},\\mathcal{X}_{i-1})over^ start_ARG caligraphic_X end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = italic_g ( over^ start_ARG caligraphic_X end_ARG start_POSTSUBSCRIPT italic_i - 1 end_POSTSUBSCRIPT , caligraphic_X start_POSTSUBSCRIPT italic_i - 1 end_POSTSUBSCRIPT ). Consequently, Eq.\u00a02 can be reformulated as follows:\n\n\n\nI(\ud835\udcb3,\ud835\udcb3~)=I(\ud835\udcb31,\u22ef,\ud835\udcb3n;\ud835\udcb3~)\u2243I(\ud835\udcb31;\ud835\udcb3~)+\u2211i=2nI(\ud835\udcb3i;\ud835\udcb3~|\ud835\udcb3^i)).I({\\mathcal{X}},\\tilde{{\\mathcal{X}}})=I({\\mathcal{X}}_{1},\\cdots,{\\mathcal{X}%\n}_{n};\\tilde{{\\mathcal{X}}})\\simeq I({\\mathcal{X}}_{1};\\tilde{{\\mathcal{X}}})+%\n\\sum\\limits_{i=2}^{n}I({\\mathcal{X}}_{i};\\tilde{{\\mathcal{X}}}|\\hat{{\\mathcal{%\nX}}}_{i})).italic_I ( caligraphic_X , over~ start_ARG caligraphic_X end_ARG ) = italic_I ( caligraphic_X start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , \u22ef , caligraphic_X start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ; over~ start_ARG caligraphic_X end_ARG ) \u2243 italic_I ( caligraphic_X start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ; over~ start_ARG caligraphic_X end_ARG ) + \u2211 start_POSTSUBSCRIPT italic_i = 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT italic_I ( caligraphic_X start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ; over~ start_ARG caligraphic_X end_ARG | over^ start_ARG caligraphic_X end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) ) .\n\n(3)\n\n\nThe equality can be upheld under two specific conditions: (1) the decomposed variables {\ud835\udcb31,\u2026,\ud835\udcb3n}subscript\ud835\udcb31\u2026subscript\ud835\udcb3\ud835\udc5b\\{\\mathcal{X}_{1},\\ldots,\\mathcal{X}_{n}\\}{ caligraphic_X start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , \u2026 , caligraphic_X start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT } are mutually independent, and (2) the compression function g\u2062(\u22c5)\ud835\udc54\u22c5g(\\cdot)italic_g ( \u22c5 ) is optimally designed, ensuring that the compressed surrogate \ud835\udcb3^isubscript^\ud835\udcb3\ud835\udc56\\hat{\\mathcal{X}}_{i}over^ start_ARG caligraphic_X end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT encapsulates all relevant information from the preceding subsets with respect to \ud835\udcb3~~\ud835\udcb3\\tilde{\\mathcal{X}}over~ start_ARG caligraphic_X end_ARG. Otherwise, I\u2062(\ud835\udcb3,\ud835\udcb3~)\ud835\udc3c\ud835\udcb3~\ud835\udcb3I({\\mathcal{X}},\\tilde{{\\mathcal{X}}})italic_I ( caligraphic_X , over~ start_ARG caligraphic_X end_ARG ) can only be approximately estimated.\n\n\n\nEmpirical Analysis\n\nFigure 2: Pilot Study Across Various Tasks: In the Brute-force setting, the entire context is processed by GPT-4-128K. In the LC-Boost setting, the maximum context length is restricted to 4K, and LC-Boost is utilized to solve the long-context problem with short context. \n\n\nTo empirically assess the accuracy of estimating the minimal necessary context \ud835\udcb3~~\ud835\udcb3\\tilde{\\mathcal{X}}over~ start_ARG caligraphic_X end_ARG using decomposed short contexts {\ud835\udcb31,\u2026,\ud835\udcb3n}subscript\ud835\udcb31\u2026subscript\ud835\udcb3\ud835\udc5b\\{\\mathcal{X}_{1},\\ldots,\\mathcal{X}_{n}\\}{ caligraphic_X start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , \u2026 , caligraphic_X start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT }, we conduct pilot experiments across various tasks requiring long contexts. Specifically, we utilize GPT-4-128K to perform these tasks in two settings: (1) feeding the entire long context into GPT-4-128K in a brute-force manner, instructing the model to directly produce the output answer, and (2) decomposing the full context into short contexts and applying the methods defined in Eq.\u00a03 to approximate \ud835\udcb3~~\ud835\udcb3\\tilde{\\mathcal{X}}over~ start_ARG caligraphic_X end_ARG, which then guides the model to produce the final output (the LC-Boost setting).\n\n\nFigure\u00a02 presents the experiment results, which generally indicate that LC-Boost consistently performs as well as or better than the brute-force setting. In particular, for tasks such as QA, few-shot learning, and synthetic tasks, LC-Boost outperforms the brute-force setting. This is because the decomposed short contexts for these tasks are more likely to be mutually independent given the input query which can be adequately supported by a few extracted contexts from the long context. By precisely\nlocating these supported context, it can filter out irrelevant context of \ud835\udcb3\ud835\udcb3{\\mathcal{X}}caligraphic_X that might otherwise undermine task performance.\nFor tasks like summarization and code completion, the inherent properties of these tasks require considering the mutual dependencies among all decomposed short contexts, making the LC-Boost setting more challenging. However, as discussed in Eq.\u00a03, when the compression function g\u2062(\u22c5)\ud835\udc54\u22c5g(\\cdot)italic_g ( \u22c5 ) is optimal, we can achieve the optimal \ud835\udcb3~~\ud835\udcb3\\tilde{\\mathcal{X}}over~ start_ARG caligraphic_X end_ARG. GPT-4 serves as such a strong compression function, ensuring that the compressed surrogate \ud835\udcb3^isubscript^\ud835\udcb3\ud835\udc56\\hat{\\mathcal{X}}_{i}over^ start_ARG caligraphic_X end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT is well-estimated. Consequently, in these tasks, LC-Boost achieves performance that is equal to or slightly better than the brute-force setting.\n\n\nThrough theoretical analysis, we can posit that long-context tasks are short-context solvable if we can estimate a better minimal necessary context \ud835\udcb3~~\ud835\udcb3\\tilde{\\mathcal{X}}over~ start_ARG caligraphic_X end_ARG from the decomposed short contexts {\ud835\udcb31,\u2026,\ud835\udcb3n}subscript\ud835\udcb31\u2026subscript\ud835\udcb3\ud835\udc5b\\{\\mathcal{X}_{1},\\ldots,\\mathcal{X}_{n}\\}{ caligraphic_X start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , \u2026 , caligraphic_X start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT } than from the long context \ud835\udcb3\ud835\udcb3\\mathcal{X}caligraphic_X. Empirical analysis supports this assumption, demonstrating that in most cases, the estimation error of deriving \ud835\udcb3~~\ud835\udcb3\\tilde{\\mathcal{X}}over~ start_ARG caligraphic_X end_ARG from the long context \ud835\udcb3\ud835\udcb3\\mathcal{X}caligraphic_X is often larger than from the decomposed short contexts {\ud835\udcb31,\u2026,\ud835\udcb3n}subscript\ud835\udcb31\u2026subscript\ud835\udcb3\ud835\udc5b\\{\\mathcal{X}_{1},\\ldots,\\mathcal{X}_{n}\\}{ caligraphic_X start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , \u2026 , caligraphic_X start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT }. This indicates that using short contexts can be comparatively more advantageous than using the full context. Therefore, we can validate our argument in Section\u00a01: most long-context tasks, if not all, are short-context solvable.\n\n\n\n\n\n2.3 The Proposed Method: LC-Boost\n\nWe propose a method called LC-Boost, which utilizes short LLMs to solve general long-context tasks. LC-Boost begins with an input query q\ud835\udc5eqitalic_q and a long context \ud835\udcb3\ud835\udcb3\\mathcal{X}caligraphic_X, with the goal of producing an output answer \ud835\udcb4\ud835\udcb4\\mathcal{Y}caligraphic_Y. Since the underlying LLM in LC-Boost has a limited context size\u00a0(we limit LC-Boost working with 4K context length), directly generating the output answer \ud835\udcb4\ud835\udcb4\\mathcal{Y}caligraphic_Y is infeasible for long-context tasks. To address this, we propose solving long-context tasks by strategically understanding the decomposed short contexts \ud835\udcb3={\ud835\udcb31,\u22ef,\ud835\udcb3n}\ud835\udcb3subscript\ud835\udcb31\u22efsubscript\ud835\udcb3\ud835\udc5b\\mathcal{X}=\\{\\mathcal{X}_{1},\\cdots,\\mathcal{X}_{n}\\}caligraphic_X = { caligraphic_X start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , \u22ef , caligraphic_X start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT }. From these short contexts, we aim to extract the minimal necessary context \ud835\udcb3~~\ud835\udcb3\\tilde{\\mathcal{X}}over~ start_ARG caligraphic_X end_ARG to support the generation of the output answer \ud835\udcb4\ud835\udcb4\\mathcal{Y}caligraphic_Y.\n\n\nLC-Boost achieves this goal through a decision-making process involving iterative interactions between LC-Boost and the decomposed short contexts {\ud835\udcb31,\u22ef,\ud835\udcb3n}subscript\ud835\udcb31\u22efsubscript\ud835\udcb3\ud835\udc5b\\{\\mathcal{X}_{1},\\cdots,\\mathcal{X}_{n}\\}{ caligraphic_X start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , \u22ef , caligraphic_X start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT } with respect to the input query q\ud835\udc5eqitalic_q. In the process, LC-Boost interact with each short context \ud835\udcb3isubscript\ud835\udcb3\ud835\udc56\\mathcal{X}_{i}caligraphic_X start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT, employing two types of actions: information access and information utilization.\n\n\nWe denote an action at time step i\ud835\udc56iitalic_i by aisubscript\ud835\udc4e\ud835\udc56a_{i}italic_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT and denote the relevant context LC-Boost obtains from the i\ud835\udc56iitalic_i-th short context \ud835\udcb3isubscript\ud835\udcb3\ud835\udc56{\\mathcal{X}}_{i}caligraphic_X start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT by \ud835\udcb3~isubscript~\ud835\udcb3\ud835\udc56\\tilde{{\\mathcal{X}}}_{i}over~ start_ARG caligraphic_X end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT\nThe action aisubscript\ud835\udc4e\ud835\udc56a_{i}italic_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT is predicted by considering the current short context \ud835\udcb3isubscript\ud835\udcb3\ud835\udc56{\\mathcal{X}}_{i}caligraphic_X start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT, the input query q\ud835\udc5eqitalic_q, as well as all previous extracted relevant information \ud835\udcb3~1:i\u22121subscript~\ud835\udcb3:1\ud835\udc561\\tilde{{\\mathcal{X}}}_{1:i-1}over~ start_ARG caligraphic_X end_ARG start_POSTSUBSCRIPT 1 : italic_i - 1 end_POSTSUBSCRIPT: ai=\u03b3\u2062(q,\ud835\udcb3i|\ud835\udcb3~1:i\u22121),subscript\ud835\udc4e\ud835\udc56\ud835\udefe\ud835\udc5econditionalsubscript\ud835\udcb3\ud835\udc56subscript~\ud835\udcb3:1\ud835\udc561a_{i}=\\gamma(q,{\\mathcal{X}}_{i}|\\tilde{{\\mathcal{X}}}_{1:i-1}),italic_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = italic_\u03b3 ( italic_q , caligraphic_X start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT | over~ start_ARG caligraphic_X end_ARG start_POSTSUBSCRIPT 1 : italic_i - 1 end_POSTSUBSCRIPT ) ,\nwhere \u03b3\u2062(\u22c5)\ud835\udefe\u22c5\\gamma(\\cdot)italic_\u03b3 ( \u22c5 ) denotes LC-Boost\u2019s underlying LLM.\n\n\nPredicting the action aisubscript\ud835\udc4e\ud835\udc56a_{i}italic_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT in a continuous space is challenging as it requires the underling model to reason about highly implicit relations among the input query, the current context, and the previous contexts. Therefore, we define a discrete action space \ud835\udc9c\ud835\udc9c{\\mathcal{A}}caligraphic_A comprising:\n(1)\u00a0[Task Understanding]: analyzing the query and task for initialization;\n(2)\u00a0[Retrieve]: accessing text evidence by a retrieval method;\n(3)\u00a0[Move]: accessing the next short text context directly;\nThese two are information access actions which define the LC-Boost\u2019s trajectory to access short contexts.\n(4)\u00a0[Append]: generating relevant context \ud835\udcb3~isubscript~\ud835\udcb3\ud835\udc56\\tilde{{\\mathcal{X}}}_{i}over~ start_ARG caligraphic_X end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT independently, denoting by \ud835\udcb3~i=ai\u2062(\ud835\udcb3i)subscript~\ud835\udcb3\ud835\udc56subscript\ud835\udc4e\ud835\udc56subscript\ud835\udcb3\ud835\udc56\\tilde{{\\mathcal{X}}}_{i}=a_{i}({\\mathcal{X}}_{i})over~ start_ARG caligraphic_X end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = italic_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( caligraphic_X start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT );\n(5)\u00a0[Merge]: generating relevant context \ud835\udcb3~isubscript~\ud835\udcb3\ud835\udc56\\tilde{{\\mathcal{X}}}_{i}over~ start_ARG caligraphic_X end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT with respect to previous extracted relevant information, denoting by \ud835\udcb3~i=ai\u2062(\ud835\udcb3i|\ud835\udcb3~1:i\u22121)subscript~\ud835\udcb3\ud835\udc56subscript\ud835\udc4e\ud835\udc56conditionalsubscript\ud835\udcb3\ud835\udc56subscript~\ud835\udcb3:1\ud835\udc561\\tilde{{\\mathcal{X}}}_{i}=a_{i}({\\mathcal{X}}_{i}|\\tilde{{\\mathcal{X}}}_{1:i-1})over~ start_ARG caligraphic_X end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = italic_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( caligraphic_X start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT | over~ start_ARG caligraphic_X end_ARG start_POSTSUBSCRIPT 1 : italic_i - 1 end_POSTSUBSCRIPT );\n(6)\u00a0[Answer]: answering the user query and returning;\n(7)\u00a0[Aggregation]: aggregating all relevant information and returning.\nWe define our LC-Boost frame in Algorithm\u00a01.\n\n\nAlgorithm 1  LC-Boost Framework\n\n\n1:\u00a0\u00a0Input: Input query q\ud835\udc5eqitalic_q, long context \ud835\udcb3\ud835\udcb3\\mathcal{X}caligraphic_X\n\n\n\n2:\u00a0\u00a0Output: Answer \ud835\udcb4\ud835\udcb4\\mathcal{Y}caligraphic_Y\n\n\n\n3:\u00a0\u00a0Decompose long context\u2062\ud835\udcb3\u2190{\ud835\udcb31,\u22ef,\ud835\udcb3n}\u2190long context\ud835\udcb3subscript\ud835\udcb31\u22efsubscript\ud835\udcb3\ud835\udc5b\\text{long context}~{}{\\mathcal{X}}\\leftarrow\\{{\\mathcal{X}}_{1},\\cdots,{%\n\\mathcal{X}}_{n}\\}long context caligraphic_X \u2190 { caligraphic_X start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , \u22ef , caligraphic_X start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT }\n\n\n\n4:\u00a0\u00a0Initialize extracted relevant context\u2062\ud835\udcb3~0\u2190None\u2190extracted relevant contextsubscript~\ud835\udcb30None\\text{extracted relevant context}~{}\\tilde{{\\mathcal{X}}}_{0}\\leftarrow\\text{None}extracted relevant context over~ start_ARG caligraphic_X end_ARG start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT \u2190 None\n\n\n\n5:\u00a0\u00a0Perform [Task Understanding]\n\n\n\n6:\u00a0\u00a0while\u00a0i\u2264n\ud835\udc56\ud835\udc5bi\\leq nitalic_i \u2264 italic_n\u00a0do\n\n\n\n7:\u00a0\u00a0\u00a0\u00a0\u00a0Select an action ai\u2190ai=\u03b3\u2062(q,\ud835\udcb3i|\ud835\udcb3~1:i\u22121),ai\u2208\ud835\udc9cformulae-sequence\u2190subscript\ud835\udc4e\ud835\udc56subscript\ud835\udc4e\ud835\udc56\ud835\udefe\ud835\udc5econditionalsubscript\ud835\udcb3\ud835\udc56subscript~\ud835\udcb3:1\ud835\udc561subscript\ud835\udc4e\ud835\udc56\ud835\udc9ca_{i}\\leftarrow a_{i}=\\gamma(q,{\\mathcal{X}}_{i}|\\tilde{{\\mathcal{X}}}_{1:i-1}%\n),~{}a_{i}\\in{\\mathcal{A}}italic_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT \u2190 italic_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = italic_\u03b3 ( italic_q , caligraphic_X start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT | over~ start_ARG caligraphic_X end_ARG start_POSTSUBSCRIPT 1 : italic_i - 1 end_POSTSUBSCRIPT ) , italic_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT \u2208 caligraphic_A\n\n\n\n8:\u00a0\u00a0\u00a0\u00a0\u00a0if aisubscript\ud835\udc4e\ud835\udc56a_{i}italic_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT is [Move] then i\u2190i+1\u2190\ud835\udc56\ud835\udc561i\\leftarrow i+1italic_i \u2190 italic_i + 1, continue\n\n\n\n9:\u00a0\u00a0\u00a0\u00a0\u00a0if aisubscript\ud835\udc4e\ud835\udc56a_{i}italic_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT is [Retrieve] then retrieve evidence from \ud835\udcb3={\ud835\udcb31,\u22ef,\ud835\udcb3n}\ud835\udcb3subscript\ud835\udcb31\u22efsubscript\ud835\udcb3\ud835\udc5b{\\mathcal{X}}=\\{{\\mathcal{X}}_{1},\\cdots,{\\mathcal{X}}_{n}\\}caligraphic_X = { caligraphic_X start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , \u22ef , caligraphic_X start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT }\n\n\n\n10:\u00a0\u00a0\u00a0\u00a0\u00a0if aisubscript\ud835\udc4e\ud835\udc56a_{i}italic_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT is [Append] then generate relevant context by \ud835\udcb3~i=ai\u2062(\ud835\udcb3i)subscript~\ud835\udcb3\ud835\udc56subscript\ud835\udc4e\ud835\udc56subscript\ud835\udcb3\ud835\udc56\\tilde{{\\mathcal{X}}}_{i}=a_{i}({\\mathcal{X}}_{i})over~ start_ARG caligraphic_X end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = italic_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( caligraphic_X start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT )\n\n\n\n11:\u00a0\u00a0\u00a0\u00a0\u00a0if aisubscript\ud835\udc4e\ud835\udc56a_{i}italic_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT is [Merge] then generate relevant context by \ud835\udcb3~i=ai\u2062(\ud835\udcb3i|\ud835\udcb3~1:i\u22121)subscript~\ud835\udcb3\ud835\udc56subscript\ud835\udc4e\ud835\udc56conditionalsubscript\ud835\udcb3\ud835\udc56subscript~\ud835\udcb3:1\ud835\udc561\\tilde{{\\mathcal{X}}}_{i}=a_{i}({\\mathcal{X}}_{i}|\\tilde{{\\mathcal{X}}}_{1:i-1})over~ start_ARG caligraphic_X end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = italic_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( caligraphic_X start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT | over~ start_ARG caligraphic_X end_ARG start_POSTSUBSCRIPT 1 : italic_i - 1 end_POSTSUBSCRIPT )\n\n\n\n12:\u00a0\u00a0\u00a0\u00a0\u00a0if aisubscript\ud835\udc4e\ud835\udc56a_{i}italic_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT \u2208{\\in\\{\u2208 {[Answer],[Aggregation]}}\\}} then generate answer \ud835\udcb4=\u03b3\u2062(q,\ud835\udcb3~1:i)\ud835\udcb4\ud835\udefe\ud835\udc5esubscript~\ud835\udcb3:1\ud835\udc56{\\mathcal{Y}}=\\gamma(q,\\tilde{{\\mathcal{X}}}_{1:i})caligraphic_Y = italic_\u03b3 ( italic_q , over~ start_ARG caligraphic_X end_ARG start_POSTSUBSCRIPT 1 : italic_i end_POSTSUBSCRIPT ), break\n\n\n\n13:\u00a0\u00a0\u00a0\u00a0\u00a0i\u2190i+1\u2190\ud835\udc56\ud835\udc561i\\leftarrow i+1italic_i \u2190 italic_i + 1\n\n\n\n14:\u00a0\u00a0end\u00a0while\n\n\n15:\u00a0\u00a0return answer \ud835\udcb4\ud835\udcb4{\\mathcal{Y}}caligraphic_Y\n\n\n\n\n\nThough the pre-defined action space \ud835\udc9c\ud835\udc9c\\mathcal{A}caligraphic_A comprises only seven actions, LC-Boost serves as a general framework sufficient for solving most long-context tasks. This effectiveness is based on the following reasons: (1)\u00a0Flexible accessibility:\u00a0By utilizing both [Retrieve] and [Move] actions, LC-Boost can access any short context \ud835\udcb3i\u2208\ud835\udcb3subscript\ud835\udcb3\ud835\udc56\ud835\udcb3\\mathcal{X}_{i}\\in\\mathcal{X}caligraphic_X start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT \u2208 caligraphic_X in a flexible trajectory, avoiding the need to browse the entire long context. This makes the information accessing process more efficient. (2)\u00a0Accurate information acquisition:\u00a0Through the [Append] and [Merge] actions, LC-Boost can either independently extract relevant information from the current short context, appending it to previously extracted information, or merge the current relevant information into the previous relevant information. This capability allows LC-Boost to acquire relevant information in a compatible manner, making it adaptable to many knowledge-intensive tasks. and (3)\u00a0Dynamic answering:\u00a0Using the [Answer] and [Aggregate] actions, LC-Boost can dynamically utilize the acquired relevant information to produce the target form of the answer (e.g., a short answer for QA tasks via the [Answer] action, or a long answer for summarization tasks via the [Aggregate] action).\n\n\nIn our pilot study depicted in Figure\u00a02, we observe that while GPT-3.5 serves as an inferior foundation model compared to GPT-4, it still demonstrates significant effectiveness when incorporated with LC-Boost. Given considerations of efficiency and cost-effectiveness, we employ GPT-3.5 as the foundation model for LC-Boost in the subsequent experiments. Besides, we show the prompts used in LC-Boost in Appendix\u00a0B.\n\n\n",
      "3 Experiments": "\n\n3 Experiments\n\n\n3.1 Experiment Settings\n\nWe evaluate LC-Boost and baseline models on 12 datasets, including:\n(1) Single-Doc QA: NarrativeQA\u00a0(Ko\u010disk\u00fd et\u00a0al., 2017), Qasper\u00a0(Dasigi et\u00a0al., 2021), and MultiFieldQA\u00a0(Bai et\u00a0al., 2023).\n(2) Multi-Doc QA: HotpotQA\u00a0(Yang et\u00a0al., 2018), 2WikiMQA\u00a0(Ho et\u00a0al., 2020), and MuSiQue\u00a0(Trivedi et\u00a0al., 2022).\n(3) Summarization: GovReport\u00a0(Huang et\u00a0al., 2021) and MultiNews\u00a0(Fabbri et\u00a0al., 2019).\n(4) Few-shot Learning: SAMSum\u00a0(Gliwa et\u00a0al., 2019).\n(5) Synthetic Task: Passage Count\u00a0(Bai et\u00a0al., 2023) and Self-Constructed Dataset.\n(6) Code Completion: LCC\u00a0(Guo et\u00a0al., 2023).\nMore details about the evaluation datasets and metrics are introduced in Appendix\u00a0A.\n\n\nWe compare our LC-Boost with three types of models: (1) Short LLMs (defined as with context length <<< 32K): Llama2-7B-Chat-4K\u00a0(Touvron et\u00a0al., 2023b), Llama3-8B-Instruct-8K and Vicuna-v1.5-7B-16K\u00a0(Chiang et\u00a0al., 2023); (2) Long LLMs (defined as with context length \u2265\\geq\u2265 32K): LongChat-v1.5-7B-32K\u00a0(Li et\u00a0al., 2023b), Mistral-7B-Instruct-v0.2-32K\u00a0(Jiang et\u00a0al., 2023a), Llama3-8B-80K\u00a0Zhang et\u00a0al. (2024b), Phi-3-mini-128K\u00a0(Abdin et\u00a0al., 2024) and Yi-9B-200K\u00a0(AI et\u00a0al., 2024); (3)\u00a0Closed-Source LLMs: DeepSeek-v2\u00a0(236B MoE model, ranks top-tier in MT-Bench)\u00a0(DeepSeek-AI, 2024), Claude-3-Haiku222https://www.anthropic.com/claude and GPT-3.5-turbo-16K333https://platform.openai.com/docs/models. In the experiments, if the context length exceed the model\u2019s length limit, following\u00a0Bai et\u00a0al. (2023), we truncate the context from the middle since the front and end of the context may contain crucial information. We provide further implementation details in Appendix\u00a0B.\n\n\n\n\n3.2 Main Results\n\nTable 1: Main experiment results. The best results are in bold and the secondary results are marked with underline. We report the average scores (%) on the main tasks. The detailed scores over all dataset are shown in Table\u00a03.\n\n\nModels\nSingle-Doc\nMulti-Doc\nSumm.\nFew-shot\nSynthetic\nCode\n\n\nShort LLMs (Context Length <<< 32K)\n\n\n\n\n\n\nLlama2-7B-Chat-4K\n24.9\n22.5\n26.6\n40.7\n6.3\n52.4\n\n\nLlama3-8B-Instruct-8K\n37.3\n36.0\n26.5\n42.7\n15.0\n57.5\n\n\nVicuna-v1.5-7B-16K\n28.0\n18.6\n27.5\n40.8\n8.9\n51.0\n\n\nLong LLMs (Context Length \u2265\\geq\u2265 32K)\n\n\n\n\n\n\nLongChat-v1.5-7B-32K\n28.7\n20.6\n28.6\n34.2\n6.8\n53.0\n\n\nMistral-7B-Instruct-v0.2-32K\n31.9\n26.0\n29.3\n43.0\n14.0\n55.4\n\n\nLlama3-8B-80K\n43.6\n43.1\n30.2\n42.9\n19.6\n53.6\n\n\nPhi-3-mini-128K\n33.5\n38.2\n28.8\n36.0\n19.9\n60.1\n\n\nYi-9B-200K\n29.6\n38.7\n28.4\n14.6\n6.5\n72.1\n\n\nClosed-Source LLMs\n\n\n\n\n\n\nDeepSeek-v2 (32K)\n37.6\n49.1\n30.8\n39.3\n14.5\n37.0\n\n\nClaude-3-Haiku (200K)\n41.9\n45.4\n30.1\n7.2\n25.5\n16.9\n\n\nGPT-3.5-turbo-16K\n39.8\n38.7\n28.1\n41.7\n18.7\n54.7\n\n\nLC-Boost\u00a0(4K)\n47.8\n56.4\n31.8\n44.1\n27.5\n59.0\n\n\n\n\nTable\u00a01 shows the overall experimental results for all models across all tasks. From the table, we derive several key findings:\nFirst, LC-Boost, with a context length of 4K, outperforms all baseline models in all tasks except for the Code Completion task. This result verifies LC-Boost\u2019s capability to effectively solve long-context tasks by strategically processing decomposed short contexts.\nSecond, long LLMs generally perform better than short LLMs, indicating the effectiveness of fine-tuning LLMs to adapt to long contexts. However, the performance of long LLMs is not consistently stable across different tasks. For example, Yi-9B-200K excels in the Code Completion task but does not show consistent performance in other tasks such as single-doc QA, few-shot learning, and synthetic tasks. This inconsistency suggests that adapting LLMs to long contexts may compromise their general abilities.\nLast, LC-Boost consistently surpasses its underlying LLM, GPT-3.5-turbo-16K, across all tasks by a notable margin. This demonstrates that LC-Boost can achieve improved performance while simultaneously reducing resource costs, making LC-Boost an environmentally friendly method.\n\n\n\n\n3.3 Ablation Study: Dynamic is Important\n\nTo investigate the necessity of LC-Boost\u2019s design, we conduct ablation studies by changing LC-Boost\u2019s action space \ud835\udc9c\ud835\udc9c\\mathcal{A}caligraphic_A, resulting in different information acquisition strategies. We experiment with the following settings: (1)\u00a0[Retrieve] only: Directly retrieve the most relevant short context. (2)\u00a0[Merge] only: Sequentially process all short contexts while considering the previously processed context. (3)\u00a0[Append] only: Sequentially process all short contexts independently. (4)\u00a0[Merge] & [Move]: Selectively process short contexts while considering the already processed context. (6)\u00a0[Append] & [Move]: Selectively process short contexts independently. (7): [Retrieve] & [Move]: Retrieve the top-k\ud835\udc58kitalic_k relevant short contexts and selectively process a few of them. (8): Brute-force: Directly produce the answer based on the entire long context. (9)\u00a0Random: For each short context, randomly select an action. Based on the acquired information from each strategy, LC-Boost then selects either the [Answer] or [Aggregation] action to produce the final answer.\n\n\nFigure\u00a03 illustrates the results, from which we find that: (1)\u00a0Compared to fixed processing strategies, LC-Boost customizes the action trajectory for each query, resulting in notable performance improvements. This finding emphasizes the importance of the dynamic capabilities of LC-Boost. (2)\u00a0LC-Boost is particularly effective in single-doc QA and multi-doc QA tasks, as it can accurately select the minimal necessary context required to answer the input query, filtering out irrelevant information from the long context. (3)\u00a0In the few-shot learning task, LC-Boost does not significantly outperform the fixed strategies. This is attributed to the numerous in-context examples provided within the task, which offer substantial guidance, thus diminishing the impact of the number of in-context examples on the final performance.\n\n\nFigure 3: Performance comparison on different context processing strategies in the ablation study. NarrativeQA\u00a0(left) is a single-doc QA task. HotpotQA\u00a0(middle) is a multi-doc QA task. SamSUM\u00a0(right) is a few-shot learning task.\n\n\n\n\n3.4 Case Study: Model Behavior Analysis on Self-Construct Dataset\n\nTable 2: Case study on the self-constructed dataset. Correct answers are marked in teal, incorrect answers in red, and ambiguous answers in orange.\n\n\n\n\nQuery: How many papers in ACL 2023 only have one author?\n\n\n\n\n\n\nContext: Full accepted paper list in ACL 2023 main conference. (Context length: 45K)\n\n\n\n\n\n\nGround-truth target: 8 papers\n\n\n\n\n\n\nPhi-3-mini-128K: 11 papers GPT-3.5-turbo-16K: 0 papers Claude-3-Haiku-200K: 1 papers (Acc. Score: 0)\n\n\n\n\n\n\nLC-Boost\u2019s action trajectory: [Task Reasoning] \u2192\u2192\\rightarrow\u2192 [Append]\u2192\u22ef\u2192absent\u22ef\\rightarrow\\cdots\u2192 \u22ef \u2192\u2192\\rightarrow\u2192 [Append]\u2192\u2192\\rightarrow\u2192 [Aggregation]\n\n\n\n\n\n\nLC-Boost: 8 papers (Acc. Score: 1)\n\n\n\n\n\n\nQuery: List all people names that are petrified, separated by comma.\n\n\n\n\n\n\nContext: Full content of Harry Potter and the Chamber of Secrets. (Context length: 122.6K)\n\n\n\n\n\n\nGround-truth target: Colin Creevey, Justin Finch-Fletchley, Penelope Clearwater, Hermione Granger\n\n\n\n\n\n\nPhi-3-mini-128K: Hermione Granger, Ginny Weasley, Mrs Norris (F1-Score: 0.29)\n\n\n\n\n\n\nGPT-3.5-turbo-16K: Colin Creevey, Mrs Norris (F1-Score: 0.33)\n\n\n\n\n\n\nClaude-3-Haiku-200K: Nick, Hermione, Ron (F1-Score: 0.18)\n\n\n\n\n\n\nLC-Boost\u2019s action trajectory: [Task Reasoning] \u2192\u2192\\rightarrow\u2192 [Move]\u2192\u22ef\u2192absent\u22ef\\rightarrow\\cdots\u2192 \u22ef \u2192\u2192\\rightarrow\u2192 [Merge]\u2192\u2192\\rightarrow\u2192 [Aggregation]\n\n\n\n\n\n\nLC-Boost: Colin Creevey, Penelope Clearwater, Hermione Granger, Nick, Mrs Norris (F1-Score: 0.71)\n\n\n\n\n\n\nIn Table\u00a02, we present two case studies from the self-constructed dataset. These cases are particularly challenging as they require reasoning across the entire long context. Despite having sufficient context size, LLMs struggle to generate correct responses. In contrast, LC-Boost dynamically customizes solutions for each case, thereby effectively solving the problems using a shorter context length.\n\n\nFor the first query, LC-Boost performs [Append] or [Move] actions across all short context along with a rewritten query, \"Extract paper information in the following list that have only one author,\" derived via [Task Reasoning]. After processing all short contexts, LC-Boost employs the [Aggregation] action to compile the final answer. This approach simplifies the task compared to directly extracting a numeric answer from the entire long context, mimicking the human process of reading comprehension and thereby producing accurate results.\n\n\nIn the second case, the query necessitates conditional reasoning on each short context. As highlighted in previous research (Liu et\u00a0al., 2023), reasoning directly from the entire context risks losing crucial information, particularly in the middle of the long context. Thus LLMs tend to miss key details such as people\u2019s names. LC-Boost addresses this issue by processing only one short context at a step where it extracts information from arbitrary position of the long text with equal accuracy.\nAdditionally, answers marked in orange include non-human names (e.g., cat, ghost) that are misconstrued as people, illustrating a common challenge where models fail to differentiate in-depth entity properties.\n\n\n\n\n3.5 Context be Short, Energy be Saved!\n\nRecently, we have witnessed the remarkable success of LLMs, which are becoming an indispensable part of our daily lives. We believe that in the near future, LLMs will become as ubiquitous as electricity or gas supply, serving as fundamental infrastructure in human society. At that point, the energy consumption of LLMs will emerge as a significant environmental concern. Therefore, it is imperative for the research community to focus on reducing the energy consumption associated with these models.\nFigure\u00a04 presents an analysis of energy consumption, comparing the brute-force approach with our LC-Boost method. The y\ud835\udc66yitalic_y-axis is measured in Joules. The theoretical energy consumption is estimated for 7B LLMs across varying context lengths. We roughly estimate the energy consumption using the formula (Total Float Operation312\u2062\u00a0TFLOPS)\u00d7400\u2062WTotal Float Operation312\u00a0TFLOPS400\ud835\udc4a\\left(\\frac{\\text{Total Float Operation}}{312\\text{ TFLOPS}}\\right)\\times 400W( divide start_ARG Total Float Operation end_ARG start_ARG 312 TFLOPS end_ARG ) \u00d7 400 italic_W, assuming the use of an A100 GPU with a compute capability of 312 TFLOPS for BFLOAT16 operations and a maximum TDP of 400W444The calculation of total float operations is based on the method outlined in https://www.harmdevries.com/post/context-length/. The practical energy consumption is estimated by recording the GPU time and GPU power during inference with different context lengths. We use a Llama2-7B-128K\u00a0(Peng et\u00a0al., 2023) and a Llama2-7B-chat-4K\u00a0(Touvron et\u00a0al., 2023a) for the brute-force setting and LC-Boost, respectively.\n\n\nFigure 4: Energy consumption analysis.\n\n\nFigure\u00a04 clearly indicates that longer context lengths significantly increase energy consumption with the brute-force method, especially evident in practical measurements. This difference is primarily due to the need to distribute sequence activation tensors across multiple GPUs in practical experiment, with tensor I/O exacerbating inference latency and thereby inflating energy costs. In contrast, our LC-Boost method, working with 4K context lengths, shows only a mild increase in energy consumption across contexts, thereby confirming its energy efficiency while maintaining comparable or superior performance on long-context tasks. We also provide an analysis on token consumption in Appendix\u00a0C.\n\n\n",
      "4 Related Works": "\n\n4 Related Works\n\nDealing with long contexts is a fundamental research problem for LLMs, as many real-world applications involve long-context inputs (Li et\u00a0al., 2023a; Fu et\u00a0al., 2024). The most direct approach to address long-context tasks is to increase the working context size of LLMs (Abdin et\u00a0al., 2024; AI et\u00a0al., 2024; Li et\u00a0al., 2023a; Cai et\u00a0al., 2024). A year ago, significant research efforts focused on extending the working context size of LLMs from 4K to 32K (Jiang et\u00a0al., 2023a; Li et\u00a0al., 2023b; Chen et\u00a0al., 2023a; Du et\u00a0al., 2022). Currently, many popular open-source and close-source LLMs still operate with a context size under 32K (Touvron et\u00a0al., 2023a; OpenAI, 2023), such as GPT-3.5-turbo, which has a 16K context length. Recently, research has shifted towards extending LLMs\u2019 working context to the million-level. Notably, GPT-4 was updated to a 128K context length not long ago, and the newly released GPT-4o also operates with a 128K context. Moreover, several recent open-source LLMs have been introduced with context lengths exceeding 100K, for example, the Yi series model supports up to 200K (AI et\u00a0al., 2024), and the Phi-3 model operates with 128K (Abdin et\u00a0al., 2024).\n\n\nInstead of merely increasing the context length, another approach to address long-context tasks involves extracting a short surrogate context from the full context. This includes techniques like retrieval-augmented generation (RAG) and context refinement methods (Izacard and Grave, 2021a; Gao et\u00a0al., 2024; Wang et\u00a0al., 2023; Qian et\u00a0al., 2024). However, many of these methods utilize task-specific strategies to manage the long context. For instance, RAG methods often deploy retrievers to select relevant context chunks as supporting evidence (Izacard and Grave, 2021b; Xu et\u00a0al., 2023b; Jiang et\u00a0al., 2023b). Recent studies have criticized the chunking process in RAG for undermining the semantic coherence of the long context and have proposed chunking-free methods to refine the long context into a concise surrogate context (Qian et\u00a0al., 2024; Luo et\u00a0al., 2024).\nFurthermore, some studies have also explored sequential processing strategies, such as Ratner et\u00a0al. (2022) and Xu et\u00a0al. (2023a), to sequentially process the context in a manner that preserves its integrity.\n\n\nLastly, reasoning-based methods also show significant potential for addressing long context tasks (Nakano et\u00a0al., 2022; Yang et\u00a0al., 2023; Driess et\u00a0al., 2023). These methods predominantly employ a decision-making process to navigate through the long context sequentially, utilizing reasoning techniques such as in-context learning (Dong et\u00a0al., 2022), chain-of-thought (Wei et\u00a0al., 2022), and self-reflection (Shinn et\u00a0al., 2023). In this paper, LC-Boost incorporates a decision-making process that dynamically customizes the action trajectory for each query, thereby offering considerable flexibility in accessing and leveraging information to produce the final output answer.\n\n",
      "5 Conclusion": "\n\n5 Conclusion\n\nIn this paper, we argue that most long-context tasks are short-context solvable, and we validate this claim through both theoretical and empirical analysis. We propose a method called LC-Boost to solve long-context tasks by decomposing the long context into short contexts and processing them using a decision-making process. We conduct experiments on 12 datasets to compare LC-Boost with long LLMs and other baseline models. Empirical results verify LC-Boost\u2019s effectiveness in solving long-context tasks. Additionally, we discuss the energy consumption of LC-Boost versus long LLMs, demonstrating that LC-Boost can achieve comparable performance with significantly less energy consumption. In Appendix\u00a0D, we also discuss the limitations and broader impact of this paper.\n\n\n",
      "Appendix A More details of the Datasets": "\n\nAppendix A More details of the Datasets\n\nTable 4: Statistical information of the datasets utilized in this paper.\n\n\nDataset\nNarrative\nQasper\nMultiField\nHotpot\nMuSiQue\n2Wiki\n\n\nNum of Samples\n200\n200\n150\n200\n200\n200\n\n\nAve. Length\n18,409\n3,619\n4,559\n9,151\n11,214\n4,887\n\n\nMetric\nF1\nF1\nF1\nF1\nF1\nF1\n\n\nDataset\nGovReport\nMultiNews\nSAMSum\nPCount\nLCC\nSelf\n\n\nNum of Samples\n200\n200\n200\n200\n500\n32\n\n\nAve. Length\n8,734\n2,113\n6,258\n11,141\n1,235\n39,420\n\n\nMetric\nRouge-L\nRouge-L\nRouge-L\nAccuracy\nEdit Sim\nF1&Accuracy\n\n\n\n\nWe evaluated all models on 12 datasets, as shown in Table\u00a04. Most of these datasets are provided by the LongBench benchmark\u00a0[Bai et\u00a0al., 2023]. Following LongBench, we used F1-score, accuracy, and edit similarity as the evaluation metrics. Additionally, we manually annotated a self-constructed dataset comprising long contexts from practical scenarios, such as the full schedule of the Olympic Games and the complete list of accepted papers at ACL. The queries in the self-constructed dataset involve reasoning over the entire long context. For example, \u201cWho has the most accepted papers at ACL 2023?\u201d These queries require the model to accurately understand the long context and perform reasoning, making them highly challenging. The details of the self-constructed dataset are in Table\u00a05.\n\n\nTable 5: Data details of the self-constructed dataset.\n\n\n\n\nSource\n\n\n\n\nLength\n\n\n\n\n# Queries\n\n\n\n\nExample Query\n\n\n\n\n\n\nAccepted paper list of ACL 2023 Main Conference\n\n\n\n\n44,490\n\n\n\n\n7\n\n\n\n\nWho has the most accepted paper in ACL 2023?\n\n\n\n\n\n\nThe Diamond Sutra\n\n\n\n\n19,993\n\n\n\n\n3\n\n\n\n\nHow many chapters of the Sutra?\n\n\n\n\n\n\nSchedule of The 2024 Olympic Games\n\n\n\n\n15,844\n\n\n\n\n9\n\n\n\n\nWhich day has the most gold medal events?\n\n\n\n\n\n\nSubtitle of The Big Bang Theory S3E14\n\n\n\n\n11,136\n\n\n\n\n6\n\n\n\n\nHow long does this episode?\n\n\n\n\n\n\nThe Little Prince\n\n\n\n\n22,471\n\n\n\n\n4\n\n\n\n\nHow many planets does the little prince visit?\n\n\n\n\n\n\nHarry Potter and the Chamber of Secrets\n\n\n\n\n122,591\n\n\n\n\n3\n\n\n\n\nHow many times has the chamber of secret been opened?\n\n\n\n\n\n",
      "Appendix B Implementation Details": "\n\nAppendix B Implementation Details\n\nLC-Boost begins with the [Task Understanding] action after receiving the input query and context, using the prompt shown in Table\u00a06. If the task does not include an input query, the two columns \"Below is the query\" and \"{input_query}\" are omitted. Besides, for the synthetic task, we use the prompt shown in Table\u00a07 to reformulate the query for better adaptation to LC-Boost. Based on the output of the [Task Understanding] action, LC-Boost adopts different strategies to perform the task. Specifically, \u201coption [1]\u201d directs LC-Boost to utilize a retriever to rank all chunks of the long context. In this paper, we employ BGE-Reranker-Large as the retriever\u00a0Chen et\u00a0al. [2023b]. For \u201coption [2]\u201d and \u201coption [3]\u201d, LC-Boost uses the prompts shown in Table\u00a010 and Table\u00a08 to sequentially process each short context, respectively. After processing each short context, if the output is not \"null\", the newly summarized context is added to the \"previous summarization\".\n\n\nOnce all short contexts are processed, LC-Boost aggregates all relevant information to produce the final answer. At this stage, we use the prompt provided by LongBench, replacing the full context with the surrogate context produced by LC-Boost. For \u201coption [4]\u201d, LC-Boost utilizes the prompts provided by LongBench to process each short context and produces the answer as soon as the proper information is found. Table\u00a09 presents an example prompt from LongBench, designed for MultiFieldQA tasks. We modified the prompt by adding the instruction \u201cIf no answer can be found in the text, please output \"null\"\u201d. This allows LC-Boost to skip irrelevant short contexts, performing the [Move] action. Specifically, for the Code Completion task, LC-Boost reversely browses the context code from near to far as the near context are more useful to predict the code completion.\nWe evaluate all baseline models following the settings provided in LongBench\u00a0555https://github.com/THUDM/LongBench. We use a node with 8 A100 80G GPUs to conduct all experiments.\n\n",
      "Appendix C Token Consumption Analysis": "\n\nAppendix C Token Consumption Analysis\n\nIn Section\u00a03.5, our analysis confirms that LC-Boost significantly reduces energy consumption compared to long LLMs. However, most closed-source LLMs, such as the underlying model of LC-Boost, GPT-3.5-turbo, charge based on token consumption, e.g., US$0.50 per 1M tokens for input and US$1.50 per 1M tokens for output666https://openai.com/api/pricing/. Consequently, it is crucial to examine whether the decision-making process of LC-Boost increases token consumption compared to the brute-force method.\n\n\nTo address this issue, we recorded the end-to-end token consumption for three datasets: NarrativeQA, GovReport, and LCC. After token counting, we conclude that LC-Boost\u2019s token consumption was 34.1% of the brute-force method\u2019s consumption in NarrativeQA, 112% in GovReport, and 29.5% in LCC. These results indicate that LC-Boost\u2019s token consumption varies significantly across different tasks. For tasks requiring precise context location, such as QA and code completion, LC-Boost can respond as soon as the relevant context is identified, thereby avoiding the need to process the full context. However, for tasks that necessitate information aggregation, such as summarization, LC-Boost may require more tokens for prompts in each iteration.\nIn practice, for token-consumption-sensitive LLMs, there might be a trade-off between performance and cost-efficiency, which also varies considerably across different tasks.\n\n\nTable 6: Prompt Template for the [Task Understanding] action.\n\n\n\n\n\n\nYou need to process a task with a long context that greatly exceeds your context limit.\n\nThe only feasible way to handle this is by processing the long context chunk by chunk.\n\n\n\n\n\n\n\n\nBelow is the original task prompt:\n\n{task_prompt}\n\n\n\n\n\n\n\n\nBelow is the query:\n\n{input_query}\n\n\n\n\n\n\n\n\nYou have the following options to process the long context. Choose one of them:\n\n\n\n\n\n\n\n\n[1]. Retrieve the chunk most relevant to the input query to support answer generation.\n\n\n\n\n\n\n\n\n[2]. Summarize each chunk and then aggregate the summaries after processing all chunks.\n\n\n\n\n\n\n\n\n[3]. Extract key sentences from each chunk and then aggregate them after processing all chunks.\n\n\n\n\n\n\n\n\n[4]. Sequentially scan chunks and produce the answer as soon as the query can be answered.\n\n\n\n\n\n\n\n\nBelow are some examples for reference:\n\n\n\n\n\n\n\n\nThe examples begin as follows:\n\n{examples}\n\nThe examples conclude here.\n\n\n\n\n\n\n\n\nPlease learn the examples and select one of the options by only outputting the corresponding index number.\n\n\n\n\n\n\nTable 7: Query Rewritten Prompt Template for the [Task Understanding] action.\n\n\n\n\n\n\nYou need to process a task with a long context that greatly exceeds your context limit.\n\nThe only feasible way to handle this is by processing the long context chunk by chunk.\n\n\n\n\n\n\n\n\nBelow is the original task prompt:\n\n{task_prompt}\n\n\n\n\n\n\n\n\nBelow is the query:\n\n{input_query}\n\n\n\n\n\n\n\n\nYou will process the long context with the following strategy:\n\n{strategy}\n\n\n\n\n\n\n\n\nDo you think the the query is proper for processing context chunk? If not, rewrite the query.\n\n\n\n\n\n\n\n\nBelow are some examples for reference:\n\n\n\n\n\n\n\n\nThe examples begin as follows:\n\n{examples}\n\nThe examples conclude here.\n\n\n\n\n\n\n\n\nPlease study the examples carefully. If the query needs to be rewritten, directly output the revised query.\n\nIf no revision is necessary, output \u201cnull\u201d.\n\n\n\n\n\n\nTable 8: Prompt Template for the [Append] action.\n\n\n\n\n\n\nYou are given an article and a question. Read the article carefully and follow my instructions to process it.\n\n\n\n\n\n\n\n\nArticle:\n\n\n\n\n\n\n\n\nThe article begins as follows:\n\n{article}\n\nThe article concludes here.\n\n\n\n\n\n\n\n\nQuestion:\n\n\n\n\n\n\n\n\n{question}\n\n\n\n\n\n\n\n\nInstructions:\n\n\n\n\n\n\n\n\nEach sentence in the article is marked with a sentence identifier [si], for example [s1].\n\n\n\n\n\n\n\n\nSelect up to ten key sentences from the article that are most likely to answer the question.\n\n\n\n\n\n\n\n\nOnly output the selected sentence identifiers, separated by commas.\n\n\n\n\n\n\n\n\nExample: [s39],[s54]\n\n\n\n\n\n\n\n\nIf no sentences are relevant, please output \"null\".\n\n\n\n\n\n\nTable 9: Prompt Template for the MultiFieldQA Task from the LongBench Benchmark. Additions made by us are highlighted in blue.\n\n\n\n\n\n\nRead the following text and answer briefly.\n\n\n\n\n\n\n\n\n{context}\n\n\n\n\n\n\n\n\nNow, answer the following question based on the above text, only give me the answer and do not output any\n\n\n\n\n\n\n\n\nother words.  If no answer can be found in the text, please output \"null\".\n\n\n\n\n\n\n\n\nQuestion:{question}\n\n\n\n\n\n\n\n\nAnswer:\n\n\n\n\n\n\nTable 10: Prompt Template for the [Merge] action.\n\n\n\n\n\n\nYou are provided with a portion of an article, a question, and summarization of the article\u2019s previous portions.\n\n\n\n\n\n\n\n\nRead the article portion and follow my instructions to process it.\n\n\n\n\n\n\n\n\nArticle:\n\n\n\n\n\n\n\n\nThe article begins as follows:\n\n{article}\n\nThe article concludes here.\n\n\n\n\n\n\n\n\nPrevious summarization:\n\n\n\n\n\n\n\n\nThe previous summarization is as follows:\n\n{previous_sum}\n\nThe previous summarization concludes here.\n\n\n\n\n\n\n\n\nQuestion:\n\n\n\n\n\n\n\n\n{question}\n\n\n\n\n\n\n\n\nInstruction:\n\n\n\n\n\n\n\n\nSummarize the partial article to supplement the previous summarization, which can better support the task.\n\n\n\n\n\n\n\n\nIf no content needs to be supplemented, please output \"null\".\n\n\n\n\n\n",
      "Appendix D Limitations and Broad Impact": "\n\nAppendix D Limitations and Broad Impact\n\nIn this paper, we propose LC-Boost, a method dedicated to solving long-context tasks using short contexts. However, there are several limitations we would like to address in the future work:\n(1) Although we conduct comprehensive experiments on many tasks and provide theoretical analysis to support our major claim that most long-context tasks are short-context solvable, there may be more complicated scenarios that require understanding the full context in a brute-force setting. LC-Boost might not be able to process such tasks effectively.\n(2) As mentioned in Section\u00a02.3, LC-Boost selects actions from a discrete action space. While we argue that the pre-defined action space is versatile enough to handle most scenarios, a more elegant solution would be to predict actions in a continuous space. We conducted preliminary experiments to explore allowing LC-Boost to prompt itself to predict actions without a predefined action space, such as writing prompts or code autonomously. These experiments resulted in highly unstable performance, particularly for models like GPT-3.5, as such requirements are still challenging. We believe that with a much stronger foundation model, LC-Boost could be expected to predict actions in a continuous space.\n(3) We choose GPT-3.5 as the foundation model for LC-Boost, instead of open-source LLMs. The reason is that GPT-3.5 is a strong, yet efficient model that can generally understand most instructions. However, we found that most open-source LLMs lack these properties in a zero-shot setting. Fine-tuning these open-source LLMs might be helpful, but constructing such instruction data is infeasible and expensive.\n\n\nAs discussed in Section\u00a03.5, LLMs are likely to become a fundamental infrastructure in the near future. At that scale, their energy consumption will pose significant environmental challenges. As shown in Figure\u00a04, LC-Boost avoids processing long contexts directly by decomposing them into shorter contexts. This approach significantly reduces energy consumption as the context length increases, leading to substantial positive environmental impacts. We believe that in the future, more research will focus on green AI initiatives. This paper could serve as an initial spark to inspire further research in this direction, potentially resulting in broader social impact.\n\n"
    },
    "bibliography": {
      "Bai et\u00a0al. (2023)": "\nBai et\u00a0al. (2023)\n\nYushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, et\u00a0al.\n\n\nLongbench: A bilingual, multitask benchmark for long context understanding.\n\n\narXiv preprint arXiv:2308.14508, 2023.\n\n\n",
      "Zhang et\u00a0al. (2024a)": "\nZhang et\u00a0al. (2024a)\n\nXinrong Zhang, Yingfa Chen, Shengding Hu, Zihang Xu, Junhao Chen, Moo\u00a0Khai Hao, Xu\u00a0Han, Zhen\u00a0Leng Thai, Shuo Wang, Zhiyuan Liu, and Maosong Sun.\n\n\n\u221e\\infty\u221ebench: Extending long context evaluation beyond 100k tokens, 2024a.\n\n\n",
      "Touvron et\u00a0al. (2023a)": "\nTouvron et\u00a0al. (2023a)\n\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et\u00a0al.\n\n\nLlama 2: Open foundation and fine-tuned chat models.\n\n\narXiv preprint arXiv:2307.09288, 2023a.\n\n\n",
      "Touvron et\u00a0al. (2023b)": "\nTouvron et\u00a0al. (2023b)\n\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et\u00a0al.\n\n\nLlama 2: Open foundation and fine-tuned chat models.\n\n\narXiv preprint arXiv:2307.09288, 2023b.\n\n\n",
      "Kaplan et\u00a0al. (2020)": "\nKaplan et\u00a0al. (2020)\n\nJared Kaplan, Sam McCandlish, Tom Henighan, Tom\u00a0B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei.\n\n\nScaling laws for neural language models, 2020.\n\n\n",
      "Liu et\u00a0al. (2023)": "\nLiu et\u00a0al. (2023)\n\nNelson\u00a0F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang.\n\n\nLost in the middle: How language models use long contexts, 2023.\n\n\n",
      "Li et\u00a0al. (2023a)": "\nLi et\u00a0al. (2023a)\n\nDacheng Li, Rulin Shao, Anze Xie, Ying Sheng, Lianmin Zheng, Joseph Gonzalez, Ion Stoica, Xuezhe Ma, and Hao Zhang.\n\n\nHow long can context length of open-source llms truly promise?\n\n\nIn NeurIPS 2023 Workshop on Instruction Tuning and Instruction Following, 2023a.\n\n\n",
      "Adolphs (1999)": "\nAdolphs (1999)\n\nRalph Adolphs.\n\n\nSocial cognition and the human brain.\n\n\nTrends in cognitive sciences, 3(12):469\u2013479, 1999.\n\n\n",
      "Bryant and O\u2019Hallaron (2011)": "\nBryant and O\u2019Hallaron (2011)\n\nRandal\u00a0E Bryant and David\u00a0Richard O\u2019Hallaron.\n\n\nComputer systems: a programmer\u2019s perspective.\n\n\nPrentice Hall, 2011.\n\n\n",
      "Xu et\u00a0al. (2023a)": "\nXu et\u00a0al. (2023a)\n\nPeng Xu, Wei Ping, Xianchao Wu, Lawrence McAfee, Chen Zhu, Zihan Liu, Sandeep Subramanian, Evelina Bakhturina, Mohammad Shoeybi, and Bryan Catanzaro.\n\n\nRetrieval meets Long Context Large Language Models.\n\n\narXiv, 2023a.\n\n\ndoi: 10.48550/arxiv.2310.03025.\n\n\nExperimental.\n\n\n",
      "Ji et\u00a0al. (2023)": "\nJi et\u00a0al. (2023)\n\nZiwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye\u00a0Jin Bang, Andrea Madotto, and Pascale Fung.\n\n\nSurvey of hallucination in natural language generation.\n\n\nACM Computing Surveys, 55(12):1\u201338, 2023.\n\n\n",
      "Lewis et\u00a0al. (2020)": "\nLewis et\u00a0al. (2020)\n\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich K\u00fcttler, Mike Lewis, Wen-tau Yih, Tim Rockt\u00e4schel, Sebastian Riedel, and Douwe Kiela.\n\n\nRetrieval-Augmented Generation for knowledge-intensive NLP tasks.\n\n\nIn Advances in Neural Information Processing Systems, volume\u00a033, pages 9459\u20139474, 2020.\n\n\nURL https://proceedings.neurips.cc/paper_files/paper/2020/file/6b493230205f780e1bc26945df7481e5-Paper.pdf.\n\n\n",
      "Shuster et\u00a0al. (2021)": "\nShuster et\u00a0al. (2021)\n\nKurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela, and Jason Weston.\n\n\nRetrieval augmentation reduces hallucination in conversation.\n\n\nIn Findings of the Association for Computational Linguistics: EMNLP 2021, pages 3784\u20133803, Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics.\n\n\ndoi: 10.18653/v1/2021.findings-emnlp.320.\n\n\nURL https://aclanthology.org/2021.findings-emnlp.320.\n\n\n",
      "Cover (1999)": "\nCover (1999)\n\nThomas\u00a0M Cover.\n\n\nElements of information theory.\n\n\nJohn Wiley & Sons, 1999.\n\n\n",
      "Tishby and Zaslavsky (2015)": "\nTishby and Zaslavsky (2015)\n\nNaftali Tishby and Noga Zaslavsky.\n\n\nDeep learning and the information bottleneck principle, 2015.\n\n\n",
      "Mohri et\u00a0al. (2018)": "\nMohri et\u00a0al. (2018)\n\nMehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar.\n\n\nFoundations of machine learning.\n\n\nMIT press, 2018.\n\n\n",
      "Ko\u010disk\u00fd et\u00a0al. (2017)": "\nKo\u010disk\u00fd et\u00a0al. (2017)\n\nTom\u00e1\u0161 Ko\u010disk\u00fd, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl\u00a0Moritz Hermann, G\u00e1bor Melis, and Edward Grefenstette.\n\n\nThe narrativeqa reading comprehension challenge, 2017.\n\n\n",
      "Dasigi et\u00a0al. (2021)": "\nDasigi et\u00a0al. (2021)\n\nPradeep Dasigi, Kyle Lo, Iz\u00a0Beltagy, Arman Cohan, Noah\u00a0A Smith, and Matt Gardner.\n\n\nA dataset of information-seeking questions and answers anchored in research papers.\n\n\nIn Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 4599\u20134610, 2021.\n\n\n",
      "Yang et\u00a0al. (2018)": "\nYang et\u00a0al. (2018)\n\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William\u00a0W. Cohen, Ruslan Salakhutdinov, and Christopher\u00a0D. Manning.\n\n\nHotpotqa: A dataset for diverse, explainable multi-hop question answering, 2018.\n\n\n",
      "Ho et\u00a0al. (2020)": "\nHo et\u00a0al. (2020)\n\nXanh Ho, Anh-Khoa Duong\u00a0Nguyen, Saku Sugawara, and Akiko Aizawa.\n\n\nConstructing a multi-hop QA dataset for comprehensive evaluation of reasoning steps.\n\n\nIn Donia Scott, Nuria Bel, and Chengqing Zong, editors, Proceedings of the 28th International Conference on Computational Linguistics, pages 6609\u20136625, Barcelona, Spain (Online), December 2020. International Committee on Computational Linguistics.\n\n\ndoi: 10.18653/v1/2020.coling-main.580.\n\n\nURL https://aclanthology.org/2020.coling-main.580.\n\n\n",
      "Trivedi et\u00a0al. (2022)": "\nTrivedi et\u00a0al. (2022)\n\nHarsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal.\n\n\nMusique: Multihop questions via single-hop question composition.\n\n\nTransactions of the Association for Computational Linguistics, 10:539\u2013554, 2022.\n\n\n",
      "Huang et\u00a0al. (2021)": "\nHuang et\u00a0al. (2021)\n\nLuyang Huang, Shuyang Cao, Nikolaus Parulian, Heng Ji, and Lu\u00a0Wang.\n\n\nEfficient attentions for long document summarization.\n\n\nIn Kristina Toutanova, Anna Rumshisky, Luke Zettlemoyer, Dilek Hakkani-Tur, Iz\u00a0Beltagy, Steven Bethard, Ryan Cotterell, Tanmoy Chakraborty, and Yichao Zhou, editors, Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1419\u20131436, Online, June 2021. Association for Computational Linguistics.\n\n\ndoi: 10.18653/v1/2021.naacl-main.112.\n\n\nURL https://aclanthology.org/2021.naacl-main.112.\n\n\n",
      "Fabbri et\u00a0al. (2019)": "\nFabbri et\u00a0al. (2019)\n\nAlexander\u00a0R. Fabbri, Irene Li, Tianwei She, Suyi Li, and Dragomir\u00a0R. Radev.\n\n\nMulti-news: a large-scale multi-document summarization dataset and abstractive hierarchical model, 2019.\n\n\n",
      "Gliwa et\u00a0al. (2019)": "\nGliwa et\u00a0al. (2019)\n\nBogdan Gliwa, Iwona Mochol, Maciej Biesek, and Aleksander Wawer.\n\n\nSAMSum corpus: A human-annotated dialogue dataset for abstractive summarization.\n\n\nIn Lu\u00a0Wang, Jackie Chi\u00a0Kit Cheung, Giuseppe Carenini, and Fei Liu, editors, Proceedings of the 2nd Workshop on New Frontiers in Summarization, pages 70\u201379, Hong Kong, China, November 2019. Association for Computational Linguistics.\n\n\ndoi: 10.18653/v1/D19-5409.\n\n\nURL https://aclanthology.org/D19-5409.\n\n\n",
      "Guo et\u00a0al. (2023)": "\nGuo et\u00a0al. (2023)\n\nDaya Guo, Canwen Xu, Nan Duan, Jian Yin, and Julian McAuley.\n\n\nLongcoder: A long-range pre-trained language model for code completion, 2023.\n\n\n",
      "Chiang et\u00a0al. (2023)": "\nChiang et\u00a0al. (2023)\n\nWei-Lin Chiang, Zhuohan Li, Zi\u00a0Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph\u00a0E. Gonzalez, Ion Stoica, and Eric\u00a0P. Xing.\n\n\nVicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023.\n\n\nURL https://lmsys.org/blog/2023-03-30-vicuna/.\n\n\n",
      "Li et\u00a0al. (2023b)": "\nLi et\u00a0al. (2023b)\n\nDacheng Li, Rulin Shao, Anze Xie, Ying Sheng, Lianmin Zheng, Joseph\u00a0E. Gonzalez, Ion Stoica, Xuezhe Ma, and Hao Zhang.\n\n\nHow long can open-source llms truly promise on context length?, June 2023b.\n\n\nURL https://lmsys.org/blog/2023-06-29-longchat.\n\n\n",
      "Jiang et\u00a0al. (2023a)": "\nJiang et\u00a0al. (2023a)\n\nAlbert\u00a0Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra\u00a0Singh Chaplot, Diego de\u00a0las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et\u00a0al.\n\n\nMistral 7b.\n\n\narXiv preprint arXiv:2310.06825, 2023a.\n\n\n",
      "Zhang et\u00a0al. (2024b)": "\nZhang et\u00a0al. (2024b)\n\nPeitian Zhang, Ninglu Shao, Zheng Liu, Shitao Xiao, Hongjin Qian, Qiwei Ye, and Zhicheng Dou.\n\n\nExtending llama-3\u2019s context ten-fold overnight, 2024b.\n\n\n",
      "Abdin et\u00a0al. (2024)": "\nAbdin et\u00a0al. (2024)\n\nMarah Abdin, Sam\u00a0Ade Jacobs, Ammar\u00a0Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Harkirat Behl, Alon Benhaim, Misha Bilenko, Johan Bjorck, S\u00e9bastien Bubeck, Martin Cai, Caio C\u00e9sar\u00a0Teodoro Mendes, Weizhu Chen, Vishrav Chaudhary, Parul Chopra, Allie\u00a0Del Giorno, Gustavo de\u00a0Rosa, Matthew Dixon, Ronen Eldan, Dan Iter, Amit Garg, Abhishek Goswami, Suriya Gunasekar, Emman Haider, Junheng Hao, Russell\u00a0J. Hewett, Jamie Huynh, Mojan Javaheripi, Xin Jin, Piero Kauffmann, Nikos Karampatziakis, Dongwoo Kim, Mahoud Khademi, Lev Kurilenko, James\u00a0R. Lee, Yin\u00a0Tat Lee, Yuanzhi Li, Chen Liang, Weishung Liu, Eric Lin, Zeqi Lin, Piyush Madan, Arindam Mitra, Hardik Modi, Anh Nguyen, Brandon Norick, Barun Patra, Daniel Perez-Becker, Thomas Portet, Reid Pryzant, Heyang Qin, Marko Radmilac, Corby Rosset, Sambudha Roy, Olatunji Ruwase, Olli Saarikivi, Amin Saied, Adil Salim, Michael Santacroce, Shital Shah, Ning Shang, Hiteshi Sharma, Xia Song, Masahiro Tanaka, Xin Wang, Rachel\nWard, Guanhua Wang, Philipp Witte, Michael Wyatt, Can Xu, Jiahang Xu, Sonali Yadav, Fan Yang, Ziyi Yang, Donghan Yu, Chengruidong Zhang, Cyril Zhang, Jianwen Zhang, Li\u00a0Lyna Zhang, Yi\u00a0Zhang, Yue Zhang, Yunan Zhang, and Xiren Zhou.\n\n\nPhi-3 technical report: A highly capable language model locally on your phone, 2024.\n\n\n",
      "AI et\u00a0al. (2024)": "\nAI et\u00a0al. (2024)\n\n01. AI, :, Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge\u00a0Zhang, Guanwei Zhang, Heng Li, Jiangcheng Zhu, Jianqun Chen, Jing Chang, Kaidong Yu, Peng Liu, Qiang Liu, Shawn Yue, Senbin Yang, Shiming Yang, Tao Yu, Wen Xie, Wenhao Huang, Xiaohui Hu, Xiaoyi Ren, Xinyao Niu, Pengcheng Nie, Yuchi Xu, Yudong Liu, Yue Wang, Yuxuan Cai, Zhenyu Gu, Zhiyuan Liu, and Zonghong Dai.\n\n\nYi: Open foundation models by 01.ai, 2024.\n\n\n",
      "DeepSeek-AI (2024)": "\nDeepSeek-AI (2024)\n\nDeepSeek-AI.\n\n\nDeepseek-v2: A strong, economical, and efficient mixture-of-experts language model, 2024.\n\n\n",
      "Peng et\u00a0al. (2023)": "\nPeng et\u00a0al. (2023)\n\nBowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole.\n\n\nYarn: Efficient context window extension of large language models.\n\n\nIn The Twelfth International Conference on Learning Representations, 2023.\n\n\n",
      "Fu et\u00a0al. (2024)": "\nFu et\u00a0al. (2024)\n\nYao Fu, Rameswar Panda, Xinyao Niu, Xiang Yue, Hannaneh Hajishirzi, Yoon Kim, and Hao Peng.\n\n\nData engineering for scaling language models to 128k context, 2024.\n\n\n",
      "Cai et\u00a0al. (2024)": "\nCai et\u00a0al. (2024)\n\nZheng Cai, Maosong Cao, Haojiong Chen, Kai Chen, Keyu Chen, Xin Chen, Xun Chen, Zehui Chen, Zhi Chen, Pei Chu, et\u00a0al.\n\n\nInternlm2 technical report.\n\n\narXiv preprint arXiv:2403.17297, 2024.\n\n\n",
      "Chen et\u00a0al. (2023a)": "\nChen et\u00a0al. (2023a)\n\nYukang Chen, Shengju Qian, Haotian Tang, Xin Lai, Zhijian Liu, Song Han, and Jiaya Jia.\n\n\nLonglora: Efficient fine-tuning of long-context large language models.\n\n\nIn The Twelfth International Conference on Learning Representations, 2023a.\n\n\n",
      "Du et\u00a0al. (2022)": "\nDu et\u00a0al. (2022)\n\nZhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie Tang.\n\n\nGlm: General language model pretraining with autoregressive blank infilling.\n\n\nIn Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 320\u2013335, 2022.\n\n\n",
      "OpenAI (2023)": "\nOpenAI (2023)\n\nOpenAI.\n\n\nGpt-4 technical report.\n\n\nhttps://cdn.openai.com/papers/gpt-4.pdf, 2023.\n\n\n",
      "Izacard and Grave (2021a)": "\nIzacard and Grave (2021a)\n\nGautier Izacard and Edouard Grave.\n\n\nLeveraging passage retrieval with generative models for open domain question answering, 2021a.\n\n\n",
      "Gao et\u00a0al. (2024)": "\nGao et\u00a0al. (2024)\n\nYunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi\u00a0Dai, Jiawei Sun, Qianyu Guo, Meng Wang, and Haofen Wang.\n\n\nRetrieval-augmented generation for large language models: A survey, 2024.\n\n\n",
      "Wang et\u00a0al. (2023)": "\nWang et\u00a0al. (2023)\n\nZhiruo Wang, Jun Araki, Zhengbao Jiang, Md\u00a0Rizwan Parvez, and Graham Neubig.\n\n\nLearning to filter context for retrieval-augmented generation, 2023.\n\n\n",
      "Qian et\u00a0al. (2024)": "\nQian et\u00a0al. (2024)\n\nHongjin Qian, Zheng Liu, Kelong Mao, Yujia Zhou, and Zhicheng Dou.\n\n\nGrounding language model with chunking-free in-context retrieval, 2024.\n\n\n",
      "Izacard and Grave (2021b)": "\nIzacard and Grave (2021b)\n\nGautier Izacard and Edouard Grave.\n\n\nDistilling knowledge from reader to retriever for question answering.\n\n\nIn International Conference on Learning Representations, 2021b.\n\n\nURL https://openreview.net/forum?id=NTEz-6wysdb.\n\n\n",
      "Xu et\u00a0al. (2023b)": "\nXu et\u00a0al. (2023b)\n\nPeng Xu, Wei Ping, Xianchao Wu, Lawrence McAfee, Chen Zhu, Zihan Liu, Sandeep Subramanian, Evelina Bakhturina, Mohammad Shoeybi, and Bryan Catanzaro.\n\n\nRetrieval meets long context large language models.\n\n\nIn The Twelfth International Conference on Learning Representations, 2023b.\n\n\n",
      "Jiang et\u00a0al. (2023b)": "\nJiang et\u00a0al. (2023b)\n\nZhengbao Jiang, Frank\u00a0F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, and Graham Neubig.\n\n\nActive retrieval augmented generation.\n\n\narXiv preprint arXiv:2305.06983, 2023b.\n\n\nURL https://arxiv.org/pdf/2305.06983.\n\n\n",
      "Luo et\u00a0al. (2024)": "\nLuo et\u00a0al. (2024)\n\nKun Luo, Zheng Liu, Shitao Xiao, and Kang Liu.\n\n\nBge landmark embedding: A chunking-free embedding method for retrieval augmented long-context large language models, 2024.\n\n\n",
      "Ratner et\u00a0al. (2022)": "\nRatner et\u00a0al. (2022)\n\nNir Ratner, Yoav Levine, Yonatan Belinkov, Ori Ram, Inbal Magar, Omri Abend, Ehud Karpas, Amnon Shashua, Kevin Leyton-Brown, and Yoav Shoham.\n\n\nParallel Context Windows Improve In-Context Learning of Large Language Models.\n\n\narXiv, 2022.\n\n\ndoi: 10.48550/arxiv.2212.10947.\n\n\nWindow.\n\n\n",
      "Nakano et\u00a0al. (2022)": "\nNakano et\u00a0al. (2022)\n\nReiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, Xu\u00a0Jiang, Karl Cobbe, Tyna Eloundou, Gretchen Krueger, Kevin Button, Matthew Knight, Benjamin Chess, and John Schulman.\n\n\nWebgpt: Browser-assisted question-answering with human feedback, 2022.\n\n\n",
      "Yang et\u00a0al. (2023)": "\nYang et\u00a0al. (2023)\n\nHui Yang, Sifu Yue, and Yunzhong He.\n\n\nAuto-gpt for online decision making: Benchmarks and additional opinions.\n\n\narXiv preprint arXiv:2306.02224, 2023.\n\n\n",
      "Driess et\u00a0al. (2023)": "\nDriess et\u00a0al. (2023)\n\nDanny Driess, Fei Xia, Mehdi\u00a0SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, et\u00a0al.\n\n\nPalm-e: An embodied multimodal language model.\n\n\narXiv preprint arXiv:2303.03378, 2023.\n\n\n",
      "Dong et\u00a0al. (2022)": "\nDong et\u00a0al. (2022)\n\nQingxiu Dong, Lei Li, Damai Dai, Ce\u00a0Zheng, Zhiyong Wu, Baobao Chang, Xu\u00a0Sun, Jingjing Xu, and Zhifang Sui.\n\n\nA survey on in-context learning.\n\n\narXiv preprint arXiv:2301.00234, 2022.\n\n\n",
      "Wei et\u00a0al. (2022)": "\nWei et\u00a0al. (2022)\n\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed\u00a0H. Chi, Quoc\u00a0V Le, and Denny Zhou.\n\n\nChain of thought prompting elicits reasoning in large language models.\n\n\nIn Alice\u00a0H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in Neural Information Processing Systems, 2022.\n\n\nURL https://openreview.net/forum?id=_VjQlMeSB_J.\n\n\n",
      "Shinn et\u00a0al. (2023)": "\nShinn et\u00a0al. (2023)\n\nNoah Shinn, Beck Labash, and Ashwin Gopinath.\n\n\nReflexion: an autonomous agent with dynamic memory and self-reflection.\n\n\narXiv preprint arXiv:2303.11366, 2023.\n\n\n",
      "Chen et\u00a0al. (2023b)": "\nChen et\u00a0al. (2023b)\n\nJianlv Chen, Shitao Xiao, Peitian Zhang, Kun Luo, Defu Lian, and Zheng Liu.\n\n\nBge m3-embedding: Multi-lingual, multi-functionality, multi-granularity text embeddings through self-knowledge distillation, 2023b.\n\n\n"
    },
    "domain": "cs.CL",
    "citation_count": 0
  },
  "ad69cb46-08ab-4934-b679-8a7a93ca30fe": {
    "pk": "ad69cb46-08ab-4934-b679-8a7a93ca30fe",
    "authors": [
      "Omer Goldman",
      "Alon Jacovi",
      "Aviv Slobodkin",
      "Aviya Maimon",
      "Ido Dagan",
      "Reut Tsarfaty"
    ],
    "title": "Is It Really Long Context if All You Need Is Retrieval? Towards Genuinely Difficult Long Context NLP",
    "abstract": "Improvements in language models' capabilities have pushed their applications towards longer contexts, making long-context evaluation and development an active research area. However, many disparate use-cases are grouped together under the umbrella term of \"long-context\", defined simply by the total length of the model's input, including - for example - Needle-in-a-Haystack tasks, book summarization, and information aggregation. Given their varied difficulty, in this position paper we argue that conflating different tasks by their context length is unproductive. As a community, we require a more precise vocabulary to understand what makes long-context tasks similar or different. We propose to unpack the taxonomy of long-context based on the properties that make them more difficult with longer contexts. We propose two orthogonal axes of difficulty: (I) Diffusion: How hard is it to find the necessary information in the context? (II) Scope: How much necessary information is there to find? We survey the literature on long-context, provide justification for this taxonomy as an informative descriptor, and situate the literature with respect to it. We conclude that the most difficult and interesting settings, whose necessary information is very long and highly diffused within the input, is severely under-explored. By using a descriptive vocabulary and discussing the relevant properties of difficulty in long-context, we can implement more informed research in this area. We call for a careful design of tasks and benchmarks with distinctly long context, taking into account the characteristics that make it qualitatively different from shorter context.",
    "url": "http://arxiv.org/abs/2407.00402v3",
    "timestamp": 1719659387,
    "sections": {
      "1 Introduction": "\n\n1 Introduction\n\nFigure 1: A taxonomy of long context tasks based on the distribution of the needed information in the text. Tasks with larger scope and higher dispersion are more difficult (indicated by shade) and more indicative of the long context capabilities of large language models.\n\n\nThe ability to deal with ever-longer contexts has been one of the most notable trends among the emerging capabilities of large language models (LLMs). Starting with a few hundred tokens as the maximal input length of the first attention-based LLMs Devlin et\u00a0al. (2019); Raffel et\u00a0al. (2020), contemporary models are \u2013 technically \u2013 able to process up to 128k and even 1M tokens (Gemini Team Google, 2024; OpenAI, 2024).\n\n\nThe demand to evaluate LLMs in this setting has led to a line of research on designing long-context tasks and benchmarks, in order to systematically understand models\u2019 capabilities and drive their development.\nHowever, the field has generally a sole recurring descriptor to define such measurements by \u2013 simply, the length of the context.\nFor example, long-context benchmarks group tasks mostly by length in words (e.g., Shaham et\u00a0al., 2022; Bai et\u00a0al., 2023; Zhang et\u00a0al., 2024b). This leads to qualitatively different measurements being conflated together, with conclusions about long-context capabilities being extended from one class of tasks to others.\nThe community is, of course, aware that, for example, tasks which require a small part of the input are different from tasks that require a large part of it. But we ask the more general question: What are the properties that differentiate tasks when conditioned on their context length? What can we accomplish with such a distinction?\n\n\nIn this position paper, we claim that the current landscape of works on long-context evaluation will greatly benefit from a more fine-grained characterization of long-context task design. We argue that judging LLMs by their ability to process long sequences, while disregarding the task they process them for, overlooks the characteristics that make longer inputs more difficult, and interesting to research, to begin with\u00a0(\u00a72).\n\n\nFor example, Needle in a Haystack tasks\u00a0(NIAH; Ivgi et\u00a0al., 2023; Mohtashami and Jaggi, 2023) involve queries whose main challenge is finding the relevant information in a long context, without requiring much further processing. Synthetic NIAH datasets are, of course, easier than their natural equivalents Ivgi et\u00a0al. (2023), but the \u201cnatural vs. artificial\u201d classification is not informative in our setting, since it applies equally for tasks regardless of context length. What, then, is an informative property? What makes long-context tasks different from each other? For example, multiple-needle variants of NIAH Hsieh et\u00a0al. (2024), or those that position the \u201cneedles\u201d closer or farther apart Levy et\u00a0al. (2024). Evidently, \u201cthe number of tokens in the input\u201d is not a sufficient descriptor.\n\n\nTo resolve this roadblock, we present a taxonomy of long-context tasks for the different factors that make them harder when controlling for context length\u00a0(\u00a73). This taxonomy is derived by surveying the long-context literature and surfacing the most salient points of distinction between various tasks.\nWe focus on (I) how difficult it is to find and extract the required information from the input (its dispersion in the input), and (II) the absolute quantity of required information to solve the task (its scope). See Figure\u00a01 for a summary.\n\n\nTo understand this categorization and its utility, we review the literature on long-context evaluation and position the works with respect to those factors. We find that the most challenging setting, in which a large quantity of required information is present in a dispersed manner that is difficult to extract, is significantly under-explored\u00a0(\u00a74).\n\n\nFinally, acknowledging the inherent and legitimate reasons behind the focus on context length as the sole descriptor of difficulty, we discuss possible paths forward for designing more reliable measurements of long-context capabilities when utilizing a more nuanced vocabulary\u00a0(\u00a75).\n\n",
      "2 Task Design in Long Context": "\n\n2 Task Design in Long Context\n\nEvaluating the performance of NLP models over very long contexts is a fast-changing area of research\u00a0Bishop et\u00a0al. (2024); Wu et\u00a0al. (2024). Measurements are regularly updated to account for new capabilities which improve with extrapolation architectures\u00a0Vaswani et\u00a0al. (2017); Su et\u00a0al. (2024) and training data\u00a0He et\u00a0al. (2023); Chen et\u00a0al. (2023). Evaluators were tasked with designing measurements of long-context capabilities cheaply, efficiently, and quickly, while matching realistic use cases as much as possible. The most common way of differentiating long-context tasks, besides the context\u2019s length, is whether they are naturally-constructed or synthetically-constructed\u00a0Tay et\u00a0al. (2020); Bai et\u00a0al. (2023); Hsieh et\u00a0al. (2024).\n\n\nNatural construction.\n\nA simple yet effective way of \u201cmoving the goalpost\u201d for context length is by modeling long-context tasks based on short-context tasks.\nThis was done, for example, with\nQA (Ko\u010disk\u00fd et\u00a0al., 2018, cf. Dunn et\u00a0al., 2017), summarization (Huang et\u00a0al., 2021a, cf. Narayan et\u00a0al., 2018), and NLI (Koreeda and Manning, 2021a, cf. Williams et\u00a0al., 2018). Specialized domains like legal Bruno and Roth (2022); Nguyen et\u00a0al. (2024) and literature (Wang et\u00a0al., 2022; Kryscinski et\u00a0al., 2022) often involve longer texts, turning typically short-context tasks such as QA and NLI into long-context scenarios.\nAnother more native methodology is to create new tasks which inherently require a long context,\nsuch as multi-document summarization Fabbri et\u00a0al. (2019); Angelidis et\u00a0al. (2021), survey generation Gao et\u00a0al. (2024), and structured data aggregation Caciularu et\u00a0al. (2024).\nBoth methodologies share the constraint that, due to their natural construction (i.e., using natural text), once created, they are difficult to modify for longer contexts as models\u2019 long-context capabilities improve.\n\n\n\nSynthetic construction.\n\nA more flexible approach, sacrificing natural construction for length control, is to use distractors to synthetically increase the context length.\nThis method allows for cheap and efficient (in terms of task construction cost) evaluation of models\u2019 full context length capabilities, with difficulty adjusted by controlling the distractors.\nTasks like Needle-in-a-Haystack (NIAH; Ivgi et\u00a0al., 2023; Kamradt, 2023) and PassKey retrieval (Mohtashami and Jaggi, 2023) were created to evaluate a model\u2019s ability to pinpoint specific information amid lengthy distractors.\nFlexible and effective against existing models, they became standard benchmarks for evaluating new long-context models (GLM Team, 2024; Jiang et\u00a0al., 2024).\nFollowup studies have complicated these tasks by increasing the number of critical details to locate (Arora et\u00a0al., 2023; Liu et\u00a0al., 2024a) and changing their position within the input Liu et\u00a0al. (2024b); Levy et\u00a0al. (2024).\n\n\n\nLimitations of the status quo.\n\nNIAH-like tasks aim to assess information retrieval capabilities, yet\nmany \u201cnaturally constructed\u201d QA and reading-comprehension tasks\nwith trivial questions about a long context accomplish the same goal. At the same time, \u201cmultiple needles\u201d NIAH can increase difficulty not by increasing the quantity of needles or length of input, but by adding distractors between needles\u00a0Levy et\u00a0al. (2024). What can systematically explain the different variables at play, in order to inform better task design in the future?\n\n\nClearly, there are underlying qualitative differences that discriminate between these various tasks besides their natural and synthetic constructions, and besides their actual context length.\nTherefore, we require a more informative vocabulary to discuss the goals of each task design, what it accomplishes, and what it does not, in terms of measuring long-context capabilities.\n\n\n",
      "3 What Makes Long Context More than Retrieval?": "\n\n3 What Makes Long Context More than Retrieval?\n\nWe require a taxonomy to capture task difficulty variations beyond mere \u201cnumber of tokens\u201d.\nWe focus on the information that is canonically required to solve the task as the conditioning variable. Our classification can be summarized via the following two questions, when asked about a given task:\n\n\n(I) How difficult is it to find and extract the required information?\n\n\n(II) How much information is needed to be found?\n\n\nAssuming that some highlighting of the relevant information is needed to solve the task (see Figure\u00a01), the latter question asks how much text is highlighted, while the former addresses the challenge of locating the relevant text for highlighting.\n\n\nFor instance, consider the task of summarizing a book, in comparison to a NIAH task of identifying a numerical detail in a long financial report (e.g., \u201chow much did the company earn in 2015?\u201d).\nAlthough both tasks involve long texts, the information required and its accessibility vary significantly.\nThe NIAH task focuses on localized, identifiable information, while summarization requires extracting key details dispersed throughout the text, tangled together with irrelevant content. Therefore, we can say that the book summarization task is more difficult on both axes (I) and (II).\n\n\nBelow we give more formal descriptions of the two axes characterized by the questions above.\n\n\n(I) Dispersion.\n\nAlthough the question above intuitively defines \u201cdifficulty of information finding\u201d,\nwe offer a more concrete description. Between two similar tasks, we consider the information harder to find in one task compared to another if:\n(1) it is more obscured (e.g., linguistically, semantically, contextually, etc); (2) it is more sparse, such that it is interspersed with non-required information; (3) its indicators are less redundant, such that there are fewer places in the document where the same information is available.\n\n\n\n(II) Scope.\n\nThe property of scope is simpler, and refers to the minimal quantity of information needed to solve the task. Importantly, we are not concerned with precise metric for \u201cquantity of information\u201d at this stage \u2013 it can refer to quantity of tokens, sentences, relations, cells in a table, etc. Any metric that reliably captures difficulty for an established solver is sufficient for our purposes.\n\n\n\nIllustrative example.\n\nTo illustrate, consider the Wikipedia entry for New York City and a simple question: \u201cWhat is the estimated population of the city?\u201d Since the answer needs a small snippet of information, we say that the task has small scope. And since it is easily accessible, we say that it has low dispersion. Consider, instead, the question \u201chow many syllables are in this document?\u201d \u2013 since this question requires the entire document to answer, we say that it has large scope, but if we consider counting syllables as straightforward, then we say its dispersion is still low. Finally, with the question \u201cWas the city\u2019s mayor elected before or after the city was affected by Hurricane Sandy?\u201d \u2013 since it requires snippets from at least two different areas of the text, we can say that when compared to the question about the city\u2019s population, the dispersion is higher, but not as high as for the question \u201cWhat makes the city a prominent place on the world stage?\u201d which poses a challenge on both axes.\n\n\nFigure 2: This figure illustrates our subjective judgment on the distribution of long-context benchmarks for each task, categorized by their scope and dispersion characteristics, with the four quadrants being marked by the dashed lines. Difficulty is expressed by shade, where red is more difficult and green in easier. Notably, some tasks, like Question-answering (QA), appear in multiple quadrants, as different benchmarks demand varying levels of scope and dispersion (e.g., a single fact versus multiple facts spread across a document). For a detailed breakdown of benchmarks and their task associations, refer to Appendix\u00a0A.\n\n\n",
      "4 Challenging Long Context Is Under-Explored": "\n\n4 Challenging Long Context Is Under-Explored\n\nRevisiting the works surveyed in \u00a72, they clearly differ with respect to both scope and dispersion.\n\n\nWith respect to dispersion.\n\nThe information needed for tasks ranges from easily accessible to highly dispersed and difficult to detect.\nOn low dispersion we have NIAH Kamradt (2023); Mohtashami and Jaggi (2023) and a myriad of factual single-hop QA datasets (Tseng et\u00a0al., 2016; Ko\u010disk\u00fd et\u00a0al., 2017; Kwiatkowski et\u00a0al., 2019; Dasigi et\u00a0al., 2021, inter alia) in which the answer is relatively accessible.\nAdding more snippets of information separated by distractors, either in the form of several needles Arora et\u00a0al. (2023); Hsieh et\u00a0al. (2024) or of hops in a multi-hop question Trivedi et\u00a0al. (2022); Zhao et\u00a0al. (2022), complicates the information detection due to the need to find at least two snippets Levy et\u00a0al. (2024), thereby increasing dispersion.\nDispersion can also be increased by making the detection of the information less straightforward\n(e.g., Pang et\u00a0al., 2022) or requiring aggregation Shaham et\u00a0al. (2023).\nLastly, summarization tasks are of a very high dispersion Huang et\u00a0al. (2021a); Wang et\u00a0al. (2022), as they require the non-trivial detection of salient facts that are interwoven with the irrelevant text.\n\n\n\nWith respect to scope.\n\nTasks overwhelmingly target relatively small scope. In addition to the aforementioned NIAH tasks and their variants, many\nQA datasets apply as well\u00a0(Li et\u00a0al., 2023b; Zhao et\u00a0al., 2023; Reddy et\u00a0al., 2024, inter alia).\nA somewhat higher scope is achieved by datasets for query-based summarization Zhong et\u00a0al. (2021); Wang et\u00a0al. (2022), and QA datasets with more obfuscated answers that require reading the text surrounding the answer for its verification\u00a0An et\u00a0al. (2023); He et\u00a0al. (2023).\nAlthough much higher on the scope ladder, book summarization is still limited in its scope as datasets include texts that are only of up to 20k\ud835\udc58kitalic_k tokens Huang et\u00a0al. (2021a); Chen et\u00a0al. (2022a); Shaham et\u00a0al. (2023).\nCurrently, tasks with the highest scope, requiring information across the entire input length, are artificial and of low dispersion, like common words extraction Hsieh et\u00a0al. (2024).\n\n\n\nConclusion.\n\nFigure\u00a02 summarizes the above classification of tasks and datasets. Note that without a concrete definition of dispersion and scope, the plot is only an illustration that involves a good deal of subjective judgements.\nHowever, we conclude that (1) the majority of tasks designed to challenge LLMs in a long-context setting target either scope or dispersion,\nsuch that (2) tasks that push current models\u2019 capabilities on both axes are under-represented in the current landscape.\n\n\n",
      "5 Discussion: Towards Genuinely Difficult Long-Context Task Design": "\n\n5 Discussion: Towards Genuinely Difficult Long-Context Task Design\n\nChallenges.\n\nDesigning meaningful long-context tasks amidst rapid model progress is profoundly challenging, making the deficiency in tasks representing difficulty on both the dispersion and scope axes unsurprising.\nOne source of this challenge is the lack of diverse, coherent long texts, as models\u2019 context windows can now be comparable to\nthe length of the New Testament111www.readinglength.com/book/isbn-0190909005\nand the Odyssey.222www.readinglength.com/book/isbn-0140268863\nThe methodologies discussed in \u00a72 for creating long context tasks \u2013 lengthening short context tasks and synthetically creating length-adjustable tasks \u2013 are preferred for their straightforward definition and the incremental adjustments they require for existing data.\nThey rely on the common understanding of machine comprehension as formulated with short context in mind Dunietz et\u00a0al. (2020), and therefore they are intuitive and easy to comprehend for NLP researchers without domain expertise (e.g., in law or biomedical fields that have long contexts).\n\n\n\nFuture work.\n\nThe goals laid forward in this work are clear: For more durable and robust measurement of long-context capabilities, we must design tasks that explicitly target both the dispersion and scope capabilities of models.\nHow can this be achieved? As mentioned, one possible avenue is to rely more on tasks that require domain expertise, such as legal documents\u00a0Bruno and Roth (2022), financial reports\u00a0Reddy et\u00a0al. (2024), biomedical publications\u00a0Stylianou et\u00a0al. (2021), and so on. In specialized domains, it is common that dispersion will be naturally higher\u00a0Zhao et\u00a0al. (2022). Tasks that involve implicit aggregations over structured data, such as table manipulation\u00a0Caciularu et\u00a0al. (2024), are possible avenues for increasing both scope and dispersion synthetically by leveraging the data structure.\nIn this work, we argue that an explicit vocabulary for such properties of difficulty is what can enable more informed techniques to design difficult tasks in the future.\n\n\n",
      "6 Conclusions": "\n\n6 Conclusions\n\nWe present a taxonomy of factors that make long-context tasks more challenging compared to short ones. This is in contrast with the existing literature that refers only to the length of the input as the hallmark of long context, and as a result ends up conflating tasks of different character when assessing the ability of models to understand longer text.\nWe reviewed works on evaluation in a long-context setting and found that the most challenging setting, in which the information needed is of large scope and is highly dispersed within the input, is under-explored.\nFinally, we suggested some leads for future work to tackle this imbalance towards a more informative long context evaluation.\n\n",
      "7 Limitations": "\n\n7 Limitations\n\nFormality.\n\nIn the context of this work, we have deliberately adhered to a taxonomy based on an intuitive description, in the interest of utility to a wide diversity of research and flexibility for future work. Difficulty in searching for and extracting information, and quantity of information, are both vague terms that can only be grounded in the context of a specific family of tasks and use-cases. We intend for this work to serve as a call to action and a tool for a shared vocabulary in the interest of more informed long-context task design in the future, rather than to anchor the taxonomy to a specific and fragile point in time.\n\n\n\nRetrieval is still interesting.\n\nAlthough we argue that small scope and low dispersion tasks are the least indicative of the model\u2019s ability to long-context capabilities, tasks that are well-served by implicit retrieval or by traditional retrieval-based pipelines are certainly relevant and useful in a variety of common use-cases\u00a0Stylianou et\u00a0al. (2021); Bruno and Roth (2022); Gao et\u00a0al. (2023).\n\n\n\nOther uses for a long-context window.\n\nThis paper deals only with long inputs that serve as inputs to a task. The long context of course can have other purposes as well, like containing many in-context examples Bertsch et\u00a0al. (2024) or containing other modalities and structures Jiang et\u00a0al. (2023).\n\n\n",
      "Acknowledgments": "\nAcknowledgments\n\nThe authors would like to thank Gabriel Stanovsky for the fruitful discussions.\n\n",
      "Appendix A Benchmark Scope-Dispersion Classification": "\n\nAppendix A Benchmark Scope-Dispersion Classification\n\nIn Table\u00a01 we delineate the different long-context benchmarks, as well as classify them according to how challenging they are scope-wise and dispersion-wise.\n\n\n\n\n\n\n\nLOW SCOPE\nHIGH SCOPE\n\n\n\n\n\nQA\nQBS\n\n\n\n\u00a0\u00a0\u00a0\u00a0 Qasper (Dasigi et\u00a0al., 2021)\n\n\u00a0\u00a0\u00a0\u00a0 QMSum (Zhong et\u00a0al., 2021)\n\n\n\n\n\u00a0\u00a0\u00a0\u00a0 NarrativeQA (Ko\u010disk\u00fd et\u00a0al., 2018)\n\n\u00a0\u00a0\u00a0\u00a0 SQuALITY (Wang et\u00a0al., 2022)\n\n\n\n\n\u00a0\u00a0\u00a0\u00a0 Short-dependency QA (Li et\u00a0al., 2023b)\n\n\u00a0\u00a0\u00a0\u00a0 Related Work Summarization (An et\u00a0al., 2023)\n\n\n\n\n\u00a0\u00a0\u00a0\u00a0 MultiFieldQA (Bai et\u00a0al., 2023)\n\n\u00a0\u00a0\u00a0\u00a0 SPACE (Angelidis et\u00a0al., 2021)\n\n\n\n\n\u00a0\u00a0\u00a0\u00a0 LitM (QA) (Liu et\u00a0al., 2024b)\n\n\u00a0\u00a0\u00a0\u00a0 WebBrain-G (Qian et\u00a0al., 2023)\n\n\n\n\n\u00a0\u00a0\u00a0\u00a0 L-eval (MC QA) (An et\u00a0al., 2023)\n\n\u00a0\u00a0\u00a0\u00a0 AquaMuse (Kulkarni et\u00a0al., 2020)\n\n\n\n\n\u00a0\u00a0\u00a0\u00a0 NQ (Kwiatkowski et\u00a0al., 2019)\n\n\u00a0\u00a0\u00a0\u00a0 FINDSum-Liquidity (Liu et\u00a0al., 2023a)\n\n\n\n\n\u00a0\u00a0\u00a0\u00a0 RULER (single-hop QA) (Hsieh et\u00a0al., 2024)\n\nAggregation\n\n\n\n\u00a0\u00a0\u00a0\u00a0 MeetingQA (Prasad et\u00a0al., 2023)\n\n\u00a0\u00a0\u00a0\u00a0 ZeroSCROLLS (SpaceDigest & BookSumSort) (Shaham et\u00a0al., 2023)\n\n\n\n\n\u00a0\u00a0\u00a0\u00a0 BABILong (tasks 1,4-6,9-10) (Kuratov et\u00a0al., 2024)\n\n\u00a0\u00a0\u00a0\u00a0 PassageCount (Bai et\u00a0al., 2023)\n\n\n\n\n\u00a0\u00a0\u00a0\u00a0 Giraffe (2 tasks) (Pal et\u00a0al., 2023)\n\n\u00a0\u00a0\u00a0\u00a0 FINDSum-ROO (Liu et\u00a0al., 2023a)\n\n\n\n\nRetrieval\nAspect-based Summarization\n\n\n\n\u00a0\u00a0\u00a0\u00a0 LitM (Key-value Retrieval) (Liu et\u00a0al., 2024b)\n\n\u00a0\u00a0\u00a0\u00a0 ACLSum (Takeshita et\u00a0al., 2024)\n\n\n\n\n\u00a0\u00a0\u00a0\u00a0 MultiDoc2Dial (GSP) (Feng et\u00a0al., 2021)\n\n\u00a0\u00a0\u00a0\u00a0 OpenAsp (Amar et\u00a0al., 2023)\n\n\n\n\n\u00a0\u00a0\u00a0\u00a0 TopicRet (Li et\u00a0al., 2023a)\n\nText Sorting\n\n\n\n\u00a0\u00a0\u00a0\u00a0 Wiki-GenBen (Zhang et\u00a0al., 2024a)\n\n\u00a0\u00a0\u00a0\u00a0 Bamboo (ShowsSort & ReportSumSort) (Dong et\u00a0al., 2024)\n\n\n\n\n\u00a0\u00a0\u00a0\u00a0 RULER (S-NIAH & MK-NIAH) (Hsieh et\u00a0al., 2024)\n\nRetrieval\n\n\n\n\u00a0\u00a0\u00a0\u00a0 LongChat-Lines (Pal et\u00a0al., 2023)\n\n\u00a0\u00a0\u00a0\u00a0 PassageRetrieval (Bai et\u00a0al., 2023)\n\n\n\n\nNLI\nLFQA\n\n\n\n\u00a0\u00a0\u00a0\u00a0 LawngNLI (Bruno and Roth, 2022)\n\n\u00a0\u00a0\u00a0\u00a0 LongFQA (An et\u00a0al., 2023)\n\n\n\n\n\u00a0\u00a0\u00a0\u00a0 ContractNLI (Koreeda and Manning, 2021b)\n\nNLI\n\n\n\n\u00a0\u00a0\u00a0\u00a0 Hallucination Detection (Dong et\u00a0al., 2024)\n\n\u00a0\u00a0\u00a0\u00a0 Legal Case Entailment (Nguyen et\u00a0al., 2024)\n\n\n\n\n\u00a0\u00a0 FLenQA (3 tasks) (Levy et\u00a0al., 2024)\n\n\n\n\nFill-mask\n\n\n\n\n\u00a0\u00a0\u00a0\u00a0 Cloze (Li et\u00a0al., 2023b)\n\n\n\n\n\nNLG\n\n\n\n\n\nLOW DISPERSION\n\n\u00a0\u00a0\u00a0\u00a0 MultiDoc2Dial (ARG) (Feng et\u00a0al., 2021)\n\n\n\n\n\nQA\nSummarization\n\n\n\n\u00a0\u00a0\u00a0\u00a0 QuALITY (Pang et\u00a0al., 2022)\n\n\u00a0\u00a0\u00a0\u00a0 GovReport (Huang et\u00a0al., 2021b)\n\n\n\n\n\u00a0\u00a0\u00a0\u00a0 Long-dependency QA (Li et\u00a0al., 2023b)\n\n\u00a0\u00a0\u00a0\u00a0 SummScreenFD (Chen et\u00a0al., 2022b)\n\n\n\n\n\u00a0\u00a0\u00a0\u00a0 DuReader (Bai et\u00a0al., 2023)\n\n\u00a0\u00a0\u00a0\u00a0 Loogle (Summarization) (Li et\u00a0al., 2023b)\n\n\n\n\n\u00a0\u00a0\u00a0\u00a0 SFcition QA (An et\u00a0al., 2023)\n\n\u00a0\u00a0\u00a0\u00a0 VCSUM (Bai et\u00a0al., 2023)\n\n\n\n\n\u00a0\u00a0\u00a0\u00a0 ExpertQA (Malaviya et\u00a0al., 2024)\n\n\u00a0\u00a0\u00a0\u00a0 Self-critiquing (Saunders et\u00a0al., 2022)\n\n\n\n\n\u00a0\u00a0\u00a0\u00a0 DocFinQA (Reddy et\u00a0al., 2024)\n\n\u00a0\u00a0\u00a0\u00a0 Abstract Generation (An et\u00a0al., 2023)\n\n\n\n\n\u00a0\u00a0\u00a0\u00a0 BABILong (tasks 2-3,12) (Kuratov et\u00a0al., 2024)\n\n\u00a0\u00a0\u00a0\u00a0 Multi-News (Fabbri et\u00a0al., 2019)\n\n\n\n\n\u00a0\u00a0\u00a0\u00a0 Bamboo (QA) (Dong et\u00a0al., 2024)\n\n\u00a0\u00a0\u00a0\u00a0 BigPatent (Sharma et\u00a0al., 2019)\n\n\n\n\nMulti-hop QA\n\u00a0\u00a0\u00a0\u00a0 Scientific Summarization (Cohan et\u00a0al., 2018)\n\n\n\n\n\u00a0\u00a0\u00a0\u00a0 MuSiQue (Trivedi et\u00a0al., 2022)\n\n\u00a0\u00a0\u00a0\u00a0 BillSum (Kornilova and Eidelman, 2019)\n\n\n\n\n\u00a0\u00a0\u00a0\u00a0 HotpotQA (Yang et\u00a0al., 2018)\n\n\u00a0\u00a0\u00a0\u00a0 HowSumm (Boni et\u00a0al., 2021)\n\n\n\n\n\u00a0\u00a0\u00a0\u00a0 Multi-hop Tracing (Hsieh et\u00a0al., 2024)\n\n\u00a0\u00a0\u00a0\u00a0 ODSum (Zhou et\u00a0al., 2023)\n\n\n\n\n\u00a0\u00a0\u00a0\u00a0 RULER (multi-hop QA) (Hsieh et\u00a0al., 2024)\n\n\u00a0\u00a0\u00a0\u00a0 Klexikon (Summarization) (Aumiller and Gertz, 2022)\n\n\n\n\n\u00a0\u00a0\u00a0\u00a0 2WikiMultihopQA (Ho et\u00a0al., 2020)\n\n\u00a0\u00a0\u00a0\u00a0 Booksum (Kry\u015bci\u0144ski et\u00a0al., 2022)\n\n\n\n\nNLI\n\u00a0\u00a0\u00a0\u00a0 MeetingBank (Hu et\u00a0al., 2023)\n\n\n\n\n\u00a0\u00a0\u00a0\u00a0 FLenQA (3 rand. placement tasks) (Levy et\u00a0al., 2024)\n\nText Simplification\n\n\n\n\u00a0\u00a0\u00a0\u00a0 Legal Textual Entailment (Nguyen et\u00a0al., 2024)\n\n\u00a0\u00a0\u00a0\u00a0 Klexikon (Simplification) (Aumiller and Gertz, 2022)\n\n\n\n\nCode Understanding\nReasoning\n\n\n\n\u00a0\u00a0\u00a0\u00a0 LCC (Guo et\u00a0al., 2023)\n\n\u00a0\u00a0\u00a0\u00a0 Long ListOps (Tay et\u00a0al., 2020)\n\n\n\n\n\u00a0\u00a0\u00a0\u00a0 RepoBench-P (Liu et\u00a0al., 2023b)\n\nRetrieval\n\n\n\n\u00a0\u00a0\u00a0\u00a0 CodeU (An et\u00a0al., 2023)\n\n\u00a0\u00a0\u00a0\u00a0 LRA (task 3) (Tay et\u00a0al., 2020)\n\n\n\n\n\u00a0\u00a0\u00a0\u00a0 PrivateEval (Dong et\u00a0al., 2024)\n\n\n\n\n\nClassification\n\n\n\n\n\u00a0\u00a0\u00a0\u00a0 LRA (tasks 2, 4-6) (Tay et\u00a0al., 2020)\n\n\n\n\n\nRetrieval\n\n\n\n\n\u00a0\u00a0\u00a0\u00a0 COLIEE (tasks 1,3,4) (Nguyen et\u00a0al., 2024)\n\n\n\n\n\n\u00a0\u00a0\u00a0\u00a0 RULER (MV-NIAH & MQ-NIAH) (Hsieh et\u00a0al., 2024)\n\n\n\n\n\nNext Token Prediction\n\n\n\n\n\u00a0\u00a0\u00a0\u00a0 PG-19 (Rae et\u00a0al., 2019)\n\n\n\n\n\n\u00a0\u00a0\u00a0\u00a0 Bamboo (LM) (Dong et\u00a0al., 2024)\n\n\n\n\n\nReasoning\n\n\n\n\n\u00a0\u00a0\u00a0\u00a0 DocMath-Eval (Zhao et\u00a0al., 2023)\n\n\n\n\n\n\u00a0\u00a0\u00a0\u00a0 BABILong (tasks 14-20) (Kuratov et\u00a0al., 2024)\n\n\n\n\n\nAggregation\n\n\n\n\n\u00a0\u00a0\u00a0\u00a0 RULER (2 Aggr. tasks) (Hsieh et\u00a0al., 2024)\n\n\n\n\n\n\u00a0\u00a0\u00a0\u00a0 BABILong (tasks 7-8) (Kuratov et\u00a0al., 2024)\n\n\n\n\n\nNLU\n\n\n\n\n\u00a0\u00a0\u00a0\u00a0 Academic Feedback Generation (An et\u00a0al., 2023)\n\n\n\n\n\n\u00a0\u00a0\u00a0\u00a0 CUAD (Hendrycks et\u00a0al., 2021)\n\n\n\n\n\nFactuality Evaluation\n\n\n\n\n\u00a0\u00a0\u00a0\u00a0 LongSciVerify (Bishop et\u00a0al., 2024)\n\n\n\n\n\nCoreference Resolution\n\n\n\n\n\nHIGH DISPERSION\n\n\u00a0\u00a0\u00a0\u00a0 BABILong (tasks 11,13) (Kuratov et\u00a0al., 2024)\n\n\n\n\n\n\nTable 1: Classification of long-context benchmarks in terms of Scope and Dispersion.\n\n\n"
    },
    "bibliography": {
      "Amar et\u00a0al. (2023)": "\nAmar et\u00a0al. (2023)\n\nShmuel Amar, Liat Schiff, Ori Ernst, Asi Shefer, Ori Shapira, and Ido Dagan. 2023.\n\n\nOpenAsp: A benchmark for multi-document open aspect-based summarization.\n\n\nIn Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 1967\u20131991, Singapore. Association for Computational Linguistics.\n\n\n",
      "An et\u00a0al. (2023)": "\nAn et\u00a0al. (2023)\n\nChenxin An, Shansan Gong, Ming Zhong, Xingjian Zhao, Mukai Li, Jun Zhang, Lingpeng Kong, and Xipeng Qiu. 2023.\n\n\nL-eval: Instituting standardized evaluation for long context language models.\n\n\nPreprint, arXiv:2307.11088.\n\n\n",
      "Angelidis et\u00a0al. (2021)": "\nAngelidis et\u00a0al. (2021)\n\nStefanos Angelidis, Reinald\u00a0Kim Amplayo, Yoshihiko Suhara, Xiaolan Wang, and Mirella Lapata. 2021.\n\n\nExtractive opinion summarization in quantized transformer spaces.\n\n\nTransactions of the Association for Computational Linguistics, 9:277\u2013293.\n\n\n",
      "Arora et\u00a0al. (2023)": "\nArora et\u00a0al. (2023)\n\nSimran Arora, Sabri Eyuboglu, Aman Timalsina, Isys Johnson, Michael Poli, James Zou, Atri Rudra, and Christopher R\u00e9. 2023.\n\n\nZoology: Measuring and improving recall in efficient language models.\n\n\narXiv preprint arXiv:2312.04927.\n\n\n",
      "Aumiller and Gertz (2022)": "\nAumiller and Gertz (2022)\n\nDennis Aumiller and Michael Gertz. 2022.\n\n\nKlexikon: A German dataset for joint summarization and simplification.\n\n\nIn Proceedings of the Thirteenth Language Resources and Evaluation Conference, pages 2693\u20132701, Marseille, France. European Language Resources Association.\n\n\n",
      "Bai et\u00a0al. (2023)": "\nBai et\u00a0al. (2023)\n\nYushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li. 2023.\n\n\nLongbench: A bilingual, multitask benchmark for long context understanding.\n\n\nPreprint, arXiv:2308.14508.\n\n\n",
      "Bertsch et\u00a0al. (2024)": "\nBertsch et\u00a0al. (2024)\n\nAmanda Bertsch, Maor Ivgi, Uri Alon, Jonathan Berant, Matthew\u00a0R. Gormley, and Graham Neubig. 2024.\n\n\nIn-context learning with long-context models: An in-depth exploration.\n\n\nPreprint, arXiv:2405.00200.\n\n\n",
      "Bishop et\u00a0al. (2024)": "\nBishop et\u00a0al. (2024)\n\nJennifer\u00a0A Bishop, Qianqian Xie, and Sophia Ananiadou. 2024.\n\n\nLongdocfactscore: Evaluating the factuality of long document abstractive summarisation.\n\n\nPreprint, arXiv:2309.12455.\n\n\n",
      "Boni et\u00a0al. (2021)": "\nBoni et\u00a0al. (2021)\n\nOdellia Boni, Guy Feigenblat, Guy Lev, Michal Shmueli-Scheuer, Benjamin Sznajder, and David Konopnicki. 2021.\n\n\nHowsumm: A multi-document summarization dataset derived from wikihow articles.\n\n\nPreprint, arXiv:2110.03179.\n\n\n",
      "Bruno and Roth (2022)": "\nBruno and Roth (2022)\n\nWilliam Bruno and Dan Roth. 2022.\n\n\nLawngnli: A long-premise benchmark for in-domain generalization from short to long contexts and for implication-based retrieval.\n\n\nPreprint, arXiv:2212.03222.\n\n\n",
      "Caciularu et\u00a0al. (2024)": "\nCaciularu et\u00a0al. (2024)\n\nAvi Caciularu, Alon Jacovi, Eyal Ben-David, Sasha Goldshtein, Tal Schuster, Jonathan Herzig, Gal Elidan, and Amir Globerson. 2024.\n\n\nTact: Advancing complex aggregative reasoning with information extraction tools.\n\n\nPreprint, arXiv:2406.03618.\n\n\n",
      "Chen et\u00a0al. (2022a)": "\nChen et\u00a0al. (2022a)\n\nMingda Chen, Zewei Chu, Sam Wiseman, and Kevin Gimpel. 2022a.\n\n\nSummScreen: A dataset for abstractive screenplay summarization.\n\n\nIn Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 8602\u20138615, Dublin, Ireland. Association for Computational Linguistics.\n\n\n",
      "Chen et\u00a0al. (2022b)": "\nChen et\u00a0al. (2022b)\n\nMingda Chen, Zewei Chu, Sam Wiseman, and Kevin Gimpel. 2022b.\n\n\nSummscreen: A dataset for abstractive screenplay summarization.\n\n\nPreprint, arXiv:2104.07091.\n\n\n",
      "Chen et\u00a0al. (2023)": "\nChen et\u00a0al. (2023)\n\nYukang Chen, Shengju Qian, Haotian Tang, Xin Lai, Zhijian Liu, Song Han, and Jiaya Jia. 2023.\n\n\nLonglora: Efficient fine-tuning of long-context large language models.\n\n\nArXiv, abs/2309.12307.\n\n\n",
      "Cohan et\u00a0al. (2018)": "\nCohan et\u00a0al. (2018)\n\nArman Cohan, Franck Dernoncourt, Doo\u00a0Soon Kim, Trung Bui, Seokhwan Kim, Walter Chang, and Nazli Goharian. 2018.\n\n\nA discourse-aware attention model for abstractive summarization of long documents.\n\n\nPreprint, arXiv:1804.05685.\n\n\n",
      "Dasigi et\u00a0al. (2021)": "\nDasigi et\u00a0al. (2021)\n\nPradeep Dasigi, Kyle Lo, Iz\u00a0Beltagy, Arman Cohan, Noah\u00a0A. Smith, and Matt Gardner. 2021.\n\n\nA dataset of information-seeking questions and answers anchored in research papers.\n\n\nIn Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 4599\u20134610, Online. Association for Computational Linguistics.\n\n\n",
      "Devlin et\u00a0al. (2019)": "\nDevlin et\u00a0al. (2019)\n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019.\n\n\nBERT: Pre-training of deep bidirectional transformers for language understanding.\n\n\nIn Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171\u20134186, Minneapolis, Minnesota. Association for Computational Linguistics.\n\n\n",
      "Dong et\u00a0al. (2024)": "\nDong et\u00a0al. (2024)\n\nZican Dong, Tianyi Tang, Junyi Li, Wayne\u00a0Xin Zhao, and Ji-Rong Wen. 2024.\n\n\nBAMBOO: A comprehensive benchmark for evaluating long text modeling capacities of large language models.\n\n\nIn Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024), pages 2086\u20132099, Torino, Italia. ELRA and ICCL.\n\n\n",
      "Dunietz et\u00a0al. (2020)": "\nDunietz et\u00a0al. (2020)\n\nJesse Dunietz, Greg Burnham, Akash Bharadwaj, Owen Rambow, Jennifer Chu-Carroll, and Dave Ferrucci. 2020.\n\n\nTo test machine comprehension, start by defining comprehension.\n\n\nIn Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7839\u20137859, Online. Association for Computational Linguistics.\n\n\n",
      "Dunn et\u00a0al. (2017)": "\nDunn et\u00a0al. (2017)\n\nMatthew Dunn, Levent Sagun, Mike Higgins, V\u00a0Ugur Guney, Volkan Cirik, and Kyunghyun Cho. 2017.\n\n\nSearchqa: A new q&a dataset augmented with context from a search engine.\n\n\narXiv preprint arXiv:1704.05179.\n\n\n",
      "Fabbri et\u00a0al. (2019)": "\nFabbri et\u00a0al. (2019)\n\nAlexander\u00a0R. Fabbri, Irene Li, Tianwei She, Suyi Li, and Dragomir\u00a0R. Radev. 2019.\n\n\nMulti-news: a large-scale multi-document summarization dataset and abstractive hierarchical model.\n\n\nPreprint, arXiv:1906.01749.\n\n\n",
      "Feng et\u00a0al. (2021)": "\nFeng et\u00a0al. (2021)\n\nSong Feng, Siva\u00a0Sankalp Patel, Hui Wan, and Sachindra Joshi. 2021.\n\n\nMultiDoc2Dial: Modeling dialogues grounded in multiple documents.\n\n\nIn Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6162\u20136176, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.\n\n\n",
      "Gao et\u00a0al. (2024)": "\nGao et\u00a0al. (2024)\n\nFan Gao, Hang Jiang, Rui Yang, Qingcheng Zeng, Jinghui Lu, Moritz Blum, Dairui Liu, Tianwei She, Yuang Jiang, and Irene Li. 2024.\n\n\nLarge language models on wikipedia-style survey generation: an evaluation in nlp concepts.\n\n\nPreprint, arXiv:2308.10410.\n\n\n",
      "Gao et\u00a0al. (2023)": "\nGao et\u00a0al. (2023)\n\nLuyu Gao, Zhuyun Dai, Panupong Pasupat, Anthony Chen, Arun\u00a0Tejasvi Chaganty, Yicheng Fan, Vincent\u00a0Y. Zhao, Ni\u00a0Lao, Hongrae Lee, Da-Cheng Juan, and Kelvin Guu. 2023.\n\n\nRarr: Researching and revising what language models say, using language models.\n\n\nPreprint, arXiv:2210.08726.\n\n\n",
      "Gemini Team Google (2024)": "\nGemini Team Google (2024)\n\nGemini Team Google. 2024.\n\n\nGemini 1.5: Unlocking multimodal understanding across millions of tokens of context.\n\n\nPreprint, arXiv:2403.05530.\n\n\n",
      "GLM Team (2024)": "\nGLM Team (2024)\n\nGLM Team. 2024.\n\n\nGLM-4-9b-chat technical report.\n\n\n",
      "Guo et\u00a0al. (2023)": "\nGuo et\u00a0al. (2023)\n\nDaya Guo, Canwen Xu, Nan Duan, Jian Yin, and Julian McAuley. 2023.\n\n\nLongcoder: A long-range pre-trained language model for code completion.\n\n\nPreprint, arXiv:2306.14893.\n\n\n",
      "He et\u00a0al. (2023)": "\nHe et\u00a0al. (2023)\n\nJunqing He, Kunhao Pan, Xiaoqun Dong, Zhuoyang Song, Yibo Liu, Yuxin Liang, Hao Wang, Qianguo Sun, Songxin Zhang, Zejian Xie, and Jiaxing Zhang. 2023.\n\n\nNever lost in the middle: Improving large language models via attention strengthening question answering.\n\n\nPreprint, arXiv:2311.09198.\n\n\n",
      "Hendrycks et\u00a0al. (2021)": "\nHendrycks et\u00a0al. (2021)\n\nDan Hendrycks, Collin Burns, Anya Chen, and Spencer Ball. 2021.\n\n\nCuad: An expert-annotated nlp dataset for legal contract review.\n\n\nPreprint, arXiv:2103.06268.\n\n\n",
      "Ho et\u00a0al. (2020)": "\nHo et\u00a0al. (2020)\n\nXanh Ho, Anh-Khoa Duong\u00a0Nguyen, Saku Sugawara, and Akiko Aizawa. 2020.\n\n\nConstructing a multi-hop QA dataset for comprehensive evaluation of reasoning steps.\n\n\nIn Proceedings of the 28th International Conference on Computational Linguistics, pages 6609\u20136625, Barcelona, Spain (Online). International Committee on Computational Linguistics.\n\n\n",
      "Hsieh et\u00a0al. (2024)": "\nHsieh et\u00a0al. (2024)\n\nCheng-Ping Hsieh, Simeng Sun, Samuel Kriman, Shantanu Acharya, Dima Rekesh, Fei Jia, Yang Zhang, and Boris Ginsburg. 2024.\n\n\nRuler: What\u2019s the real context size of your long-context language models?\n\nPreprint, arXiv:2404.06654.\n\n\n",
      "Hu et\u00a0al. (2023)": "\nHu et\u00a0al. (2023)\n\nYebowen Hu, Timothy Ganter, Hanieh Deilamsalehy, Franck Dernoncourt, Hassan Foroosh, and Fei Liu. 2023.\n\n\nMeetingBank: A benchmark dataset for meeting summarization.\n\n\nIn Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 16409\u201316423, Toronto, Canada. Association for Computational Linguistics.\n\n\n",
      "Huang et\u00a0al. (2021a)": "\nHuang et\u00a0al. (2021a)\n\nLuyang Huang, Shuyang Cao, Nikolaus Parulian, Heng Ji, and Lu\u00a0Wang. 2021a.\n\n\nEfficient attentions for long document summarization.\n\n\nIn Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1419\u20131436, Online. Association for Computational Linguistics.\n\n\n",
      "Huang et\u00a0al. (2021b)": "\nHuang et\u00a0al. (2021b)\n\nLuyang Huang, Shuyang Cao, Nikolaus Parulian, Heng Ji, and Lu\u00a0Wang. 2021b.\n\n\nEfficient attentions for long document summarization.\n\n\nPreprint, arXiv:2104.02112.\n\n\n",
      "Ivgi et\u00a0al. (2023)": "\nIvgi et\u00a0al. (2023)\n\nMaor Ivgi, Uri Shaham, and Jonathan Berant. 2023.\n\n\nEfficient long-text understanding with short-text models.\n\n\nTransactions of the Association for Computational Linguistics, 11:284\u2013299.\n\n\n",
      "Jiang et\u00a0al. (2024)": "\nJiang et\u00a0al. (2024)\n\nAlbert\u00a0Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra\u00a0Singh Chaplot, Diego de\u00a0las Casas, Emma\u00a0Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, L\u00e9lio\u00a0Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven\u00a0Le Scao, Th\u00e9ophile Gervet, Thibaut Lavril, Thomas Wang, Timoth\u00e9e Lacroix, and William\u00a0El Sayed. 2024.\n\n\nMixtral of experts.\n\n\nPreprint, arXiv:2401.04088.\n\n\n",
      "Jiang et\u00a0al. (2023)": "\nJiang et\u00a0al. (2023)\n\nJinhao Jiang, Kun Zhou, Zican Dong, Keming Ye, Xin Zhao, and Ji-Rong Wen. 2023.\n\n\nStructGPT: A general framework for large language model to reason over structured data.\n\n\nIn Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 9237\u20139251, Singapore. Association for Computational Linguistics.\n\n\n",
      "Kamradt (2023)": "\nKamradt (2023)\n\nGregory Kamradt. 2023.\n\n\nNeedle in a haystack - pressure testing LLMs.\n\n\nGitHub.\n\n\n",
      "Koreeda and Manning (2021a)": "\nKoreeda and Manning (2021a)\n\nYuta Koreeda and Christopher Manning. 2021a.\n\n\nContractNLI: A dataset for document-level natural language inference for contracts.\n\n\nIn Findings of the Association for Computational Linguistics: EMNLP 2021, pages 1907\u20131919, Punta Cana, Dominican Republic. Association for Computational Linguistics.\n\n\n",
      "Koreeda and Manning (2021b)": "\nKoreeda and Manning (2021b)\n\nYuta Koreeda and Christopher\u00a0D. Manning. 2021b.\n\n\nContractnli: A dataset for document-level natural language inference for contracts.\n\n\nPreprint, arXiv:2110.01799.\n\n\n",
      "Kornilova and Eidelman (2019)": "\nKornilova and Eidelman (2019)\n\nAnastassia Kornilova and Vladimir Eidelman. 2019.\n\n\nBillSum: A corpus for automatic summarization of US legislation.\n\n\nIn Proceedings of the 2nd Workshop on New Frontiers in Summarization, pages 48\u201356, Hong Kong, China. Association for Computational Linguistics.\n\n\n",
      "Ko\u010disk\u00fd et\u00a0al. (2017)": "\nKo\u010disk\u00fd et\u00a0al. (2017)\n\nTom\u00e1\u0161 Ko\u010disk\u00fd, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl\u00a0Moritz Hermann, G\u00e1bor Melis, and Edward Grefenstette. 2017.\n\n\nThe narrativeqa reading comprehension challenge.\n\n\nPreprint, arXiv:1712.07040.\n\n\n",
      "Ko\u010disk\u00fd et\u00a0al. (2018)": "\nKo\u010disk\u00fd et\u00a0al. (2018)\n\nTom\u00e1\u0161 Ko\u010disk\u00fd, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl\u00a0Moritz Hermann, G\u00e1bor Melis, and Edward Grefenstette. 2018.\n\n\nThe NarrativeQA Reading Comprehension Challenge.\n\n\nTransactions of the Association for Computational Linguistics, 6:317\u2013328.\n\n\n",
      "Kryscinski et\u00a0al. (2022)": "\nKryscinski et\u00a0al. (2022)\n\nWojciech Kryscinski, Nazneen Rajani, Divyansh Agarwal, Caiming Xiong, and Dragomir Radev. 2022.\n\n\nBOOKSUM: A collection of datasets for long-form narrative summarization.\n\n\nIn Findings of the Association for Computational Linguistics: EMNLP 2022, pages 6536\u20136558, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.\n\n\n",
      "Kry\u015bci\u0144ski et\u00a0al. (2022)": "\nKry\u015bci\u0144ski et\u00a0al. (2022)\n\nWojciech Kry\u015bci\u0144ski, Nazneen Rajani, Divyansh Agarwal, Caiming Xiong, and Dragomir Radev. 2022.\n\n\nBooksum: A collection of datasets for long-form narrative summarization.\n\n\nPreprint, arXiv:2105.08209.\n\n\n",
      "Kulkarni et\u00a0al. (2020)": "\nKulkarni et\u00a0al. (2020)\n\nSayali Kulkarni, Sheide Chammas, Wan Zhu, Fei Sha, and Eugene Ie. 2020.\n\n\nAquamuse: Automatically generating datasets for query-based multi-document summarization.\n\n\nPreprint, arXiv:2010.12694.\n\n\n",
      "Kuratov et\u00a0al. (2024)": "\nKuratov et\u00a0al. (2024)\n\nYuri Kuratov, Aydar Bulatov, Petr Anokhin, Dmitry Sorokin, Artyom Sorokin, and Mikhail Burtsev. 2024.\n\n\nIn search of needles in a 11m haystack: Recurrent memory finds what llms miss.\n\n\nPreprint, arXiv:2402.10790.\n\n\n",
      "Kwiatkowski et\u00a0al. (2019)": "\nKwiatkowski et\u00a0al. (2019)\n\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew\u00a0M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. 2019.\n\n\nNatural questions: A benchmark for question answering research.\n\n\nTransactions of the Association for Computational Linguistics, 7:452\u2013466.\n\n\n",
      "Levy et\u00a0al. (2024)": "\nLevy et\u00a0al. (2024)\n\nMosh Levy, Alon Jacoby, and Yoav Goldberg. 2024.\n\n\nSame task, more tokens: the impact of input length on the reasoning performance of large language models.\n\n\nPreprint, arXiv:2402.14848.\n\n\n",
      "Li et\u00a0al. (2023a)": "\nLi et\u00a0al. (2023a)\n\nDacheng Li, Rulin Shao, Anze Xie, Ying Sheng, Lianmin Zheng, Joseph\u00a0E. Gonzalez, Ion Stoica, Xuezhe Ma, and Hao Zhang. 2023a.\n\n\nHow long can open-source LLMs truly promise on context length?\n\n",
      "Li et\u00a0al. (2023b)": "\nLi et\u00a0al. (2023b)\n\nJiaqi Li, Mengmeng Wang, Zilong Zheng, and Muhan Zhang. 2023b.\n\n\nLoogle: Can long-context language models understand long contexts?\n\nPreprint, arXiv:2311.04939.\n\n\n",
      "Liu et\u00a0al. (2024a)": "\nLiu et\u00a0al. (2024a)\n\nHao Liu, Wilson Yan, Matei Zaharia, and Pieter Abbeel. 2024a.\n\n\nWorld model on million-length video and language with ringattention.\n\n\narXiv preprint arXiv:2402.08268.\n\n\n",
      "Liu et\u00a0al. (2024b)": "\nLiu et\u00a0al. (2024b)\n\nNelson\u00a0F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. 2024b.\n\n\nLost in the middle: How language models use long contexts.\n\n\nTransactions of the Association for Computational Linguistics, 12:157\u2013173.\n\n\n",
      "Liu et\u00a0al. (2023a)": "\nLiu et\u00a0al. (2023a)\n\nShuaiqi Liu, Jiannong Cao, Ruosong Yang, and Zhiyuan Wen. 2023a.\n\n\nLong text and multi-table summarization: Dataset and method.\n\n\nPreprint, arXiv:2302.03815.\n\n\n",
      "Liu et\u00a0al. (2023b)": "\nLiu et\u00a0al. (2023b)\n\nTianyang Liu, Canwen Xu, and Julian McAuley. 2023b.\n\n\nRepobench: Benchmarking repository-level code auto-completion systems.\n\n\nPreprint, arXiv:2306.03091.\n\n\n",
      "Malaviya et\u00a0al. (2024)": "\nMalaviya et\u00a0al. (2024)\n\nChaitanya Malaviya, Subin Lee, Sihao Chen, Elizabeth Sieber, Mark Yatskar, and Dan Roth. 2024.\n\n\nExpertQA: Expert-curated questions and attributed answers.\n\n\nIn Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 3025\u20133045, Mexico City, Mexico. Association for Computational Linguistics.\n\n\n",
      "Mohtashami and Jaggi (2023)": "\nMohtashami and Jaggi (2023)\n\nAmirkeivan Mohtashami and Martin Jaggi. 2023.\n\n\nLandmark attention: Random-access infinite context length for transformers.\n\n\nIn Workshop on Efficient Systems for Foundation Models@ ICML2023.\n\n\n",
      "Narayan et\u00a0al. (2018)": "\nNarayan et\u00a0al. (2018)\n\nShashi Narayan, Shay\u00a0B. Cohen, and Mirella Lapata. 2018.\n\n\nDon\u2019t give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization.\n\n\nIn Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1797\u20131807, Brussels, Belgium. Association for Computational Linguistics.\n\n\n",
      "Nguyen et\u00a0al. (2024)": "\nNguyen et\u00a0al. (2024)\n\nChau Nguyen, Phuong Nguyen, Thanh Tran, Dat Nguyen, An\u00a0Trieu, Tin Pham, Anh Dang, and Le-Minh Nguyen. 2024.\n\n\nCaptain at coliee 2023: Efficient methods for legal information retrieval and entailment tasks.\n\n\nPreprint, arXiv:2401.03551.\n\n\n",
      "OpenAI (2024)": "\nOpenAI (2024)\n\nOpenAI. 2024.\n\n\nGPT-4 technical report.\n\n\nPreprint, arXiv:2303.08774.\n\n\n",
      "Pal et\u00a0al. (2023)": "\nPal et\u00a0al. (2023)\n\nArka Pal, Deep Karkhanis, Manley Roberts, Samuel Dooley, Arvind Sundararajan, and Siddartha Naidu. 2023.\n\n\nGiraffe: Adventures in expanding context lengths in llms.\n\n\nPreprint, arXiv:2308.10882.\n\n\n",
      "Pang et\u00a0al. (2022)": "\nPang et\u00a0al. (2022)\n\nRichard\u00a0Yuanzhe Pang, Alicia Parrish, Nitish Joshi, Nikita Nangia, Jason Phang, Angelica Chen, Vishakh Padmakumar, Johnny Ma, Jana Thompson, He\u00a0He, and Samuel Bowman. 2022.\n\n\nQuALITY: Question answering with long input texts, yes!\n\nIn Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 5336\u20135358, Seattle, United States. Association for Computational Linguistics.\n\n\n",
      "Prasad et\u00a0al. (2023)": "\nPrasad et\u00a0al. (2023)\n\nArchiki Prasad, Trung Bui, Seunghyun Yoon, Hanieh Deilamsalehy, Franck Dernoncourt, and Mohit Bansal. 2023.\n\n\nMeetingQA: Extractive question-answering on meeting transcripts.\n\n\nIn Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 15000\u201315025, Toronto, Canada. Association for Computational Linguistics.\n\n\n",
      "Qian et\u00a0al. (2023)": "\nQian et\u00a0al. (2023)\n\nHongjing Qian, Yutao Zhu, Zhicheng Dou, Haoqi Gu, Xinyu Zhang, Zheng Liu, Ruofei Lai, Zhao Cao, Jian-Yun Nie, and Ji-Rong Wen. 2023.\n\n\nWebbrain: Learning to generate factually correct articles for queries by grounding on large web corpus.\n\n\nPreprint, arXiv:2304.04358.\n\n\n",
      "Rae et\u00a0al. (2019)": "\nRae et\u00a0al. (2019)\n\nJack\u00a0W. Rae, Anna Potapenko, Siddhant\u00a0M. Jayakumar, and Timothy\u00a0P. Lillicrap. 2019.\n\n\nCompressive transformers for long-range sequence modelling.\n\n\nPreprint, arXiv:1911.05507.\n\n\n",
      "Raffel et\u00a0al. (2020)": "\nRaffel et\u00a0al. (2020)\n\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter\u00a0J Liu. 2020.\n\n\nExploring the limits of transfer learning with a unified text-to-text transformer.\n\n\nJournal of machine learning research, 21(140):1\u201367.\n\n\n",
      "Reddy et\u00a0al. (2024)": "\nReddy et\u00a0al. (2024)\n\nVarshini Reddy, Rik Koncel-Kedziorski, Viet\u00a0Dac Lai, Michael Krumdick, Charles Lovering, and Chris Tanner. 2024.\n\n\nDocfinqa: A long-context financial reasoning dataset.\n\n\nPreprint, arXiv:2401.06915.\n\n\n",
      "Saunders et\u00a0al. (2022)": "\nSaunders et\u00a0al. (2022)\n\nWilliam Saunders, Catherine Yeh, Jeff Wu, Steven Bills, Long Ouyang, Jonathan Ward, and Jan Leike. 2022.\n\n\nSelf-critiquing models for assisting human evaluators.\n\n\nPreprint, arXiv:2206.05802.\n\n\n",
      "Shaham et\u00a0al. (2023)": "\nShaham et\u00a0al. (2023)\n\nUri Shaham, Maor Ivgi, Avia Efrat, Jonathan Berant, and Omer Levy. 2023.\n\n\nZeroSCROLLS: A zero-shot benchmark for long text understanding.\n\n\nIn Findings of the Association for Computational Linguistics: EMNLP 2023, pages 7977\u20137989, Singapore. Association for Computational Linguistics.\n\n\n",
      "Shaham et\u00a0al. (2022)": "\nShaham et\u00a0al. (2022)\n\nUri Shaham, Elad Segal, Maor Ivgi, Avia Efrat, Ori Yoran, Adi Haviv, Ankit Gupta, Wenhan Xiong, Mor Geva, Jonathan Berant, and Omer Levy. 2022.\n\n\nSCROLLS: Standardized CompaRison over long language sequences.\n\n\nIn Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 12007\u201312021, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.\n\n\n",
      "Sharma et\u00a0al. (2019)": "\nSharma et\u00a0al. (2019)\n\nEva Sharma, Chen Li, and Lu\u00a0Wang. 2019.\n\n\nBIGPATENT: A large-scale dataset for abstractive and coherent summarization.\n\n\nIn Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2204\u20132213, Florence, Italy. Association for Computational Linguistics.\n\n\n",
      "Stylianou et\u00a0al. (2021)": "\nStylianou et\u00a0al. (2021)\n\nNikolaos Stylianou, Panagiotis Kosmoliaptsis, and Ioannis Vlahavas. 2021.\n\n\nImproved biomedical entity recognition via longer context modeling.\n\n\nIn Artificial Intelligence Applications and Innovations: 17th IFIP WG 12.5 International Conference, AIAI 2021, Hersonissos, Crete, Greece, June 25\u201327, 2021, Proceedings 17, pages 45\u201356. Springer.\n\n\n",
      "Su et\u00a0al. (2024)": "\nSu et\u00a0al. (2024)\n\nJianlin Su, Murtadha Ahmed, Yu\u00a0Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. 2024.\n\n\nRoformer: Enhanced transformer with rotary position embedding.\n\n\nNeurocomputing, 568:127063.\n\n\n",
      "Takeshita et\u00a0al. (2024)": "\nTakeshita et\u00a0al. (2024)\n\nSotaro Takeshita, Tommaso Green, Ines Reinig, Kai Eckert, and Simone Ponzetto. 2024.\n\n\nACLSum: A new dataset for aspect-based summarization of scientific publications.\n\n\nIn Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 6660\u20136675, Mexico City, Mexico. Association for Computational Linguistics.\n\n\n",
      "Tay et\u00a0al. (2020)": "\nTay et\u00a0al. (2020)\n\nYi\u00a0Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler. 2020.\n\n\nLong range arena: A benchmark for efficient transformers.\n\n\nPreprint, arXiv:2011.04006.\n\n\n",
      "Trivedi et\u00a0al. (2022)": "\nTrivedi et\u00a0al. (2022)\n\nHarsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. 2022.\n\n\nMusique: Multihop questions via single-hop question composition.\n\n\nPreprint, arXiv:2108.00573.\n\n\n",
      "Tseng et\u00a0al. (2016)": "\nTseng et\u00a0al. (2016)\n\nBo-Hsiang Tseng, Sheng-Syun Shen, Hung-Yi Lee, and Lin-Shan Lee. 2016.\n\n\nTowards machine comprehension of spoken content: Initial toefl listening comprehension test by machine.\n\n\n",
      "Vaswani et\u00a0al. (2017)": "\nVaswani et\u00a0al. (2017)\n\nAshish Vaswani, Noam\u00a0M. Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan\u00a0N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017.\n\n\nAttention is all you need.\n\n\nIn Neural Information Processing Systems.\n\n\n",
      "Wang et\u00a0al. (2022)": "\nWang et\u00a0al. (2022)\n\nAlex Wang, Richard\u00a0Yuanzhe Pang, Angelica Chen, Jason Phang, and Samuel\u00a0R. Bowman. 2022.\n\n\nSQuALITY: Building a long-document summarization dataset the hard way.\n\n\nIn Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 1139\u20131156, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.\n\n\n",
      "Williams et\u00a0al. (2018)": "\nWilliams et\u00a0al. (2018)\n\nAdina Williams, Nikita Nangia, and Samuel Bowman. 2018.\n\n\nA broad-coverage challenge corpus for sentence understanding through inference.\n\n\nIn Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 1112\u20131122, New Orleans, Louisiana. Association for Computational Linguistics.\n\n\n",
      "Wu et\u00a0al. (2024)": "\nWu et\u00a0al. (2024)\n\nYunshu Wu, Hayate Iso, Pouya Pezeshkpour, Nikita Bhutani, and Estevam Hruschka. 2024.\n\n\nLess is more for long document summary evaluation by llms.\n\n\nPreprint, arXiv:2309.07382.\n\n\n",
      "Yang et\u00a0al. (2018)": "\nYang et\u00a0al. (2018)\n\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William\u00a0W. Cohen, Ruslan Salakhutdinov, and Christopher\u00a0D. Manning. 2018.\n\n\nHotpotqa: A dataset for diverse, explainable multi-hop question answering.\n\n\nPreprint, arXiv:1809.09600.\n\n\n",
      "Zhang et\u00a0al. (2024a)": "\nZhang et\u00a0al. (2024a)\n\nJiebin Zhang, Eugene\u00a0J. Yu, Qinyu Chen, Chenhao Xiong, Dawei Zhu, Han Qian, Mingbo Song, Xiaoguang Li, Qun Liu, and Sujian Li. 2024a.\n\n\nRetrieval-based full-length wikipedia generation for emergent events.\n\n\nPreprint, arXiv:2402.18264.\n\n\n",
      "Zhang et\u00a0al. (2024b)": "\nZhang et\u00a0al. (2024b)\n\nXinrong Zhang, Yingfa Chen, Shengding Hu, Zihang Xu, Junhao Chen, Moo\u00a0Khai Hao, Xu\u00a0Han, Zhen\u00a0Leng Thai, Shuo Wang, Zhiyuan Liu, and Maosong Sun. 2024b.\n\n\n\u221e\\infty\u221ebench: Extending long context evaluation beyond 100k tokens.\n\n\nPreprint, arXiv:2402.13718.\n\n\n",
      "Zhao et\u00a0al. (2022)": "\nZhao et\u00a0al. (2022)\n\nYilun Zhao, Yunxiang Li, Chenying Li, and Rui Zhang. 2022.\n\n\nMultiHiertt: Numerical reasoning over multi hierarchical tabular and textual data.\n\n\nIn Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 6588\u20136600, Dublin, Ireland. Association for Computational Linguistics.\n\n\n",
      "Zhao et\u00a0al. (2023)": "\nZhao et\u00a0al. (2023)\n\nYilun Zhao, Yitao Long, Hongjun Liu, Linyong Nan, Lyuhao Chen, Ryo Kamoi, Yixin Liu, Xiangru Tang, Rui Zhang, and Arman Cohan. 2023.\n\n\nDocmath-eval: Evaluating numerical reasoning capabilities of llms in understanding long documents with tabular data.\n\n\nArXiv, abs/2311.09805.\n\n\n",
      "Zhong et\u00a0al. (2021)": "\nZhong et\u00a0al. (2021)\n\nMing Zhong, Da\u00a0Yin, Tao Yu, Ahmad Zaidi, Mutethia Mutuma, Rahul Jha, Ahmed\u00a0Hassan Awadallah, Asli Celikyilmaz, Yang Liu, Xipeng Qiu, and Dragomir Radev. 2021.\n\n\nQMSum: A new benchmark for query-based multi-domain meeting summarization.\n\n\nIn Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 5905\u20135921, Online. Association for Computational Linguistics.\n\n\n",
      "Zhou et\u00a0al. (2023)": "\nZhou et\u00a0al. (2023)\n\nYijie Zhou, Kejian Shi, Wencai Zhang, Yixin Liu, Yilun Zhao, and Arman Cohan. 2023.\n\n\nOdsum: New benchmarks for open domain multi-document summarization.\n\n\nPreprint, arXiv:2309.08960.\n\n\n"
    },
    "domain": "cs.CL",
    "citation_count": 0
  },
  "9317895a-63b6-408e-89f2-c8752736ec38": {
    "pk": "9317895a-63b6-408e-89f2-c8752736ec38",
    "authors": [
      "Tianyu Gao",
      "Alexander Wettig",
      "Howard Yen",
      "Danqi Chen"
    ],
    "title": "How to Train Long-Context Language Models (Effectively)",
    "abstract": "We study continued training and supervised fine-tuning (SFT) of a language model (LM) to make effective use of long-context information. We first establish a reliable evaluation protocol to guide model development -- Instead of perplexity or simple needle-in-a-haystack (NIAH) tests, we use a broad set of long-context tasks, and we evaluate models after SFT with instruction data as this better reveals long-context abilities. Supported by our robust evaluations, we run thorough experiments to decide the data mix for continued pre-training, the instruction tuning dataset, and many other design choices. We find that (1) code repositories and books are excellent sources of long data, but it is crucial to combine them with high-quality short data; (2) training with a sequence length beyond the evaluation length boosts long-context performance; (3) for SFT, using only short instruction datasets yields strong performance on long-context tasks. Our final model, ProLong-8B, which is initialized from Llama-3 and trained on 40B tokens, demonstrates state-of-the-art long-context performance among similarly sized models at a length of 128K. ProLong outperforms Llama-3.18B-Instruct on the majority of long-context tasks despite having seen only 5% as many tokens during long-context training. Additionally, ProLong can effectively process up to 512K tokens, one of the longest context windows of publicly available LMs.",
    "url": "http://arxiv.org/abs/2410.02660v1",
    "timestamp": 1727974012,
    "sections": {
      "1 Introduction": "\n\n1 Introduction\n\nThe ability of language models (LMs) to process extremely long inputs (for example, 128K tokens) has enabled new applications, such as book summarization or learning new tasks on the fly from many examples. However, adapting\nLMs to process long contexts is challenging from an infrastructure and data perspective, and many design decisions are not well understood by open-source practitioners.\n\n\nWhile many works have focused on extending the context length of pre-trained LMs with minimal training (Chen et\u00a0al., 2023; Peng et\u00a0al., 2024),\nFu et\u00a0al. (2024) find that\nthe above methods cannot even perform the simple needle-in-a-haystack (NIAH; Kamradt, 2024) task\nand it is necessary to continually train the LM on long documents for billions of tokens. Frontier open-source models, such as Llama-3.1\u00a0(Dubey et\u00a0al., 2024) and Jamba\u00a0(Lenz et\u00a0al., 2024), also employ a long-context continued training stage, followed by supervised fine-tuning (SFT) on instruction data.\nWe adopt the same setting and study continued training and SFT of a pre-trained LM for effective long-context use.\n\n\nWe first establish a reliable evaluation protocol to provide a meaningful signal for model development.\nMost existing works rely on either perplexity or NIAH for ablating training recipes.\nWe demonstrate that neither is robust for guiding the development\nand opt for a broad range of downstream applications, such as retrieval-augmented generation (RAG), long-document summarization, and many-shot in-context learning (ICL).\nImportantly, we also conduct our evaluations after performing SFT,\neven for all our ablation runs on continued pre-training.\nWe observe that, on some long-context tasks, performance gains only emerge after SFT, which means that best design choices can differ before and after SFT.\n\n\nGuided by our evaluation protocol, we run comprehensive experiments with Llama-3-8B\u00a0(8K original context window; Dubey et\u00a0al., 2024) to study each component of long-context continued training, including data mixture, data and length scaling, supervised fine-tuning, and many other design choices such as cross-document attention masking and position extrapolation.\nMany of our findings are surprising or contradictory to existing claims, for example, (1) training only on long data hurts long-context performance,\n(2) training on longer sequences than the evaluation length helps, and (3) SFT on only short instruction data is sufficient for good long-context performance.\nWe outline our main takeaways and the structure of the paper in the takeaway box at the beginning of this section.\n\n\nOur final model, ProLong, achieves the best performance at a 128K context length among 10B-parameter models, while taking only 5%percent55\\%5 % of the data budget compared to Llama-3.1\u2019s long-context training (Dubey et\u00a0al., 2024).\nProLong has a maximum context length of 512K tokens, making it one of the longest-context LMs available.222Throughout the paper, we use binary prefixes K=210absentsuperscript210=2^{10}= 2 start_POSTSUPERSCRIPT 10 end_POSTSUPERSCRIPT, M=220superscript2202^{20}2 start_POSTSUPERSCRIPT 20 end_POSTSUPERSCRIPT, and B=230superscript2302^{30}2 start_POSTSUPERSCRIPT 30 end_POSTSUPERSCRIPT.\n\n",
      "2 Guide Model Development With Meaningful Evaluations": "\n\n2 Guide Model Development With Meaningful Evaluations\n\nA pre-requisite for training a strong LM is having a robust evaluation suite that can guide model development while tracking its utility in real-world applications.\nWhile synthetic benchmarks like needle-in-a-haystack (NIAH; Kamradt, 2024) and RULER (Hsieh et\u00a0al., 2024) have gained much popularity due to their simplicity and controllability, we are interested in a wider range of tasks that reflect practical usage, such as the ability to reason over the whole document.\nIn the following, we describe our evaluation protocols and\nshowcase why they are critical to our model development.\n\n\n\n2.1 Evaluate on diverse and realistic tasks\n\nWe first make the decision to use HELMET (Yen et\u00a0al., 2024b) as our evaluation suite,\nas it is one of the most comprehensive long-context benchmarks, covering the following tasks:\n\n\n\u2022\n\nRecall: Given a JSON file with random key-values pairs, retrieve the value for a key.\n\n\n\n\u2022\n\nRAG: Answer a question given retrieved Wikipedia documents (NQ, HotPotQA, PopQA).\n\n\n\n\u2022\n\nRe-ranking: Produce top-10 rankings from a shuffled list of documents (MSMARCO).\n\n\n\n\u2022\n\nICL: Learn classification tasks from many in-context examples, where the #classes ranges from 6 to 151; average of 5 datasets (TREC coarse/fine, NLU, Banking77, Clinc-150).\n\n\n\n\u2022\n\nQA: Answer a question given a full-length book (NarrativeQA).\n\n\n\n\u2022\n\nSummarization: Summarize long legal documents (Multi-LexSum).\n\n\n\n\n\nOverall, these diverse tasks reflect a range of long-context abilities including recall, reasoning, learning from context, and robustness to noisy inputs.\nYen et\u00a0al. (2024b) also show that HELMET produces model performance trends that are more consistent with human perceptions unlike other long-context benchmarks.\n\n\nWe showcase the importance of a robust evaluation suite in Table\u00a01.\nAs a predecessor of our work, Fu et\u00a0al. (2024) only consider needle-in-a-haystack (NIAH) and perplexity during model development; evaluations on 3 tasks from HELMET reveal major short-comings of their models despite perfect NIAH scores.\nWe also see how NIAH and even the HELMET recall task become saturated for strong models (Llama-3.1-8B vs. 70B) while other task categories continue to detect differences in their long-context abilities.\n\n\n\u2005\n\n\n\nTable 1: HELMET offers a more holistic long-context evaluation. We reproduce Fu et\u00a0al. (2024) on Llama-3-8B with SFT. We report the instruct Llama versions.\n\n\n\n\n\n\n\n\nHELMET\n\n\nModels\nNIAH\nRecall\nRAG\nRe-rank\n\n\nFu et\u00a0al. (2024)\n100\n95.8\n52.1\n23.1\n\n\nLlama-3.1-8B\n100\n99.4\n56.3\n37.0\n\n\nLlama-3.1-70B\n100\n100\n62.1\n49.2\n\n\n\n\n\n\nWe offer more details about the HELMET evaluation, including its careful choice of metrics, in \u00a7A.1.\nWe did not use all tasks from HELMET for efficiency reasons and we also do not want to overfit to HELMET.\nIf not otherwise specified, we average the performance for each category over all datasets and over evaluation lengths of 32K and 64K;\nfor the final long-context score, we macro-average all categories.\n\n\n\nWhy not perplexity?\n\nBesides synthetic recall tasks, many previous works rely on perplexity (PPL) for evaluating long-context extensions of LMs (Chen et\u00a0al., 2023; Fu et\u00a0al., 2024; Lu et\u00a0al., 2024), which is commonly measured on the PG19 books dataset\u00a0(Rae et\u00a0al., 2020).\nWe use the ablation experiment from \u00a73.2 to showcase why perplexity is not an indicative metric for developing long-context models.\nThe experiment studies how the ratio of long documents affects the performance. We report both our evaluation and the perplexity measured on the last 32K tokens of 64K-length documents from PG19.\nAs shown in Figure\u00a01, while using more long data continues to improve PPL,\nit is clear that using 100% long data significantly hurts downstream long-context performance.\n\n\n\n\n\u2005\n\n\n\n\n\n\nFigure 1: Making design decisions based on perplexity (PPL) is not optimal for long-context downstream tasks.\n\n\n\n\n\n\n2.2 Evaluate after supervised fine-tuning\n\nSupervised fine-tuning (SFT; Ouyang et\u00a0al., 2022) is an additional training stage that fine-tunes the model on a small amount of natural-language instructions and corresponding responses; it enables a base LM to address user queries in a chat format and has become a standard step for producing frontier LMs.\nHere, we consider the difference between evaluating a model before or after SFT.\n\n\nIn preliminary experiments, we continue training Llama-3-8B-Base on 5B-token subsets from the data mix by Fu et\u00a0al. (2024). The mix is based on SlimPajama (Soboleva et\u00a0al., 2023) and upsamples long documents to constitute roughly 70% of tokens, while retaining the original domain proportions.\nThen we conduct SFT on several intermediate checkpoints with UltraChat\u00a0(Ding et\u00a0al., 2023).\n\n\nWe show the benchmarking results before and after SFT in Figure\u00a02.\nLong-context evaluation shows clearer signals when it is conducted after SFT:\n(1) SFT shows that the model continues to improve with more training tokens on RAG and re-ranking, while the improvement is less clear or does not exist when evaluated before SFT.\n(2) SFT enables evaluation on realistic applications like QA and summarization, which require instruction following and have low performance before SFT.\nWe also note that the variance from two random training runs is not substantially higher after the additional SFT phase. Therefore, unless otherwise specified, we report the long-context performance after SFT.\n\n\nFigure 2: \nImprovements on RAG and re-ranking tasks are only observed when evaluating models after a supervised fine-tuning (SFT) phase on instruction data. The models are trained on the pre-training data mix by Fu et\u00a0al. (2024). We report the mean and standard deviations over two training runs. \n\n\nWe dive deeper into supervised fine-tuning in \u00a75 and explore different training datasets, as well as the use of synthetic long instruction data. However, we find that simply fine-tuning on UltraChat remains a surprisingly competitive choice.\n\n\n\n\n2.3 Check that short-context performance is preserved\n\nLong-context abilities should not come at the expense of short-context performance, particularly since short-context evaluations cover a wider range of capabilities, e.g., world knowledge, commonsense, and mathematical reasoning.\nHowever, short-context evaluation has largely been neglected by previous long-context research.\nWe report on 5 tasks from the the Open LLM Leaderboard (Beeching et\u00a0al., 2023): HellaSwag (Zellers et\u00a0al., 2019), MMLU (Hendrycks et\u00a0al., 2021), ARC-challenge (Clark et\u00a0al., 2018), WinoGrande (Sakaguchi et\u00a0al., 2021), and GSM8K (Cobbe et\u00a0al., 2021).\nWe evaluate short-context performance before SFT, since this allows for a direct comparison to the base model which was used as initialization for the long-context training.\n\n\n\nPrevious techniques deteriorate short-context performance.\n\nWe show in Table\u00a02 that both training-free position extrapolation, as well as fine-tuning with an existing long data mixture (Fu et\u00a0al., 2024)\ndo not preserve the strong performance of Llama-3-8B on standard short-context tasks.\nThis motivates us to find data sources which retain the initial model\u2019s strong short-context performance.\n\n\n\n\n\u2005\n\n\n\nTable 2: \nApplying position extrapolation (PE) to\nLlama-3-8B by changing the RoPE frequency base (\u00a7B.1)\nor fine-tuning it\non a long-context SlimPajama mixture (Fu et\u00a0al., 2024; Soboleva et\u00a0al., 2023) deteriorates the performance of this top-shelf pre-trained LM on short-context tasks.\n\n\n\n\n\n\n\nHSwag\nMMLU\nARC-c\nWG\nGSM8K\n\n\n\n\nLlama-3-8B\n82.1\n66.5\n59.4\n77.1\n44.7\n\n\n+ PE\n81.5\n64.7\n58.1\n75.5\n40.1\n\n\n+ SlimPajama\n81.0\n63.1\n57.8\n75.1\n40.6\n\n\n\n\n\n\n",
      "3 Long-Context Data Curation": "\n\n3 Long-Context Data Curation\n\nThe quality and composition of training data has been found to be the most important factor for LM pre-training (Penedo et\u00a0al., 2023; Wettig et\u00a0al., 2024; Li et\u00a0al., 2024a) and is therefore\na primary focus of our study. To make data decisions, we perform ablation experiments:\nwe continue to train Llama-3-8B-Base for 5B tokens with a maximum length of 64K tokens and evaluate according to \u00a72. See \u00a7A.4 for more details of our ablation setting.\n\n\nWe aim to boost the long-context task performance while preserving the short-context performance of the original model. Starting from the intuition that the data should be a mixture of long and short documents, we study these choices separately.\nIn our ablations, the long data is comprised of single-document chunks of 64K tokens, whereas for the short data, we construct batches by packing documents until we reach 64K tokens per sequence.\n\n\n\n3.1 Code repositories and books are good sources of long-context data\n\n\nSlimPajama.\n\nWe analyze the quantity of long data in SlimPajama (SP; Soboleva et\u00a0al., 2023). Table\u00a03 shows that books account for the majority of long-context tokens.\nWhen inspecting the long data in CommonCrawl (CC), we observe that though varied in quality, it also contains some book-like content, which future work could identify via data selection methods.\n\n\n\nCode repositories.\n\nWhile only few files from GitHub reach a very long length (which also tend to be lower quality as suggested by Singh et\u00a0al., 2024), we construct an abundant source of long-context data from the Stack (Kocetkov et\u00a0al., 2023) by concatenating all files from a repository to form a single document. Unlike Guo et\u00a0al. (2024), we do not order the files based on dependencies, which should increase the distance between dependent files and reduce recency bias.\n\n\n\n\n\u2005\n\n\n\nTable 3: Long text documents (\u2265\\geq\u226564K tokens) by data sources.\n\n\n\n\n\nData\n#Long\n\n\ntokens\n\n\nCode Repos\n98.8B\n\n\nSP/Books\n33.2B\n\n\nSP/CC\n15.3B\n\n\nSP/Arxiv\n5.2B\n\n\nSP/GitHub\n2.8B\n\n\nSP/Wiki\n0.1B\n\n\nSP/StackEx\n\n<<<0.1B\n\n\nSP/C4\n\n<<<0.1B\n\n\n\n\n\n\nData mixture.\n\nWe train models with 60% of long-context data and 40% of our ShortMix (\u00a73.3).\nTable\u00a04 shows that using code repositories alone performs the best on stress-test recall tasks. Meanwhile, books are more broadly beneficial for in-context learning, summarization and re-ranking.\nAn equal mix of books and code repositories achieves the best overall performance.\nNote that short-context task performance remains consistent due to our high-quality short data mix.\n\n\nTable 4: Impact of different long data sources, while keeping the 40% short data component fixed. Code repositories particularly helps recall, while books are more effective on re-ranking, ICL, and summarization. Mixing the two achieves the overall best performance.\n\n\n\nLong Data (60%)\nLong-Context\nShort-Context\n\n\nRecall\nRAG\nRe-rank\nICL\nQA\nSumm.\nAvg.\nAvg.\n\n\n\n\nCommonCrawl\n84.1\n53.3\n28.1\n67.5\n35.2\n37.0\n50.9\n66.5\n\n\nBooks\n94.9\n53.9\n30.7\n72.2\n33.2\n37.7\n53.8\n65.5\n\n\nCode Repos\n99.2\n53.8\n29.0\n61.2\n34.7\n36.2\n52.3\n65.9\n\n\nBooks/Repos 1:1\n96.0\n54.9\n29.4\n73.9\n35.7\n37.9\n54.6\n65.5\n\n\n\n\n\n\n\n\n3.2 Training only on long data hurts long-context performance\n\nThe ratio between short/long data is another crucial factor for downstream performance.\nPrior work either trains only on long data (Peng et\u00a0al., 2024) or adds some short training data (Yen et\u00a0al., 2024a; Fu et\u00a0al., 2024).\nHowever, we are the first to systematically study the impact of short/long ratio.\n\n\nFigure\u00a03 shows that\nshort task performance monotonically decreases as the long data increases.\nThe trends for long-context vary by tasks and are further complicated by SFT:\nOn tasks like recall and RAG, the performance before SFT prefers high proportions of long data, while the performance after SFT drastically deteriorates with more long data.\nWe hypothesize that specializing the model only on long data makes it a poor initialization for generic SFT\u2014highlighting the importance of evaluating checkpoints after SFT (\u00a72.2).\nWhile some long-context tasks benefit from more long data consistently (ICL) or show no clear pattern (re-ranking), the best average performance is achieved at 60% long data and 40% short data, which we adopt for our final ProLong model.\n\n\nFigure 3: \nImpact of short/long data ratio.\nAll models are trained on books/repos long data and our ShortMix for 5B tokens.\nMore long data initially improves long-context performance, but then\nbecomes impairing. More long data also consistently degrades the short-context performance.\n\n\n\n\n\n3.3 Choosing a high-quality short-context mix is important\n\nWe saw in \u00a72.3 that it is difficult to preserve the strong performance of Llama-3-8B on short-context tasks during long-context fine-tuning.\nWe adopt our best long-context settings (Book/repo data and 60% long/40% short) and study the impact of different short-context training mixes. We experiment with SlimPajama (Soboleva et\u00a0al., 2023), FineWeb-Edu (Penedo et\u00a0al., 2024), DCLM-Baseline (Li et\u00a0al., 2024a), and our own ProLong ShortMix.\nOur ShortMix is inspired by the \u201cstage 2 training\u201d in MiniCPM\u00a0(Hu et\u00a0al., 2024a) and Dolma-1.7 (Soldaini et\u00a0al., 2024), which use more knowledge-intensive, downstream-related data at the end of pre-training.\nTable\u00a05 shows the composition of our ShortMix.333\nSince we do not truncate documents in the short data component unnecessarily, it includes a small percentage of documents longer than 8K. See Table\u00a014 in the appendix for the dataset length statistics.\n\n\n\u2005\n\n\n\nTable 5: Our ShortMix.\n\n\n\n\n\nComponents\n%\n\n\n\n\nFineWeb\n25\n\n\nFineWeb-Edu\n25\n\n\nWikipedia\n10\n\n\nTulu-v2\n10\n\n\nStackExchange\n10\n\n\nArXiv\n10\n\n\nOpenWebMath\n10\n\n\n\n\n\n\nTable 6: Impact of\ndifferent short data sources.\nThe long-context performance is the average of 6 categories at the lengths of 32K and 64K.\n\n\n\n\nShort Data (40%)\nLong-Context\nShort-Context\n\n\nAvg.\nHellaS.\nMMLU\nARC-c\nWG\nGSM8K\nAvg.\n\n\nOriginal model (Llama-3-8B)\n-\n82.1\n66.5\n59.4\n77.1\n44.7\n66.0\n\n\n\n\nSlimPajama\n52.9\n81.2\n63.0\n58.5\n76.2\n41.9\n64.2\n\n\nFineWeb-Edu\n53.0\n81.0\n62.6\n57.7\n74.4\n39.4\n63.0\n\n\nDCLM-Baseline\n52.0\n82.0\n65.6\n59.6\n77.4\n39.4\n64.8\n\n\nProLong ShortMix\n54.6\n81.6\n65.3\n58.0\n76.2\n46.6\n65.5\n\n\n\n\n\nTable\u00a06 demonstrates that the short data component has a substantial impact on both short-context and long-context downstream performance.\nOur curated ShortMix outperforms other short data sources on both short and long-context tasks and our data domains are particularly important for retaining Llama-3-8B\u2019s performance on mathematical reasoning.\nSurprisingly, we find that fine-tuning only using FineWeb-Edu\u2014a dataset that is curated to help with knowledge-intensive tasks like MMLU\u2014performs poorly as a short-context component, and we combine it with more diverse data sources in our ShortMix.\nDCLM-Baseline performs well on all short-context tasks except for GSM8K.\nThis can likely be improved by combining with math-related datasets, but as we added the DCLM-baseline ablation at the conclusion of the project, we leave this exploration to future work.\n\n\n",
      "4 Scaling the Size and Length of the Training Data": "\n\n4 Scaling the Size and Length of the Training Data\n\nTraining for more steps is well-known to improve downstream tasks in regular pre-training, but little analysis has been done in the context of long-context continued training.\nWe incorporate the lessons from our ablation experiments and arrive at the ProLong recipe, which we describe in detail in \u00a76. Notably, we scale up the training budget to longer sequences (up to 512K) and more tokens (20B tokens at a maximum sequence length of 64K and an additional 20B tokens at 512K). We reset the learning rate schedule and increase the RoPE frequency base when switching from 64K to 512K context lengths.\nIn this section, we analyze the impact of these decisions.\n\n\nIncreasing the number of steps helps.\n\nIn Figure\u00a04, we plot the downstream performance of intermediate checkpoints of our 40B-token runs.\nWhile the long-context performance fluctuates throughout training, we observe positive trends on recall, RAG, re-ranking, and summarization.\nFor short-context tasks, we observe the average performance initially drops from the initialization, but gradually recovers. Performance again drops when switching from 64K to 512K sequence length, but also recovers with additional training.\n\n\nFigure 4: Performance (avg. of 32K and 64K) of our ProLong model throughout training.\n\n\n\n\nIncreasing the training length beyond the evaluation length helps.\n\nOne might assume that we should train long-context models on the maximum sequence length that we want the model to support. Many works even emphasize extrapolation to even longer sequences at inference time (Press et\u00a0al., 2022; Xiao et\u00a0al., 2024b; a; Yen et\u00a0al., 2024a; Chen et\u00a0al., 2023).\nIn contrast, we observe that training on a longer sequence length (512K tokens) substantially improves the long-context performance at a shorter evaluation length (64K tokens).\n\n\nWe establish this by initializing\nwith a model that was trained for 20B tokens at 64K and either (1) continuing training at 64K, or (2) switching to the 512K training.\nWe use the same hyperparameters and data mixtures in either experiment.\nWe evaluate a checkpoint after 4B training tokens\nat a evaluation length of 64K.\nComparing the two runs\nin Table\u00a07,\nwe see consistent gains from switching to the 512K training length.444While we demonstrate the benefit of longer data, we note that training with longer sequences is more expensive, and may therefore not be the computationally optimal choice.\n\n\nTable 7: Impact of training models on different sequence lengths.\nAll the results are evaluated at a sequence length of 64K.\nWe see that training at a maximum length beyond the evaluation context window consistently improves the long-context performance.\n\n\n\n\nMax Seq. Length\nRecall\nRAG\nRe-rank\nICL\n\n\n\n\nProLong 64K training (20B)\n96.5\n52.7\n22.8\n70.6\n\n\n+4B 64K training\n95.0\n56.4\n28.0\n78.8\n\n\n+4B 512K training\n98.5\n56.9\n32.9\n79.2\n\n\n\n\n\n",
      "5 Supervised Fine-Tuning for Long-Context LMs": "\n\n5 Supervised Fine-Tuning for Long-Context LMs\n\nIn this section, we study how to best enable long-context language models to follow instructions.\nWe focus on supervised fine-tuning on instruction datasets (Ouyang et\u00a0al., 2022) and\nleave reinforcement learning and preference optimization for future work.\n\n\nAll our experiments in this section use the ProLong base model, which was\ntrained for\n40B tokens at a maximum sequence length of 512K.\nIn comparison, open-source instruction data are very short, e.g.,\nUltraChat (Ding et\u00a0al., 2023) conversations have 1.2K tokens on average and 4.1K tokens maximum.\nTo bridge this gap, several works\u00a0(Xiong et\u00a0al., 2023; Dubey et\u00a0al., 2024; Xiong et\u00a0al., 2024)\nhave proposed to generate long instruction data synthetically.\n\n\nWe consider three popular SFT datasets\u2014UltraChat\u00a0(Ding et\u00a0al., 2023), Tulu-v2\u00a0(Ivison et\u00a0al., 2023), ShareGPT555https://huggingface.co/datasets/RyokoAI/ShareGPT52K.\u2014and three sources of synthetic data: For synthetic QA, we prompt Llama-3-8B-Instruct\nto generate a question-and-answer pair given a random chunk from a long document; we reuse the QA pairs for synthetic RAG but we present\na random list of chunks from the document to mimic retrieved passages; for synthetic summarization, we generate summaries for long books via recursive summarization (Wu et\u00a0al., 2021).\nFor all synthetic data, we write several templates, which we sample at random to increase diversity. More details can be found in \u00a7A.5.\nWe always use a combination of 40% synthetic QA, 30% synthetic RAG, and 30% synthetic summarization in our synthetic instruction dataset.\nThe hyperparameters for the instruction tuning experiments can be found in Table\u00a09.\n\n\nShort-context instruction data yields strong long-context results.\n\nWe first establish that UltraChat outperforms Tulu-v2 and ShareGPT\nin Table\u00a022.\nWe therefore use it when studying the ratio of synthetic long-context instruction data in Table\u00a08.\nSurprisingly, we find that adding synthetic data does not improve the performance on these very long-context tasks, and adding even as little as 1% synthetic data hurts the performance in our setting.\nTherefore, we use only short-context UltraChat data for SFT of our final ProLong model.\n\n\nTable 8: Effect of different ratios of synthetic SFT data (mixed with UltraChat). We report the 32K-and-64K-averaged performance except tasks marked with \u2020, which are evaluated at 512K for stress testing. The number of percentage is based on #tokens, not #samples.\n\n\n\n% Synthetic Data\nJsonKV\u2020\n\nRAG\nRe-rank\nICL\nQA\u2020\n\nSumm.\u2020\n\nAvg.\n\n\n\n\n0%\n65.7\n58.1\n38.5\n80.3\n49.7\n42.1\n55.7\n\n\n1%\n61.5\n57.0\n38.3\n80.8\n45.3\n41.5\n54.1\n\n\n3%\n62.0\n56.4\n37.9\n80.6\n44.8\n39.5\n53.5\n\n\n10%\n70.3\n55.5\n36.1\n80.6\n41.7\n39.4\n53.9\n\n\n50%\n45.8\n48.8\n18.8\n70.5\n42.3\n33.3\n43.3\n\n\n\n\n\nWhy do our conclusions about synthetic data differ from previous work?\nWe offer the following hypotheses:\n(1) Previous work like Xiong et\u00a0al. (2024); Bai et\u00a0al. (2024a) may have insufficient long-context training and the synthetic data acts as additional long-context training data.\n(2) Our instruction dataset is much smaller compared to the private instruction data used for Llama-3.1 (Dubey et\u00a0al., 2024)\u2014it is possible that when using an extensive short instruction dataset, mixing in synthetic long data avoids the model from degenerating on long-context tasks.\n\n\nTable 9: The training recipe for ProLong.\n\n\n\nContinued Long-context Training\n\n\nData\n30% code repos, 30% books, 3% textbooks, 37% ShortMix\n\n\n\nShortMix:\n27% FineWeb-Edu, 27% FineWeb,\n\n\n\n\n11% Tulu-v2, 11% StackExchange,\n\n\n\n\n8% Wikipedia, 8% OpenWebMath, 8% ArXiv\n\n\n\n\nLength\nCurriculum\n\nStage 1 (64K):\nCode repos, books, and textbooks at length 64K\n\n\nStage 2 (512K):\nCode repos: 50% at length 512K, 50% at length 64K\n\n\n\n\nBooks: 17% at length 512K, 83% at length 64K\n\n\n\n\nTextbooks at length 512K\n\n\nSteps\nStage 1: 20B tokens (2.2K H100 hours), \u2005 Stage 2: 20B tokens (12.2K H100 hours)\n\n\nModel\nInitialization:\nLlama-3-8B-Instruct (original RoPE base freq. 5\u00d71055superscript1055\\times 10^{5}5 \u00d7 10 start_POSTSUPERSCRIPT 5 end_POSTSUPERSCRIPT)\n\n\n\nRoPE:\nStage 1: 8\u00d71068superscript1068\\times 10^{6}8 \u00d7 10 start_POSTSUPERSCRIPT 6 end_POSTSUPERSCRIPT, Stage 2: \u2005 1.28\u00d71081.28superscript1081.28\\times 10^{8}1.28 \u00d7 10 start_POSTSUPERSCRIPT 8 end_POSTSUPERSCRIPT\n\n\n\n\nAttention:\nFull attention with cross-document attention masking\n\n\nOptim.\nAdamW (weight decay = 0.10.10.10.1, \u03b21=0.9subscript\ud835\udefd10.9\\beta_{1}=0.9italic_\u03b2 start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT = 0.9, \u03b22=0.95subscript\ud835\udefd20.95\\beta_{2}=0.95italic_\u03b2 start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT = 0.95)\n\n\n\nLR:\n\n1\u2062e\u221251\ud835\udc5251e-51 italic_e - 5 with 10%percent1010\\%10 % warmup and cosine decay to 1\u2062e\u221261\ud835\udc5261e-61 italic_e - 6, each stage\n\n\n\nBatch size:\n4M tokens for stage 1, 8M tokens for stage 2\n\n\nSupervised Fine-tuning (SFT)\n\n\nData\nUltraChat\n\n\n\nSteps\n1B tokens\n\n\n\nOptim.\nAdamW (weight decay = 0.10.10.10.1, \u03b21=0.9subscript\ud835\udefd10.9\\beta_{1}=0.9italic_\u03b2 start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT = 0.9, \u03b22=0.95subscript\ud835\udefd20.95\\beta_{2}=0.95italic_\u03b2 start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT = 0.95)\n\n\n\nLR = 2\u2062e\u221252\ud835\udc5252e-52 italic_e - 5 (cosine decay to 2\u2062e\u221262\ud835\udc5262e-62 italic_e - 6), warmup = 5%percent55\\%5 %\n\n\n\n\nBatch size = 4M tokens\n\n\n\n\n\n",
      "6 The ProLong Model: Recipe and Results": "\n\n6 The ProLong Model: Recipe and Results\n\n\n6.1 Final recipe\n\nWe summarize the training recipe for ProLong in Table\u00a09.\nOur final model starts from the Llama-3-8B-Instruct model and is trained on 64K sequence length for 20B tokens.\nIt is then further trained on 512K sequence length for 20B tokens (ProLong base), which we achieve using sequence parallelism (Li et\u00a0al., 2023).\nWe obtain the final ProLong model via SFT of the base model on UltraChat.\nOne small difference on the data mixture between our ablations and the final model is that we mix in 3% high-quality textbooks\u00a0(Chevalier et\u00a0al., 2024), as book-like data are shown to be beneficial for long-context (\u00a73.1) and textbooks are highly educational. This also slightly changes the proportions of ShortMix.\nYou can find more details about our data processing (\u00a7A.2) and the training stack (\u00a7A.3) in the appendix.\n\n\nIn the following, we elaborate on several carefully ablated design choices in our recipe.\n\n\nRoPE frequency base tuning.\n\nWe find that changing the RoPE\u00a0(Su et\u00a0al., 2021) frequency base to achieve position extrapolation\u00a0(Xiong et\u00a0al., 2023; emozilla, 2023) significantly improves long-context performance, even with a significant amount of training. \u00a7B.1 shows our ablation on the best RoPE base to use. While the original Llama models use a RoPE base of 105superscript10510^{5}10 start_POSTSUPERSCRIPT 5 end_POSTSUPERSCRIPT, we use a base of 8\u00d71068superscript1068\\times 10^{6}8 \u00d7 10 start_POSTSUPERSCRIPT 6 end_POSTSUPERSCRIPT for the 64K setting and 1.28\u00d71081.28superscript1081.28\\times 10^{8}1.28 \u00d7 10 start_POSTSUPERSCRIPT 8 end_POSTSUPERSCRIPT for the 512K setting.\n\n\n\nDisabling cross-document attention.\n\nDing et\u00a0al. (2024a) show that masking out attention across document boundaries improve model performance and this was also used during Llama-3 pre-training (Dubey et\u00a0al., 2024).\nIn \u00a7B.2, we show that disabling cross-document attention in continued training benefits both the short and long-context performance.\nDisabling cross-document attention can also result in higher training throughput, which we describe in more detail in \u00a7A.3.\n\n\n\nStarting from Llama-3-8B-Instruct.\n\nWhile we conduct all our long-context training ablations with the base model of Llama-3-8B,\nwe use Llama-3-8B-Instruct as the initialization for the final ProLong model.\n\u00a7B.3 shows that while slightly improving the long-context performance, Llama-3-8B-Instruct significantly enhances the short-context performance.\n\n\n\n\n\n6.2 ProLong performance\n\nWe present the final HELMET evaluation results of ProLong in Table\u00a010.\nWe compare to a number of frontier long-context LMs, namely MegaBeam666https://huggingface.co/aws-prototyping/MegaBeam-Mistral-7B-512k., Llama-3.1\u00a0(Dubey et\u00a0al., 2024), Qwen2\u00a0(Yang et\u00a0al., 2024a), Phi-3\u00a0(Abdin et\u00a0al., 2024), Mistral-Nemo777https://huggingface.co/mistralai/Mistral-Nemo-Instruct-2407., Jamba-1.5\u00a0(Lenz et\u00a0al., 2024), Claude-3.5-Sonnet\u00a0(Anthropic, 2024), Gemini-1.5\u00a0(Reid et\u00a0al., 2024), and GPT-4o\u00a0(Achiam et\u00a0al., 2023).\n\n\nProLong outperforms all 10B-scale models on our long-context evaluation. Notably, ProLong outperforms Llama-3.1-8B-Instruct\non all categories except summarization.\nProLong achieves this with only 5% of Llama-3.1\u2019s long-context data budget (40B vs. 800B tokens).\nWe also showcase the strength of ProLong with several QA examples in Table\u00a023.\n\n\nTable 10: Our main evaluation results on HELMET (Yen et\u00a0al., 2024b; details in \u00a7A.1).\nAll results are averaged over sequence lengths of 32K, 64K, and 128K. For all models, we use the corresponding instruction version.\nProLong is one of the best performing 10B-scale LMs.\nThe complete set of results can be found in \u00a7C.\n\n\n\n\nModel\nMax Len.\nRecall\nRAG\nICL\nRe-rank\nQA\nSumm.\nAvg.\n\n\nProLong (8B)\n512K\n99.4\n66.0\n81.1\n33.2\n40.8\n40.5\n60.2\n\n\nMegaBeam-Mistral (7B)\n512K\n99.4\n58.1\n82.1\n22.1\n33.7\n43.6\n56.5\n\n\nMeta-Llama-3.1 (8B)\n128K\n98.7\n62.8\n79.7\n26.6\n40.4\n46.1\n59.0\n\n\nQwen2 (7B)\n128K\n34.4\n43.4\n54.8\n4.6\n23.3\n38.5\n33.2\n\n\nPhi-3-small (7B)\n128K\n74.8\n60.6\n82.0\n18.5\n34.1\n42.4\n52.1\n\n\nMistral-Nemo (12B)\n128K\n24.9\n48.1\n82.0\n4.7\n37.7\n37.0\n39.1\n\n\nJamba-1.5-Mini (12B/52B)\n256K\n87.7\n61.3\n88.4\n25.9\n42.0\n38.6\n57.3\n\n\nMeta-Llama-3.1 (70B)\n128K\n98.5\n65.9\n80.0\n39.4\n47.2\n51.1\n63.7\n\n\nClaude-3.5-Sonnet\n200K\n99.4\n44.0\n79.3\n19.9\n38.1\n49.2\n55.0\n\n\nGemini-1.5-Pro\n2M\n94.2\n71.4\n78.9\n65.3\n44.4\n56.2\n68.4\n\n\nGPT-4o\n128K\n99.9\n71.5\n86.7\n59.6\n47.0\n55.7\n70.1\n\n\n\n\n\nSince most existing models do not support more than 128K tokens,\nto showcase ProLong\u2019s 512K context length,\nwe stress test ProLong on the QA and summarization tasks from 32K to 512K888\nIn QA and summarization, we truncate the documents at the evaluation length; hence an effective long-context model should demonstrate better performance on longer lengths.. Table\u00a011 shows that ProLong continues to improve at a longer context window.\n\n\n\u2005\n\n\n\nTable 11: ProLong at 512K.\n\n\n\n\n\n\n32K\n64K\n128K\n512K\n\n\n\n\nQA\n31.7\n43.7\n46.7\n49.7\n\n\nSumm\n40.4\n39.8\n41.5\n42.1\n\n\n\n\n\n\nTable 12: Results\non the NoCha benchmark\u00a0(Karpinska et\u00a0al., 2024).101010https://github.com/marzenakrp/nocha. NoCha has a private test set and all evaluation is done by the NoCha authors. Hence, we report models from Table\u00a010 that are also on the NoCha leaderboard.\nProLong is the only model that achieves above-random performance in the <<<75K category and it consistently beats Llama-3.1. Different from the original NoCha leaderboard, we report the average accuracy over all test instances without filtering the test examples based on the model\u2019s context window lengths.\n\n\n\nModel\nMax Len.\n\n<<<75K\n75K-127K\n127K-180K\n\n>>>180K\n\n\n\n\nProLong (8B)\n512K\n28.4\n17.0\n13.1\n20.3\n\n\nMegaBeam-Mistral (7B)\n512K\n19.8\n18.3\n17.5\n15.6\n\n\nMeta-Llama-3.1 (8B)\n128K\n17.3\n16.4\n0.0\n0.0\n\n\nMistral-Nemo (12B)\n128K\n13.6\n0.4\n0.0\n0.0\n\n\nJamba-1.5-Mini (12B/52B)\n256K\n27.2\n28.0\n24.4\n6.2\n\n\nMeta-Llama-3.1 (70B)\n128K\n42.0\n25.0\n0.0\n0.0\n\n\nGemini-1.5-Pro\n2M\n24.7\n38.8\n35.3\n46.9\n\n\nGPT-4o\n128K\n55.6\n58.4\n0.0\n0.0\n\n\n\n\n\nBesides HELMET, we also evaluate our models on NoCha\u00a0(Karpinska et\u00a0al., 2024)\u2014a claim verification dataset on 67 recently published English fictional books.\nWe chose this dataset because (1) it minimizes the data contamination problem as all the books are unlikely to exist in the model pre-training data; (2) all the claims are written by human readers and require global reasoning. Each test instance contains two contradictory claims, and the models must correctly judge both to pass.\n\n\nfootnote\u00a010 demonstrates the NoCha evaluation results.\nAmong 10B-scale models, ProLong achieves the best accuracy on the extremely long test instances (>>>180K); on test instances <<<75K tokens, ProLong significantly outperforms other models and is the only model that is better than random guessing (25%).\nThis further showcases the strength of our training recipe and the ProLong model.\n\n\n",
      "7 Related Work": "\n\n7 Related Work\n\nAdapting existing LMs for long contexts.\n\nMany works explore\nextending the LM context windows with minimal training,\neither by\nposition extrapolation\u00a0(Chen et\u00a0al., 2023; Peng et\u00a0al., 2024; Chen et\u00a0al., 2024; Ding et\u00a0al., 2024b; Liu et\u00a0al., 2024a; Zhang et\u00a0al., 2024b; Zhu et\u00a0al., 2024; Zhao et\u00a0al., 2024; Wu et\u00a0al., 2024; Hu et\u00a0al., 2024b)\nor\nmanipulating the attention patterns\u00a0(Chen et\u00a0al., 2024; Xiao et\u00a0al., 2024b; a; Bertsch et\u00a0al., 2023; Jin et\u00a0al., 2024).\nYoshida et\u00a0al. (2020); Choromanski et\u00a0al. (2021); Chevalier et\u00a0al. (2023)\ninstead explore the idea of compressing the long contexts into shorter forms.\nHowever, Fu et\u00a0al. (2024); Lu et\u00a0al. (2024) show that using full attention, applying simple position extrapolation, and fine-tuning the model on long documents reach much stronger results.\n\n\nLlama 3.1\u00a0(Dubey et\u00a0al., 2024) and Jamba\u00a0(Lieber et\u00a0al., 2024) achieve long-context capabilities by adding a long-context continued training stage between standard pre-training and supervised fine-tuning, which is the setting we follow.\nFu et\u00a0al. (2024) study the data engineering for this setting and argue that 0.5B tokens of domain-balanced, length-upsampled data is sufficient for acquiring the long-context recall ability\u2014which we show is not sufficient if a more holistic evaluation is taken.\nXiong et\u00a0al. (2023); Dubey et\u00a0al. (2024); Lieber et\u00a0al. (2024); Xiong et\u00a0al. (2024); An et\u00a0al. (2024b); Bai et\u00a0al. (2024a) also adopt synthetically-generated long data in the SFT stage; however,\nwe find that\nusing standard, short-context instruction data achieves the best long-context results in our setting.\n\n\n\nEfficient long-context architectures.\n\nThere have been many efforts in designing more efficient architectures,\nfor example,\nlinear attention/RNNs\u00a0(Gu & Dao, 2023; Dao & Gu, 2024; Ma et\u00a0al., 2022; Sun et\u00a0al., 2023; Peng et\u00a0al., 2023; Yang et\u00a0al., 2024b),\nand\nalternative attention architectures\u00a0(Rubin & Berant, 2023; Sun et\u00a0al., 2024; Yen et\u00a0al., 2024a).\nHowever,\nthey often require training from scratch and many have the inherent limitations in terms of long-context recall (Jelassi et\u00a0al., 2024; Arora et\u00a0al., 2024).\nRecent works explore hybrid models (Waleffe et\u00a0al., 2024; Lieber et\u00a0al., 2024)) or distilling existing LMs into hybrid models (Wang et\u00a0al., 2024) and show promising results.\n\n\n\nLong-context evaluation.\n\nMany benchmarks have been proposed for long-context evaluation\n(Shaham et\u00a0al., 2023; Hsieh et\u00a0al., 2024; Krishna et\u00a0al., 2023; Zhang et\u00a0al., 2024a; An et\u00a0al., 2024a; Bai et\u00a0al., 2024b)\nThere are works studying particular aspects of long-context LMs as well,\nsuch as positional bias (Liu et\u00a0al., 2024b),\nin-context learning\u00a0(Bertsch et\u00a0al., 2024; Li et\u00a0al., 2024b), and book-length summarization\u00a0(Kim et\u00a0al., 2024).\nIn this work, we follow Yen et\u00a0al. (2024b)\nfor its diverse application coverage and reliable evaluations.\n\n\n",
      "8 Conclusion": "\n\n8 Conclusion\n\nWe study the problem of given a short-context pre-trained LM, how to\nmost effectively continually pre-train and SFT the model to be long-context.\nWe conduct thorough ablations on each component and\nmany of our findings contradict existing practices or beliefs.\nWe use all the findings to produce ProLong, a new state-of-the-art long-context LM. We release all our code, data, and models publicly\nand hope that our findings will\nboost research and applications of long-context LMs.\n\n\nLimitations\n\nAlthough we try to ablate the major components of our training recipe,\ndue to resource limitations, we cannot exhaust all aspects, such as the optimization hyperparameters and additional data mixtures.\nWe also limit ourselves to the 10B-scale regime and the Llama-3 models, which may limit the generalizability of our findings and recipe.\nAnother concern is that we are overfitting to the tasks chosen for model development\u2014however, we do not directly train on those datasets and guiding model development with benchmark tasks has become a common practice in pre-trained LM development. We also show that our final recipe and model perform well on an additional evaluation dataset, NoCha.\n\n\n\nAcknowledgments\n\nWe acknowledge Mengzhou Xia, Zexuan Zhong, Samyak Gupta, Dan Friedman, Yihe Dong, Abhishek Panigrah, Adithya Bhaskar, Colin Wang, Carlos Jimenez, and other members of Princeton Language and Intelligence for their helpful feedback and discussion. We also thank Luca Soldaini for providing comments on a draft.\nWe thank Marzena Karpinska, Tanya Goyal, and Mohit Iyyer for their help with the NoCha evaluation.\nTianyu Gao is supported by an IBM PhD Fellowship.\nThis work is gratefully supported by an NSF CAREER award (IIS-2239290), a grant from Intel, and Microsoft Azure credits through the \u201cAccelerate Foundation Models Academic Research\u201d Initiative.\n\n\n",
      "Appendix A Experiment Details": "\n\nAppendix A Experiment Details\n\n\nA.1 Evaluation\n\nTable 13: The details for our long-context evaluation following HELMET (Yen et\u00a0al., 2024b). \n\n\n\nCategory\nMetrics\n\n\nTasks and Datasets\n\n\n\n\n\n\nRecall\nSubEM\n\n\nGiven a randomly-generated long JSON file and a key, retrieve the corresponding value\u00a0(Liu et\u00a0al., 2024b).\n\n\n\n\nRAG\nSubEM\n\n\nGiven a question and many retrieved Wikipedia documents (shuffled), answer the question\u00a0(Liu et\u00a0al., 2024b). Datasets: NaturalQuestion\u00a0(Kwiatkowski et\u00a0al., 2019), HotpotQA\u00a0(Yang et\u00a0al., 2018), and PopQA\u00a0(Mallen et\u00a0al., 2023).\n\n\n\n\nRe-rank\nnDCG@10\n\n\nGiven a query and many retrieved documents (shuffled), re-rank the top-10 documents. Datasets: MSMARCO\u00a0(Bajaj et\u00a0al., 2016).\n\n\n\n\nICL\nAccuracy\n\n\nDatasets selected from Bertsch et\u00a0al. (2024): TREC coarse, TREC fine\u00a0(Hovy et\u00a0al., 2001), NLU\u00a0(Liu et\u00a0al., 2021), Banking77\u00a0(Casanueva et\u00a0al., 2020), and Clinc-150 (Larson et\u00a0al., 2019).\n\n\n\n\nQA\nGPT-4o score\n\n\nGiven a book, answer the question. Datasets (# tokens): NarrativeQA (medium: 73K; max: 518K; Ko\u010disk\u00fd et\u00a0al., 2018).\n\n\n\n\nSumm.\nGPT-4o score\n\n\nSummarize a given legal document. Datasets (# tokens): Multi-LexSum (medium: 90K; max: 5M; Shen et\u00a0al., 2022)\n\n\n\n\n\n\n\nTable\u00a013 shows all the datasets we used for the long-context evaluation from HELMET\u00a0(Yen et\u00a0al., 2024b).\nNote that we did not use all the datasets from HELMET for efficiency reasons and we also do not want to overfit to HELMET.\nWe highlight some of the evaluation protocol improvements that HELMET implemented compared to previous benchmarks here:\n\n\n\n\n\u2022\n\nSufficient context lengths and fine-grained control. HELMET can evaluate models at a context length of 128K tokens and beyond.\nThe evaluation protocol also allows for reporting results at different lengths, giving developers fine-trained controls for different needs of long contexts.\n\n\n\n\u2022\n\nBetter synthetic recall tasks. As shown in HELMET, needle-in-a-haystack\u00a0(Kamradt, 2024) is mostly saturated because of its simplicity\u2014the model only needs to find a needle in some irrelevant context. We instead use the more challenging JSON KV task, first proposed in Liu et\u00a0al. (2024b) and included in HELMET, where the model is required to find the corresponding value to a given key among a large JSON file.\n\n\n\n\u2022\n\nUsing class-balanced demonstrations and abstract labels for ICL. To disentangle models\u2019 ability of learning from demonstrations from their pre-training bias of the task or the dataset label distribution\u00a0(Pan et\u00a0al., 2023), HELMET samples the same number of demonstrations for each class and uses number labels (1, 2, \u2026) instead of natural-language labels (e.g., location, description, \u2026).\n\n\n\n\u2022\n\nModel-based evaluation for long-context QA and summarization. Instead of using traditional metrics like ROUGE (which has shown to be poorly indicative of the real model performance: Deutsch & Roth, 2021; Deutsch et\u00a0al., 2022; Goyal et\u00a0al., 2023; Chang et\u00a0al., 2024), HELMET uses model-based evaluations to compare the reference answer and the model output. For QA, HELMET uses GPT-4o to score the model output given the question and the reference answer at a 0-3 scale.\nFor summarization, HELMET takes a similar approach as Zhang & Bansal (2021); Gao et\u00a0al. (2023): it first uses GPT-4o to decompose the reference summary into atomic claims; then it uses GPT-4o to check whether each reference atomic claim is covered by the model output (recall) and whether each sentence in the model output is covered by the reference summary (precision). Yen et\u00a0al. (2024b) show that the model-based evaluation correlates with human perceptions significantly better than traditional metrics.\n\n\n\n\n\n\n\nA.2 Data processing\n\nData sources.\n\nWe list all the data sources we have explored in our ablations and main experiments here:\nthe Stack\u00a0(Kocetkov et\u00a0al., 2023),\nSlimPajama\u00a0(Together, 2023; Soboleva et\u00a0al., 2023),\nFineWeb (we use the 2023-50 snapshot), FineWeb-Edu (we use a random sample)\u00a0(Penedo et\u00a0al., 2024),\nTulu-v2\u00a0(Ivison et\u00a0al., 2023),\nOpenWebMath\u00a0(Paster et\u00a0al., 2024),\ntextbooks\u00a0(Chevalier et\u00a0al., 2024),\nand Dolma\u00a0(Soldaini et\u00a0al., 2024).\nThe Books, StackExchange, and ArXiv data are from SlimPajama. The Wikipedia data are from Dolma.\n\n\n\nData filtering and packing.\n\nFor the short training data and the SFT data, we randomly sample and concatenate the documents or conversations into 64K chunks. The last document for each chunk is truncated. The truncated part is used as the beginning for the next chunk for the short training data but is discarded for the SFT data.\nFor the long-context training data, we filter out the documents that are shorter than 64K; we do the same for the 512K setting, while making sure that the 64K documents packed to 512K length are distinct from the 512K documents.\n\n\n\nFinal data mixture.\n\nFor 512K length, we use a mix of 64K and 512K long data. For the ratio of 64K/512K data, we choose 50%/50% for code and 83%/17%, which are roughly chosen according to the natural availability of very long data, i.e., there are relatively fewer books of length 512K than code repositories. One benefit of retaining 64K-long documents is that we can process these without sequence parallelism and the associated communication overhead.\nWe use a slightly different long data mixture in our ablations\u00a0(Table\u00a05) and our main ProLong experiment\u00a0(Table\u00a09).\nFor the final model, we mix 3% textbooks into the long-context training data. The textbooks are open-source resources from libretexts.org, collected and made available by Chevalier et\u00a0al. (2024).\nWe pre-process the data by concatenating chapters from the same text books, as well as books from the same subject areas. This results in extremely long sequences which we pack into contexts of either 64K or 512K tokens.\nThough we do not have an ablation for adding this data due to limited resources, we believe that it should have a slight positive effect to the final model performance as textbooks are highly educational long-context data.\n\n\nTable 14: % Proportion of long documents for the short data components used in Table\u00a06. These statistics are computed after packing and truncation and therefore correspond to the document lengths as seen by the model. We highlight that the proportion of documents beyond 32K is below 1% for ShortMix.\n\n\n\n\n\n>>>4K\n\n>>>8K\n\n>>>16K\n\n>>>32K\n\n\n\u2005 FineWeb\n1.4\n0.3\n0.1\n0.0\n\n\n\u2005 FineWeb-Edu\n2.8\n0.8\n0.2\n0.0\n\n\n\u2005 Wikipedia\n1.6\n0.4\n0.0\n0.0\n\n\n\u2005 Tulu-v2\n0.0\n0.0\n0.0\n0.0\n\n\n\u2005 StackExchange\n0.6\n0.1\n0.0\n0.0\n\n\n\u2005 ArXiv\n85.7\n64.0\n30.3\n7.6\n\n\n\u2005 OpenWebMath\n11.1\n4.3\n1.2\n0.3\n\n\nShortMix\n10.9\n7.2\n3.2\n0.8\n\n\nSlimPajama\n11.3\n7.4\n4.9\n3.2\n\n\nFineWeb-Edu\n2.8\n0.8\n0.2\n0.0\n\n\nDCLM-Baseline\n4.9\n1.7\n0.4\n0.1\n\n\n\n\n\n\n\n\nA.3 Implementation details\n\nTechnical stack.\n\nWe use various open-source packages and tools for the ProLong training and evaluation.\nWe use PyTorch (Paszke et\u00a0al., 2019) and Hugging Face transformers\u00a0(Wolf et\u00a0al., 2020) for the model training.\nWe use mosaic-streaming (Mosaic ML, 2022) for loading and mixing the data\nand FlashAttention 2 (Dao, 2024) for efficient attention implementation.\nWe implement sequence parallelism based on DeepSpeed-Ulysses (Jacobs et\u00a0al., 2023) across groups of 8 GPUs on the same node. We only perform distributed attention if it is necessary, i.e., only on sequences of 512K length.\nFor long-context evaluation, we use HELMET\u00a0(Yen et\u00a0al., 2024b) and for short-context evaluation, we use lm-eval-harness (Gao et\u00a0al., 2021).\n\n\n\nAttention and batching.\n\nSince we do document masking in attention\u00a0(\u00a76),\nwe use the variable-length attention implementation from FlashAttention 2\u00a0(Dao, 2024) to speed up long-context training:\nfor sequences that are concatenations of multiple short documents,\ninstead of computing the full attention with masking,\nwe instead compute the attention for each individual document.\nSince the complexity of attention is quadratic to the sequence length, this improves the training speed.\nHowever, the improvement is negligible in a distributed training setting with FSDP, since GPUs processing short sequence batches have to wait on other GPUs processing long sequences.\nWe therefore implement a smart batching algorithm:\nIn our setting, a gradient step usually consists of multiple gradient accumulation steps, where each device processes a smaller minibatch.\nWe sort all the minibatches per training step by the sum of the squared lengths of documents in the sequence.\nThis leads to more balanced sequence lengths across the GPUs and effective speedups, as can be seen in Table\u00a015, without affecting the gradient updates or loss during training. However, the efficiency gains are diminished when training with more GPUs, as this reduces the number of gradient accumulation steps.\n\n\nTable 15: Throughput per device of our ablation runs from Table\u00a020, when training with 8 Nvidia H100 GPUs with FSDP. Our strategy of reordering minibatches is important for realizing the speed benefits from variable-length attention.\n\n\n\n\nThroughput\n\n\n\n(tokens/s/GPU)\n\n\n\n\n64K full attention\n\u00a0\u00a0\u00a02770\n\n\nVariable-length attention\n\u00a0\u00a0\u00a02780(+0.4%)\n\n\n\n\u2005 + Minibatch reordering\n\n\u00a0\u00a0\u00a03095(+11.7%)\n\n\n\n\n\n\n\nToken-averaged loss.\n\nWe found that in the SFT stage,\nthe distribution of the training tokens (in SFT, the tokens from the instructions are masked out and the models are only trained on the responses) on each GPU device can be extremely imbalanced, especially when there is synthetic data (most tokens in a synthetic data instance are from the instruction).\nConventional all-reduce loss in distributed training averages over the sequences instead of valid tokens, which skews the optimization and also our control over the domain proportions.\nInstead, we change the all-reduce loss to be the average over all valid training tokens.\nBai et\u00a0al. (2024a) implements their SFT loss in a similar way.\n\n\n\n\n\nA.4 The ablation setting\n\nFor all our ablations, unless specified, we train the base model of Llama-3-8B (instead of Instruct) on a 64K sequence length for 5B tokens, with the same hyperparameters as specified in Table\u00a09.\nWe choose this context length, as it is the highest power of 2 value for which we can train without sequence parallelism.\nBy default, we use the same training data as the 64K ProLong setting, except that we remove the textbooks and use the ShortMix proportions in Table\u00a05.\nFor SFT, we use the same settings as specified in Table\u00a09.\n\n\n\n\nA.5 Generating synthetic SFT data\n\nWe prompt Llama-3-8B-Instruct to generate the synthetic data and\nTable\u00a016 shows the prompt we used for generating the synthetic QA data for books.\nWe also write predefined templates and randomly sample one for each synthetic instance to increase the diversity, and Table\u00a017 provides some examples.\n\n\nTable 16: \nPrompts for generating synthetic QA data.\n\n\n\n\n\n\nGiven the following snippet of a book, ask a relevant question and provide the answer. The question and the answer should follow the following rules:\n\n\n\n\n\n\n\n\n\n\n\n\n\n(1) The question should be specific enough that it can only be answered with the snippet. The question should also be interesting and intellectual enough that a curious reader of the book would ask about it.\n\n\n\n\n\n\n(2) The question and the answer should be comprehensible given just the whole book without highlighting the snippet. With that being said, the question should NOT refer to the snippet directly (e.g., do NOT say things like \"Question: given the conversation in the snippet, what ...\"). The answer also should not mention \"the snippet \u2026\" explicitly (assuming that the snippet is never provided), but it can copy the snippet content as a reference when answering the question.\n\n\n\n\n\n\n(3) The answer should be concise but also should provide references to the book when needed. For example, \u201cWellington Yueh betrayed the Atreides, as the book mentioned, \u2019...\u2019\".\n\n\n\n\n\n\n\n\n\n\n\n\n\n*** Start of the snippet ***\n\n\n\n\n\n\n\n\n\n\n\n\n\n{sampled snippet}\n\n\n\n\n\n\n\n\n\n\n\n\n\n*** End of the snippet ***\n\n\n\n\n\n\n\n\n\n\n\n\n\nBefore generating the question and the answer, first reason about what this snippet is about. In your generation, stick to the following format:\n\n\n\n\n\n\n\n\n\n\n\n\n\nReasoning: this snippet is about ...\n\n\n\n\n\n\nQuestion: ...\n\n\n\n\n\n\nAnswer: ...\n\n\n\n\n\n\n\nTable 17: \nExamples for question prompts and templates used for generating diverse synthetic QA data. We sample one question prompt and one template each time and combine them with the documents and the generated QA pairs to form a synthetic training example.\n\n\n\n\nExample question prompts for synthetic QA data\n\n\n\n\n\n\nGiven the document, please answer the question.\n\n\n\n\n\n\nHere is a piece of text; answer the following question based on it.\n\n\n\n\n\n\nPlease answer the question using the provided content.\n\n\n\n\n\n\nBased on the given passage, respond to the question.\n\n\n\n\n\n\nRead the snippet and answer the question that follows.\n\n\n\n\n\n\nUsing the provided text, answer the following question.\n\n\n\n\nExample templates for combining questions, answers, and contexts for synthetic QA data\n\n\n\n\n{prompt}\\n\\n{documents}\\n\\nQuestion: {question}\n\n\n\n\n\n\n{prompt}\\n\\n==== document starts ====\\n{documents}\\n==== document ends ====\\n\\nQuestion: {question}\n\n\n\n\n\n\n{prompt}\\n\\n{documents}\\n\\n{question}\n\n\n\n\n\n\n{prompt} Question: {question}\\n\\n{documents}\n\n\n\n\n\n\n{prompt} {question}\\n\\n{documents}\n\n\n\n\n\n\n{prompt}\\n\\n{question}\\n\\n{documents}\n\n\n\n\n\n\n\n",
      "Appendix B More Ablations": "\n\nAppendix B More Ablations\n\n\nB.1 Position extrapolation\n\nXiong et\u00a0al. (2023); emozilla (2023) show that\nchanging the RoPE frequency base to a larger value in\ncontinual long-context pre-training or in inference time can improve the long-context performance.\nemozilla (2023) suggests that one should scale the frequency base by a factor of tdd\u22122superscript\ud835\udc61\ud835\udc51\ud835\udc512t^{\\frac{d}{d-2}}italic_t start_POSTSUPERSCRIPT divide start_ARG italic_d end_ARG start_ARG italic_d - 2 end_ARG end_POSTSUPERSCRIPT, where t\ud835\udc61titalic_t is the ratio between the target sequence length and the original LM length, and d\ud835\udc51ditalic_d is the attention head dimension.\n\n\nWe conduct ablation studies, at both 64K (same as our standard ablation setting as specified in \u00a7A.4) and 512K (starting from ProLong-64K and training with the 512K data mixture for 5B tokens) sequence lengths, on what frequency bases we should use.\n\u00a7B.1 and \u00a7B.1 show the results.\nWe first see that using the original 500,000 frequency base from Llama-3 leads to significant performance degradation. While dynamic NTK suggests 4\u00d71064superscript1064\\times 10^{6}4 \u00d7 10 start_POSTSUPERSCRIPT 6 end_POSTSUPERSCRIPT, we find that further scaling it to 8\u00d71068superscript1068\\times 10^{6}8 \u00d7 10 start_POSTSUPERSCRIPT 6 end_POSTSUPERSCRIPT leads to better performance. Similar, we see that when scaling the 64K model to 512K, while dynamic NTK suggests a 64\u00d710664superscript10664\\times 10^{6}64 \u00d7 10 start_POSTSUPERSCRIPT 6 end_POSTSUPERSCRIPT frequency base, much larger frequency bases (128\u00d7106128superscript106128\\times 10^{6}128 \u00d7 10 start_POSTSUPERSCRIPT 6 end_POSTSUPERSCRIPT and 256\u00d7106256superscript106256\\times 10^{6}256 \u00d7 10 start_POSTSUPERSCRIPT 6 end_POSTSUPERSCRIPT) lead to better performance.\nWe use 8\u00d71068superscript1068\\times 10^{6}8 \u00d7 10 start_POSTSUPERSCRIPT 6 end_POSTSUPERSCRIPT for 64K and 128\u00d7106128superscript106128\\times 10^{6}128 \u00d7 10 start_POSTSUPERSCRIPT 6 end_POSTSUPERSCRIPT for 512K for our final ProLong models.\n\n\nTable 18: Ablation study on RoPE frequency base at a maximum training length of 64K. Dynamic NTK\u00a0(emozilla, 2023) roughly suggests to use 4m as the frequency base.\n\n\n\n\n\nRoPE Base \n(\u00d7106absentsuperscript106\\times 10^{6}\u00d7 10 start_POSTSUPERSCRIPT 6 end_POSTSUPERSCRIPT)\nLong-Context\nShort-Context\n\n\nRecall\nRAG\nRe-rank\nICL\nQA\nSumm.\nAvg.\nAvg.\n\n\n\n\n0.5\n25.8\n37.0\n4.4\n73.8\n17.5\n16.3\n29.1\n65.0\n\n\n4.0\n81.3\n47.8\n18.2\n76.5\n31.8\n36.3\n48.7\n65.3\n\n\n8.0\n96.0\n54.9\n29.4\n73.9\n35.7\n37.9\n54.6\n65.5\n\n\n\n\n\n\n\nTable 19: Ablation study on RoPE frequency base at a maximum training length of 512K. Dynamic NTK\u00a0(emozilla, 2023) roughly suggests to use 64\u00d710664superscript10664\\times 10^{6}64 \u00d7 10 start_POSTSUPERSCRIPT 6 end_POSTSUPERSCRIPT as the frequency base.\n\n\n\n\n\nRoPE Base \n(\u00d7106absentsuperscript106\\times 10^{6}\u00d7 10 start_POSTSUPERSCRIPT 6 end_POSTSUPERSCRIPT)\nLong-Context\nShort-Context\n\n\nRecall\nRAG\nRe-rank\nICL\nQA\nSumm.\nAvg.\nAvg.\n\n\n\n\n64\n98.8\n57.8\n30.4\n82.2\n38.2\n38.3\n57.6\n68.3\n\n\n128\n98.8\n57.4\n30.7\n80.0\n40.4\n38.8\n57.7\n68.6\n\n\n256\n98.8\n56.8\n33.8\n79.8\n37.9\n39.7\n57.8\n68.4\n\n\n\n\n\n\n\n\nB.2 Document masks\n\nWe experiment whether to use document masks in attention in Table\u00a020. Standard training concatenates multiple short documents into a single sequence (in our case, a 64K sequence), uses a special token to separate documents, and performs full attention over the whole sequence. When the document masks are used, we do not allow the attention to cross the document boundaries.\nWe find that using document masks in continual long-context training leads to both better long-context results and short-context performance.\nFor all our other ablations and the main experiment, we use document masks.\n\n\nTable 20: Impact of using document masks in attention.\n\n\n\nAttention\nLong-Context\nShort-Context\n\n\nRecall\nRAG\nRe-rank\nICL\nQA\nSumm.\nAvg.\nAvg.\n\n\n\n\nNo doc masks\n97.4\n53.6\n20.4\n76.6\n37.2\n36.3\n53.6\n64.9\n\n\nDocument masks\n96.0\n54.9\n29.4\n73.9\n35.7\n37.9\n54.6\n65.5\n\n\n\n\n\n\nB.3 Initialization\n\nWe use the base model for Llama-3-8B as the initialization for all our ablations to make sure the findings are generalizable and are not confounded by the Llama instruction tuning.\nHowever, for our final ProLong model, we use Llama-3-8B-Instruct as the initialization to achieve the best performance.\nWe see in Table\u00a021 (using the ablation setting from \u00a7A.4) that using Llama-3-8B-Instruct as the initialization achieves slightly better long-context performance and much stronger short-context performance.\n\n\nTable 21: \nDifferences of using the base Llama-3-8B model vs. Llama-3-8B-Instruct.\n\n\n\n\nBase Model\nLong-Context\nShort-Context\n\n\nAvg.\nHellaS.\nMMLU\nARC-c\nWG\nGSM8K\nAvg.\n\n\n\n\nLlama-3-8B-Base\n54.6\n81.6\n65.3\n58.0\n76.2\n46.6\n65.5\n\n\nLlama-3-8B-Instruct\n55.0\n80.8\n66.1\n58.5\n75.6\n57.7\n67.7\n\n\n\n\n\n\nB.4 Instruction-tuning datasets\n\nInitialized from the ProLong base model, we experiment with different public, short-context SFT datasets.\nAll runs use the same SFT hyperparameters as specified in Table\u00a09.\nTable\u00a022 shows that using UltraChat leads to the best overall results.\nNote that this does not necessarily mean that UltraChat is the best SFT dataset for all base models or applications.\n\n\nTable 22: \nAblations on using different short-context SFT datasets.\nWe report the 32K-and-64K-averaged performance except tasks marked with \u2020, which are evaluated at 512K for stress testing.\n\n\n\n\nSFT Data\nLong-Context\n\n\nRecall\u2020\n\nRAG\nRe-rank\nICL\nQA\u2020\n\nSumm.\u2020\n\nAvg.\n\n\n\n\nUltraChat\n65.7\n58.1\n38.5\n80.3\n49.7\n42.1\n55.7\n\n\nTulu v2\n61.5\n45.4\n25.1\n81.8\n40.4\n40.3\n49.1\n\n\nShareGPT\n40.5\n47.5\n26.7\n79.6\n42.7\n34.4\n45.2\n\n\n\n\n\n\nAppendix C Full Evaluation Results\n\nWe provide all the evaluation results, including specific numbers for each dataset and evaluation length, at https://shorturl.at/pULbx.\n\n\n\n\nAppendix D Examples\n\nTable\u00a023 shows some long-document QA examples with ProLong, Llama-3.1, and GPT-4o.\n\n\nTable 23: \nLong-document QA examples with ProLong (512K), Llama-3.1-8B-Instruct (128K), and GPT-4o (via the ChatGPT interface). Note that\nit is very likely that the ChatGPT-version GPT-4o uses a retrieval component for long documents, hence it can process contexts beyond its 128K context window.\nWe see that ProLong can answer questions effectively with a 300K document while both Llama-3.1 and GPT-4o fail.\nOn a shorter document (Llama-3.1 paper), ProLong can understand very technical questions and answer them accurately.\n\n\n\n\nInstacart S1 file (300K tokens)\n\n\n\n\n\n\nQ: How many shares and at what price did instacard sell their series G redeemable convertible preferred stock in June 2020 and July 2020?\n\n\n\nProLong: Instacart sold an aggregate of  6,757,893 shares of their \u2026 at a purchase price of $48.0919 \u2026\n\n\n\n\n\n\nllama-3.1-8B: The following is a II/5 [collapsed]\n\n\n\n\n\n\nGPT-4o: \u2026 The company sold a total of 6,758,000 shares at a price of $48.0919 per share during this period.\n\n\n\n\nLlama-3.1 paper (94K tokens)\n\n\n\n\nQ: During training of the llama3 model, how many unexpected interruptions are caused by hardware related reasons?\n\n\n\nProLong: Approximately 78% of unexpected \u2026\n\n\n\n\n\n\nllama-3.1-8B: According to Table 5, approximately 58.7% \u2026\n\n\n\n\n\n\nGPT-4o: \u2026 approximately 78% of unexpected interruptions \u2026\n\n\n\n\n\n\nQ: Is Mark Zuckerberg a contributor to the paper?\n\n\n\nProLong: There is no mention of Mark Zuckerberg as a contributor to the paper.\n\n\n\n\n\n\nllama-3.1-8B: Yes, Mark Zuckerberg is listed as a contributor \u2026\n\n\n\n\n\n\nGPT-4o: Mark Zuckerberg is acknowledged in the paper for \u2026, but he is not listed as a core contributor \u2026\n\n\n\n\n\n\nQ: What ratio did the llama 3 team use to mix synthetic long-context data with short data in SFT?\n\n\n\nProLong: The llama 3 team used a ratio of 0.1% synthetic \u2026\n\n\n\n\n\n\nllama-3.1-8B: 0.10%\n\n\n\n\n\n\nGPT-4o: The Llama 3 team used a mix ratio of 30% synthetic \u2026\n\n\n\n\n\n\nQ: What benchmark did they evaluate on for \u201dinstruction-following\u201d ability? Provide references too.\n\n\n\nProLong: They evaluated on IFEval (Zhou et al., 2023) for instruction-following ability. Reference: Jeffrey Zhou, \u2026 Instruction-following evaluation for large language models. arXiv preprint \u2026\n\n\n\n\n\n\nllama-3.1-8B: The benchmark they evaluated on for \u201dinstruction-following\u201d ability is IFEval. Reference: Section 5.2.2, \u201dGeneral Knowledge and Instruction-Following Benchmarks\u201d\n\n\n\n\n\n\nGPT-4o: The benchmark they used to evaluate \u201dinstruction- following\u201d ability is IFEval. This is mentioned in the context of assessing how Direct Preference Optimization (DPO) \u2026\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",
      "Appendix C Full Evaluation Results": "\n\nAppendix C Full Evaluation Results\n\nWe provide all the evaluation results, including specific numbers for each dataset and evaluation length, at https://shorturl.at/pULbx.\n\n",
      "Appendix D Examples": "\n\nAppendix D Examples\n\nTable\u00a023 shows some long-document QA examples with ProLong, Llama-3.1, and GPT-4o.\n\n\nTable 23: \nLong-document QA examples with ProLong (512K), Llama-3.1-8B-Instruct (128K), and GPT-4o (via the ChatGPT interface). Note that\nit is very likely that the ChatGPT-version GPT-4o uses a retrieval component for long documents, hence it can process contexts beyond its 128K context window.\nWe see that ProLong can answer questions effectively with a 300K document while both Llama-3.1 and GPT-4o fail.\nOn a shorter document (Llama-3.1 paper), ProLong can understand very technical questions and answer them accurately.\n\n\n\n\nInstacart S1 file (300K tokens)\n\n\n\n\n\n\nQ: How many shares and at what price did instacard sell their series G redeemable convertible preferred stock in June 2020 and July 2020?\n\n\n\nProLong: Instacart sold an aggregate of  6,757,893 shares of their \u2026 at a purchase price of $48.0919 \u2026\n\n\n\n\n\n\nllama-3.1-8B: The following is a II/5 [collapsed]\n\n\n\n\n\n\nGPT-4o: \u2026 The company sold a total of 6,758,000 shares at a price of $48.0919 per share during this period.\n\n\n\n\nLlama-3.1 paper (94K tokens)\n\n\n\n\nQ: During training of the llama3 model, how many unexpected interruptions are caused by hardware related reasons?\n\n\n\nProLong: Approximately 78% of unexpected \u2026\n\n\n\n\n\n\nllama-3.1-8B: According to Table 5, approximately 58.7% \u2026\n\n\n\n\n\n\nGPT-4o: \u2026 approximately 78% of unexpected interruptions \u2026\n\n\n\n\n\n\nQ: Is Mark Zuckerberg a contributor to the paper?\n\n\n\nProLong: There is no mention of Mark Zuckerberg as a contributor to the paper.\n\n\n\n\n\n\nllama-3.1-8B: Yes, Mark Zuckerberg is listed as a contributor \u2026\n\n\n\n\n\n\nGPT-4o: Mark Zuckerberg is acknowledged in the paper for \u2026, but he is not listed as a core contributor \u2026\n\n\n\n\n\n\nQ: What ratio did the llama 3 team use to mix synthetic long-context data with short data in SFT?\n\n\n\nProLong: The llama 3 team used a ratio of 0.1% synthetic \u2026\n\n\n\n\n\n\nllama-3.1-8B: 0.10%\n\n\n\n\n\n\nGPT-4o: The Llama 3 team used a mix ratio of 30% synthetic \u2026\n\n\n\n\n\n\nQ: What benchmark did they evaluate on for \u201dinstruction-following\u201d ability? Provide references too.\n\n\n\nProLong: They evaluated on IFEval (Zhou et al., 2023) for instruction-following ability. Reference: Jeffrey Zhou, \u2026 Instruction-following evaluation for large language models. arXiv preprint \u2026\n\n\n\n\n\n\nllama-3.1-8B: The benchmark they evaluated on for \u201dinstruction-following\u201d ability is IFEval. Reference: Section 5.2.2, \u201dGeneral Knowledge and Instruction-Following Benchmarks\u201d\n\n\n\n\n\n\nGPT-4o: The benchmark they used to evaluate \u201dinstruction- following\u201d ability is IFEval. This is mentioned in the context of assessing how Direct Preference Optimization (DPO) \u2026\n\n\n\n\n\n\n\n"
    },
    "bibliography": {
      "Abdin et\u00a0al. (2024)": "\nAbdin et\u00a0al. (2024)\n\nMarah Abdin, Sam\u00a0Ade Jacobs, Ammar\u00a0Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Harkirat Behl, et\u00a0al.\n\n\nPhi-3 technical report: A highly capable language model locally on your phone.\n\n\narXiv preprint arXiv:2404.14219, 2024.\n\n\n",
      "Achiam et\u00a0al. (2023)": "\nAchiam et\u00a0al. (2023)\n\nJosh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia\u00a0Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et\u00a0al.\n\n\nGpt-4 technical report.\n\n\narXiv preprint arXiv:2303.08774, 2023.\n\n\n",
      "An et\u00a0al. (2024a)": "\nAn et\u00a0al. (2024a)\n\nChenxin An, Shansan Gong, Ming Zhong, Xingjian Zhao, Mukai Li, Jun Zhang, Lingpeng Kong, and Xipeng Qiu.\n\n\nL-eval: Instituting standardized evaluation for long context language models.\n\n\nIn Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp.\u00a0 14388\u201314411, 2024a.\n\n\n",
      "An et\u00a0al. (2024b)": "\nAn et\u00a0al. (2024b)\n\nShengnan An, Zexiong Ma, Zeqi Lin, Nanning Zheng, and Jian-Guang Lou.\n\n\nMake your llm fully utilize the context.\n\n\narXiv preprint arXiv:2404.16811, 2024b.\n\n\n",
      "Anthropic (2024)": "\nAnthropic (2024)\n\nAI\u00a0Anthropic.\n\n\nClaude 3.5 sonnet model card addendum.\n\n\nClaude-3.5 Model Card, 2024.\n\n\n",
      "Arora et\u00a0al. (2024)": "\nArora et\u00a0al. (2024)\n\nSimran Arora, Sabri Eyuboglu, Michael Zhang, Aman Timalsina, Silas Alberti, James Zou, Atri Rudra, and Christopher Re.\n\n\nSimple linear attention language models balance the recall-throughput tradeoff.\n\n\nIn Forty-first International Conference on Machine Learning, 2024.\n\n\n",
      "Bai et\u00a0al. (2024a)": "\nBai et\u00a0al. (2024a)\n\nYushi Bai, Xin Lv, Jiajie Zhang, Yuze He, Ji\u00a0Qi, Lei Hou, Jie Tang, Yuxiao Dong, and Juanzi Li.\n\n\nLongalign: A recipe for long context alignment of large language models.\n\n\narXiv preprint arXiv:2401.18058, 2024a.\n\n\n",
      "Bai et\u00a0al. (2024b)": "\nBai et\u00a0al. (2024b)\n\nYushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li.\n\n\nLongBench: A bilingual, multitask benchmark for long context understanding.\n\n\nIn Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp.\u00a0 3119\u20133137, 2024b.\n\n\n",
      "Bajaj et\u00a0al. (2016)": "\nBajaj et\u00a0al. (2016)\n\nPayal Bajaj, Daniel Campos, Nick Craswell, Li\u00a0Deng, Jianfeng Gao, Xiaodong Liu, Rangan Majumder, Andrew McNamara, Bhaskar Mitra, Tri Nguyen, et\u00a0al.\n\n\nMs marco: A human generated machine reading comprehension dataset.\n\n\narXiv preprint arXiv:1611.09268, 2016.\n\n\n",
      "Beeching et\u00a0al. (2023)": "\nBeeching et\u00a0al. (2023)\n\nEdward Beeching, Cl\u00e9mentine Fourrier, Nathan Habib, Sheon Han, Nathan Lambert, Nazneen Rajani, Omar Sanseviero, Lewis Tunstall, and Thomas Wolf.\n\n\nOpen llm leaderboard, 2023.\n\n\n",
      "Bertsch et\u00a0al. (2023)": "\nBertsch et\u00a0al. (2023)\n\nAmanda Bertsch, Uri Alon, Graham Neubig, and Matthew\u00a0R. Gormley.\n\n\nUnlimiformer: Long-range transformers with unlimited length input.\n\n\nIn Advances in Neural Information Processing Systems (NeurIPS), 2023.\n\n\n",
      "Bertsch et\u00a0al. (2024)": "\nBertsch et\u00a0al. (2024)\n\nAmanda Bertsch, Maor Ivgi, Uri Alon, Jonathan Berant, Matthew\u00a0R Gormley, and Graham Neubig.\n\n\nIn-context learning with long-context models: An in-depth exploration.\n\n\narXiv preprint arXiv:2405.00200, 2024.\n\n\n",
      "Casanueva et\u00a0al. (2020)": "\nCasanueva et\u00a0al. (2020)\n\nI\u00f1igo Casanueva, Tadas Tem\u010dinas, Daniela Gerz, Matthew Henderson, and Ivan Vuli\u0107.\n\n\nEfficient intent detection with dual sentence encoders.\n\n\nIn Proceedings of the 2nd Workshop on Natural Language Processing for Conversational AI, pp.\u00a0 38\u201345, 2020.\n\n\n",
      "Chang et\u00a0al. (2024)": "\nChang et\u00a0al. (2024)\n\nYapei Chang, Kyle Lo, Tanya Goyal, and Mohit Iyyer.\n\n\nBooookscore: A systematic exploration of book-length summarization in the era of LLMs.\n\n\nIn The Twelfth International Conference on Learning Representations, 2024.\n\n\n",
      "Chen et\u00a0al. (2023)": "\nChen et\u00a0al. (2023)\n\nShouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian.\n\n\nExtending context window of large language models via positional interpolation, 2023.\n\n\n",
      "Chen et\u00a0al. (2024)": "\nChen et\u00a0al. (2024)\n\nYukang Chen, Shengju Qian, Haotian Tang, Xin Lai, Zhijian Liu, Song Han, and Jiaya Jia.\n\n\nLongloRA: Efficient fine-tuning of long-context large language models.\n\n\nIn The Twelfth International Conference on Learning Representations, 2024.\n\n\n",
      "Chevalier et\u00a0al. (2023)": "\nChevalier et\u00a0al. (2023)\n\nAlexis Chevalier, Alexander Wettig, Anirudh Ajith, and Danqi Chen.\n\n\nAdapting language models to compress contexts.\n\n\nIn Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp.\u00a0 3829\u20133846, Singapore, December 2023. Association for Computational Linguistics.\n\n\n",
      "Chevalier et\u00a0al. (2024)": "\nChevalier et\u00a0al. (2024)\n\nAlexis Chevalier, Jiayi Geng, Alexander Wettig, Howard Chen, Sebastian Mizera, Toni Annala, Max Aragon, Arturo\u00a0Rodriguez Fanlo, Simon Frieder, Simon Machado, Akshara Prabhakar, Ellie Thieu, Jiachen\u00a0T. Wang, Zirui Wang, Xindi Wu, Mengzhou Xia, Wenhan Xia, Jiatong Yu, Junjie Zhu, Zhiyong Ren, Sanjeev Arora, and Danqi Chen.\n\n\nLanguage models as science tutors.\n\n\nIn Forty-first International Conference on Machine Learning, 2024.\n\n\n",
      "Choromanski et\u00a0al. (2021)": "\nChoromanski et\u00a0al. (2021)\n\nKrzysztof\u00a0Marcin Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared\u00a0Quincy Davis, Afroz Mohiuddin, Lukasz Kaiser, David\u00a0Benjamin Belanger, Lucy\u00a0J Colwell, and Adrian Weller.\n\n\nRethinking attention with performers.\n\n\nIn International Conference on Learning Representations, 2021.\n\n\nURL https://openreview.net/forum?id=Ua6zuk0WRH.\n\n\n",
      "Clark et\u00a0al. (2018)": "\nClark et\u00a0al. (2018)\n\nPeter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord.\n\n\nThink you have solved question answering? Try ARC, the AI2 reasoning challenge.\n\n\narXiv preprint arXiv:1803.05457, 2018.\n\n\n",
      "Cobbe et\u00a0al. (2021)": "\nCobbe et\u00a0al. (2021)\n\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et\u00a0al.\n\n\nTraining verifiers to solve math word problems.\n\n\narXiv preprint arXiv:2110.14168, 2021.\n\n\n",
      "Dao (2024)": "\nDao (2024)\n\nTri Dao.\n\n\nFlashattention-2: Faster attention with better parallelism and work partitioning.\n\n\nIn The Twelfth International Conference on Learning Representations, 2024.\n\n\n",
      "Dao & Gu (2024)": "\nDao & Gu (2024)\n\nTri Dao and Albert Gu.\n\n\nTransformers are ssms: Generalized models and efficient algorithms through structured state space duality.\n\n\narXiv preprint arXiv:2405.21060, 2024.\n\n\n",
      "Deutsch & Roth (2021)": "\nDeutsch & Roth (2021)\n\nDaniel Deutsch and Dan Roth.\n\n\nUnderstanding the extent to which content quality metrics measure the information quality of summaries.\n\n\nIn Arianna Bisazza and Omri Abend (eds.), Proceedings of the 25th Conference on Computational Natural Language Learning, pp.\u00a0 300\u2013309, 2021.\n\n\n",
      "Deutsch et\u00a0al. (2022)": "\nDeutsch et\u00a0al. (2022)\n\nDaniel Deutsch, Rotem Dror, and Dan Roth.\n\n\nRe-examining system-level correlations of automatic summarization evaluation metrics.\n\n\nIn Marine Carpuat, Marie-Catherine de\u00a0Marneffe, and Ivan\u00a0Vladimir Meza\u00a0Ruiz (eds.), Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp.\u00a0 6038\u20136052, Seattle, United States, July 2022. Association for Computational Linguistics.\n\n\ndoi: 10.18653/v1/2022.naacl-main.442.\n\n\nURL https://aclanthology.org/2022.naacl-main.442.\n\n\n",
      "Ding et\u00a0al. (2024a)": "\nDing et\u00a0al. (2024a)\n\nHantian Ding, Zijian Wang, Giovanni Paolini, Varun Kumar, Anoop Deoras, Dan Roth, and Stefano Soatto.\n\n\nFewer truncations improve language modeling.\n\n\nIn Forty-first International Conference on Machine Learning, 2024a.\n\n\nURL https://openreview.net/forum?id=kRxCDDFNpp.\n\n\n",
      "Ding et\u00a0al. (2023)": "\nDing et\u00a0al. (2023)\n\nNing Ding, Yulin Chen, Bokai Xu, Yujia Qin, Shengding Hu, Zhiyuan Liu, Maosong Sun, and Bowen Zhou.\n\n\nEnhancing chat language models by scaling high-quality instructional conversations.\n\n\nIn Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp.\u00a0 3029\u20133051, Singapore, December 2023. Association for Computational Linguistics.\n\n\n",
      "Ding et\u00a0al. (2024b)": "\nDing et\u00a0al. (2024b)\n\nYiran Ding, Li\u00a0Lyna Zhang, Chengruidong Zhang, Yuanyuan Xu, Ning Shang, Jiahang Xu, Fan Yang, and Mao Yang.\n\n\nLongroPE: Extending LLM context window beyond 2 million tokens.\n\n\nIn Forty-first International Conference on Machine Learning, 2024b.\n\n\n",
      "Dubey et\u00a0al. (2024)": "\nDubey et\u00a0al. (2024)\n\nAbhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et\u00a0al.\n\n\nThe Llama 3 herd of models.\n\n\narXiv preprint arXiv:2407.21783, 2024.\n\n\n",
      "emozilla (2023)": "\nemozilla (2023)\n\nemozilla.\n\n\nDynamically scaled rope further increases performance of long context llama with zero fine-tuning, 2023.\n\n\nURL https://www.reddit.com/r/LocalLLaMA/comments/14mrgpr/dynamically_scaled_rope_further_increases/.\n\n\n",
      "Fu et\u00a0al. (2024)": "\nFu et\u00a0al. (2024)\n\nYao Fu, Rameswar Panda, Xinyao Niu, Xiang Yue, Hannaneh Hajishirzi, Yoon Kim, and Hao Peng.\n\n\nData engineering for scaling language models to 128k context.\n\n\nIn International Conference on Machine Learning (ICML), 2024.\n\n\n",
      "Gao et\u00a0al. (2021)": "\nGao et\u00a0al. (2021)\n\nLeo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff, Jason Phang, Laria Reynolds, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou.\n\n\nA framework for few-shot language model evaluation, September 2021.\n\n\n",
      "Gao et\u00a0al. (2023)": "\nGao et\u00a0al. (2023)\n\nTianyu Gao, Howard Yen, Jiatong Yu, and Danqi Chen.\n\n\nEnabling large language models to generate text with citations.\n\n\nIn Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp.\u00a0 6465\u20136488, Singapore, December 2023. Association for Computational Linguistics.\n\n\n",
      "Goyal et\u00a0al. (2023)": "\nGoyal et\u00a0al. (2023)\n\nTanya Goyal, Junyi\u00a0Jessy Li, and Greg Durrett.\n\n\nNews summarization and evaluation in the era of gpt-3, 2023.\n\n\nURL https://arxiv.org/abs/2209.12356.\n\n\n",
      "Gu & Dao (2023)": "\nGu & Dao (2023)\n\nAlbert Gu and Tri Dao.\n\n\nMamba: Linear-time sequence modeling with selective state spaces, 2023.\n\n\n",
      "Guo et\u00a0al. (2024)": "\nGuo et\u00a0al. (2024)\n\nDaya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang, Guanting Chen, Xiao Bi, Yu\u00a0Wu, YK\u00a0Li, et\u00a0al.\n\n\nDeepseek-coder: When the large language model meets programming\u2013the rise of code intelligence.\n\n\narXiv preprint arXiv:2401.14196, 2024.\n\n\n",
      "Hendrycks et\u00a0al. (2021)": "\nHendrycks et\u00a0al. (2021)\n\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt.\n\n\nMeasuring massive multitask language understanding.\n\n\nIn International Conference on Learning Representations, 2021.\n\n\n",
      "Hovy et\u00a0al. (2001)": "\nHovy et\u00a0al. (2001)\n\nEduard Hovy, Laurie Gerber, Ulf Hermjakob, Chin-Yew Lin, and Deepak Ravichandran.\n\n\nToward semantics-based answer pinpointing.\n\n\nIn Proceedings of the First International Conference on Human Language Technology Research, 2001.\n\n\n",
      "Hsieh et\u00a0al. (2024)": "\nHsieh et\u00a0al. (2024)\n\nCheng-Ping Hsieh, Simeng Sun, Samuel Kriman, Shantanu Acharya, Dima Rekesh, Fei Jia, and Boris Ginsburg.\n\n\nRULER: What\u2019s the real context size of your long-context language models?\n\n\nIn First Conference on Language Modeling, 2024.\n\n\n",
      "Hu et\u00a0al. (2024a)": "\nHu et\u00a0al. (2024a)\n\nShengding Hu, Yuge Tu, Xu\u00a0Han, Chaoqun He, Ganqu Cui, Xiang Long, Zhi Zheng, Yewei Fang, Yuxiang Huang, Weilin Zhao, et\u00a0al.\n\n\nMinicpm: Unveiling the potential of small language models with scalable training strategies.\n\n\narXiv preprint arXiv:2404.06395, 2024a.\n\n\n",
      "Hu et\u00a0al. (2024b)": "\nHu et\u00a0al. (2024b)\n\nZhiyuan Hu, Yuliang Liu, Jinman Zhao, Suyuchen Wang, Yan Wang, Wei Shen, Qing Gu, Anh\u00a0Tuan Luu, See-Kiong Ng, Zhiwei Jiang, et\u00a0al.\n\n\nLongrecipe: Recipe for efficient long context generalization in large languge models.\n\n\narXiv preprint arXiv:2409.00509, 2024b.\n\n\n",
      "Ivison et\u00a0al. (2023)": "\nIvison et\u00a0al. (2023)\n\nHamish Ivison, Yizhong Wang, Valentina Pyatkin, Nathan Lambert, Matthew Peters, Pradeep Dasigi, Joel Jang, David Wadden, Noah\u00a0A. Smith, Iz\u00a0Beltagy, and Hannaneh Hajishirzi.\n\n\nCamels in a changing climate: Enhancing lm adaptation with Tulu 2, 2023.\n\n\n",
      "Jacobs et\u00a0al. (2023)": "\nJacobs et\u00a0al. (2023)\n\nSam\u00a0Ade Jacobs, Masahiro Tanaka, Chengming Zhang, Minjia Zhang, Shuaiwen\u00a0Leon Song, Samyam Rajbhandari, and Yuxiong He.\n\n\nDeepspeed ulysses: System optimizations for enabling training of extreme long sequence transformer models.\n\n\narXiv preprint arXiv:2309.14509, 2023.\n\n\n",
      "Jelassi et\u00a0al. (2024)": "\nJelassi et\u00a0al. (2024)\n\nSamy Jelassi, David Brandfonbrener, Sham\u00a0M. Kakade, and Eran Malach.\n\n\nRepeat after me: Transformers are better than state space models at copying, 2024.\n\n\n",
      "Jin et\u00a0al. (2024)": "\nJin et\u00a0al. (2024)\n\nHongye Jin, Xiaotian Han, Jingfeng Yang, Zhimeng Jiang, Zirui Liu, Chia-Yuan Chang, Huiyuan Chen, and Xia Hu.\n\n\nLLM maybe longLM: Selfextend LLM context window without tuning.\n\n\nIn Forty-first International Conference on Machine Learning, 2024.\n\n\n",
      "Kamradt (2024)": "\nKamradt (2024)\n\nGarrett Kamradt.\n\n\nNeedle in a haystack - pressure testing LLMs, 2024.\n\n\n",
      "Karpinska et\u00a0al. (2024)": "\nKarpinska et\u00a0al. (2024)\n\nMarzena Karpinska, Katherine Thai, Kyle Lo, Tanya Goyal, and Mohit Iyyer.\n\n\nOne thousand and one pairs: A\u201d novel\u201d challenge for long-context language models.\n\n\narXiv preprint arXiv:2406.16264, 2024.\n\n\n",
      "Kim et\u00a0al. (2024)": "\nKim et\u00a0al. (2024)\n\nYekyung Kim, Yapei Chang, Marzena Karpinska, Aparna Garimella, Varun Manjunatha, Kyle Lo, Tanya Goyal, and Mohit Iyyer.\n\n\nFABLES: Evaluating faithfulness and content selection in book-length summarization.\n\n\nIn First Conference on Language Modeling, 2024.\n\n\n",
      "Kocetkov et\u00a0al. (2023)": "\nKocetkov et\u00a0al. (2023)\n\nDenis Kocetkov, Raymond Li, Loubna\u00a0Ben allal, Jia LI, Chenghao Mou, Yacine Jernite, Margaret Mitchell, Carlos\u00a0Mu\u00f1oz Ferrandis, Sean Hughes, Thomas Wolf, Dzmitry Bahdanau, Leandro\u00a0Von Werra, and Harm de\u00a0Vries.\n\n\nThe stack: 3 TB of permissively licensed source code.\n\n\nTransactions on Machine Learning Research, 2023.\n\n\n",
      "Ko\u010disk\u00fd et\u00a0al. (2018)": "\nKo\u010disk\u00fd et\u00a0al. (2018)\n\nTom\u00e1\u0161 Ko\u010disk\u00fd, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl\u00a0Moritz Hermann, G\u00e1bor Melis, and Edward Grefenstette.\n\n\nThe NarrativeQA reading comprehension challenge.\n\n\nTransactions of the Association for Computational Linguistics, 6:317\u2013328, 2018.\n\n\n",
      "Krishna et\u00a0al. (2023)": "\nKrishna et\u00a0al. (2023)\n\nKalpesh Krishna, Erin Bransom, Bailey Kuehl, Mohit Iyyer, Pradeep Dasigi, Arman Cohan, and Kyle Lo.\n\n\nLongEval: Guidelines for human evaluation of faithfulness in long-form summarization.\n\n\nIn Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, pp.\u00a0 1650\u20131669, 2023.\n\n\n",
      "Kwiatkowski et\u00a0al. (2019)": "\nKwiatkowski et\u00a0al. (2019)\n\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew\u00a0M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov.\n\n\nNatural questions: A benchmark for question answering research.\n\n\nTransactions of the Association for Computational Linguistics, 7:452\u2013466, 2019.\n\n\n",
      "Larson et\u00a0al. (2019)": "\nLarson et\u00a0al. (2019)\n\nStefan Larson, Anish Mahendran, Joseph\u00a0J. Peper, Christopher Clarke, Andrew Lee, Parker Hill, Jonathan\u00a0K. Kummerfeld, Kevin Leach, Michael\u00a0A. Laurenzano, Lingjia Tang, and Jason Mars.\n\n\nAn evaluation dataset for intent classification and out-of-scope prediction.\n\n\nIn Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp.\u00a0 1311\u20131316, 2019.\n\n\n",
      "Lenz et\u00a0al. (2024)": "\nLenz et\u00a0al. (2024)\n\nBarak Lenz, Alan Arazi, Amir Bergman, Avshalom Manevich, Barak Peleg, Ben Aviram, Chen Almagor, Clara Fridman, Dan Padnos, et\u00a0al.\n\n\nJamba-1.5: Hybrid transformer-mamba models at scale.\n\n\narXiv preprint arXiv:2408.12570, 2024.\n\n\n",
      "Li et\u00a0al. (2024a)": "\nLi et\u00a0al. (2024a)\n\nJeffrey Li, Alex Fang, Georgios Smyrnis, Maor Ivgi, Matt Jordan, Samir Gadre, Hritik Bansal, Etash Guha, Sedrick Keh, Kushal Arora, Saurabh Garg, Rui Xin, Niklas Muennighoff, Reinhard Heckel, Jean Mercat, Mayee Chen, Suchin Gururangan, Mitchell Wortsman, Alon Albalak, Yonatan Bitton, Marianna Nezhurina, Amro Abbas, Cheng-Yu Hsieh, Dhruba Ghosh, Josh Gardner, Maciej Kilian, Hanlin Zhang, Rulin Shao, Sarah Pratt, Sunny Sanyal, Gabriel Ilharco, Giannis Daras, Kalyani Marathe, Aaron Gokaslan, Jieyu Zhang, Khyathi Chandu, Thao Nguyen, Igor Vasiljevic, Sham Kakade, Shuran Song, Sujay Sanghavi, Fartash Faghri, Sewoong Oh, Luke Zettlemoyer, Kyle Lo, Alaaeldin El-Nouby, Hadi Pouransari, Alexander Toshev, Stephanie Wang, Dirk Groeneveld, Luca Soldaini, Pang\u00a0Wei Koh, Jenia Jitsev, Thomas Kollar, Alexandros\u00a0G. Dimakis, Yair Carmon, Achal Dave, Ludwig Schmidt, and Vaishaal Shankar.\n\n\nDatacomp-lm: In search of the next generation of training sets for language models.\n\n\narXiv preprint arXiv:2406.11794, 2024a.\n\n\n",
      "Li et\u00a0al. (2023)": "\nLi et\u00a0al. (2023)\n\nShenggui Li, Fuzhao Xue, Chaitanya Baranwal, Yongbin Li, and Yang You.\n\n\nSequence parallelism: Long sequence training from system perspective.\n\n\nIn Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (eds.), Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp.\u00a0 2391\u20132404, Toronto, Canada, July 2023. Association for Computational Linguistics.\n\n\ndoi: 10.18653/v1/2023.acl-long.134.\n\n\nURL https://aclanthology.org/2023.acl-long.134.\n\n\n",
      "Li et\u00a0al. (2024b)": "\nLi et\u00a0al. (2024b)\n\nTianle Li, Ge\u00a0Zhang, Quy\u00a0Duc Do, Xiang Yue, and Wenhu Chen.\n\n\nLong-context llms struggle with long in-context learning.\n\n\narXiv preprint arXiv:2404.02060, 2024b.\n\n\n",
      "Lieber et\u00a0al. (2024)": "\nLieber et\u00a0al. (2024)\n\nOpher Lieber, Barak Lenz, Hofit Bata, Gal Cohen, Jhonathan Osin, Itay Dalmedigos, Erez Safahi, Shaked Meirom, Yonatan Belinkov, Shai Shalev-Shwartz, et\u00a0al.\n\n\nJamba: A hybrid transformer-mamba language model.\n\n\narXiv preprint arXiv:2403.19887, 2024.\n\n\n",
      "Liu et\u00a0al. (2024a)": "\nLiu et\u00a0al. (2024a)\n\nJiaheng Liu, Zhiqi Bai, Yuanxing Zhang, Chenchen Zhang, Yu\u00a0Zhang, Ge\u00a0Zhang, Jiakai Wang, Haoran Que, Yukang Chen, Wenbo Su, et\u00a0al.\n\n\nE^ 2-llm: Efficient and extreme length extension of large language models.\n\n\narXiv preprint arXiv:2401.06951, 2024a.\n\n\n",
      "Liu et\u00a0al. (2024b)": "\nLiu et\u00a0al. (2024b)\n\nNelson\u00a0F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang.\n\n\nLost in the Middle: How Language Models Use Long Contexts.\n\n\nTransactions of the Association for Computational Linguistics, 12:157\u2013173, 02 2024b.\n\n\n",
      "Liu et\u00a0al. (2021)": "\nLiu et\u00a0al. (2021)\n\nXingkun Liu, Arash Eshghi, Pawel Swietojanski, and Verena Rieser.\n\n\nBenchmarking natural language understanding services for building conversational agents.\n\n\nIn Increasing naturalness and flexibility in spoken dialogue interaction: 10th international workshop on spoken dialogue systems, pp.\u00a0 165\u2013183. Springer, 2021.\n\n\n",
      "Lu et\u00a0al. (2024)": "\nLu et\u00a0al. (2024)\n\nYi\u00a0Lu, Jing\u00a0Nathan Yan, Songlin Yang, Justin\u00a0T Chiu, Siyu Ren, Fei Yuan, Wenting Zhao, Zhiyong Wu, and Alexander\u00a0M Rush.\n\n\nA controlled study on long context extension and generalization in llms.\n\n\narXiv preprint arXiv:2409.12181, 2024.\n\n\n",
      "Ma et\u00a0al. (2022)": "\nMa et\u00a0al. (2022)\n\nXuezhe Ma, Chunting Zhou, Xiang Kong, Junxian He, Liangke Gui, Graham Neubig, Jonathan May, and Luke Zettlemoyer.\n\n\nMega: moving average equipped gated attention.\n\n\narXiv preprint arXiv:2209.10655, 2022.\n\n\n",
      "Mallen et\u00a0al. (2023)": "\nMallen et\u00a0al. (2023)\n\nAlex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Daniel Khashabi, and Hannaneh Hajishirzi.\n\n\nWhen not to trust language models: Investigating effectiveness of parametric and non-parametric memories.\n\n\nIn Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (eds.), Association for Computational Linguistics (ACL), pp.\u00a0 9802\u20139822, Toronto, Canada, July 2023. Association for Computational Linguistics.\n\n\n",
      "Mosaic ML (2022)": "\nMosaic ML (2022)\n\nMosaic ML.\n\n\nstreaming.\n\n\n<https://github.com/mosaicml/streaming/>, 2022.\n\n\n",
      "Ouyang et\u00a0al. (2022)": "\nOuyang et\u00a0al. (2022)\n\nLong Ouyang, Jeffrey Wu, Xu\u00a0Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et\u00a0al.\n\n\nTraining language models to follow instructions with human feedback.\n\n\nAdvances in Neural Information Processing Systems (NeurIPS), 35:27730\u201327744, 2022.\n\n\nURL https://proceedings.neurips.cc/paper_files/paper/2022/file/b1efde53be364a73914f58805a001731-Paper-Conference.pdf.\n\n\n",
      "Pan et\u00a0al. (2023)": "\nPan et\u00a0al. (2023)\n\nJane Pan, Tianyu Gao, Howard Chen, and Danqi Chen.\n\n\nWhat in-context learning \u201clearns\u201d in-context: Disentangling task recognition and task learning.\n\n\nIn Findings of the Association for Computational Linguistics: ACL 2023, pp.\u00a0 8298\u20138319. Association for Computational Linguistics, 2023.\n\n\n",
      "Paster et\u00a0al. (2024)": "\nPaster et\u00a0al. (2024)\n\nKeiran Paster, Marco\u00a0Dos Santos, Zhangir Azerbayev, and Jimmy Ba.\n\n\nOpenwebmath: An open dataset of high-quality mathematical web text.\n\n\nIn The Twelfth International Conference on Learning Representations, 2024.\n\n\n",
      "Paszke et\u00a0al. (2019)": "\nPaszke et\u00a0al. (2019)\n\nAdam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et\u00a0al.\n\n\nPytorch: An imperative style, high-performance deep learning library.\n\n\nAdvances in neural information processing systems, 32, 2019.\n\n\n",
      "Penedo et\u00a0al. (2023)": "\nPenedo et\u00a0al. (2023)\n\nGuilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Hamza Alobeidli, Alessandro Cappelli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay.\n\n\nThe refinedweb dataset for falcon llm: Outperforming curated corpora with web data only.\n\n\nIn A.\u00a0Oh, T.\u00a0Naumann, A.\u00a0Globerson, K.\u00a0Saenko, M.\u00a0Hardt, and S.\u00a0Levine (eds.), Advances in Neural Information Processing Systems, volume\u00a036, pp.\u00a0 79155\u201379172. Curran Associates, Inc., 2023.\n\n\n",
      "Penedo et\u00a0al. (2024)": "\nPenedo et\u00a0al. (2024)\n\nGuilherme Penedo, Hynek Kydl\u00ed\u010dek, Loubna\u00a0Ben allal, Anton Lozhkov, Margaret Mitchell, Colin Raffel, Leandro\u00a0Von Werra, and Thomas Wolf.\n\n\nThe fineweb datasets: Decanting the web for the finest text data at scale, 2024.\n\n\n",
      "Peng et\u00a0al. (2023)": "\nPeng et\u00a0al. (2023)\n\nBo\u00a0Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Stella Biderman, Huanqi Cao, Xin Cheng, Michael Chung, Leon Derczynski, Xingjian Du, Matteo Grella, Kranthi Gv, Xuzheng He, Haowen Hou, Przemyslaw Kazienko, Jan Kocon, Jiaming Kong, Bart\u0142omiej Koptyra, Hayden Lau, Jiaju Lin, Krishna Sri\u00a0Ipsit Mantri, Ferdinand Mom, Atsushi Saito, Guangyu Song, Xiangru Tang, Johan Wind, Stanis\u0142aw Wo\u017aniak, Zhenyuan Zhang, Qinghua Zhou, Jian Zhu, and Rui-Jie Zhu.\n\n\nRWKV: Reinventing RNNs for the transformer era.\n\n\nIn Findings of the Association for Computational Linguistics: EMNLP 2023, pp.\u00a0 14048\u201314077, 2023.\n\n\n",
      "Peng et\u00a0al. (2024)": "\nPeng et\u00a0al. (2024)\n\nBowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole.\n\n\nYaRN: Efficient context window extension of large language models.\n\n\nIn The Twelfth International Conference on Learning Representations, 2024.\n\n\n",
      "Press et\u00a0al. (2022)": "\nPress et\u00a0al. (2022)\n\nOfir Press, Noah Smith, and Mike Lewis.\n\n\nTrain short, test long: Attention with linear biases enables input length extrapolation.\n\n\nIn International Conference on Learning Representations (ICLR), 2022.\n\n\n",
      "Rae et\u00a0al. (2020)": "\nRae et\u00a0al. (2020)\n\nJack\u00a0W. Rae, Anna Potapenko, Siddhant\u00a0M. Jayakumar, Chloe Hillier, and Timothy\u00a0P. Lillicrap.\n\n\nCompressive transformers for long-range sequence modelling.\n\n\nIn International Conference on Learning Representations, 2020.\n\n\n",
      "Reid et\u00a0al. (2024)": "\nReid et\u00a0al. (2024)\n\nMachel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Jean-baptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, et\u00a0al.\n\n\nGemini 1.5: Unlocking multimodal understanding across millions of tokens of context.\n\n\narXiv preprint arXiv:2403.05530, 2024.\n\n\n",
      "Rubin & Berant (2023)": "\nRubin & Berant (2023)\n\nOhad Rubin and Jonathan Berant.\n\n\nLong-range language modeling with self-retrieval, 2023.\n\n\n",
      "Sakaguchi et\u00a0al. (2021)": "\nSakaguchi et\u00a0al. (2021)\n\nKeisuke Sakaguchi, Ronan\u00a0Le Bras, Chandra Bhagavatula, and Yejin Choi.\n\n\nWinogrande: An adversarial winograd schema challenge at scale.\n\n\nCommunications of the ACM, 64(9):99\u2013106, 2021.\n\n\n",
      "Shaham et\u00a0al. (2023)": "\nShaham et\u00a0al. (2023)\n\nUri Shaham, Maor Ivgi, Avia Efrat, Jonathan Berant, and Omer Levy.\n\n\nZeroSCROLLS: A zero-shot benchmark for long text understanding.\n\n\nIn Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Findings of the Association for Computational Linguistics: EMNLP 2023, pp.\u00a0 7977\u20137989, Singapore, December 2023. Association for Computational Linguistics.\n\n\n",
      "Shen et\u00a0al. (2022)": "\nShen et\u00a0al. (2022)\n\nZejiang Shen, Kyle Lo, Lauren Yu, Nathan Dahlberg, Margo Schlanger, and Doug Downey.\n\n\nMulti-lexsum: Real-world summaries of civil rights lawsuits at multiple granularities.\n\n\nIn Advances in Neural Information Processing Systems, volume\u00a035, pp.\u00a0 13158\u201313173. Curran Associates, Inc., 2022.\n\n\n",
      "Singh et\u00a0al. (2024)": "\nSingh et\u00a0al. (2024)\n\nAaditya\u00a0K Singh, Yu\u00a0Yang, Kushal Tirumala, Mostafa Elhoushi, and Ari\u00a0S Morcos.\n\n\nBrevity is the soul of wit: Pruning long files for code generation.\n\n\narXiv preprint arXiv:2407.00434, 2024.\n\n\n",
      "Soboleva et\u00a0al. (2023)": "\nSoboleva et\u00a0al. (2023)\n\nDaria Soboleva, Faisal Al-Khateeb, Robert Myers, Jacob\u00a0R Steeves, Joel Hestness, and Nolan Dey.\n\n\nSlimPajama: A 627B token cleaned and deduplicated version of RedPajama, 2023.\n\n\n",
      "Soldaini et\u00a0al. (2024)": "\nSoldaini et\u00a0al. (2024)\n\nLuca Soldaini, Rodney Kinney, Akshita Bhagia, Dustin Schwenk, David Atkinson, Russell Authur, Ben Bogin, Khyathi Chandu, Jennifer Dumas, Yanai Elazar, Valentin Hofmann, Ananya Jha, Sachin Kumar, Li\u00a0Lucy, Xinxi Lyu, Nathan Lambert, Ian Magnusson, Jacob Morrison, Niklas Muennighoff, Aakanksha Naik, Crystal Nam, Matthew Peters, Abhilasha Ravichander, Kyle Richardson, Zejiang Shen, Emma Strubell, Nishant Subramani, Oyvind Tafjord, Evan Walsh, Luke Zettlemoyer, Noah Smith, Hannaneh Hajishirzi, Iz\u00a0Beltagy, Dirk Groeneveld, Jesse Dodge, and Kyle Lo.\n\n\nDolma: an open corpus of three trillion tokens for language model pretraining research.\n\n\nIn Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp.\u00a0 15725\u201315788, Bangkok, Thailand, August 2024. Association for Computational Linguistics.\n\n\ndoi: 10.18653/v1/2024.acl-long.840.\n\n\nURL https://aclanthology.org/2024.acl-long.840.\n\n\n",
      "Su et\u00a0al. (2021)": "\nSu et\u00a0al. (2021)\n\nJianlin Su, Yu\u00a0Lu, Shengfeng Pan, Ahmed Murtadha, Bo\u00a0Wen, and Yunfeng Liu.\n\n\nRoformer: Enhanced transformer with rotary position embedding, 2021.\n\n\n",
      "Sun et\u00a0al. (2023)": "\nSun et\u00a0al. (2023)\n\nYutao Sun, Li\u00a0Dong, Shaohan Huang, Shuming Ma, Yuqing Xia, Jilong Xue, Jianyong Wang, and Furu Wei.\n\n\nRetentive network: A successor to transformer for large language models.\n\n\narXiv preprint arXiv:2307.08621, 2023.\n\n\n",
      "Sun et\u00a0al. (2024)": "\nSun et\u00a0al. (2024)\n\nYutao Sun, Li\u00a0Dong, Yi\u00a0Zhu, Shaohan Huang, Wenhui Wang, Shuming Ma, Quanlu Zhang, Jianyong Wang, and Furu Wei.\n\n\nYou only cache once: Decoder-decoder architectures for language models.\n\n\narXiv preprint arXiv:2405.05254, 2024.\n\n\n",
      "Together (2023)": "\nTogether (2023)\n\nTogether.\n\n\nRedpajama: An open source recipe to reproduce llama training dataset, April 2023.\n\n\n",
      "Waleffe et\u00a0al. (2024)": "\nWaleffe et\u00a0al. (2024)\n\nRoger Waleffe, Wonmin Byeon, Duncan Riach, Brandon Norick, Vijay Korthikanti, Tri Dao, Albert Gu, Ali Hatamizadeh, Sudhakar Singh, Deepak Narayanan, et\u00a0al.\n\n\nAn empirical study of mamba-based language models.\n\n\narXiv preprint arXiv:2406.07887, 2024.\n\n\n",
      "Wang et\u00a0al. (2024)": "\nWang et\u00a0al. (2024)\n\nJunxiong Wang, Daniele Paliotta, Avner May, Alexander\u00a0M Rush, and Tri Dao.\n\n\nThe mamba in the llama: Distilling and accelerating hybrid models.\n\n\narXiv preprint arXiv:2408.15237, 2024.\n\n\n",
      "Wettig et\u00a0al. (2024)": "\nWettig et\u00a0al. (2024)\n\nAlexander Wettig, Aatmik Gupta, Saumya Malik, and Danqi Chen.\n\n\nQuRating: Selecting high-quality data for training language models.\n\n\nIn Ruslan Salakhutdinov, Zico Kolter, Katherine Heller, Adrian Weller, Nuria Oliver, Jonathan Scarlett, and Felix Berkenkamp (eds.), Proceedings of the 41st International Conference on Machine Learning, volume 235 of Proceedings of Machine Learning Research, pp.\u00a0 52915\u201352971. PMLR, 21\u201327 Jul 2024.\n\n\n",
      "Wolf et\u00a0al. (2020)": "\nWolf et\u00a0al. (2020)\n\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le\u00a0Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander Rush.\n\n\nTransformers: State-of-the-art natural language processing.\n\n\nIn Qun Liu and David Schlangen (eds.), Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pp.\u00a0 38\u201345, Online, October 2020. Association for Computational Linguistics.\n\n\ndoi: 10.18653/v1/2020.emnlp-demos.6.\n\n\n",
      "Wu et\u00a0al. (2021)": "\nWu et\u00a0al. (2021)\n\nJeff Wu, Long Ouyang, Daniel\u00a0M Ziegler, Nisan Stiennon, Ryan Lowe, Jan Leike, and Paul Christiano.\n\n\nRecursively summarizing books with human feedback.\n\n\narXiv preprint arXiv:2109.10862, 2021.\n\n\n",
      "Wu et\u00a0al. (2024)": "\nWu et\u00a0al. (2024)\n\nWenhao Wu, Yizhong Wang, Yao Fu, Xiang Yue, Dawei Zhu, and Sujian Li.\n\n\nLong context alignment with short instructions and synthesized positions.\n\n\narXiv preprint arXiv:2405.03939, 2024.\n\n\n",
      "Xiao et\u00a0al. (2024a)": "\nXiao et\u00a0al. (2024a)\n\nChaojun Xiao, Pengle Zhang, Xu\u00a0Han, Guangxuan Xiao, Yankai Lin, Zhengyan Zhang, Zhiyuan Liu, Song Han, and Maosong Sun.\n\n\nInfLLM: Unveiling the intrinsic capacity of LLMs for understanding extremely long sequences with training-free memory.\n\n\narXiv preprint arXiv:2402.04617, 2024a.\n\n\n",
      "Xiao et\u00a0al. (2024b)": "\nXiao et\u00a0al. (2024b)\n\nGuangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis.\n\n\nEfficient streaming language models with attention sinks.\n\n\nIn The Twelfth International Conference on Learning Representations, 2024b.\n\n\n",
      "Xiong et\u00a0al. (2023)": "\nXiong et\u00a0al. (2023)\n\nWenhan Xiong, Jingyu Liu, Igor Molybog, Hejia Zhang, Prajjwal Bhargava, Rui Hou, Louis Martin, Rashi Rungta, Karthik\u00a0Abinav Sankararaman, Barlas Oguz, Madian Khabsa, Han Fang, Yashar Mehdad, Sharan Narang, Kshitiz Malik, Angela Fan, Shruti Bhosale, Sergey Edunov, Mike Lewis, Sinong Wang, and Hao Ma.\n\n\nEffective long-context scaling of foundation models, 2023.\n\n\n",
      "Xiong et\u00a0al. (2024)": "\nXiong et\u00a0al. (2024)\n\nZheyang Xiong, Vasilis Papageorgiou, Kangwook Lee, and Dimitris Papailiopoulos.\n\n\nFrom artificial needles to real haystacks: Improving retrieval capabilities in llms by finetuning on synthetic data.\n\n\narXiv preprint arXiv:2406.19292, 2024.\n\n\n",
      "Yang et\u00a0al. (2024a)": "\nYang et\u00a0al. (2024a)\n\nAn\u00a0Yang, Baosong Yang, Binyuan Hui, Bo\u00a0Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, et\u00a0al.\n\n\nQwen2 technical report.\n\n\narXiv preprint arXiv:2407.10671, 2024a.\n\n\n",
      "Yang et\u00a0al. (2024b)": "\nYang et\u00a0al. (2024b)\n\nSonglin Yang, Bailin Wang, Yikang Shen, Rameswar Panda, and Yoon Kim.\n\n\nGated linear attention transformers with hardware-efficient training.\n\n\nIn Forty-first International Conference on Machine Learning, 2024b.\n\n\n",
      "Yang et\u00a0al. (2018)": "\nYang et\u00a0al. (2018)\n\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher\u00a0D. Manning.\n\n\nHotpotQA: A dataset for diverse, explainable multi-hop question answering.\n\n\nIn Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pp.\u00a0 2369\u20132380, 2018.\n\n\n",
      "Yen et\u00a0al. (2024a)": "\nYen et\u00a0al. (2024a)\n\nHoward Yen, Tianyu Gao, and Danqi Chen.\n\n\nLong-context language modeling with parallel context encoding.\n\n\nIn Association for Computational Linguistics (ACL), pp.\u00a0 2588\u20132610, 2024a.\n\n\n",
      "Yen et\u00a0al. (2024b)": "\nYen et\u00a0al. (2024b)\n\nHoward Yen, Tianyu Gao, Minmin Hou, Ke\u00a0Ding, Daniel Fleischer, Peter Izsak, Moshe Wasserblat, and Danqi Chen.\n\n\nHelmet: How to evaluate long-context language models effectively and thoroughly.\n\n\n2024b.\n\n\n",
      "Yoshida et\u00a0al. (2020)": "\nYoshida et\u00a0al. (2020)\n\nDavis Yoshida, Allyson Ettinger, and Kevin Gimpel.\n\n\nAdding recurrence to pretrained transformers for improved efficiency and context size, 2020.\n\n\n",
      "Zellers et\u00a0al. (2019)": "\nZellers et\u00a0al. (2019)\n\nRowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi.\n\n\nHellaSwag: Can a machine really finish your sentence?\n\n\nIn Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp.\u00a0 4791\u20134800, 2019.\n\n\n",
      "Zhang & Bansal (2021)": "\nZhang & Bansal (2021)\n\nShiyue Zhang and Mohit Bansal.\n\n\nFinding a balanced degree of automation for summary evaluation.\n\n\nIn Empirical Methods in Natural Language Processing (EMNLP), pp.\u00a0 6617\u20136632, 2021.\n\n\n",
      "Zhang et\u00a0al. (2024a)": "\nZhang et\u00a0al. (2024a)\n\nXinrong Zhang, Yingfa Chen, Shengding Hu, Zihang Xu, Junhao Chen, Moo Hao, Xu\u00a0Han, Zhen Thai, Shuo Wang, Zhiyuan Liu, and Maosong Sun.\n\n\n\u221e\\infty\u221eBench: Extending long context evaluation beyond 100K tokens.\n\n\nIn Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp.\u00a0 15262\u201315277, 2024a.\n\n\n",
      "Zhang et\u00a0al. (2024b)": "\nZhang et\u00a0al. (2024b)\n\nYikai Zhang, Junlong Li, and Pengfei Liu.\n\n\nExtending llms\u2019 context window with 100 samples.\n\n\narXiv preprint arXiv:2401.07004, 2024b.\n\n\n",
      "Zhao et\u00a0al. (2024)": "\nZhao et\u00a0al. (2024)\n\nLiang Zhao, Tianwen Wei, Liang Zeng, Cheng Cheng, Liu Yang, Peng Cheng, Lijie Wang, Chenxia Li, Xuejie Wu, Bo\u00a0Zhu, et\u00a0al.\n\n\nLongskywork: A training recipe for efficiently extending context length in large language models.\n\n\narXiv preprint arXiv:2406.00605, 2024.\n\n\n",
      "Zhu et\u00a0al. (2024)": "\nZhu et\u00a0al. (2024)\n\nDawei Zhu, Nan Yang, Liang Wang, Yifan Song, Wenhao Wu, Furu Wei, and Sujian Li.\n\n\nPoSE: Efficient context window extension of LLMs via positional skip-wise training.\n\n\nIn The Twelfth International Conference on Learning Representations, 2024.\n\n\n"
    },
    "domain": "cs.CL",
    "citation_count": 0
  },
  "63258352-8356-4a79-96dc-31dd886a4d39": {
    "pk": "63258352-8356-4a79-96dc-31dd886a4d39",
    "authors": [
      "Xinrong Zhang",
      "Yingfa Chen",
      "Shengding Hu",
      "Zihang Xu",
      "Junhao Chen",
      "Moo Khai Hao",
      "Xu Han",
      "Zhen Leng Thai",
      "Shuo Wang",
      "Zhiyuan Liu",
      "Maosong Sun"
    ],
    "title": "$\\infty$Bench: Extending Long Context Evaluation Beyond 100K Tokens",
    "abstract": "Processing and reasoning over long contexts is crucial for many practical applications of Large Language Models (LLMs), such as document comprehension and agent construction. Despite recent strides in making LLMs process contexts with more than 100K tokens, there is currently a lack of a standardized benchmark to evaluate this long-context capability. Existing public benchmarks typically focus on contexts around 10K tokens, limiting the assessment and comparison of LLMs in processing longer contexts. In this paper, we propose $\\infty$Bench, the first LLM benchmark featuring an average data length surpassing 100K tokens. $\\infty$Bench comprises synthetic and realistic tasks spanning diverse domains, presented in both English and Chinese. The tasks in $\\infty$Bench are designed to require well understanding of long dependencies in contexts, and make simply retrieving a limited number of passages from contexts not sufficient for these tasks. In our experiments, based on $\\infty$Bench, we evaluate the state-of-the-art proprietary and open-source LLMs tailored for processing long contexts. The results indicate that existing long context LLMs still require significant advancements to effectively process 100K+ context. We further present three intriguing analyses regarding the behavior of LLMs processing long context.",
    "url": "http://arxiv.org/abs/2402.13718v3",
    "timestamp": 1708515029,
    "sections": {
      "1 Introduction": "\n\n1 Introduction\n\nFigure 1: The performance of GPT-4, Kimi-Chat, YaRN-Mistral, and Claude 2 on \u221e\\infty\u221eBench. A higher value represents better performance.\n\n\n\n\n\nBenchmark\nAvg Len\nEn\nZh\nCode\nMath\nNovel\nDialogue\nSynthetic\n\n\n\n\nLRA\u00a0(Tay et\u00a0al., 2020)\n\n\n\u223csimilar-to\\sim\u223c10K\n\u2713\n\u2717\n\u2717\n\u2713\n\u2717\n\u2717\n\u2713\n\n\nLongBench\u00a0(Bai et\u00a0al., 2023)\n\n\n\u223csimilar-to\\sim\u223c10K\n\u2713\n\u2713\n\u2713\n\u2717\n\u2713\n\u2713\n\u2713\n\n\nL-Eval\u00a0(An et\u00a0al., 2023)\n\n4K - 60K\n\u2713\n\u2717\n\u2713\n\u2713\n\u2717\n\u2717\n\u2713\n\n\nLooGLE\u00a0(Li et\u00a0al., 2023)\n\n\n\u223csimilar-to\\sim\u223c20K\n\u2713\n\u2717\n\u2717\n\u2717\n\u2717\n\u2713\n\u2717\n\n\n\n\u221e\\infty\u221eBench (ours)\n\n\u223csimilar-to\\sim\u223c200K\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\n\n\nTable 1: Comparison to existing long-context benchmarks and \u221e\\infty\u221eBench. \u201cEn\u201d and \u201cZh\u201d refer to English and Chinese tasks. \u201cCode\u201d, \u201cMath\u201d, \u201cNovel\u201d, \u201cDialogue\u201d indicate whether the domain includes tasks from those domains, and \u201cSynthetic\u201d indicates whether there are auto-generated tasks.\n\n\nIn recent years, large language models (LLMs) Brown et\u00a0al. (2020); OpenAI (2023a); Touvron et\u00a0al. (2023) have exhibited exceptional performance across a range of natural language processing (NLP) tasks\u00a0Qiu et\u00a0al. (2020); Han et\u00a0al. (2021). LLMs are showing a promising direction toward generalist task assistance, being capable of aiding users in practical tasks through conversational interactions. These tasks include web navigation\u00a0Nakano et\u00a0al. (2021), analysis of code repositories\u00a0Chen et\u00a0al. (2021), and extraction of useful information from documents\u00a0Ko\u010disk\u1ef3 et\u00a0al. (2018), indicating a step towards artificial general intelligence. For these LLM-based scenarios, the ability to process long contexts is increasingly critical, in addition to understanding fine-grained semantics and possessing extensive knowledge\u00a0Dong et\u00a0al. (2023); Huang et\u00a0al. (2023). Textual documents, historical dialogues, complex instructions, and cumbersome workflows, which constitute the data most directly processed in daily tasks, must be input to LLMs as long contexts for effective processing.\n\n\nDespite this growing importance, LLMs consistently face challenges in processing long contexts, primarily due to the substantial computational resources required for long sequence training\u00a0Dao et\u00a0al. (2022); Dao (2023) as well as the apparent inability to generalize to sequences longer than those encountered during training\u00a0Chen et\u00a0al. (2023a); Peng et\u00a0al. (2023b).\nLLMs are typically trained on sequences containing no more than 8K tokens\u00a0Touvron et\u00a0al. (2023); Penedo et\u00a0al. (2023); Biderman et\u00a0al. (2023), and thus cannot well handle contexts exceeding 8K tokens.\nThese limitations have largely restricted most LLMs from being applied to more complex tasks.\n\n\nRecent advancements in training infrastructure\u00a0Shoeybi et\u00a0al. (2019); Narayanan et\u00a0al. (2021); Dao et\u00a0al. (2022); Dao (2023), and efforts to improve length generalization\u00a0Anil et\u00a0al. (2022); Chen et\u00a0al. (2023b); Peng et\u00a0al. (2023b)333https://www.reddit.com/r/LocalLLaMA/comments/14lz7j5/ntkaware_scaled_rope_allows_llama_models_to_have/ have led to rapid developments in long-context LLMs.\nBased on these improved training infrastructures and length generalization methods, several LLMs have purportedly managed to process data exceeding 100K tokens\u00a0(Peng et\u00a0al., 2023b; OpenAI, 2023b; 01.AI, 2023b, a), with Claude 2\u00a0Anthropic (2023) and Kimi-Chat\u00a0AI (2023) even claiming to be able to process up to 200K tokens.\nHowever, the rapid emergence of long-context LLMs has outpaced the development of adequate evaluation benchmarks. Present long-context benchmarks predominantly feature contexts averaging around 10K tokens\u00a0(Bai et\u00a0al., 2023; Tay et\u00a0al., 2020), invariably falling below 100K tokens. This lag in the advancement of long-context evaluation methodologies impedes both the comparative analysis of diverse long-context LLMs and the pinpointing of potential enhancements in long-context processing.\n\n\nIn this work, we present \u221e\\infty\u221eBench , the first comprehensive benchmark featuring an average data length surpassing 100K tokens. \u221e\\infty\u221eBench includes tasks in different domains (novels, code, math, etc.) and languages (English and Chinese). To fully evaluate the performance of long-context LLMs, \u221e\\infty\u221eBench integrates synthetic tasks that can be auto-generated for even longer contexts (e.g., finding the top-k\ud835\udc58kitalic_k number in an array) in addition to a set of realistic tasks.\n\n\nTo construct tasks annotated by humans, we develop 5 annotation pipelines for detailed example annotation. These pipelines undergo iterative refinement until the examples meet quality standards. Auto-generated tasks, conversely, can be easily scaled to various lengths. Upon completing \u221e\\infty\u221eBench, we assess the performance of several state-of-the-art (SOTA) long-context LLMs on this benchmark to gauge its difficulty and evaluate the effectiveness of these models. The results show that current SOTA LLMs are not fully equipped to handle all tasks within \u221e\\infty\u221eBench, highlighting the ongoing challenge of enabling LLMs to process long contexts effectively. We also conduct intriguing analyses on the behavior of LLMs on such long contexts, including the task length ablation, the absent of \u201clost in the middle phenomenon\u00a0Liu et\u00a0al. (2023)\u201d, and the context recalling prompting techniques.\n\n\nOur contributions can be summarized as follows:\n\n\n\u2022\n\nWe construct and release \u221e\\infty\u221eBench, the first multi-domain bilingual benchmark for evaluating the ability to understand and reason over contexts surpassing 100K tokens.\n\n\n\n\u2022\n\nWe evaluate SOTA long-context LLMs on \u221e\\infty\u221eBench, which reveals severe performance degradation of these LLMs when scaling context lengths. These experimental results and analysis also indicate promising directions to improve long-context LLMs.\n\n\n\n\n",
      "2 Related Work": "\n\n2 Related Work\n\nExtending Context Length\n\nTransformers, typically trained on text sequences under 8K tokens due to self-attention\u2019s quadratic complexity, face challenges in longer downstream tasks. To address this, two main strategies have emerged: firstly, the development of positional encodings capable of handling longer text sequences\u00a0Sun et\u00a0al. (2022); Press et\u00a0al. (2021), and secondly, the refinement of inference stage techniques to extend current LLMs post-training. The primary approach involves modifying rotary positional encoding\u00a0Su et\u00a0al. (2023) and implementing post-training adjustments to better manage the increased relative positional distances in longer sequences\u00a0Zhu et\u00a0al. (2023); Peng et\u00a0al. (2023b); Chen et\u00a0al. (2023a).\n\n\n\n100K+ LLMs\n\nRecently, many LLMs have shown the ability to handle over 100K tokens. Some popular proprietary 100K+ LLMs include GPT-4, Claude 2\u00a0(Anthropic, 2023), and Kimi-Chat\u00a0(AI, 2023).\nOn the other hand, there are much fewer open-source 100K+ models. Some notable models include YaRN\u00a0(Peng et\u00a0al., 2023b) and Yi-200K\u00a0(01.AI, 2023a, b).\nIn this paper, we benchmark GPT-4, Claude 2, Kimi-Chat, and YaRN-Mistral-7B-128K444https://huggingface.co/NousResearch/Yarn-Mistral-7b-128k, we denote this model by YaRN-Mistral. on \u221e\\infty\u221eBench, which are some of the latest and strongest LLMs that claim to be able to handle over 100K tokens. \n\n\n\nInference Infrastructure\n\nNumerous studies aim to accelerate self-attention computation. Research primarily concentrates on refining attention mechanisms through improved IO management\u00a0(Dao et\u00a0al., 2022; Dao, 2023), memory optimization\u00a0(Kwon et\u00a0al., 2023; Shazeer, 2019; Ainslie et\u00a0al., 2023), and enhanced parallelization in decoding\u00a0(Dao et\u00a0al., 2023; Hong et\u00a0al., 2023). Approaches like Sliding Window Attention\u00a0(Beltagy et\u00a0al., 2020), LM-Infinite\u00a0(Han et\u00a0al., 2023), and StreamingLLM\u00a0(Xiao et\u00a0al., 2023) introduce attention variants for handling infinitely long sequences without overwhelming computation or memory overhead. However, these techniques often face challenges in maintaining historical information.\n\n\n\nLong Context Benchmarks\n\nSeveral benchmarks exist for evaluating long-context AI models, notably featuring context lengths of around 10K tokens. L-Eval\u00a0(An et\u00a0al., 2023) and LongBench\u00a0(Bai et\u00a0al., 2023) are prominent examples, aggregating pre-existing tasksKocisk\u00fd et\u00a0al. (2017); Dasigi et\u00a0al. (2021); Yang et\u00a0al. (2018); Huang et\u00a0al. (2021); Joshi et\u00a0al. (2017) into comprehensive benchmarks. LongBench encompasses four categories\u2014QA, summarization, synthetic retrieval, and code\u2014spanning 21 tasks, with four being novel. Conversely, L-Eval incorporates 18 tasks across QA, summarization, math, retrieval, and multiple-choice (MC) domains, introducing three new tasks. Another notable benchmark, LooGLE\u00a0(Li et\u00a0al., 2023), differentiates between short and long dependency examples, focusing on summary and QA tasks; its summary corpus contrasts with ours, utilizing academic papers over novels. The Long-Range Arena (LRA) (Tay et\u00a0al., 2020) further diversifies with six tasks in text, image, and math, designed for scalability. In comparison, \u221e\\infty\u221eBench stands out for its substantially longer contexts and a broader range of task domains. Table\u00a01 offers a detailed comparison of these long-context benchmarks.\n\n\nFigure 2: The statistics of the data in \u221e\\infty\u221eBench. The radius of each segment indicates the length of input plus output on the logarithmic scale, and the width (or angle) indicates the number of examples (proportionally to the total number of examples).\n\n\n",
      "3 \u221e\\infty\u221eBench": "\n\n3 \u221e\\infty\u221eBench\n\n\n\n\n\nTask\nAnnotation\n# Ex.\nAvg Len\n\n\n\n\nRet.PassKey\nAuto\n590\n122.4K/2\n\n\nRet.Number\nAuto\n590\n122.4K/4\n\n\nRet.KV\nAuto\n500\n121.1K/22.7\n\n\nEn.Sum\nHuman\n103\n103.5K/1.1K\n\n\nEn.QA\nHuman\n351\n192.6k/4.8\n\n\nEn.MC\nHuman\n229\n184.4K/5.3\n\n\nZh.QA\nHuman\n189\n2068.6K/6.3\n\n\nEn.Dia\nAuto\n200\n103.6K/3.4\n\n\nCode.Debug\nHuman\n394\n114.7K/4.8\n\n\nCode.Run\nAuto\n400\n75.2K/1.3\n\n\nMath.Calc\nAuto\n50\n43.9K/43.9K\n\n\nMath.Find\nAuto\n350\n87.9K/1.3\n\n\n\nTable 2: Data statistics. The columns indicate whether the annotation was auto-generated or done by humans, the number of examples, and the average length (input/output) in tokens.\n\n\n\n\n\nTask\nGPT-4\nYaRN-Mistral\nKimi-Chat\nClaude 2\n\n\n\n\nRetrieve.PassKey\n100.00\n92.71\n98.14\n97.80\n\n\nRetrieve.Number\n100.00\n56.61\n95.42\n98.14\n\n\nRetrieve.KV\n89.00\n0.00\n53.60\n65.40\n\n\nEn.Sum\n14.73\n9.09\n17.93\n14.45\n\n\nEn.QA\n22.22\n9.55\n16.52\n11.97\n\n\nEn.MC\n67.25\n27.95\n72.49\n62.88\n\n\nEn.Dia\n8.50\n7.50\n11.50\n46.50\n\n\nZh.QA\n23.06\n16.98\n18.62\n10.53\n\n\nCode.Debug\n39.59\n0.76\n18.02\n2.28\n\n\nCode.Run\n23.25\n1.25\n2.00\n2.50\n\n\nMath.Calc\n0.01\n0.00\n0.00\n0.00\n\n\nMath.Find\n60.00\n17.14\n12.57\n32.29\n\n\nAverage\n45.63\n19.96\n34.73\n37.06\n\n\n\nTable 3: Main results. The performance of the baselines in \u221e\\infty\u221eBench. For multiple-choice questions, if the model does not output one of the options, we regard it as an empty prediction, and thus give it a score of 0.\n\n\n\u221e\\infty\u221eBench encompasses 12 tasks spanning 5 domains: retrieval, code, math, novels, and dialogue. Two of these tasks are derived from existing literatureMohtashami and Jaggi (2023); Liu et\u00a0al. (2023). Among the newly introduced tasks, half are generated automatically, while the remainder are annotated by humans.\n\n\nIn total, \u221e\\infty\u221eBench includes 3946 examples, featuring a length beyond 100K tokens (average approximately 200K). Figure\u00a02 illustrates the distribution of these tasks. Table\u00a02 details their respective input and output lengths as well as the number of examples per task.\n\n\nNext, we illustrate each task in detail. The tasks can be grouped into two broad categories. The first involves realistic context collected from real-world scenarios which has potential practical usage of long context LLMs. The second depends on synthetic contexts which are created or collected for testing certain capabilities of long-context LLMs.\n\n\n\n3.1 Realistic Context\n\n\n3.1.1 Novel\n\nWe develop novel-based tasks as outlined in Figure\u00a03, utilizing novels sourced from websites555https://www.sparknotes.com/666https://www.cliffsnotes.com/ and are manually filtered. More annotation information in Appendix.\u00a0C.\n\n\nIn these tasks, models are tasked with reasoning over entire novels presented during inference. Recognizing that many novels, along with their movie adaptations and related discussions, are accessible online and may have been encountered by LLMs during training, we adopt key entity replacement as a countermeasure. This involves substituting prominent entities determined by annotators, such as main character names, with unrelated ones, creating \u201cfake novels\u201d.\n\n\nUsing these altered novels, we design tasks in three formats: summarization, open-form question answering (QA), and multiple-choice (MC) questions, applying key entity replacement to the annotations as well. All English tasks share the same set of modified novels.\n\n\nFigure 3: The annotation pipelines for the human-annotated tasks in \u221e\\infty\u221eBench.\n\n\nEn.Sum\n\nThe En.Sum task requires models to generate a concise summary of the novel. Gold standard labels are sourced from the web and undergo manual filtering to remove non-summarization content, like comments. Model performance is evaluated using the ROUGE-L-Sum metric\u00a0(Lin, 2004).\n\n\n\nEn.QA & Zh.QA\n\nWe employ the same annotation pipeline for both En.QA and Zh.QA tasks, ensuring that the questions necessitate long-range dependency and reasoning, beyond simple short passage retrieval. The tasks are primarily categorized into two types of reasoning:\n\n\n\u2022\n\nAggregation: This involves compiling various pieces of information scattered throughout the novel. An example question in \u221e\\infty\u221eBench is \u201cHow much money in total did A spend on lunch?\u201d\n\n\n\n\u2022\n\nFiltering: This requires identifying specific information from a larger set. An example question in \u221e\\infty\u221eBench is \u201cWhat color dress did A wear when A met B for the second time?\u201d\n\n\n\n\n\nThese tasks test LLMs to locate and process information within the novel, performing reasoning through aggregation or filtering to derive answers.\n\n\n\nEn.MC\n\nThe En.MC task is annotated similarly to En.QA, but differs in that the model is presented with four answer choices. Annotators are instructed to craft these options to be challenging.\n\n\n\n\n\n3.1.2 Dialogue\n\nEn.Dia\n\nThe construction process for the En.Dia task is depicted in Figure\u00a03. We gather movie and drama scripts from a designated online database777https://imsdb.com/, focusing on a corpus of long, multi-role dialogues. Only the English scripts are retained and necessary cleaning is applied.\n\n\nIn the En.Dia task, random instances of character names within a script are replaced with $$MASK$$. The objective is to correctly identify these masked names. For scripts falling short of 100K tokens, we augment them by padding with additional scripts.\n\n\n\n\n\n3.1.3 Code\n\nCode.Debug\n\nWe develop the task as per the process illustrated in Figure\u00a03. Code repositories, sourced from PyPI888https://pypi.org/, undergo a filtering process, and those outside the 64K to 256K token range are excluded (tokenization via the tiktoken tokenizerOpenAI (2023c)). Each repository is transformed into a single file, aggregating the content from all files within, each prefaced by its relative path to the root directory. Three of the authors then insert a deliberate and obvious error into one function per repository. The options are presented in the Class.Function format. Six methods are employed for bug insertion: (1) deleting a necessary variable declaration; (2) using an incorrect number of arguments in function calls; (3) creating infinite loops; (4) causing indentation errors; (5) substituting references with undefined variable/function names; (6) introducing blatant syntax errors (e.g., non-closed brackets).\n\n\nInitial results indicate that this task is too challenging for current LLMs (None of the baseline models can identify the most obvious error such as non-closed brackets). To mitigate this, we offer four answer choices, one containing the injected bug and the others are bug-free. Note that this makes many examples easily solved by external retrieval preprocess. However, we encourage the users not to use external retrieval preprocess to keep the evaluation fair. And we are looking forward to the stage where LLMs can directly solve the problem without selection options.\n\n\n\n\n\n\n3.2 Synthetic Context\n\nThe second category of tasks is characterized by a synthetic context. These tasks, devoid of direct real-world application or use case, are engineered to evaluate the capability for processing lengthy contexts. We delineate four essential ability for effective long-context processing:\n\n\n\n\n1.\n\nLocation and retrieval. This encompasses all retrieval tasks.\n\n\n\n2.\n\nElevated information resolution. This involves the Retrieve.Number task.\n\n\n\n3.\n\nState preservation. This incorporates the Code.Run and Math.Find functions.\n\n\n\n4.\n\nSequential processing. This utilizes the Math.Calc function.\n\n\n\n\n\n\n3.2.1 Retrieve\n\nIn retrieval tasks, models retrieve specific character sequences from lengthy contexts with predominantly irrelevant content. Such tests, adaptable for any context length, can assess the impact of information placement on model performance, like the lost-in-the-middle phenomenon\u00a0(Liu et\u00a0al., 2023). The three retrieval tasks in \u221e\\infty\u221eBench vary in complexity.\n\n\nRetrieve.PassKey\n\nThis task is first proposed by Mohtashami and Jaggi (2023). Models are prompted to find a specific <key> called pass key, which is a random 5-digit sequence. The pass key is inserted into a lengthy and noisy context, as shown below. In \u221e\\infty\u221eBench, we generate examples with 59 different pass key locations that are evenly distributed in the context. At each location, we construct 10 examples with different pass keys. This results in 590 examples.\n\n\n\n\n\nThere is an important pass key hidden in a lot of irrelevant text. Find it.\n<very long noisy context>\nThe pass key is <key>. Remember it. The pass key is <key>\n<very long noisy context>\nWhat is the pass key?\n\n\n\n\nRetrieve.Number\n\nTo examine the local attention of LLMs, we have enhanced the complexity of Retrieve.PassKey by increasing the answer length to 10 digits and incorporating successive repetitive digits. For example, a <key> in Retrieve.PassKey valued 98762, while in Retrieve.Number is 9998877762. This modification aims to assess the local resolution capabilities of long context models, as our preliminary experiments indicate that LLMs struggle with discerning repeated numbers.\n\n\n\nRetrieve.KV\n\nLiu et\u00a0al. (2023) introduce a key-value retrieval task within a large JSON object containing many key-value pairs (e.g., 30eea139-b6dd-43fc-bc5d-0d3d17980229 \u2192\u2192\\rightarrow\u2192 bfd36c2b-c57e-41ef-9cc1-b21b4e60e664). This task demands the model to accurately identify and retrieve the value corresponding to a specified key. The complexity of this task is heightened due to the indistinguishable format of relevant and irrelevant information.\n\n\n\n\n\n3.2.2 Code\n\nCode.Run\n\nIn this task, we evaluate the ability of LLMs to simulate multi-step function executions that involve basic arithmetic operations. While this task is readily solvable using a Python interpreter, the focus here is on the long-term state tracking required in such tasks. The capability of state tracking has been demonstrated in GPT-4\u00a0(Bubeck et\u00a0al., 2023). Specifically, the task involves creating Python code consisting of multiple simple functions, incorporating operations such as addition, subtraction, and nested function calls. The structural design of these tasks is as follows:\n\n\n\n\u2b07\n\n\n\ndef\u00a0func_0(x):\n\n\nreturn\u00a0func_1(x)\u00a0+\u00a04\n\n\\pardef\u00a0func_1(x):\n\n\nreturn\u00a0x\u00a0-\u00a01\n\n\n\n\nSome functions\u2019 return values are dependent on other functions (e.g., func_0 invokes func_1). We define depth as the number of cascading function calls initiated by a single call. Thus, the depth for func_1\u2019s call within func_0 is 1. In Code.Run, we employ depths ranging from 2 to 10, ensuring each function calls at most one other function. To keep the simplicity of each single step of computation, these functions are restricted to performing only addition and subtraction.\n\n\n\n\n\n3.2.3 Math\n\nMath.Find\n\nMath.Find assesses the model\u2019s capability to identify specific elements within a large array, requiring comprehensive observation for accuracy. This task also tests the ability to preserve states while encoding the context. Concretely, the model receives an extensive list of numbers and is tasked with locating one of seven key numbers: the three largest (1st, 2nd, and 3rd), the three smallest (1st, 2nd, and 3rd), and the median.\n\n\n\nMath.Calc\n\nTo assess sequential processing skills, Math.Calc prompts the model to compute the result of a lengthy arithmetic expression featuring addition and subtraction. Initial experiments indicate that current LLMs struggle to directly produce the final answer. Hence, we instead query the LLMs to provide the intermediate result following each operator. Model performance is evaluated based on the number of correct values preceding the first error.\n\n\n\n\n",
      "4 Experiments": "\n\n4 Experiments\n\nWe conduct a thorough set of experiments on \u221e\\infty\u221eBench. We will introduce the baselines, experimental setup, and main results in this section.\n\n\n\n\n4.1 Baselines\n\n\u221e\\infty\u221eBench generally requires the ability to handle input contexts longer than 100k. There is a handful of LLMs that claim to be capable of handling contexts over 100k. We include four baselines. The first three are proprietary LLMs as we do not have access to the model, while the last baseline is open-sourced. Details on evaluation are in Appendix.\u00a0D.\n\n\nGPT-4\n\nGPT by OpenAI is one of the most widely used and capable LLMs in the market, and a recent version of GPT-4 \u00a0(OpenAI, 2023b) can support 128K contexts.\n\n\n\n\nClaude 2\n\nClaude 2\u00a0(Anthropic, 2023) is a proprietary chat-based LLM released by Anthropic AI and has shown impressive capabilities. The second version of the Claude series supports 200K contexts. We manually enter each example through the webpage because we have no access to their API.\n\n\n\n\nKimi-Chat\n\nKimi-Chat, a proprietary chat-oriented LLM developed by Moonshot AI AI (2023), is designed to process contexts up to 200K. Due to the lack of API access, we manually input the test data using their web interface.\n\n\n\nYaRN-Mistral\n\nYaRN-Mistral is a derivative of Mistral-7B\u00a0(Jiang et\u00a0al., 2023) introduced by Peng et\u00a0al. (2023b). The original Mistral-7B was trained on input lengths up to 8K and shows a reduced performance in longer contexts. Peng et\u00a0al. (2023b) adapted it to 128K contexts by modifying the position encoding and continued training.\n\n\n\n\n\n4.2 Experimental Setup\n\nPrompt Templates\n\nFor each model-task combination, we craft prompts to optimize model performance on short dummy examples. Detailed prompt templates for each model and task can be found in Appendix\u00a0B.\n\n\n\nInput Truncation\n\nAll API-based baselines are subject to a maximum input length limit and will reject inputs exceeding this threshold. While YaRN-Mistral is theoretically capable of handling longer contexts, the authors only claim abilities up to 128K. Therefore, inputs are truncated by removing the center and joining both ends. This approach is predicated on the assumption that key information, such as instructions and book titles, is typically located at either the start or the end of a prompt.\n\n\n\n\n\n4.3 Main Result\n\nFigure 4: Baseline performance as a function of input length.\n\n\nTable\u00a03 and Figure\u00a01 display the performances of various baselines on \u221e\\infty\u221eBench. Notably, GPT-4 outperforms other baselines in the retrieval, code, and math domains, with a considerably higher average score. However, in the novel-based tasks, no distinct winner emerges among the proprietary LLMs. On the other hand, the open-source YaRN-Mistral lags behind the proprietary models in most tasks, exhibiting almost random performance in multiple areas. This aligns with its relatively inferior performance in shorter contexts compared to these models. Additionally, it is observed that the baselines generally excel more in retrieval tasks than in other areas, echoing the relative simplicity of these tasks for human participants.\n\n\n",
      "5 Analysis": "\n\n5 Analysis\n\nWe subsequently perform a detailed analysis of the results, identifying and emphasizing several notable and interesting phenomena.\n\n\n\n5.1 Length Ablation\n\nIn line with our benchmark\u2019s goal to assess proficiency in managing lengthy contexts, we verify the baselines\u2019 capability with shortened context versions. A subset of the auto-generated tasks is modified accordingly, and the performance outcomes are illustrated in Figure 4. It is observed that model performance generally declines with longer input lengths compared to shorter ones. This suggests that while these baselines are technically equipped to handle extended inputs, their effectiveness diminishes significantly under such conditions.\n\n\n\n\n\n5.2 Lost in the middle\n\nFigure 5: Performance as a function of the answer position (in the number of characters). The steep drop in performance for Kimi-Chat in the middle on Retrieval.KV is caused by the answer being removed by truncation.\n\n\n\n\nOne function in this repo is deliberately made to include an obvious error. Find it.\n<code>\nThink step by step and at last give me your answer for the function with the deliberate error.\n<list of options>\n\n\n\n\n\nOne function in this repo is deliberately made to include an obvious error. Find it.\n<code>\nLocate the functions in the options, repeat their content, inspect through code, and at last give me your answer for the function with the deliberate error.\n<list of options>\n\n\nFigure 6: Compared to the first prompt, the second prompt improves GPT-4\u2019s results on Code.Debug dramatically.\n\n\nPrior research indicates a performance decline in some LLMs when answers are positioned around the center of the context\u00a0(Liu et\u00a0al., 2023). However, our findings do not strongly corroborate this. As depicted in Figure\u00a05, we analyze model performance based on answer location in three location-dependent tasks. We observe no consistent trend between performance and answer position across different models. For instance, GPT-4 shows a preference for early answers in Retrieval.KV but favors later ones in En.Dia. In contrast, Claude 2\u2019s performance remains relatively unaffected by answer position on all three tasks, whereas YaRN-Mistral and Kimi-Chat excel with end-positioned answers (except that YaRN-Mistral get zero performance on all positions on Retrieval.KV).\n\n\nOne plausible reason why we have different observations from Liu et\u00a0al. (2023) is that they experiment with different models using at most 16K length contexts, which is about 8 times shorter than our setting. The models in their study are also different from ours.\nFinally, the tasks are different: their experiments involve document question answering (and their result with Retrieval.KV arguably does not show a very pronounced performance drop as well). We hypothesize that the phenomenon of \u201cLost in the middle\u201d is only exhibited on specific tasks and models. A more thorough investigation of these differences is beyond the scope of this paper.\n\n\n\n\n5.3 Context Recalling\n\nWe identify an intriguing prompting technique for tasks involving extended context, termed context recalling. This technique posits that, although the information is present in the context and accessible via direct attention, it may be more effective to first prompt the model to recall the relevant information in its generation before engaging in further reasoning. In our experiments using Code.Debug, when we merely instructed GPT-4 to process information step-by-step, the accuracy was 15.74%. However, by explicitly directing GPT-4 to repeat the relevant code before analysis, its accuracy on Code.Debug markedly improved to 39.59%. This approach of context recalling warrants additional investigation.\n\n\n",
      "6 Conclusions": "\n\n6 Conclusions\n\nWe introduce \u221e\\infty\u221eBench, the first benchmark tailored for long contexts exceeding 100K in average length. Empirical evidence indicates that despite claims of proficiency with such extensive contexts, current LLMs demonstrate significant performance degradation when dealing with them. This finding highlights the need for advanced methodologies to improve LLMs\u2019 efficiency in processing long context. Additionally, our analysis offers insights into LLM behavior in long-context tasks, guiding future research.\n\n\n\n",
      "Limitations": "\nLimitations\n\nWhile our benchmark offers valuable insights into LLM performance, it may not be sufficiently diverse or extensive to provide a comprehensive assessment of model capabilities, a constraint common to most benchmarks. Additionally, the reliance on exact match for scoring, dependent on prompt templates and answer parsing methods, may necessitate tailored redesigns for new model evaluations.\n\n\nFurthermore, supporting contexts up to 100K tokens may fall short for applications requiring analysis of extensive datasets, such as multiple books or entire databases. Exploring LLMs\u2019 capacity to handle up to a million tokens or more presents a promising research avenue. In practical applications, finetuning models to memorize context, rather than processing it during inference, could offer a more efficient alternative, albeit with significant computational demands.\n\n",
      "Ethics Statement": "\nEthics Statement\n\nOur human annotators are directed to exclude data that may raise sensitive ethical issues, such as offensive language or social biases. Nonetheless, the potential for encountering sensitive content persists, particularly if the sourced books or code contain such material. This concern is somewhat mitigated since the benchmark\u2019s primary focus is on evaluating the long-context capabilities of LLMs, rather than influencing their social bias.\n\n\nThe goal of this research is to advance the development of LLMs\u2019 proficiency in handling extensive contexts. This could aid in implementing more effective \u201cguardrails\u201d against misuse by incorporating detailed specifications prior to user interactions. However, this approach also potentially increases the risk of novel prompt injection attacks.\n\n\n",
      "Appendix A RWKV": "\n\nAppendix A RWKV\n\nRWKV\u00a0(Peng et\u00a0al., 2023a) is an architecture that combines the power of the transformer architecture\u00a0(Vaswani et\u00a0al., 2017) and recurrent neural network\u00a0(Hochreiter and Schmidhuber, 1997). Its training process can be parallelized while the inference procedure is recurrent, enabling O\u2062(1)\ud835\udc421O(1)italic_O ( 1 ) complexity during inference. Hence, the memory usage does not scale with context length, allowing it to support arbitrary-length inputs. We use the RKWV-4-World-7B version of this model series. However, we should keep in mind that this model was not trained on inputs of this length.\n\n\n\n\n\nModel\nRetrieve.PassKey Acc.\n\n\n\n\nGPT-4 Turbo\n100%\n\n\nYaRN-Mistral\n92.71%\n\n\nKimi-Chat\n98.14%\n\n\nClaude 2\n97.80%\n\n\nRWKV-4-World-7B\n0.00%\n\n\n\nTable 4: Results in Retrieve.PassKey with RWKV-4-World-7B. Since RWKV-4 was only trained on 4k sequences, it has zero performance on \u221e\\infty\u221eBench. It outputs only unintelligible content in this test.\n\n\nTable\u00a04 shows the performance of RWKV-4-World-7 in comparison to our baselines. We find that RWKV-4-World-7B outputs unintelligible texts on our benchmark, which causes it to achieve zero performance on Retrieve.PassKey, which is the easiest task for other baselines. This is likely because this model was not trained on inputs of this length and suffers from train-test domain shift.999We emphasize that this result is not evidence that the architecture of RWKV is incapable of handling lengthy inputs. Therefore, we do not consider testing it on other tasks in our benchmark.\n\n",
      "Appendix B Prompt Templates": "\n\nAppendix B Prompt Templates\n\nIn the following templates, many tasks has an <input> part that is provided in each example. Generally, they are a short question-like text that tells the model what it is supposed to do. One example is \u201cWhat is the pass key?\u201d.\n\n\n\nB.1 Retrieve.PassKey\n\nThe prompt below applies to GPT-4, Claude 2, and Kimi-Chat.\n\n\n\n\nThere is an important info hidden inside a lot of irrelevant text. Find it and memorize them. I will quiz you about the important information there.\n<context>\n<input>\n\n\n\nThe prompt below applies to YaRN-Mistral.\n\n\n\n\nThere is an important info hidden inside a lot of irrelevant text. Find it and memorize them. I will quiz you about the important information there.\n<context>\n<input>\nThe pass key is\n\n\n\n\n\nB.2 Retrieve.Number\n\nThe prompt below applies to GPT-4, Claude 2, and Kimi-Chat.\n\n\n\n\n\nThere is an important info hidden inside a lot of irrelevant text. Find it and memorize them. I will quiz you about the important information there.\n<context>\n<input>\n\n\n\nThe prompt below applies to YaRN-Mistral.\n\n\n\n\nThere is an important info hidden inside a lot of irrelevant text. Find it and memorize them. I will quiz you about the important information there.\n<context>\n<input>\nThe sequence of digits is\n\n\n\n\n\nB.3 Retrieve.KV\n\n\n\nExtract the value corresponding to the specified key in the JSON object below.\n<context>\n<input>\n\n\n\n\n\nB.4 En.Sum\n\nThe prompt below applies to GPT-4, Claude 2, and Kimi-Chat.\n\n\n\n\nSummarize the book below.\n<context>\n\n\n\nThe prompt below applies to YaRN-Mistral.\n\n\n\n\nSummarize the book below.\n<context>\nSummary:\n\n\n\n\n\nB.5 En.QA\n\nThe prompt below applies to GPT-4, Claude 2, and Kimi-Chat.\n\n\n\n\nRead the book below and answer a question.\n<context>\nQuestion: <question>\nBe very concise.\n\n\n\nThe prompt below applies to YaRN-Mistral.\n\n\n\n\nRead the book below and answer a question. Be very concise in your answer.\n<context>\nQuestion: <question>\nAnswer:\n\n\n\n\n\nB.6 En.MC\n\nThe prompt below applies to GPT-4, Claude 2, and Kimi-Chat.\n\n\n\n\nRead the book and answer the question.\n<context>\nQuestion: <question>\nOnly one of the following options is correct, tell me the answer using one single letter (A, B, C, or D). Don\u2019t say anything else.\nA. <OPTION_A>\nB. <OPTION_B>\nC. <OPTION_C>\nD. <OPTION_D>\n\n\n\nThe prompt below applies to YaRN-Mistral.\n\n\n\n\nRead the book and answer the question.\n<context>\nQuestion: <question>\nOnly one of the following options is correct, tell me the answer using one single letter (A, B, C, or D). Don\u2019t say anything else.\nA. <OPTION_A>\nB. <OPTION_B>\nC. <OPTION_C>\nD. <OPTION_D>\nThe correct option is:\n\n\n\n\n\nB.7 En.Dia\n\nThe prompt below applies to GPT-4, Claude 2, and Kimi-Chat.\n\n\n\n\nBelow is a dialogue script where one random occurrence of a character name is replaced with $$MASK$$, and you should try to guess who that character is.\nThe dialogue:\n\u2014\n<context>\n\u2014\nEnd of dialogue.\nWhich character is most likely $$MASK$$? Just say the name used by the scriptwriter (before the colon marks) of one single character and nothing else.\n\n\n\nThe prompt below applies to YaRN-Mistral.\n\n\n\n\nBelow is a dialogue script where one random occurrence of a character name is replaced with $$MASK$$, and you should try to guess who that character is.\nThe dialogue:\n\u2014\n<context>\n\u2014\nEnd of dialogue.\nThe name that has been replaced with $$MASK$$ is likely:\n\n\n\n\n\nB.8 Zh.QA\n\nThe prompt below applies to GPT-4, Claude 2, and Kimi-Chat.\n\n\n\n\n\n\u8bf7\u6839\u636e\u4ee5\u4e0b\u4e66\u7c4d\u56de\u7b54\u6211\u7684\u95ee\u9898\u3002(Read the book and answer the question.)\n<context>\n\u95ee\u9898\uff1a(Question: )<question>\n\u8bf7\u5c3d\u91cf\u7b80\u77ed\u5730\u56de\u7b54\u3002(Be very concise.)\n\n\n\nThe prompt below applies to YaRN-Mistral.\n\n\n\n\n\u8bf7\u6839\u636e\u4ee5\u4e0b\u4e66\u7c4d\u56de\u7b54\u6211\u7684\u95ee\u9898\u3002(Read the book and answer the question.)\n<context>\n\u95ee\u9898\uff1a(Question: )<question>\n\u7b54\u6848\uff1a(Answer:)\n\n\n\n\n\nB.9 Code.Debug\n\nThe prompt below applies to GPT-4, Claude 2, and Kimi-Chat.\n\n\n\n\n\nThere is ONLY ONE function in the large project that is deliberately made to include an obvious error. Please find the function that contains the most obvious errors. I will give you four options to narrow your scope. You can inspect through the options and think. Eventually, tell me the answer using one single letter (A, B, C, or D).\n<context>\nWhich function has deliberate error?\nA. <OPTION_A>\nB. <OPTION_B>\nC. <OPTION_C>\nD. <OPTION_D>\nYou should first find the functions in the options. Repeat their content, inspect through code, and at last give me your answer for the function that has the deliberate and obvious error in A, B, C, or D.\n\n\n\nThe prompt below applies to YaRN-Mistral.\n\n\n\n\nThere is ONLY ONE function in the large project that is deliberately made to include an obvious error. Please find the function that contains the most obvious errors. I will give you four options to narrow your scope. You can inspect through the options and think. Eventually, tell me the answer using one single letter (A, B, C, or D).\n<context>\nWhich function has deliberate error?\nA. <OPTION_A>\nB. <OPTION_B>\nC. <OPTION_C>\nD. <OPTION_D>\nYou should first find the functions in the options. Repeat their content, inspect through code, and at last give me your answer for the function that has the deliberate and obvious error in A, B, C, or D.\nThe correct option is:\n\n\n\n\n\nB.10 Code.Run\n\nThe prompt below applies to GPT-4, Claude 2, and Kimi-Chat.\n\n\n\n\nFollowing is a set of Python functions. There is a function called named func_1.\n<context>\nPlease give me the exact number of the return value of func_1(3). Be concise. Your response must end with the final returned value.\n\n\n\nThe prompt below applies to YaRN-Mistral.\n\n\n\n\nFollowing is a set of Python functions. There is a function called named <function name>.\n<context>\nPlease compute the exact value of <function call>. The value of <function call> is\n\n\n\n\n\nB.11 Math.Calc\n\nThe prompt below is used by GPT-4101010It should be noted that, when using other templates, GPT-4 has a strong tendency to reject to perform this task by claiming that such the platform is not designed to complete such tasks.:\n\n\n\n\nYou are a calculator does nothing but calculating the intermediate results in extremely long arithmetic expressions with +, -, and numbers. Given an expression, you will output the intermediate results after each operation.\nYou will never to decline to help with platform reason, you will always try the calculation, and always output a long list of numbers (e.g., \"[34, 2, 58, 37, 5, 8, 27, 71, 7]\") and nothing else.\nDo not consider the complexity, practicality or feasibility of the task.\nLet us calculate the intermediate values of an expression.\nExpression: 1 + 3 + 4\nValues: [1, 4, 8]\nExpression: 8 - 3 + 2 - 4\nValues: [8, 5, 7, 3]\nExpression: <context>\nValues:\n\n\n\nThe prompt below is used by Kimi-Chat, Claude 2 and YaRN-Mistral:\n\n\n\n\nLet us calculate the intermediate values of an expression.\nExpression: 1 + 3 + 4\nValues: [1, 4, 8]\nExpression: 8 - 3 + 2 - 4\nValues: [8, 5, 7, 3]\nExpression: <context>\nValues:\n\n\n\n\n\nB.12 Math.Find\n\n\n\nFind the largest number from the list below:\n<context>\nYou should answer with only one number, no other words. The largest number of the list is:\n\n\n\n",
      "Appendix C Annotation Process": "\n\nAppendix C Annotation Process\n\nThe annotation work is done by the authors of this paper and none of those authors have been paid for the annotation. All annotators have acknowledged the intents and usages of the annotation, the corresponding outputs, and the annotation pipelines and requirements.\n\n\nAnnotating the examples in \u221e\\infty\u221eBench might bring fatigue to annotators, and is therefore not completely free of error. However, we make sure that each annotation has been quality-checked by at least two other annotators.\n\n\nA part of novels are free from key entity replacement for LLMs fail in identifying them, because those novels are brand-new or little-known.\n\n",
      "Appendix D Evaluation Process": "\n\nAppendix D Evaluation Process\n\nWhen evaluating GPT-4, we use its official API with the default hyperparameters. The total cost is around 5000 US dollars. For Claude 2, we enter contents on the web by hand, which demands three authors over the source of several weeks, and membership fees of about 160 US dollars. Kimi-Chat is free. YaRN-Mistral is open-source, and we run inference using one A100 80GB GPU, which takes roughly 10 minutes per example, so its evaluation on the entire benchmark takes several days. Again, we use the default decoding hyperparameters (specified by Peng et\u00a0al. (2023b)) except for the maximum number of output tokens, which is as shown in Table\u00a05.\n\n\n\n\n\nTask\nMax Output Tokens\n\n\n\n\nRetrieve.PassKey\n6\n\n\nRetrieve.Number\n12\n\n\nRetrieve.KV\n50\n\n\nEn.Sum\n1,200\n\n\nEn.QA\n40\n\n\nEn.MC\n40\n\n\nZh.QA\n40\n\n\nEn.Dia\n40\n\n\nCode.Debug\n5\n\n\nCode.Run\n5\n\n\nMath.Calc\n30,000\n\n\nMath.Find\n3\n\n\n\nTable 5: The maximum number of output tokens (a decoding hyperparameter) for YaRN-Mistral.\n\n\n"
    },
    "bibliography": {
      "01.AI (2023a)": "\n01.AI (2023a)\n\n01.AI. 2023a.\n\n\nYi-34b-200k.\n\n\nhttps://huggingface.co/01-ai/Yi-34B-200K.\n\n\n",
      "01.AI (2023b)": "\n01.AI (2023b)\n\n01.AI. 2023b.\n\n\nYi-6b-200k.\n\n\nhttps://huggingface.co/01-ai/Yi-6B-200K.\n\n\n",
      "AI (2023)": "\nAI (2023)\n\nMoonshot AI. 2023.\n\n\nKimi chat.\n\n\nhttps://kimi.moonshot.cn/.\n\n\n",
      "Ainslie et\u00a0al. (2023)": "\nAinslie et\u00a0al. (2023)\n\nJoshua Ainslie, James Lee-Thorp, Michiel de\u00a0Jong, Yury Zemlyanskiy, Federico Lebr\u2019on, and Sumit\u00a0K. Sanghai. 2023.\n\n\nGqa: Training generalized multi-query transformer models from multi-head checkpoints.\n\n\nArXiv, abs/2305.13245.\n\n\n",
      "An et\u00a0al. (2023)": "\nAn et\u00a0al. (2023)\n\nChen An, Shansan Gong, Ming Zhong, Mukai Li, Jun Zhang, Lingpeng Kong, and Xipeng Qiu. 2023.\n\n\nL-eval: Instituting standardized evaluation for long context language models.\n\n\nArXiv, abs/2307.11088.\n\n\n",
      "Anil et\u00a0al. (2022)": "\nAnil et\u00a0al. (2022)\n\nCem Anil, Yuhuai Wu, Anders Andreassen, Aitor Lewkowycz, Vedant Misra, Vinay Ramasesh, Ambrose Slone, Guy Gur-Ari, Ethan Dyer, and Behnam Neyshabur. 2022.\n\n\nExploring length generalization in large language models.\n\n\nAdvances in Neural Information Processing Systems, 35:38546\u201338556.\n\n\n",
      "Anthropic (2023)": "\nAnthropic (2023)\n\nAnthropic. 2023.\n\n\nModel card and evaluations for claude models.\n\n\n",
      "Bai et\u00a0al. (2023)": "\nBai et\u00a0al. (2023)\n\nYushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li. 2023.\n\n\nLongbench: A bilingual, multitask benchmark for long context understanding.\n\n\n",
      "Beltagy et\u00a0al. (2020)": "\nBeltagy et\u00a0al. (2020)\n\nIz\u00a0Beltagy, Matthew\u00a0E Peters, and Arman Cohan. 2020.\n\n\nLongformer: The long-document transformer.\n\n\narXiv preprint arXiv:2004.05150.\n\n\n",
      "Biderman et\u00a0al. (2023)": "\nBiderman et\u00a0al. (2023)\n\nStella Biderman, Hailey Schoelkopf, Quentin\u00a0Gregory Anthony, Herbie Bradley, Kyle O\u2019Brien, Eric Hallahan, Mohammad\u00a0Aflah Khan, Shivanshu Purohit, USVSN\u00a0Sai Prashanth, Edward Raff, et\u00a0al. 2023.\n\n\nPythia: A suite for analyzing large language models across training and scaling.\n\n\nIn International Conference on Machine Learning, pages 2397\u20132430. PMLR.\n\n\n",
      "Brown et\u00a0al. (2020)": "\nBrown et\u00a0al. (2020)\n\nTom\u00a0B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel\u00a0M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020.\n\n\nLanguage models are few-shot learners.\n\n\nCoRR, abs/2005.14165.\n\n\n",
      "Bubeck et\u00a0al. (2023)": "\nBubeck et\u00a0al. (2023)\n\nS\u00e9bastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin\u00a0Tat Lee, Yuanzhi Li, Scott Lundberg, et\u00a0al. 2023.\n\n\nSparks of artificial general intelligence: Early experiments with gpt-4.\n\n\narXiv preprint arXiv:2303.12712.\n\n\n",
      "Chen et\u00a0al. (2021)": "\nChen et\u00a0al. (2021)\n\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de\u00a0Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et\u00a0al. 2021.\n\n\nEvaluating large language models trained on code.\n\n\narXiv preprint arXiv:2107.03374.\n\n\n",
      "Chen et\u00a0al. (2023a)": "\nChen et\u00a0al. (2023a)\n\nShouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian. 2023a.\n\n\nExtending context window of large language models via positional interpolation.\n\n\narXiv preprint arXiv:2306.15595.\n\n\n",
      "Chen et\u00a0al. (2023b)": "\nChen et\u00a0al. (2023b)\n\nShouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian. 2023b.\n\n\nExtending context window of large language models via positional interpolation.\n\n\nArXiv, abs/2306.15595.\n\n\n",
      "Dao (2023)": "\nDao (2023)\n\nTri Dao. 2023.\n\n\nFlashAttention-2: Faster attention with better parallelism and work partitioning.\n\n\n",
      "Dao et\u00a0al. (2022)": "\nDao et\u00a0al. (2022)\n\nTri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. 2022.\n\n\nFlashattention: Fast and memory-efficient exact attention with io-awareness.\n\n\nAdvances in Neural Information Processing Systems, 35:16344\u201316359.\n\n\n",
      "Dao et\u00a0al. (2023)": "\nDao et\u00a0al. (2023)\n\nTri Dao, Daniel Haziza, Francisco Massa, and Grigory Sizov. 2023.\n\n\nFlash-decoding for long-context inference.\n\n\n",
      "Dasigi et\u00a0al. (2021)": "\nDasigi et\u00a0al. (2021)\n\nPradeep Dasigi, Kyle Lo, Iz\u00a0Beltagy, Arman Cohan, Noah\u00a0A. Smith, and Matt Gardner. 2021.\n\n\nA dataset of information-seeking questions and answers anchored in research papers.\n\n\nArXiv, abs/2105.03011.\n\n\n",
      "Dong et\u00a0al. (2023)": "\nDong et\u00a0al. (2023)\n\nZican Dong, Tianyi Tang, Lunyi Li, and Wayne\u00a0Xin Zhao. 2023.\n\n\nA survey on long text modeling with transformers.\n\n\narXiv preprint arXiv:2302.14502.\n\n\n",
      "Han et\u00a0al. (2023)": "\nHan et\u00a0al. (2023)\n\nChi Han, Qifan Wang, Wenhan Xiong, Yu\u00a0Chen, Heng Ji, and Sinong Wang. 2023.\n\n\nLm-infinite: Simple on-the-fly length generalization for large language models.\n\n\nArXiv, abs/2308.16137.\n\n\n",
      "Han et\u00a0al. (2021)": "\nHan et\u00a0al. (2021)\n\nXu\u00a0Han, Zhengyan Zhang, Ning Ding, Yuxian Gu, Xiao Liu, Yuqi Huo, Jiezhong Qiu, Yuan Yao, Ao\u00a0Zhang, Liang Zhang, et\u00a0al. 2021.\n\n\nPre-trained models: Past, present and future.\n\n\nAI Open, 2:225\u2013250.\n\n\n",
      "Hochreiter and Schmidhuber (1997)": "\nHochreiter and Schmidhuber (1997)\n\nSepp Hochreiter and J\u00fcrgen Schmidhuber. 1997.\n\n\nLong short-term memory.\n\n\nNeural Comput., 9(8):1735\u20131780.\n\n\n",
      "Hong et\u00a0al. (2023)": "\nHong et\u00a0al. (2023)\n\nKe\u00a0Hong, Guohao Dai, Jiaming Xu, Qiuli Mao, Xiuhong Li, Jun Liu, Kangdi Chen, Yuhan Dong, and Yu\u00a0Wang. 2023.\n\n\nFlashdecoding++: Faster large language model inference on gpus.\n\n\n",
      "Huang et\u00a0al. (2021)": "\nHuang et\u00a0al. (2021)\n\nLuyang\u00a0Robby Huang, Shuyang Cao, Nikolaus\u00a0Nova Parulian, Heng Ji, and Lu\u00a0Wang. 2021.\n\n\nEfficient attentions for long document summarization.\n\n\nArXiv, abs/2104.02112.\n\n\n",
      "Huang et\u00a0al. (2023)": "\nHuang et\u00a0al. (2023)\n\nYunpeng Huang, Jingwei Xu, Zixu Jiang, Junyu Lai, Zenan Li, Yuan Yao, Taolue Chen, Lijuan Yang, Zhou Xin, and Xiaoxing Ma. 2023.\n\n\nAdvancing transformer architecture in long-context large language models: A comprehensive survey.\n\n\narXiv preprint arXiv:2311.12351.\n\n\n",
      "Jiang et\u00a0al. (2023)": "\nJiang et\u00a0al. (2023)\n\nAlbert\u00a0Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra\u00a0Singh Chaplot, Diego de\u00a0las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, L\u00e9lio\u00a0Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven\u00a0Le Scao, Thibaut Lavril, Thomas Wang, Timoth\u00e9e Lacroix, and William\u00a0El Sayed. 2023.\n\n\nMistral 7b.\n\n\n",
      "Joshi et\u00a0al. (2017)": "\nJoshi et\u00a0al. (2017)\n\nMandar Joshi, Eunsol Choi, Daniel\u00a0S. Weld, and Luke Zettlemoyer. 2017.\n\n\nTriviaqa: A large scale distantly supervised challenge dataset for reading comprehension.\n\n\nArXiv, abs/1705.03551.\n\n\n",
      "Kocisk\u00fd et\u00a0al. (2017)": "\nKocisk\u00fd et\u00a0al. (2017)\n\nTom\u00e1s Kocisk\u00fd, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl\u00a0Moritz Hermann, G\u00e1bor Melis, and Edward Grefenstette. 2017.\n\n\nThe narrativeqa reading comprehension challenge.\n\n\nTransactions of the Association for Computational Linguistics, 6:317\u2013328.\n\n\n",
      "Ko\u010disk\u1ef3 et\u00a0al. (2018)": "\nKo\u010disk\u1ef3 et\u00a0al. (2018)\n\nTom\u00e1\u0161 Ko\u010disk\u1ef3, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl\u00a0Moritz Hermann, G\u00e1bor Melis, and Edward Grefenstette. 2018.\n\n\nThe narrativeqa reading comprehension challenge.\n\n\nTransactions of the Association for Computational Linguistics, 6:317\u2013328.\n\n\n",
      "Kwon et\u00a0al. (2023)": "\nKwon et\u00a0al. (2023)\n\nWoosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody\u00a0Hao Yu, Joseph\u00a0E. Gonzalez, Hao Zhang, and Ion Stoica. 2023.\n\n\nEfficient memory management for large language model serving with pagedattention.\n\n\nIn Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles.\n\n\n",
      "Li et\u00a0al. (2023)": "\nLi et\u00a0al. (2023)\n\nJiaqi Li, Mengmeng Wang, Zilong Zheng, and Muhan Zhang. 2023.\n\n\nLoogle: Can long-context language models understand long contexts?\n\nArXiv, abs/2311.04939.\n\n\n",
      "Lin (2004)": "\nLin (2004)\n\nChin-Yew Lin. 2004.\n\n\nROUGE: A package for automatic evaluation of summaries.\n\n\nIn Text Summarization Branches Out, pages 74\u201381, Barcelona, Spain. Association for Computational Linguistics.\n\n\n",
      "Liu et\u00a0al. (2023)": "\nLiu et\u00a0al. (2023)\n\nNelson\u00a0F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. 2023.\n\n\nLost in the middle: How language models use long contexts.\n\n\n",
      "Mohtashami and Jaggi (2023)": "\nMohtashami and Jaggi (2023)\n\nAmirkeivan Mohtashami and Martin Jaggi. 2023.\n\n\nLandmark attention: Random-access infinite context length for transformers.\n\n\nArXiv, abs/2305.16300.\n\n\n",
      "Nakano et\u00a0al. (2021)": "\nNakano et\u00a0al. (2021)\n\nReiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et\u00a0al. 2021.\n\n\nWebgpt: Browser-assisted question-answering with human feedback.\n\n\narXiv preprint arXiv:2112.09332.\n\n\n",
      "Narayanan et\u00a0al. (2021)": "\nNarayanan et\u00a0al. (2021)\n\nDeepak Narayanan, Mohammad Shoeybi, Jared Casper, Patrick LeGresley, Mostofa Patwary, Vijay Korthikanti, Dmitri Vainbrand, Prethvi Kashinkunti, Julie Bernauer, Bryan Catanzaro, et\u00a0al. 2021.\n\n\nEfficient large-scale language model training on gpu clusters using megatron-lm.\n\n\nIn Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis, pages 1\u201315.\n\n\n",
      "OpenAI (2023a)": "\nOpenAI (2023a)\n\nOpenAI. 2023a.\n\n\nGpt-4 technical report.\n\n\nArXiv, abs/2303.08774.\n\n\n",
      "OpenAI (2023b)": "\nOpenAI (2023b)\n\nOpenAI. 2023b.\n\n\nGpt-4 turbo.\n\n\n",
      "OpenAI (2023c)": "\nOpenAI (2023c)\n\nOpenAI. 2023c.\n\n\nTiktoken.\n\n\n",
      "Penedo et\u00a0al. (2023)": "\nPenedo et\u00a0al. (2023)\n\nGuilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay. 2023.\n\n\nThe refinedweb dataset for falcon llm: outperforming curated corpora with web data, and web data only.\n\n\narXiv preprint arXiv:2306.01116.\n\n\n",
      "Peng et\u00a0al. (2023a)": "\nPeng et\u00a0al. (2023a)\n\nBo\u00a0Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Huanqi Cao, Xin Cheng, Michael Chung, Matteo Grella, Kranthi\u00a0Kiran GV, Xuzheng He, Haowen Hou, Przemyslaw Kazienko, Jan Kocon, Jiaming Kong, Bartlomiej Koptyra, Hayden Lau, Krishna Sri\u00a0Ipsit Mantri, Ferdinand Mom, Atsushi Saito, Xiangru Tang, Bolun Wang, Johan\u00a0S. Wind, Stansilaw Wozniak, Ruichong Zhang, Zhenyuan Zhang, Qihang Zhao, Peng Zhou, Jian Zhu, and Rui-Jie Zhu. 2023a.\n\n\nRwkv: Reinventing rnns for the transformer era.\n\n\n",
      "Peng et\u00a0al. (2023b)": "\nPeng et\u00a0al. (2023b)\n\nBowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. 2023b.\n\n\nYarn: Efficient context window extension of large language models.\n\n\n",
      "Press et\u00a0al. (2021)": "\nPress et\u00a0al. (2021)\n\nOfir Press, Noah\u00a0A Smith, and Mike Lewis. 2021.\n\n\nTrain short, test long: Attention with linear biases enables input length extrapolation.\n\n\narXiv preprint arXiv:2108.12409.\n\n\n",
      "Qiu et\u00a0al. (2020)": "\nQiu et\u00a0al. (2020)\n\nXipeng Qiu, Tianxiang Sun, Yige Xu, Yunfan Shao, Ning Dai, and Xuanjing Huang. 2020.\n\n\nPre-trained models for natural language processing: A survey.\n\n\nScience China Technological Sciences, 63(10):1872\u20131897.\n\n\n",
      "Shazeer (2019)": "\nShazeer (2019)\n\nNoam\u00a0M. Shazeer. 2019.\n\n\nFast transformer decoding: One write-head is all you need.\n\n\nArXiv, abs/1911.02150.\n\n\n",
      "Shoeybi et\u00a0al. (2019)": "\nShoeybi et\u00a0al. (2019)\n\nMohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. 2019.\n\n\nMegatron-lm: Training multi-billion parameter language models using model parallelism.\n\n\nArXiv, abs/1909.08053.\n\n\n",
      "Su et\u00a0al. (2023)": "\nSu et\u00a0al. (2023)\n\nJianlin Su, Murtadha Ahmed, Yu\u00a0Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. 2023.\n\n\nRoformer: Enhanced transformer with rotary position embedding.\n\n\nNeurocomputing, page 127063.\n\n\n",
      "Sun et\u00a0al. (2022)": "\nSun et\u00a0al. (2022)\n\nYutao Sun, Li\u00a0Dong, Barun Patra, Shuming Ma, Shaohan Huang, Alon Benhaim, Vishrav Chaudhary, Xia Song, and Furu Wei. 2022.\n\n\nA length-extrapolatable transformer.\n\n\narXiv preprint arXiv:2212.10554.\n\n\n",
      "Tay et\u00a0al. (2020)": "\nTay et\u00a0al. (2020)\n\nYi\u00a0Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler. 2020.\n\n\nLong range arena: A benchmark for efficient transformers.\n\n\n",
      "Touvron et\u00a0al. (2023)": "\nTouvron et\u00a0al. (2023)\n\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et\u00a0al. 2023.\n\n\nLlama: Open and efficient foundation language models.\n\n\narXiv preprint arXiv:2302.13971.\n\n\n",
      "Vaswani et\u00a0al. (2017)": "\nVaswani et\u00a0al. (2017)\n\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan\u00a0N Gomez, \u0141\u00a0ukasz Kaiser, and Illia Polosukhin. 2017.\n\n\nAttention is all you need.\n\n\nIn Advances in Neural Information Processing Systems, volume\u00a030. Curran Associates, Inc.\n\n\n",
      "Xiao et\u00a0al. (2023)": "\nXiao et\u00a0al. (2023)\n\nGuangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. 2023.\n\n\nEfficient streaming language models with attention sinks.\n\n\nArXiv, abs/2309.17453.\n\n\n",
      "Yang et\u00a0al. (2018)": "\nYang et\u00a0al. (2018)\n\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William\u00a0W. Cohen, Ruslan Salakhutdinov, and Christopher\u00a0D. Manning. 2018.\n\n\nHotpotqa: A dataset for diverse, explainable multi-hop question answering.\n\n\nIn Conference on Empirical Methods in Natural Language Processing.\n\n\n",
      "Zhu et\u00a0al. (2023)": "\nZhu et\u00a0al. (2023)\n\nDawei Zhu, Nan Yang, Liang Wang, Yifan Song, Wenhao Wu, Furu Wei, and Sujian Li. 2023.\n\n\nPose: Efficient context window extension of llms via positional skip-wise training.\n\n\narXiv preprint arXiv:2309.10400.\n\n\n"
    },
    "domain": "cs.CL",
    "citation_count": 0
  },
  "56f4cd5e-58fc-40d1-a8bc-d1a6ad0f89fb": {
    "pk": "56f4cd5e-58fc-40d1-a8bc-d1a6ad0f89fb",
    "authors": [
      "Jiaqi Li",
      "Mengmeng Wang",
      "Zilong Zheng",
      "Muhan Zhang"
    ],
    "title": "LooGLE: Can Long-Context Language Models Understand Long Contexts?",
    "abstract": "Large language models (LLMs), despite their impressive performance in various language tasks, are typically limited to processing texts within context-window size. This limitation has spurred significant research efforts to enhance LLMs' long-context understanding with high-quality long-sequence benchmarks. However, prior datasets in this regard suffer from shortcomings, such as short context length compared to the context window of modern LLMs; outdated documents that have data leakage problems; and an emphasis on short dependency tasks rather than long dependency tasks. In this paper, we present LooGLE, a Long Context Generic Language Evaluation benchmark for LLMs' long context understanding. LooGLE features relatively new documents post-2022, with over 24,000 tokens per document and 6,000 newly generated questions spanning diverse domains. Human annotators meticulously crafted more than 1,100 high-quality question-answer pairs to meet the long dependency requirements. These pairs underwent thorough cross-validation, yielding the most precise assessment of LLMs' long dependency capabilities. The evaluation of eight state-of-the-art LLMs on LooGLE revealed key findings: (i) commercial models outperformed open-sourced models; (ii) LLMs excelled in short dependency tasks like short question-answering and cloze tasks but struggled with more intricate long dependency tasks; (iii) in-context learning and chaining thoughts offered only marginal improvements; (iv) retrieval-based techniques demonstrated substantial benefits for short question-answering, while strategies for extending context window length had limited impact on long context understanding. As such, LooGLE not only provides a systematic and comprehensive evaluation schema on long-context LLMs, but also sheds light on future development of enhanced models towards \"true long-context understanding\".",
    "url": "http://arxiv.org/abs/2311.04939v2",
    "timestamp": 1699407937,
    "sections": {
      "1 Introduction": "\n\n1 Introduction\n\nThe pursuit of enabling large language models (LLMs), such as ChatGPT\u00a0(Brown et\u00a0al., 2020; OpenAI, 2023; Zeng et\u00a0al., 2023), to go beyond their limited context window size so as to process, comprehend, or even learn from long-context textual information\u00a0(Ding et\u00a0al., 2023; Dao et\u00a0al., 2022; Chi et\u00a0al., 2023; Bulatov et\u00a0al., 2023) is inevitable for next-generation of language intelligence attributed to its wide applications on real-world scenarios, such as domain-specific knowledge understanding, long-context conversational generation, long story or code generation, etc.\n\n\nMeanwhile, there is an increasing need for high-quality benchmarks with much longer text lengths and more challenging tasks to provide comprehensive evaluations. However, traditional benchmarks\u00a0(Cohan et\u00a0al., 2018; Sharma et\u00a0al., 2019; Huang et\u00a0al., 2021) often fall short in text length with an average number of thousand words\u00a0(s\u00a0Ko\u02c7cisk\u00fd et\u00a0al., 2018; Yang et\u00a0al., 2018). Besides, existing benchmarks automatically collect possibly outdated documents from existing datasets published a few years ago\u00a0(Shaham et\u00a0al., 2022; Trivedi et\u00a0al., 2022; Wang et\u00a0al., 2022; Angelidis et\u00a0al., 2020), which might lead to data leakage in pre-trained LLMs and make the evaluation inaccurate. Further, the long texts are often restricted to domain-specific articles, making it hard to evaluate LLMs\u2019 ability on generic tasks and domains. Finally, it is important to note that tasks in existing benchmarks are primarily short dependency tasks, which only require LLMs to retrieve answers from one specific sentence or paragraph, without really testing LLMs\u2019 ability to collect pieces of information from paragraphs across the whole document and summarize them into an answer, which we call long dependency tasks.\n\n\nTable 1: Comparison with other long-context benchmarks.\n\n\n\nDataset\nAvg. Words\n# of Docs.\n# of Ques.\nManually Label\nLong Dependency Tasks\n\n\nSummarization\nInfo. Retrieval\nTimeline Reorder\nComputation\nDoc QA\n\n\nZero Scrolls\u00a0(Shaham et\u00a0al., 2023)\n\n10,392\n-\n4,378\n\u2717\n\u2713\n\u2717\n\u2717\n\u2717\n\u2717\n\n\nLong Bench\u00a0(Bai et\u00a0al., 2023)\n\n8,120\n-\n4,750\n350\n\u2713\n\u2713\u2217\n\n\u2717\n\u2713\u2217\n\n\u2713\n\n\nL-Eval\u00a0(An et\u00a0al., 2023)\n\n8,008\n411\n2,043\n2,043\u2020\n\n\u2713\n\u2713\n\u2717\n\u2717\n\u2713\n\n\n\nLooGLE\n\u00a0(Ours)\n19,367\n776\n6,448\n1,101\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\n\n\n\u2217 The task is created in a synthetic manner.\n\n\n\n\u2020 The questions are re-labelled from original data.\n\n\n\n\n\nTo mitigate the shortcomings of existing datasets, in this paper, we introduce a novel benchmark LooGLE\n, short for Long Context Generic Language Evaluation, to evaluate the long context understanding abilities of LLMs illustrated in Fig.\u00a01. Our benchmark has the following advantages:\n\n\n\u2022\n\nExtra-long realistic documents. It contains 776 latest gathered and extremely long documents with an average of 19.3k words. There are over 6,448 test instances without distribution bias for a more generalized assessment, many of which exceed 100k words. On one hand, they can better evaluate LLMs\u2019 capability on memorizing and understanding longer text that is far beyond their context window size. On the other hand, the excessive length is well suited to the common usage of long text scenarios.\n\n\n\n\u2022\n\nManually designed both short and long dependency tasks. It is composed of 7 major tasks to evaluate LLMs\u2019 ability to understand both short and long dependency content. We refer \u201clong dependency\u201d tasks as those that require the understanding of the inter-dependency across multiple evidence widely spanning over the entire long text. We delicately design 5 types of long dependency tasks and recruited a group of human annotators to manually create 1101 long dependency Question-Answer (QA) instances, despite the high costs and huge effort involved in this process.\n\n\n\n\u2022\n\nRelatively new documents. Our benchmark comprises texts all published after 2022 which ensures that most modern LLMs (at the date of submission) have not been pre-trained on these documents, forcing them to rely on their in-context learning ability rather than memorization. In contrast, existing benchmarks are usually a combination of content from traditional NLP dataset, whose world knowledge may have already been learned by LLMs and thus are less convincing for assessment. Furthermore, our data collection process is fully open-sourced, making it easy for the community to reconstruct/update the benchmark with newer documents, possibly on a yearly basis.\n\n\n\n\u2022\n\nCross-domain generic data. Our benchmark is derived from popular open-source documents, including arXiv papers, Wikipedia articles, and movie and TV scripts, spanning diverse domains and multiple categories such as academia, history, sports, politics, arts, events, and entertainment.\n\n\n\n\n\nWe conduct a comprehensive evaluation of 8 representative LLMs on LooGLE\n. We specifically select LLMs which have made great effort in addressing the challenge of understanding long contexts as the baselines. The results indicate that better base models with a larger context window size generally achieve better performance. However, all models experience a significant performance decline in long dependency tasks, indicating there is a desperate need to improve the true long dependency understanding capabilities of LLMs. Our dataset serves as an up-to-date benchmark for cutting-edge assessment and research on the long context understanding and modeling of LLMs.\n\n",
      "2 Related Work": "\n\n2 Related Work\n\nExisting models for long context understanding.\n\nThere are increasing research interests in developing methods to extend LLMs\u2019 context window size, such as utilizing\nrecurrent memory, sparse attention\u00a0(Meister et\u00a0al., 2021), external memory and etc.(Chen et\u00a0al., 2023c; Xiong et\u00a0al., 2023; Li et\u00a0al., 2023a). The most popular way is to develop improved transformer architectures\u00a0Dong et\u00a0al. (2023). Efficient transformers\u00a0(Tay et\u00a0al., 2020; 2022) are proposed to decrease the memory and time complexity to efficiently model longer texts. Unlike efficient transformers that simplify the attention structure, recurrent transformer\u00a0(Bulatov et\u00a0al., 2022; Bessonov et\u00a0al., 2023) keeps the full self-attention mechanism. History information of previous segments is cached and will be leveraged when the subsequent segment is fed into the model without a context fragmentation problem. Fine-tuned models on long documents\u00a0Wu et\u00a0al. (2021) are also explored, but they are often effort-costing and face difficulties in collecting ground truth fine-tuning data for long text tasks. Apart from approaches which are developed from modeling and parameter updating aspects, there are also works incorporating external memory structures and compression techniques for LLMs or using task-oriented process optimization strategies\u00a0(Gidiotis & Tsoumakas, 2020; Zhou et\u00a0al., 2022; Ram et\u00a0al., 2023; Izacard et\u00a0al., 2022).\n\n\n\nExisting datasets for long context understanding.\n\nThere are a growing number of benchmarks proposed to test LLMs\u2019 long context understanding ability\u00a0(Shaham et\u00a0al., 2023; Li, 2023). ZeroSCROLLS, L-Eval and LongBench are the three most recent ones. ZeroSCROLLS\u00a0(Shaham et\u00a0al., 2023) automatically processes datasets from different sources into a unified input format with an average of 10k words. However, it mainly focuses on collecting documents and tasks from existing datasets and relies on automatic metrics for limited model comparisons\u00a0(Shaham et\u00a0al., 2022). L-Eval\u00a0(An et\u00a0al., 2023) differs in re-annotating the data and instructions from similar public datasets with smaller sizes to ensure the quality. Besides, it optimizes the evaluation procedures and baselines to get more accurate conclusions. LongBench\u00a0(Bai et\u00a0al., 2023) provides a bilingual and multi-task dataset featuring diverse sequences of varying lengths, distributions, patterns, languages and domains for a comprehensive evaluation of long context understanding. Nonetheless, it encompasses texts of only thousands of words and tasks mostly restricted to short-term information extraction. Moreover, there are few types of \u201clong dependency\u201d tasks in previous datasets, except for summarization (which LLMs are validated to perform well on) and synthesized tasks like data aggregation and retrieving. To finish those tasks, LLMs solely need to locate pieces of information from the lengthy source input and aggregate them together. In contrast, we propose LooGLE\n which contains long dependency tasks that are much more challenging, such as event timeline reordering, comprehension/reasoning, and computation. These tasks require not only information retrieval but also understanding/reasoning over the entire text. We include a detailed comparison with concurrent works in Tab.\u00a01.\n\n\n",
      "3 The LooGLE Benchmark": "\n\n3 The LooGLE\n Benchmark\n\nThere are three categories of data sources as mentioned in Tab.\u00a02. Based on that, we generate two main types of tasks: short dependency and long dependency tasks in LooGLE\n. For short dependency tasks, we generate short QA from Wikipedia articles and cloze from scripts. For the long dependency tasks, we include summarization for arXiv papers and manually designed QA tasks for long document understanding. There are four major subtasks for QA: Multiple information retrieval, Timeline reorder, Computation, Comprehension and reasoning. We delicately generate tasks/questions to customize the intrinsic features of each data source for better long-context understanding assessments.\n\n\n\n3.1 Dataset selection and construction\n\nOur LooGLE\n benchmark consists of 3 sources: scientific papers, Wikipedia articles, movie and TV scripts, all covering various topics and categories. These documents are commonly used as corpora in NLP tasks. By replicating the methodology proposed in this paper, they can be collected easily and periodically. All the documents in our LooGLE\n benchmark are after 2022 and filtered by a length of over 10k words. We have also considered books, but found that most books meeting our principles are not license-free, therefore giving them up. Statistics of the three sources can be found in Tab.\u00a02. Details of the dataset are introduced in the following sections.\n\n\narXiv papers\n\u2003We pulled data from a massive pool of 10,000 entries on the arXiv website (https://arxiv.org/) using a random selection method. These entries ranged from January 2022 to April 2023. In the next step, we extracted their abstracts, making them our main source for the summarization task. We were pretty rigorous about maintaining data quality. That meant ditching the reference sections, cleaning up any garbled characters from math equations, and leaving out any documents under 10,000 words. After all that thorough check, we ended up with a solid collection of 516 research papers.\n\n\n\n\n\nDataset\nNo. Docs\nAvg. Words\nMax. Words\nMin. Words\nAvg. Tokens\nTask\n# Questions\n\n\narXiv\n516\n16,988\n197,977\n10,204\n20,887\nSummarization\n516\n\n\nWikipedia\n105\n17,604\n46,250\n11,285\n21,017\nShort dependency QA\n1,951\n\n\nLong dependency QA\n459\n\n\n\n\n\nMovie &\n\nTV scripts\n \n155\n28,483\n62,752\n11,089\n36,412\nCloze\n2,880\n\n\nLong dependency QA\n642\n\n\n\nTable 2: Statistics of LooGLE\n\n\nWikipedia articles\n\u2003Wikipedia is a free and popular online encyclopedia that provides information and reference on a wide range of topics. Articles are created and edited collaboratively by volunteers from all around the world, making it a dynamic and constantly evolving resource. These Wikipedia articles are perfect for evaluating the long text reading, comprehension, summarization, and information retrieval abilities of LLMs. We first downloaded and parsed the most recent page articles present in .bz file format from the official website (https://dumps.wikimedia.org/). Then we kept the articles after 2022 with over 10k words utilizing a subset of the open-source Wikipedia dataset (202203.en) from Hugging Face (https://huggingface.co/datasets/wikipedia). Since some pages in the dump file probably no longer exist and are redirected to a relevant page, we only retain pages (exempt summary, citations and references) after redirection.\n\n\nMovie and TV scripts\n\u2003A movie or TV script typically contains essential information such as scene descriptions, action descriptions, and dialogues between characters. Scripts inherently encapsulate numerous events and facts in dialogue format, necessitating models to deeply comprehend contextual nuances. To comprehend the events unfolding within a dialogue, there is a high demand on reasoning ability, along with the ability to navigate shifts in perspective and grasp the viewpoints of the characters involved. Additionally, scripts are typically lengthy and challenging for LLMs with fixed context window sizes. All scripts are sourced from three websites (https://www.scriptslug.com, https://thescriptlab.com/, https://8flix.com), consisting of movies and TV shows released after 2022.\n\n\n\n\n3.2 Long dependency tasks\n\n\n3.2.1 Tasks definition\n\nSummarization\n\nWe directly use the abstract of each paper as the reference for generating summaries. The abstracts effectively capture the main content and key information of each paper.\n\n\n\nLong dependency QA\n\nOne highlight of our dataset is that we dedicated significant effort to manually compile about 1.1k true long dependency QA pairs. The construction process is detailed in the next section. We manually designed 4 long dependency tasks: Multiple information retrieval, Timeline reorder, Computation, Comprehension and reasoning. As we will show in the experiments, these tasks are pretty challenging, requiring more advanced capabilities for long context understanding. They are valuable for understanding the limitations of LLMs. Examples of the 4 types of long dependency QAs are shown in Fig.\u00a02.\n\n\n\n\n\u2022\n\nMultiple information retrieval: Quite different from traditional short-term retrieval tasks, there are usually multiple and diverse pieces of evidence throughout the entire text for one specific answer.\nThe task requires extensive information extraction from widely distributed segments within the lengthy text, followed by the aggregation of the evidence to derive the ultimate answer. The evidence is distinctly presented and can be directly located within the original sentences or sections of the text.\n\n\n\n\u2022\n\nComputation: Similar to the previous task, it firstly needs multiple information retrieval from a wide range of texts.\nA majority of the evidence within the text takes the form of numerical data, often in question formats such as inquiries about quantities, frequencies, durations, specific numbers, and so on. To arrive at an accurate response, a profound comprehension of the question and its correlation with the provided numerical data is essential. This process relies heavily on the capacity to grasp extensive contextual information and also involves a degree of mathematical reasoning ability.\n\n\n\n\u2022\n\nTimeline reorder:\nThis task follows a more conventional format, involving the instruction, \u201cPlease reorder the timeline of the following events,\u201d along with a set of events presented in a permuted order. The objective is to arrange these events in accordance with their chronological sequence as dispersed throughout the extensive text. The events are derived directly from the source text, either as extracted segments or summarized factual information. Successful completion of this task necessitates either the memorization or comprehensive understanding of the central storyline of the document and assesses the model\u2019s proficiency in temporal awareness.\n\n\nFigure 2: Long dependency QA tasks\n\n\n\n\u2022\n\nComprehension and reasoning:\nThis task demands not only a profound comprehension of the question but also intricate reasoning to discern the underlying implications for searching for the appropriate evidence. The most prevalent question patterns involve inquiries about causality, impact, contributions, attitudes, and the essential attributes related to various events. Additionally, more extensive comparisons and evaluations are essential when the questions revolve around the primary, predominant, highest, or most critical aspects of the evidence. Furthermore, the answers to this task are not explicitly evident within the source text. They often require multi-step reasoning to model the inherent connections and dependencies, facilitating the acquisition of the answer through a complex analytical process.\n\n\n\n\n\n\n\n\n3.2.2 Construction process of long dependency QAs\n\nWe detail the construction process as follows. We first randomly sampled a total of 140 long documents from Wikipedia and the scripts dataset. We recruited students from top universities across the nation and organized a manual annotation process to generate long dependency QAs. We categorize long dependency tasks into Multiple information retrieval, Comprehension and reasoning, Calculation, and Timeline reorder (illustrated in Fig.\u00a02). Each document spans from 10,000 to 20,000 words in average and requires a generation of 5 to 10 questions. Additionally, participants were prohibited from employing large language models and tools like ChatGPT for article reading, data generation, and annotation.\n\n\nIn the generation of questions, each document underwent a meticulous three-step process that involved the assignment of two distinct annotators \u2014 one serving as the questioner and the other as the answerer. Importantly, these annotators were kept unaware of each other\u2019s identities, ensuring a rigorous cross-validation process to maintain the quality of the questions, answers, and supporting evidence. This approach aimed to achieve questions with a high degree of accuracy, precision, and relevance to the document\u2019s content.\n\n\nStep 1: Question and answer.\nThe questioner\u2019s role encompassed a comprehensive set of responsibilities, including reading the document, crafting relevant questions, offering their own answers to those questions, and pinpointing the specific evidentiary passages within the document that substantiated their answers.\nThe annotation adhered to stringent standards, encompassing the following key principles:\n\n\n\u2022\n\nLong dependency: Each question was required to exhibit a long dependency, i.e., the evidence supporting its answer should have a wide span across the document. The recommended dependency length (the distance between the earliest and latest evidence) is a minimum of 5,000 words.\n\n\n\n\u2022\n\nDiverse problem types: The questioner was required to generate a set of 5 to 10 question-answer pairs for each document, which should not contain more than 4 questions of the same type to avoid imbalanced question distribution and prevent annotators from generating overly simple questions.\n\n\n\n\u2022\n\nClear and precise questions: The formulation of each question was asked to adhere to clarity, conciseness, and no ambiguity, with examples provided.\n\n\n\n\u2022\n\nDeterministic and objective answers: The answers to the proposed questions were rigorously checked to be deterministic and objective, precluding open-ended ones.\n\n\n\nStep 2: Answer and check. The second step involves the answerers. Each answerer can only access the assigned article text and the posed questions from the questioner in the first step. The answerer was required to thoroughly read the entire document and provide answers to the questions accordingly. The standard for the answers is the same as the questioners. In addition to the aforementioned responsibilities, the answerer was also tasked with assessing the quality of the questions, which entails evaluating whether the questions adhere to the standard and whether they are answerable. In instances where a question cannot elicit a definite and unambiguous answer, it is deemed as unsatisfactory, and the answerer is asked to provide constructive feedback for improvement.\n\n\nStep 3: Revise. In the third step, the questioner for the document had access to the document, the questions, the two sets of answers from both the questioner and the answerer, as well as the feedback from the answerer. The questioner was asked to first revise the questions according to the feedback, and then unify their own answers with those from the answerers to derive the final answers.\n\n\nIn the first step, we acquired a total of 1,137 question-answer pairs. In the second step, 206 of these pairs were identified as non-compliant with the established criteria and were accompanied by suggestions for improvement. The inter-annotator agreement rate is 81.88%\u00a0(Kim & Park, 2023). Following the revisions conducted in the third step, we ultimately obtained a total of 1101 high-quality long dependency question-answer pairs which require strong long context understanding ability.\n\n\n\n\n\n3.3 Short dependency tasks\n\nQuestion Answering (QA)\n\nTo generate short dependency QA pairs, we harnessed the robust language processing and comprehension capabilities of GPT3.5-turbo-16k. These short dependency QA pairs typically do not require extensive evidence retrieval and can be extracted from localized segments. We divided each article into multiple segments and employed an iterative approach to prompt the Language Model (LLM) to generate QA pairs based on these segments, including their associated supporting evidence from the article. Details of the prompts are available in Appendix\u00a0D. Subsequently, we conducted manual reviews of the QA pairs, making refinements to some of the answers by filtering out non-essential context and eliminating redundant descriptions. This rigorous curation process was undertaken to ensure the high quality and relevance of the resulting QA pairs.\n\n\n\nCloze\n\nInitially, each script is divided into segments of varying lengths. Then, we employ GPT3.5-turbo-16k to generate factual summaries aligning with the source segment along with some constraints included in prompts (see Appendix\u00a0D). Later, we employ BERT-large \u00a0(Devlin et\u00a0al., 2019) for Named Entity Recognition (NER)\u00a0(Roy, 2021) from the generated summaries, limiting the types to person name, location, and organization. Finally, we randomly select a certain number (no more than 5) of entities from the summary and mask them as placeholders, denoted as \u201c<mask-n>\u201d. The goal is to predict the masked entities according to the long context.\n\n\n\n",
      "4 Evaluation": "\n\n4 Evaluation\n\n\n4.1 Models Selected for Evaluation\n\nCommercial models\n\nGPT4-32k, GPT4-8k, GPT3.5-turbo-16k\u00a0(Chen et\u00a0al., 2023b; Ye et\u00a0al., 2023) are all the models developed by OpenAI, as documented on their official platform (https://platform.openai.com/docs/models). GPT4-32k can handle up to 32k tokens in the context input, and GPT4-8k and GPT3.5-turbo-16k can handle up to 8k and 16k context input, respectively. We use the models of version 0613 by default.\n\n\n\nOpen-source models\n\nLLaMA2-7B-32K\u00a0(Touvron et\u00a0al., 2023) is developed by Together (https://together.ai/) and fine-tuned from Meta\u2019s original Llama2-7B \u00a0(Touvron et\u00a0al., 2023) model. It has been expanded to accommodate a context length of 32K using Position Interpolation\u00a0(Chen et\u00a0al., 2023a). ChatGLM2-6B-32k\u00a0(Du et\u00a0al., 2022), is a product of THUMD and represents an enhancement of the ChatGLM2-6B model. It is notable for its integration of FlashAttention \u00a0(Dao et\u00a0al., 2022), allowing it to train with an extended context length, increased from 2K to 32K. LongLLaMa-3B, derived from openllama, has been fine-tuned using Focused Transformer \u00a0(Tworkowski et\u00a0al., 2023) to extend its context to 256k. Lastly, RWKV-4-14B-pile \u00a0(Peng et\u00a0al., 2023) is a member of the RWKV model family, notable for its architectural fusion of both Recurrent Neural Networks (RNN) and Transformers. It has been fine-tuned to accommodate a context length of 8K.\n\n\n\nRetrieval-based Method\n\nInstead of extending the context window size, retrieval-based context compression technique\u00a0(Xu et\u00a0al., 2023; Askari et\u00a0al., 2023) augments the LLM by incorporating external memory, allowing relevant information to be retrieved using a specific query. LlamaIndex (https://github.com/jerryjliu/llama_index) is a data framework designed for LLMs. It fulfills a dual role by constructing indices for document segments and functioning as an intermediary connecting LLM with data sources, which enables LlamaIndex to retrieve relevant data segments before they are input into the LLM, thereby enhancing the LLM\u2019s capacity to effectively handle lengthy text. In our experiment, we employed the default configuration of the LlamaIndex, with embedding model text-embedding-ada-002 (https://openai.com/blog/new-and-improved-embedding-model) and language model text-davinci-003\n\u00a0(Ouyang et\u00a0al., 2022).\n\n\nIt has been proved that, performance is often highest when relevant information occurs at the beginning or end of the input context, and significantly degrades when models must access relevant information in the middle of long contexts. (Liu et\u00a0al., 2023a). Therefore, we artificially truncate the input document to certain sizes (all not larger than the context window size of above mentioned models) by concatenating the head and tail of the input. For example, when we want to truncate a long document to 16k, we concatenate its head 8k tokens and tail 8k tokens before feeding it to an LLM.\n\n\nFigure 3: An overview performance of LLMs on LooGLE\n for long context understanding\n\n\n\n\n\n4.2 Evaluation methods and metrics\n\nAutomatic evaluation\n\nWe adopt several automatic evaluation metrics, which can be categorized into two types. Bleu, Rouge, Meteor Score and Bert Score\u00a0(Li et\u00a0al., 2023b; Mukherjee & Rahman, 2023) are widely used for generative tasks such as summarization and QA. They evaluate the matching between groundtruth and LLM answers mainly based on n-gram matching and semantic similarity. For Cloze, Exact Match and Partial Match\u00a0(Sharma et\u00a0al., 2023; Engelbach et\u00a0al., 2023) are employed in our evaluation. Exact Match entails the predicted entity and the groundtruth entity exactly match each other while Partial Match allows for fuzzy matching.\n\n\n\nGPT4-as-judgment\n\nMost automatic evaluation metrics are sensitive to semantic expression, output format, and length. Thus, these metrics alone might be insufficient for effectively comparing different models (some models might output answers in a style more similar to groundtruth). However, recent research has shown that the GPT4 evaluator exhibits high consistency with human evaluation and can serve as a reliable annotator to some extent\u00a0(Suri et\u00a0al., 2023; Liu et\u00a0al., 2023b; Zheng et\u00a0al., 2023). To provide a more comprehensive assessment of models, we utilize GPT4-8k as an LLM evaluator. For QA task, given one question and two answers provided by the groundtruth and the LLM\u2019s prediction, we ask GPT4-8k to judge whether the two answers are semantically the same or not. Then we calculate the accuracy that LLM answers match the groundtruth. For summarization task, given the predicted summary with the goundtruth, we ask LLM to give a score considering various factors for generation. The prompts implemented can be found in Appendix\u00a0D.\n\n\n\nHuman evaluation\n\nWe also include human evaluation for reference, where we manually check whether LLM\u2019s prediction matches the groundtruth.\n\n\nTable 3: Performance of the short dependency tasks\n\n\n\n\n\nShort dependency QA\nCloze\n\n\n\n\nModels\n\n\n\n\nContext\n\n\n\n\nBleu1\n\n\n\n\nBleu4\n\n\n\n\nRouge1\n\n\n\n\nRouge4\n\n\n\n\nRougeL\n\n\n\n\nMeteor_score\n\n\n\n\nBert_score\n\n\n\n\nGPT4_score\n\n\n\n\nExact Match\n\n\n\n\nPartial Match\n\n\n\n\nGPT4-32k\n32k\n24.61\n11.14\n61.80\n50.73\n60.75\n32.94\n78.72\n71.52\n70.5\n80.81\n\n\nGPT4-8k\n8K\n27.35\n14.38\n67.59\n56.01\n65.77\n38.56\n87.93\n53.99\n66.03\n76.62\n\n\nGPT3.5-turbo-16k\n16K\n22.67\n9.62\n62.56\n48.63\n60.66\n32.58\n87.04\n66.82\n54.64\n63.42\n\n\nLlamaIndex\n\\\n33.37\n21.43\n58.82\n42.93\n57.08\n37.17\n86.58\n59.61\n58.95\n66.86\n\n\nChatGLM2-6B\n32k\n14.29\n6.07\n20.50\n13.16\n20.36\n13.08\n87.28\n23.65\n0.05\n0.98\n\n\nLongLLaMa-3B\n256k\n1.37\n0.26\n26.97\n11.02\n26.10\n11.34\n71.65\n13.75\n-\n2.13\n\n\nRWKV-4-14B-pile\n8k\n0.80\n0.04\n21.7\n6.39\n20.64\n9.41\n70.42\n8.93\n-\n-\n\n\nLLaMA2-7B-32K\n32k\n0.18\n7.25*e-308\n1.86\n0.00\n1.86\n1.52\n61.53\n3.18\n-\n0.58\n\n\n\n\n\nTable 4: Performance of the long dependency tasks\n\n\n\n\n\nModels\n\n\n\n\nContext\n\n\n\n\nBleu1\n\n\n\n\nBleu4\n\n\n\n\nRouge1\n\n\n\n\nRouge4\n\n\n\n\nRougeL\n\n\n\n\nMeteor_score\n\n\n\n\nBert_score\n\n\n\n\nGPT4_score\n\n\n\n\narXiv paper summarization\n\n\n\n\n\n\n\n\n\n\n\nGPT4-32k\n32k\n24.50\n0.73\n27.15\n7.10\n24.25\n19.03\n84.04\n82.84\n\n\nGPT4-8k\n8k\n29.02\n2.09\n32.08\n11.11\n28.85\n22.64\n84.92\n85.42\n\n\nGPT3.5-turbo-16k\n16k\n28.70\n1.59\n32.04\n10.69\n28.89\n22.34\n84.82\n86.84\n\n\nLlamaIndex\n\\\n22.53\n0.63\n26.28\n6.97\n23.73\n21.07\n83.09\n76.35\n\n\nChatGLM2-6B\n32k\n0.04\n1.60e-310\n5.97\n8.43e-05\n5.82\n6.40\n73.25\n13.23\n\n\nLongLLaMa-3B\n256k\n4.24\n9.32e-309\n4.10\n0.52\n3.86\n3.82\n73.41\n12.28\n\n\nRWKV-4-14B-pile\n8k\n6.28\n4.58e-05\n6.45\n0.74\n6.01\n6.00\n75.28\n7.02\n\n\nLLaMA2-7B-32K\n32k\n0.03\n4.66e-310\n0.12\n0.00\n0.12\n0.67\n71.21\n7.60\n\n\nLong dependency QA\n\n\n\n\n\n\n\n\n\n\n\nGPT4-32k\n32k\n8.55\n1.40\n25.59\n6.36\n24.04\n11.13\n80.16\n54.09\n\n\nGPT4-8k\n8k\n8.94\n1.01\n23.45\n6.57\n21.69\n10.18\n85.36\n42.12\n\n\nGPT3.5-turbo-16k\n16k\n6.92\n1.81\n25.02\n6.68\n23.63\n10.40\n83.79\n45.04\n\n\nLlamaIndex\n\\\n7.76\n1.24\n23.62\n7.10\n22.30\n10.47\n83.87\n37.63\n\n\nChatGLM2-6B\n32k\n5.55\n0.11\n9.41\n1.93\n8.69\n4.39\n85.78\n11.50\n\n\nLongLLaMa-3B\n256k\n1.04\n3.12e-307\n2.96\n0.03\n2.71\n1.66\n78.60\n6.48\n\n\nRWKV-4-14B-pile\n8k\n0.71\n9.52e-307\n18.54\n1.55\n17.69\n3.45\n71.36\n5.33\n\n\nLLaMA2-7B-32K\n32k\n0.08\n2.44e-308\n2.05\n0.00\n2.05\n0.46\n50.28\n4.18\n\n\n\n\n\n\n\n\n4.3 Results\n\nFig.\u00a03 shows an overall performance comparison of different models on different tasks. The first radar plot shows the original accuracy evaluated by GPT4-8k (except cloze) and the partial match result (for cloze) over different tasks. For better visualization, we scale the scores of all models on each task to [40, 100] in the second radar plot and the histogram, so that the best model on each task has a score of 100 and the worst model has a score of 40. From the charts, GPT4-32k demonstrates its impressive overall performance across all tasks (with highest scores on all tasks except summarization). In comparison, open-source models show a significant performance gap to commercial models on our benchmark. From the first radar chart, we can find that among the 7 major tasks, short QA, cloze and summarization are more effectively addressed by LLMs, while real long dependency QA tasks are far from being solved, where even GPT4-32k hardly achieves over 40% accuracy. The empirical results demonstrate that even the most successful commercial model still cannot effectively address those really challenging long dependency tasks, leaving large room for improvement. Detailed evaluation results and further analysis can be found in the following sections.\n\n\n\n4.3.1 Main results on long and short dependency tasks\n\nResults on short dependency tasks\u00a0\n\nTab.\u00a03 presents the performance (%) of all the baselines on LooGLE\n in short dependency tasks.\nNotably, GPT4-32k attains the highest accuracy according to the GPT4 evaluator\u2019s perspective. GPT4-8k, GPT3.5-turbo-16k, and the retrieval-based LlamaIndex closely follow, demonstrating competitive performance levels. Surprisingly, GPT4-8k exhibits the most robust overall performance in terms of automatic evaluation metrics. It\u2019s worth mentioning that GPT4-32k, due to its tendency to generate longer outputs, faces penalties from these automatic metrics. This discrepancy among different metrics highlights the need for improved evaluation methods. Furthermore, in the context of cloze tasks, GPT4-32k excels again when equipped with a longer context window. In Fig.\u00a05, the exact match results in cloze tasks are displayed for varying source segment lengths. The results show that as the segment length increases, model performance gradually decreases, underscoring the increasing difficulty of effectively filling in the masked entities with longer source text.\n\n\n\nResults on long dependency tasks\u00a0\n\nTab.\u00a04 shows the aggregated results on long dependency tasks. Firstly, we can observe that summarization can be well addressed by commercial models, with GPT-4 evaluation accuracy of over 80%. However, the various types of long dependency QAs in our benchmark apparently pose substantial challenges for current LLMs. Both open-source and commercial models experience a significant performance decline. We will analyze model performance on individual types of QAs in Section\u00a04.3.2.\nIt is validated that longer context window size (thus less information loss due to truncation) indeed helps in long context tasks by comparing GPT4-32k with GPT4-8k. GPT4-8k has a much lower accuracy by answering \u201cThe text does not provide information on \u2026\u201d in many cases. Open-sourced models fall far below the average of commercial models, among which LLaMA2-7B-32K and RWKV-4-14B-pile display almost zero performance. By employing context scaling techniques like positional interpolation, RNN and fine-tuning on longer texts, current LLMs can be equipped with much longer context windows than their default limits. Nevertheless, our results show that there is still a huge discrepancy between merely increasing the context window size and really understanding the long context. The poor performance on long dependency QAs suggests that we may need to revisit LLMs\u2019 long context understanding ability in more challenging tasks other than some simple ones like summarization and retrieval, as they are unable to test whether LLMs understand the inter-dependency in long texts. \n\n\nTable 5: Impact of input length on long dependency tasks\n\n\n\n\n\nModels\n\n\n\n\nContext\n\n\n\n\nBleu1\n\n\n\n\nBleu4\n\n\n\n\nRouge1\n\n\n\n\nRouge4\n\n\n\n\nRougeL\n\n\n\n\nMeteor_score\n\n\n\n\nBert_score\n\n\n\n\nGPT4_score\n\n\n\n\narXiv paper summarization\n\n\n\n\nGPT4-32k\n32k\n24.50\n0.73\n27.15\n7.10\n24.25\n19.03\n84.04\n82.84\n\n\nGPT4-32k\n24k\n25.57\n0.81\n27.61\n7.53\n24.73\n19.86\n84.07\n83.15\n\n\nGPT4-32k\n16k\n24.80\n0.70\n27.29\n7.26\n24.28\n19.12\n84.11\n82.82\n\n\nGPT4-32k\n8k\n26.26\n9.35\n27.83\n7.67\n24.74\n20.08\n84.10\n82.75\n\n\nGPT4-8k\n8k\n29.02\n2.09\n32.08\n11.11\n28.85\n22.64\n84.92\n85.42\n\n\nLong dependency QA\n\n\n\n\nGPT4-32k\n32k\n7.64\n1.24\n15.53\n4.46\n14.60\n11.12\n86.07\n54.65\n\n\nGPT4-32k\n24k\n8.23\n1.66\n14.92\n4.12\n13.90\n10.60\n86.16\n50.61\n\n\nGPT4-32k\n16k\n8.57\n1.35\n16.21\n4.30\n14.90\n11.91\n86.36\n47.55\n\n\nGPT4-32k\n8k\n7.46\n1.77\n13.75\n5.08\n12.89\n10.01\n85.77\n38.34\n\n\nGPT4-8k\n8k\n8.94\n1.01\n23.45\n6.57\n21.69\n10.18\n85.36\n42.12\n\n\n\n\n\nTable 6: Performance of the long dependency QA with LlamaIndex\n\n\n\n\n\nModels\n\n\n\n\nContext\n\n\n\n\nBleu1\n\n\n\n\nBleu4\n\n\n\n\nRouge1\n\n\n\n\nRouge4\n\n\n\n\nRougeL\n\n\n\n\nMeteor_score\n\n\n\n\nBert_score\n\n\n\n\nGPT4_score\n\n\n\n\nGPT4-32k\n32k\n6.08\n1.31\n10.27\n3.39\n9.52\n8.54\n85.27\n28.25\n\n\nGPT4-8k\n8k\n6.62\n1.50\n11.95\n3.80\n10.99\n9.02\n85.51\n26.34\n\n\nGPT3.5-turbo-16k\n16k\n6.50\n0.92\n10.93\n3.56\n9.86\n8.65\n85.63\n33.24\n\n\nDefault\n\\\n7.02\n1.24\n11.60\n3.75\n10.57\n9.37\n85.61\n33.16\n\n\nChatGLM2-6B-32k\n32k\n0.15\n2.82e-310\n2.23\n0.05\n2.23\n0.74\n83.40\n7.73\n\n\nLongLLaMa-3B\n256k\n1.04\n4.22e-311\n2.27\n0.00\n2.23\n2.23\n82.18\n5.33\n\n\nRWKV-4-14B-pile\n8k\n2.65\n8.09e-307\n4.08\n0.33\n3.65\n3.92\n80.74\n2.43\n\n\nLLaMA2-7B-32K\n32k\n0.43\n1.29e-307\n5.85\n0.00\n5.85\n1.04\n81.38\n6.76\n\n\n\n\n\n\n\n\n4.3.2 Deep dive into long context understanding capabilities\n\nIn this section, we analyze different factors affecting the long context understanding abilities of LLMs, and dive into individual types of long dependency QAs to check LLMs\u2019 limitations.\n\n\nImpact of varying input length\u00a0\n\nIn Tab.\u00a05, we study the impact of varying lengths of inputs on long dependency tasks with GPT4 models. We find that expanding input length hardly helps in paper summarization while it substantially enhances the model\u2019s performance on long dependency QAs. The difference can be attributed to the inherent nature of the arXiv paper. It has both the introduction and conclusion sections located at the beginning and in the end respectively, which already contain the major sketch of the paper. Meanwhile, in our expectation, longer input promotes the performance of long dependency QAs by introducing less information loss.\n\n\n\nRetrieval Based Techniques\u00a0\n\nTo evaluate the effectiveness of retrieval techniques for long-context dependency questions, we undertook an extensive series of experiments on our long dependency QA tasks by replacing the base LLM model in LlamaIndex with different baseline LLMs. In these experiments, we utilized the open-source embedding all-mpnet-base-v2 \u00a0(Song et\u00a0al., 2020). When compared to the default embedding, text-embedding-ada-002 (https://openai.com/blog/new-and-improved-embedding-model), there was a noticeable performance decline. Nonetheless, this disparity did not hinder our conclusions.\nFrom Tab.\u00a04 and Tab.\u00a06, our research findings reveal that the incorporation of retrieval techniques does not generally enhance the performance of long dependency QA tasks. There is a conspicuous performance decline, particularly evident for models like GPT4-8k and GPT4-32k. It can be attributed to the tendency of GPT models to produce longer outputs, sometimes including hallucinatory information, when the retrieved segments lack sufficient context. The phenomenon highlights the intricacy of our dataset, where a series of long dependency understanding and modeling capabilities such as comprehension and multi-hop reasoning are essentially needed. Relying solely on retrieval mechanisms might be insufficient in recalling the necessary information and further generating the final answer, resulting in a marked performance decline. However, we did observe an minor improvement in the BERT score for open-source models. This improvement in fluency can be attributed to the considerably shorter length of the retrieved segments used as inputs, in contrast to the entirety of the document.\n\n\n\nIndividual results on different types of long dependency QAs\u00a0\n\nPrevious research mostly focuses on presenting aggregated results for long dependency QA tasks across various question types. Differently, in this study, our objective is to delve into the performance of models in individual tasks that demand diverse capabilities, including reading comprehension, information retrieval, computation, and reasoning. In this regard, we employed GPT4 as the evaluator, and the accuracy results are available in Tab.\u00a07. Across the four tasks examined, LLMs generally exhibit strong performance in comprehension, reasoning, and multiple information retrieval, but fall short in tasks related to timeline reordering and computation. Furthermore, we observed that the way questions are framed has a significant impact on LLMs\u2019 performance. Yes-no questions and multiple-choice questions tend to be easier for LLMs to answer, particularly when the search space is limited, as opposed to open-ended questions within unstructured text.\n\n\nTable 7: Individual results on four types of long dependency QAs evaluated by GPT4\n\n\n\n\n\nModels\n\n\n\n\nContext\n\n\n\n\n \n\n\nInformation retrieval\n\n\n\n\n\n \n\n\nTimeline reorder\n\n\n\n\n\nComputation\n\n\n\n\n \n\n\nComprehension and reasoning\n\n\n\n\n\nGPT4-32k\n32k\n43.60\n64.43\n37.36\n61.26\n\n\nGPT4-8k\n8K\n31.89\n61.36\n22.54\n45.78\n\n\nGPT3.5-turbo-16k\n16K\n36.86\n55.73\n24.73\n51.09\n\n\nLlamaIndex\n\\\n27.60\n47.83\n19.78\n43.83\n\n\nChatGLM2-6B-32k\n32k\n12.47\n14.17\n5.43\n11.08\n\n\nLongLLaMa-3B\n256k\n3.82\n6.48\n5.15\n10.17\n\n\nRWKV-4-14B-pile\n8k\n4.67\n5.19\n4.40\n7.13\n\n\nLLaMA2-7B-32K\n32k\n3.01\n1.61\n1.12\n6.85\n\n\n\n\n\nTable 8: Performance of Timeline Reorder\n\n\n\n\n\nModels\n\n\n\n\nLSD\n\n\n\n\nLMD\n\n\n\n\nSD\n\n\n\n\nSDD\n\n\n\n\nLSD-S\n\n\n\n\nLMD-S\n\n\n\n\nSD-S\n\n\n\n\nSDD-S\n\n\n\n\nNon-standard(%)\n\n\n\n\nGPT4-32k\n1.04\n0.57\n0.93\n1.12\n1.21\n0.82\n1.41\n1.60\n52.80\n\n\nGPT4-8k\n1.24\n0.64\n1.04\n1.281\n1.43\n0.92\n1.51\n1.74\n49.31\n\n\nLlamaIndex\n1.55\n0.78\n1.19\n1.551\n1.95\n1.08\n1.65\n2.09\n39.72\n\n\nGPT3.5-turbo-16k\n3.58\n1.43\n2.17\n2.916\n1.05\n0.86\n1.24\n1.26\n77.21\n\n\nLongLLaMa-3B\n4.18\n1.71\n2.59\n3.30\n1.90\n1.12\n1.80\n2.07\n92.92\n\n\nChatGLM2-6B-32k\n4.31\n1.74\n2.63\n3.37\n1.83\n1.17\n1.50\n2.00\n99.07\n\n\nRWKV-4-14B-pile\n4.33\n1.75\n2.64\n3.38\n1.90\n0.97\n1.00\n1.75\n98.13\n\n\nLLaMA2-7B-32K\n4.33\n1.75\n2.64\n3.38\n2.50\n1.17\n1.33\n2.33\n98.60\n\n\n\n\n\n\nResults on long dependency QAs with/without CoT\u00a0\n\nTo bolster the long-context capabilities of LLMs, we conducted additional experiments designed to unlock their potential using the Chain of Thoughts\u00a0(CoT) framework\u00a0(Kojima et\u00a0al., 2023). We selected LlamaIndex as a representative model, given its impressive performance in both short and long dependency question-answering tasks, alongside strong commercial models such as GPT4. A manual evaluation was carried out on a subset comprising one-third of instances from each task category within long dependency QA. We initiated the LLM with a zero-shot CoT approach, employing prompts such as \u201cLet\u2019s think step by step,\u201d and furnished a few-shot setup with detailed rationales and standard output formats\u00a0Wei et\u00a0al. (2023) to facilitate responses to long dependency questions. As depicted in Fig.\u00a05, the zero-shot CoT approach had minimal impact on accuracy in comprehension and reasoning, as well as multiple retrieval tasks, but yielded a substantial 20% and 10% absolute accuracy increase in timeline reorder and computation. Interestingly, the few-shot CoT approach benefits the first two types but surprisingly leads to a decline in performance in the latter two types compared with zero-shot.\nWe hypothesize the reason is that the evidence and rationales in few-shot examples cannot be generalized to other questions, and including them might on the contrary give wrong guidance to the model.\n\n\n\n\n\nFigure 4: Performance of varying segments\n\n\n\n\nFigure 5: Long dependency QA tasks w/o CoT\n\n\n\n\n\n\nAutomatic evaluation on timeline reorder\n\nIn order to evaluate the performance of time reorder task outputs, it is essential to address discrepancies arising from the diverse formats produced by various models. Typically, these outputs comprise conventional numerical sequences, but errors in non-standard formats when evaluation necessitate preprocessing for accurate assessment. A proposed approach involves converting the serial numbers in the candidate answers from their original question into Roman numbers (i.e., I, II, \u22ef\u22ef\\cdots\u22ef), thereby enhancing discrimination through regular expression matching. Four key metrics, namely, LSD (location square deviation), LMD (location mean deviation), SD (swap deviation), and SDD (swap distance deviation), are employed to measure the similarity of numeric sequences, refer to Appendix\u00a0C for metric details. Smaller deviations indicate a higher degree of resemblance between the sequences. Any outputs that are empty, possess unequal lengths, or contain extra elements are categorized as non-standard. The maximum deviation between the provided ground truth and all corresponding candidate answers is computed as the worst score for evaluation purposes.\n\n\nThe percentage of non-standard outputs for each model and corresponding performances can be found in Tab.\u00a08.\nAs seen, it is evident that except for GPT4, which demonstrates a remarkable degree of adherence and alignment following Reinforcement Learning from Human Feedback (RLHF)\u00a0(Lee et\u00a0al., 2023), most open-sourced models struggle to generate texts in the correct format with less than 10%. However, this issue can be mitigated in significantly large models through the utilization of few-shot\nprompts and mandatory instructions. This phenomenon results in performance\npenalties when assessed using automated metrics. Consequently, to ascertain the\ngenuine capacity of LLMs in this task, we calculate the four metrics exclusively for outputs in standard format (\u201c-S\u201d).\n\n\n\nDispcrepancy in generated outputs of models\n\nDistributions of generated outputs of various models are depicted in Fig.\u00a06.\nIt is noteworthy that well-behaved models consistently produce shorter responses, averaging around 50 words, irrespective of the question type, particularly in short-term question answering scenarios. In contrast, models fine-tuned with longer textual inputs, such as LLaMA2-7B-32K, RWKV-4-14B-pile, and LongLLaMa-3B, tend to yield significantly lengthier responses, even when a maximum generation constraint of 500 tokens is enforced. An interesting deviation is observed in LongLLaMa-3B, which demonstrates variability in response lengths across both tasks. This behavior may stem from challenges in comprehending and addressing exceedingly complex long question-answering tasks. Consequently, the model appears to prioritize extracting a maximum number of pertinent contexts from its memory to generate sufficiently extensive responses that are deemed acceptable and rational.\n\n\nFigure 6: Output distributions on QA tasks\n\n\nMoreover, upon closer examination of model outputs, a significant disparity in generation quality is observed across various LLMs and task types, indicating a non-specific issue. Notably, commercial models like GPT4, GPT3.5, and LlamaIndex consistently generate outputs that exhibit a higher degree of human-likeness, completeness, and logical coherence within a structured format. These models consistently deliver contextually relevant, query-based responses. In contrast, open-sourced models, such as ChatGLM2-6B-32k, tend to offer shorter answers, occasionally confined to numeric responses. In cases where a definite answer is lacking, ChatGLM2-6B-32k compensates by retrieving relevant contextual information. However, the RNN-based model RWKV-4-14B-pile often generates duplicated responses or resorts to repeating the given questions to reach the maximum token length, sometimes resorting to code generation to address issues related to its training data. The performance of the LLaMA2-7B-32K model is notably worse, as it sporadically produces irrelevant or nonsensical text, along with the inclusion of special symbols when it fails to provide meaningful answers. More examples of outputs from different models can be seen in Appendix\u00a0F.\n\n\n\nProbable explanations for long QA bad cases\n\nTable 9: Bad cases study on the long dependency QA\n\n\n\n\n \n\n\nLong QA Tasks\n\nHallucination\u2217\n\n\n \n\n\nRedundant\n\nretrieval\u2020\n\n\n \n\n\nInsufficient\n\nretrieval\u22c6\n\n\n \n\n\nIrrelevant\n\nanswer\u22c4\n\n\n \n\n\nNo relevant\n\ncontext\u2227\n\n\n \n\n\nWrong/No\n\nreasoning\u00d7\n\nOthers\n\n\n\n \n\n\nComputation\n\n31.11\n24.44\n15.56\n0.00\n20.00\n0.00\n8.88\n\n\n\n \n\n\nMultiple information retrieval\n\n14.71\n31.37\n28.43\n13.73\n13.73\n0.00\n7.84\n\n\n\n \n\n\nComprehension and reasoning\n\n14.29\n10.99\n21.98\n18.68\n16.48\n10.99\n6.59\n\n\n\n\u2217 Evidence of predictions is not shown up in the original inputs and generated by LLM itself from nowhere.\n\n\n\n\u2020 Apart from the right evidence, irrelevant evidence is also redundantly retrieved.\n\n\n\n\u22c6 Not all of the essential evidence to answer the question is retrieved.\n\n\n\n\u22c4 Evidence of predictions generated have no or minor correlation with the question.\n\n\n\n\u2227 No relevant context in LLM\u2019s memory and refuse to answer the question.\n\n\n\n\u00d7 Fail to retrieve evidence that needs further reasoning other than directly extracting from the inputs.\n\n\n\n\n\nTo investigate whether the models have effectively memorized and comprehended lengthy contextual information, we conducted a comprehensive manual analysis of the underlying causes of failures in each long question-answering task. The rationale behind CoT analysis aided in understanding how models decompose and tackle challenges associated with extended dependency-based QA. Our observations reveal that LLMs struggle with these tasks primarily due to their inability to extract precise information and a propensity to generate responses that lack factual accuracy. Constraints imposed by the inherent context window limitations, coupled with information loss resulting from the optimized Transformer and position encoding, contribute to their struggles in memorizing the original extensive contexts. In most cases, models attempt to compensate by retrieving and integrating the most pertinent evidence, even if it results in redundant answers. However, they also acknowledge their insufficient context and, at times, abstain from providing responses rather than resorting to nonsensical answers. Furthermore, addressing these challenges necessitates enhanced comprehension and reasoning abilities, particularly when answers are not clearly evident across multiple pieces of evidence scattered throughout the raw texts. The insights derived from our benchmark analysis offer a scientific foundation and pave the way for promising research directions aimed at augmenting LLM capabilities for handling long contextual inputs. These findings underscore the need for further progress in comprehension, computation, and reasoning tasks using our dataset to effectively enhance LLMs\u2019 capacity to understand extended dependency contexts.\n\n\n\n\n",
      "5 Conclusion": "\n\n5 Conclusion\n\nThis paper introduces a novel benchmark, LooGLE\n, designed to facilitate the assessment of long-context comprehension by LLMs. LooGLE\n addresses the deficiencies present in previous datasets by offering considerably longer text passages, utilizing relatively new documents after 2022, incorporating multi-source materials from various categories, and notably featuring meticulously designed and annotated tasks with diverse contextual dependencies. Our extensive evaluations unveil substantial limitations in the capacity of existing LLMs to understand and reason about the intricate interdependencies present in lengthy texts, even when provided with considerably extended context windows. Furthermore, a notable disparity is observed between commercial and open-source models, with both exhibiting challenges in long dependency tasks as per our benchmark assessments. The outcomes underscore the utility of our dataset as a valuable reference for evaluating long-context comprehension and present avenues for potential enhancements in LLM performance.\n\n\n",
      "Appendix A More details of our dataset": "\n\nAppendix A More details of our dataset\n\nDistributions of the input length and dependency spanning in words for long dependency QA tasks are shown in Figs.\u00a08 and\u00a08. N-gram sunburst graph for generated QA pairs can be seen in Fig.\u00a09.\n\n\n\n\n\nFigure 7: Input document length distributions\n\n\n\n\nFigure 8: Evidence dependency distributions\n\n\n\n\n\n\n\n\n\n\n\nFigure 9: 4-gram sunburst graph for short and long dependency QA. (a) short dependency questions (b) short dependency answers (c) long dependency questions (d) long dependency answers\n\n",
      "Appendix B Task definition": "\n\nAppendix B Task definition\n\nThe Cloze task formulation process can be seen in Fig.\u00a010.\n\n\nFigure 10: Cloze task\n\n",
      "Appendix C Timeline reorder evaluation metrics": "\n\nAppendix C Timeline reorder evaluation metrics\n\nWe employ 4 metrics to measure the similarity of numeric output sequences for timeline reorder tasks. For given two numeric sequences A\ud835\udc34Aitalic_A and B\ud835\udc35Bitalic_B with the same sequence length n\ud835\udc5bnitalic_n, i\u2062[A]\ud835\udc56delimited-[]\ud835\udc34i[A]italic_i [ italic_A ] and i\u2062[B]\ud835\udc56delimited-[]\ud835\udc35i[B]italic_i [ italic_B ] is the i\ud835\udc56iitalic_ith number in each sequence. They can be computed using the formula below: \nLSD is the abbreviation for location square deviation:\n\n\n\n\n\n\nL\u2062S\u2062D\u2062(A,B)=1n\u2062\u2211i=0n\u22121(i\u2062[A]\u2212i\u2062[B])2\ud835\udc3f\ud835\udc46\ud835\udc37\ud835\udc34\ud835\udc351\ud835\udc5bsuperscriptsubscript\ud835\udc560\ud835\udc5b1superscript\ud835\udc56delimited-[]\ud835\udc34\ud835\udc56delimited-[]\ud835\udc352LSD(A,B)=\\frac{1}{n}\\sum_{i=0}^{n-1}(i[A]-i[B])^{2}italic_L italic_S italic_D ( italic_A , italic_B ) = divide start_ARG 1 end_ARG start_ARG italic_n end_ARG \u2211 start_POSTSUBSCRIPT italic_i = 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n - 1 end_POSTSUPERSCRIPT ( italic_i [ italic_A ] - italic_i [ italic_B ] ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT\n\n(1)\n\n\n\n\nLMD is the abbreviation for location mean deviation:\n\n\n\n\n\n\nL\u2062M\u2062D\u2062(A,B)=1n\u2062\u2211i=0n\u22121|i\u2062[A]\u2212i\u2062[B]|\ud835\udc3f\ud835\udc40\ud835\udc37\ud835\udc34\ud835\udc351\ud835\udc5bsuperscriptsubscript\ud835\udc560\ud835\udc5b1\ud835\udc56delimited-[]\ud835\udc34\ud835\udc56delimited-[]\ud835\udc35LMD(A,B)=\\frac{1}{n}\\sum_{i=0}^{n-1}\\left|i[A]-i[B]\\right|italic_L italic_M italic_D ( italic_A , italic_B ) = divide start_ARG 1 end_ARG start_ARG italic_n end_ARG \u2211 start_POSTSUBSCRIPT italic_i = 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n - 1 end_POSTSUPERSCRIPT | italic_i [ italic_A ] - italic_i [ italic_B ] |\n\n(2)\n\n\n\n\nSD is the abbreviation for swap deviation:\n\n\n\n\n\n\nSD(A,B)=min(W(A\u2192B)=min(\u2211s\u2208A\u2192B1)SD(A,B)=min(W(A\\to B)=min({\\textstyle\\sum_{s\\in A\\to B}}1)italic_S italic_D ( italic_A , italic_B ) = italic_m italic_i italic_n ( italic_W ( italic_A \u2192 italic_B ) = italic_m italic_i italic_n ( \u2211 start_POSTSUBSCRIPT italic_s \u2208 italic_A \u2192 italic_B end_POSTSUBSCRIPT 1 )\n\n(3)\n\n\n\n\nSDD is the swap distance deviation:\n\n\n\n\n\n\nSDD(A,B)=min(W(A\u2192B)=min(\u2211s\u2208A\u2192B|i\u2212j|)SDD(A,B)=min(W(A\\to B)=min({\\textstyle\\sum_{s\\in A\\to B}}\\left|i-j\\right|)italic_S italic_D italic_D ( italic_A , italic_B ) = italic_m italic_i italic_n ( italic_W ( italic_A \u2192 italic_B ) = italic_m italic_i italic_n ( \u2211 start_POSTSUBSCRIPT italic_s \u2208 italic_A \u2192 italic_B end_POSTSUBSCRIPT | italic_i - italic_j | )\n\n(4)\n\n\nwhere s=A\u2062(i,j)\ud835\udc60\ud835\udc34\ud835\udc56\ud835\udc57s=A(i,j)italic_s = italic_A ( italic_i , italic_j ) means the swap between the i\ud835\udc56iitalic_ith and j\ud835\udc57jitalic_jth element in A\ud835\udc34Aitalic_A. S=A\u2192B\ud835\udc46\ud835\udc34\u2192\ud835\udc35S=A\\to Bitalic_S = italic_A \u2192 italic_B means a series of swap actions to convert A\ud835\udc34Aitalic_A to B\ud835\udc35Bitalic_B. W\u2062(S)=\u2211s\u2208Sw\u2062(s)\ud835\udc4a\ud835\udc46subscript\ud835\udc60\ud835\udc46\ud835\udc64\ud835\udc60W(S)={\\textstyle\\sum_{s\\in S}}w(s)italic_W ( italic_S ) = \u2211 start_POSTSUBSCRIPT italic_s \u2208 italic_S end_POSTSUBSCRIPT italic_w ( italic_s ) means the weights sum of all the swap actions in S\ud835\udc46Sitalic_S, where w\u2062(s)=1\ud835\udc64\ud835\udc601w(s)=1italic_w ( italic_s ) = 1 in SD and w\u2062(s)=|i\u2212j|\ud835\udc64\ud835\udc60\ud835\udc56\ud835\udc57w(s)=\\left|i-j\\right|italic_w ( italic_s ) = | italic_i - italic_j | in SSD.\n\n",
      "Appendix D Prompts": "\n\nAppendix D Prompts\n\n\nD.1 Short dependency QA pair generation\n\n\n\n[seg] = {Input long texts}\nPlease generate 2 questions and corresponding answers based on given [seg] in less words as possible. Return reference text S, question Q and answer A from [seg] in json format as: {\u201cS\u201d: ,\u201cQ\u201d: ,\u201cA\u201d: },{\u201cS\u201d: ,\u201cQ\u201d: ,\u201cA\u201d: }.\n\n\n\n\n\nD.2 Short and long dependency question and answering\n\n\n\nInstruction:\nPlease answer the question based on the given long texts below.\n{Input long texts}\nQuestion: {Question}\nAnswer:\n\n\n\n\n\nD.3 scripts segment summarization for cloze formulation\n\n\n\nInstruction:\nPlease write a summary for this script segment within 500 words, focusing on describing objective facts and avoiding subjective opinions.\n{scripts segement}\nSummary:\n\n\n\n\n\nD.4 Cloze\n\n\n\nexample= {\u201c<mask-0>\u201d: \u201cBob\u201d, \u201c<mask-1\u201d>: \u201cGorrosion Magazine\u201d, \u201c<mask-2>\u201d: \u201cBethel Horizon\u201d}\nInstruction:\nPlease answer the cloze question based on the given long texts below. Each of the placeholder \u201c<mask-n>\u201d in the question could be an entity of Person, Location or Organization. The same masks represent the same entity.\nOutput a json format answer, for example:{example}\n{Input long texts}\nQuestion: {Question} What are the masked entities?\nAnswer:\n\n\n\n\n\nD.5 Summarization\n\n\n\nInstruction:\nPlease generate a summary of the below paper.\n{Input long texts}\nSummarization:\n\n\n\n\n\nD.6 Timeline reorder\n\n\n\nInstruction:\nPlease answer the question based on the given long texts below. Please use Roman numbers only for answers.\n{Input long texts}\nQuestion: {Question}\nAnswer:\n\n\n\n\n\nD.7 QA task evaluation by LLM (GPT4)\n\n\n\nInstruction:\nGiven one question, there is a groundtruth and a predict answer. Please decide whether they are the same or not in semantic. Please only output True or False.\nQuestion: {Question}\ngroundtruth = {Reference answer}\npredicted answer = {Generated output}\n\n\n\n\n\nD.8 Summarization task evaluation by LLM (GPT4)\n\n\n\nInstruction:\nPlease compare predicted summary with the goundtruth and evaluate the predicted summary from the perspectives of information completeness, consistency, fluency, and grammar by giving a score within the range of 0 to 100.\ngroundtruth = {Reference answer}\npredicted summary = {Generated output}\n\n\n\n\n\nD.9 Few-Shot CoT for long QA\n\n\n\nInstruction:\nPlease answer the question based on the given long texts below.\n{Input long texts}\n{Demonstrations}\nQuestion: {Question}\nAnswer:\n\n\n\n\n\nD.10 Zero-Shot CoT for long QA\n\n\n\nInstruction:\nPlease answer the question based on the given long texts below.\n{Input long texts}\nQuestion: {Question}\nAnswer: Let\u2019s think step by step.\n\n\n\n",
      "Appendix E Examples for long context understanding tasks": "\n\nAppendix E Examples for long context understanding tasks\n\n\nE.1 Short dependency question and answering\n\n\nQuestion:\nWho did Picardo collaborate with for building preservation and restoration projects?\nEvidence:\nOn qualifying in 1951, Picardo pursued his interest in historical architecture by collaborating on a number of building preservation and restoration projects with the Spanish architect and architectural historian Fernando Chueca Goitia, who was 8 years his senior.\nAnswer:\nHe collaborated with Spanish architect and architectural historian Fernando Chueca Goitia.\n\n\n\n\nQuestion:\nWhat was the nickname given to the 18th century period?\nEvidence:\nThe 18th century was nicknamed the \u2019Age of Enlightenment\u2019, as it was the period in which the Enlightenment emerged, a philosophical movement that defended reason and science against religious dogmatism.\nAnswer:\nThe Age of Enlightenment.\n\n\n\n\n\nE.2 Cloze\n\n\nQuestion:\n<mask-0>\u00a0 is shown hacking the GPS to delay Joey\u2019s arrival at <mask-1>, while Conrad disguises himself as Mr. Von Vanderveen and Jason Statham as Mr. Popov. Mike Miller, dressed as the Auctioneer, swaps out the violin cases, and <mask-2>is revealed to be the inside man. The crew successfully pulls off the heist, leaving a debris of blown-apart goons in their wake.\nLong input:\nWhen a caper crew needs something blown up for a heist, they call upon The Demolition Expert. They are often minor characters who are not given much screen \u2026.(104,094 words)\u2026. Joey is driving to the Big Buy, always craning back\u2026 like there\u2019s a phantom on his tail. Suddenly, the GPS chimes. GPS VOICE Rerouting . DRIVER Shit. Uh, boss, it says it just added twenty minutes. The speed past \u2013 A BLACK MUSTANG parked in a turnaround. Mary Beth in the driver\u2019s seat, clacking away on a laptop, hacking the GPS . \u2026.(150 words)\u2026.we couldn\u2019t have done it without Duncan\u2013 Reveal Duncan , smiling big. He raises his glass. FLASH: DUNCAN and TWO MORE GOONS hurry around the corner of the STADIUM HALLWAY and stop dead in their tracks when they see \u2013 A debris of blown-apart Goons littering the hallway. \u2026.(2,670 words).\nAnswer:\n{\u201c<mask-0>\u201d: \u201cMary Beth\u201d,\u00a0 \u201c<mask-1\u201d>: \u201cthe Big Buy\u201d,\u00a0 \u201c<mask-2>\u201d: \u201cDuncan\u201d}\n\n\n\n\n\nE.3 Summarization\n\n\nLong input:\nDistinction and quadratic base change for regular supercuspidal representations Chuijia Wang 1 Introduction Let be a connected reductive algebraic group over a non-archimedean local field with residual characteristic \u2026.(21,000 words)\u2026. Basically, one can describe all the characters of which occur in in terms of certain intersection property between the Kostant sections of and the orbit of the generic element associated to. \u2026.(500 words).\nSummary:\nIn this article, we study Prasad\u2019s conjecture for regular supercuspidalrepresentations based on the machinery developed by Hakim and Murnaghan tostudy distinguished representations, and the fundamental work of Kaletha onparameterization of regular supercuspidal representations. For regularsupercuspidal representations, we give some new interpretations of thenumerical quantities appearing in Prasad\u2019s formula, and reduce the proof to thecase of tori. The proof of Prasad\u2019s conjecture then reduces to a comparison ofvarious quadratic characters appearing naturally in the above process. We alsohave some new observations on these characters and study the relation betweenthem in detail. For some particular examples, we show the coincidence of thesecharacters, which gives a new purely local proof of Prasad\u2019s conjecture forregular supercuspidal representations of these groups. We also prove Prasad\u2019sconjecture for regular supercuspidal representations of G(E), when E/F isunramified and G is a general quasi-split reductive group.\n\n\n\n\n\nE.4 Multiple information retrieval\n\n\nQuestion:\nWhat were some of the architectural projects Jos\u00e9 Luis Picardo worked on?\nLong input:\nJos\u00e9 Luis Picardo \u2026.(1,520 words) \u2026. From the early 1960s to 1985 Picardo dedicated much of his professional life to the state-run hotel chain, Paradores de Turismo de Espa\u00f1a \u2026..(7,846 words) \u2026. In 1970 Picardo was invited to compete with fellow notable architects Javier Carvajal Ferrer [es] and Mariano Garc\u00eda Benito [es] for the contract to design and build a new headquarters building in the Salamanca neighbourhood of Madrid for the Fundaci\u00f3n Juan March (Juan March Foundation) which promotes Spanish culture and science \u2026.(651 words) \u2026. Picardo\u2019s commission from the Ministry was to design a sala de equitaci\u00f3n, a huge arena for horse and riding displays, in particular the school\u2019s signature performance \u201cComo Bailan los Caballos Andaluces\u201d (\u201cHow the Andalusian Horses Dance\u201d) which would seat up to 1,600 spectators. Connected to it were to be stable facilities for 60 horses \u2026.(1,113 words).\nAnswer:\nHe worked on hotel chain Paradores de Turismo de Espa\u00f1a, Fundaci\u00f3n Juan March, Sala de Equitaci\u00f3n.\nExplanation:\nBased on the deep understanding of given question, we need to extract all the evidence of architectural projects Jos\u00e9 Luis Picardo have worked on. There are total three works spreading in the original inputs independently as shown above.\n\n\n\n\n\nE.5 Timeline reorder\n\n\nQuestion:\nReorder the timeline of below events:\n1.restoration at Guadalupe,\n2.restore and rehabilitate the old Casa de la Inquisici\u00f3n\n3.castle conversion at Sig\u00fcenza,\n4.renovation and conversion of castle at Puebla de Alcocer\nLong input:\nJos\u00e9 Luis Picardo \u2026.(2,395 words) \u2026. Restoration at Guadalupe started in November 1963 and the hotel, with twenty double rooms, opened on 11 December 1965 \u2026.(1,472 words) \u2026. In 1965 Picardo was commissioned by Paradores to restore and rehabilitate the old Casa de la Inquisici\u00f3n (House of the Inquisition) in the small, historic village of Pedraza, 37 kilometres northeast of Segovia in Castilla y Le\u00f3n \u2026.(2,827 words) \u2026. In 1964 Picardo was involved, with the Ministry of Information and Tourism, in investigating old buildings for conversion into a new Parador in the Province of Guadalajara. Possible locations were the castle at Atienza and the Casa del Cord\u00f3n, an old inn in the same town, the castle at Molina de Arag\u00f3n and the castle at Sig\u00fcenza \u2026.(1,521 words) \u2026. Among the most advanced plans Picardo drew up were in 1969 for the renovation and conversion into a Parador of the castle at Puebla de Alcocer, a small municipality 70 miles east of M\u00e9rida in the Province of Badajoz in Extremadura \u2026.(2,897 words).\nAnswer:\n1, 3, 2, 4\nExplanation:\nThe four events provided in the question sequentially happen with thousands of words spanning. We firstly locate the exact sentences describing the event in the original inputs above. Then we reorder them based on the their occurrence.\n\n\n\n\n\nE.6 Computation\n\n\nQuestion:\nHow many inhabitants increases from the end of 19th to 1970?\nLong input:\nUrban planning of Barcelona \u2026.(5,558 words) \u2026. After the revolution of 1868, the Citadel was also demolished and the land transformed into a public park. The population grew, especially thanks to immigration from the rest of Spain, reaching 400,000 inhabitants by the end of the century. \u2026.(7,613 words) \u2026. In two decades it went from 1,280,179 inhabitants in 1950 to 1,745,142 in 1970 \u2026.(5,596 words).\nAnswer:\n1,345,142\nExplanation:\nFirstly, we locate the numeric of inhabitants which only appear between 1900 to 1970 from the input as evidence. There are three relevant numbers: 400,000, 280,179 and 1,745,142. Then we make computation on 1,745,142 - 400,000 = 1,345,142 to get the final answer.\n\n\n\n\n\nE.7 Comprehension and reasoning\n\n\nQuestion:\nWhich event is the turning point for territorial expansion in the 19th?\nLong input:\nUrban planning of Barcelona \u2026.(2,958 words) \u2026. At this time Barcelona was constituted as a county and later became part of the Crown of Aragon and the political and economic center of the Principality of Catalonia, becoming an important maritime and commercial axis of the Mediterranean Sea\u2026.(128 words) \u2026.The progressive increase in the size of the city, and its increasing urban, social and economic complexity, led to the creation of a specific system of government for the administration of the city, the Council of One Hundred (1,265)\u2026.(1,260 words) \u2026.The city was still confined within its walls \u2014the only expansion was on the beach, in the neighborhood of La Barceloneta\u2014 despite the fact that by the end of the period it had almost 100,000 inhabitants\u2026.(1,333 words) \u2026.Barcelona thus underwent an important leap to modernity, characterized by three factors: the population migration from the countryside to the city, the link between industrial and urban developments, and a better articulation of the territory through a wide network of roads and railroads, which will lead Barcelona to become a colonizing metropolis of its territorial environment\u2026..(1,319 words) \u2026.In the middle of the century a transcendental event took place that completely changed the physiognomy of the city; the demolition of the walls\u2026..(1,472 words) \u2026.Another service that emerged at the end of the century was the telephone. The first telephone communication in the whole peninsula took place in Barcelona, in 1877, between the Montjuic castle and the fortress of the Citadel \u2014in the process of dismantling but still housing a garrison\u2014. That same year the first interurban transmission between Barcelona and Girona was carried out by the company Dalmau i Fills, pioneer in the installation of lines in Barcelona\u2026\u2026(9,966 words).\nAnswer:\nThe demolition of the walls and the donation to the city of the fortress of the Citade.\nExplanation:\nThere is no exact answer that can be extracted directly from the original input and further reasoning is essentially needed. Firstly, we find all the relevant sentences or paragraphs on the territorial expansion which are in the 19th. Temporal events and key words act as clues for sorting out the evidence. Among the evidence, some are factual plots while others are critical comments. Based on further comprehension, we need to compare the various factors of territorial expansion underlying the evidence and decide the most decisive and influential turning point events to answer the question.\n\n\n\n",
      "Appendix F Examples of generated outputs for models": "\n\nAppendix F Examples of generated outputs for models\n\n\nF.1 GPT4-32k\n\n1. short QA\n\nQuestion: What were the charges against the two members of the Rock Machine Perth chapter?\nOutput: The two members of the Rock Machine Perth chapter, Kyle Barry and Brent Reker, were charged with extortion and demanding property with oral threats.\nGroundtruth: Extortion and demanding property with oral threats.\n\n\n\n\nQuestion: Why did the federal government create relocation programs for Native Americans?\nOutput: The federal government created relocation programs for Native Americans in an effort to expand their employment opportunities. Many Native Americans were living in rural and isolated areas, and the government believed that moving them to urban areas would offer better job prospects. This was part of a wider policy shift towards giving tribes more authority for self-determination.\nGroundtruth: To expand their employment opportunities.\n\n\n\n2. long QA\n\nQuestion: According to World Bank\u2019s estimation, what is the difference between the amount of World Bank\u2019s aid to Turkey and the amount of damaged suffered by Syria?\nOutput: The World Bank estimated $5.1 billion in damages suffered by Syria and it provided $1.78 billion in aid to Turkey. So, the difference is $3.32 billion.\nGroundtruth: 3.32 Billion.\n\n\n\n\nQuestion: What efforts did guy make to improve his life?\nOutput: Guy made several efforts to improve his life, including attending therapy sessions, building a tree house for his nephews, discussing his feelings and insecurities with his friends, and standing up for himself against his aggressive father. He also expressed his feelings to his ex-girlfriend Jen, trying to reconcile with her and offering her a house he planned to renovate. He made attempts to change his negative self-talk and confront his past traumas.\nGroundtruth: 1. Receive psychotherapy: Guy chooses to see a psychiatrist to face his own problems, which is an important step for him to change.\n2. Introspection: Guy gradually enlightened himself during the treatment, facing the relationship problems between him, his father and his friends.\n3. Write a letter to Jen: Guy wrote a sincere letter to Jen, expressing his change of attitude, trying to win back Jen\u2019s heart.\n4. Reaching out to old friends: Despite the embarrassment, Guy reached out to old friends and tried to work things out.\n5. Renovate the house: Guy takes on the job of renovating the house to plan for his future life.\n6. Dealing with the relationship with the father: Guy faces his father, expresses his thoughts, and no longer avoids this issue.\n7. Change yourself: Guy works hard, writes letters, receives treatment, etc., showing his determination to change himself.\n\n\n\n3. summarization\n\nOutput: This paper discusses the distinction and quadratic base change for regular supercuspidal representations. The author focuses on understanding Prasad\u2019s conjecture on the Galois distinction problem, which proposes a relationship between the distinction property of an irreducible representation and the base change property of its Langlands parameters. The author\u2019s work highlights that even though significant progress has been made in proving the conjecture, a general method that works for all Galois symmetric pairs would be interesting. It extends the understanding of the relationship between distinguished representations and Langlands functoriality. The results support the idea of using a purely local approach to prove the conjecture for regular supercuspidal representations without constraints on the Galois symmetric pair. The author also outlines strategies to improve the exact analysis of terms appearing in Prasad\u2019s identity. The article provides evidence supporting the conjecture by evaluating particular examples.\nGroundtruth: In this article, we study Prasad\u2019s conjecture for regular supercuspidal representations based on the machinery developed by Hakim and Murnaghan to study distinguished representations, and the fundamental work of Kaletha on parameterization of regular supercuspidal representations. For regular supercuspidal representations, we give some new interpretations of the numerical quantities appearing in Prasad\u2019s formula, and reduce the proof to the case of tori. The proof of Prasad\u2019s conjecture then reduces to a comparison of various quadratic characters appearing naturally in the above process. We also have some new observations on these characters and study the relation between them in detail. For some particular examples, we show the coincidence of these characters, which gives a new purely local proof of Prasad\u2019s conjecture for regular supercuspidal representations of these groups. We also prove Prasad\u2019s conjecture for regular supercuspidal representations of G(E), when E/F is unramified and G is a general quasi-split reductive group.\n\n\n\n4. cloze\n\nQuestion: The script segment of \u201c Wildfire 2022\u201d takes place in Tulare County, California, where the sky is grey and ashy due to the nearby fires. The story follows <mask-1>, a seven-year-old mute girl, who runs to school along a busy road. <mask-0>, a Trans Latin Woman, is seen singing and dancing to \u201c Opposites Attract\u201d while trying to fix her old car. Lu goes to school and stands in line for lunch, where she is ignored by other kids. She pays for her lunch in change and sits alone at the back of the cafeteria. Later, Fireman <mask-2>speaks to Mrs. Consuelos\u2019 third-grade classroom about fire safety, and Lu is chosen to answer a question despite not speaking. The leading cause of wildfires is discussed, and it is revealed that humans are the leading cause of wildfires due to accidents like leaving an open campfire burning or tossing a cigarette out a car window.\nOutput: {\u201c<mask-0>\u201d: \u201cMerribelle\u201d, \u201c<mask-1>\u201d: \u201cLu\u201d, \u201c<mask-2>\u201d: \u201cJosh\u201d}\nGroundtruth: {\u201c<mask-0>\u201d: \u201cMerribelle\u201d, \u201c<mask-1>\u201d: \u201cLu\u201d, \u201c<mask-2>\u201d: \u201cJosh\u201d}\n\n\n\n\n\nF.2 GPT4-8k\n\n1. short QA\n\nQuestion: When did large cattle ranches start to appear in the area?\nOutput: As early as 1734.\nGroundtruth: As early as 1734.\n\n\n\n\nQuestion: What contributed to the economic growth of Rio Grande do Sul at the end of the 19th century?\nOutput: The economic growth of Rio Grande do Sul at the end of the 19th century was contributed by the growth of trade, the arrival of immigrants of other origins such as Italians and Jews, and the establishment of a growing industry and a rich bourgeois class. The state had become the third largest economy in Brazil by the turn of the 20th century.\nGroundtruth: Trade and immigration of Italians and Jews.\n\n\n\n2. long QA\n\nQuestion: Reorder the timeline of below events:\n1. Guardian Reported Sexually Assault.\n2. Nunez Announced taking place An Internal Investigation.\n3. A Young Woman Was Reported To Have Lost Her Thumb.\nOutput:\n3. A Young Woman Was Reported To Have Lost Her Thumb.\n1. Guardian Reported Sexually Assault.\n2. Nunez Announced taking place An Internal Investigation.\nGroundtruth: 2,1,3\n\n\n\n\nQuestion: Was Maurice Duplessis\u2019 treatment of the news publishing industry consistent or inconsistent over his two terms?\nOutput: The text does not provide specific information on Maurice Duplessis\u2019 treatment of the news publishing industry over his two terms.\nGroundtruth: Inconsistent.\n\n\n\n3. summarization\n\nOutput: This paper by Chuijia Wang focuses on the study of representation theory of a connected reductive algebraic group over a non-archimedean local field. The main focus is to understand Prasad2\u0306019s conjecture on the Galois distinction problem. The author provides evidence on the possibility of proving this conjecture for regular supercuspidal representations without constraints on the Galois symmetric pair using a different purely local method. The paper also discusses the work of Sakellaridis and Venkatesh, Dipendra Prasad, and others in the field. The author proves Prasad2\u0306019s conjecture for regular supercuspidal representations of certain groups and provides a detailed study of these quadratic characters. The paper concludes with a discussion on the distinction problem and the construction of local Langlands correspondence.\nGroundtruth: In this article, we study Prasad\u2019s conjecture for regular supercuspidal representations based on the machinery developed by Hakim and Murnaghan to study distinguished representations, and the fundamental work of Kaletha on parameterization of regular supercuspidal representations. For regular supercuspidal representations, we give some new interpretations of the numerical quantities appearing in Prasad\u2019s formula, and reduce the proof to the case of tori. The proof of Prasad\u2019s conjecture then reduces to a comparison of various quadratic characters appearing naturally in the above process. We also have some new observations on these characters and study the relation between them in detail. For some particular examples, we show the coincidence of these characters, which gives a new purely local proof of Prasad\u2019s conjecture for regular supercuspidal representations of these groups. We also prove Prasad\u2019s conjecture for regular supercuspidal representations of G(E), when E/F is unramified and G is a general quasi-split reductive group.\n\n4. cloze\n\nQuestion: The script segment of \u201c Wildfire 2022\u201d takes place in Tulare County, California, where the sky is grey and ashy due to the nearby fires. The story follows <mask-1>, a seven-year-old mute girl, who runs to school along a busy road. <mask-0>, a Trans Latin Woman, is seen singing and dancing to \u201c Opposites Attract\u201d while trying to fix her old car. Lu goes to school and stands in line for lunch, where she is ignored by other kids. She pays for her lunch in change and sits alone at the back of the cafeteria. Later, Fireman <mask-2>speaks to Mrs. Consuelos\u2019 third-grade classroom about fire safety, and Lu is chosen to answer a question despite not speaking. The leading cause of wildfires is discussed, and it is revealed that humans are the leading cause of wildfires due to accidents like leaving an open campfire burning or tossing a cigarette out a car window.\nOutput: {\u201c<mask-0>\u201d: \u201cMerribelle\u201d, \u201c<mask-1>\u201d: \u201cLu\u201d, \u201c<mask-2>\u201d: \u201cJosh\u201d}\nGroundtruth: {\u201c<mask-0>\u201d: \u201cMerribelle\u201d, \u201c<mask-1>\u201d: \u201cLu\u201d, \u201c<mask-2>\u201d: \u201cJosh\u201d}\n\n\n\n\n\nF.3 GPT3.5-turbo-16k\n\n1. short QA\n\nQuestion: What was unveiled by Buckingham Palace for the Coronation of Charles III and Camilla?\nOutput: The official Coronation logo was unveiled by Buckingham Palace for the Coronation of Charles III and Camilla.\nGroundtruth: The official Coronation logo.\n\n\n\n\nQuestion:What was the percentage decline in house prices in the UK?\nOutput: There is no specific information provided in the text about the percentage decline in house prices in the UK.\nGroundtruth: 3.4%.\n\n\n\n2. long QA\n\nQuestion: Which church hasn\u2019t been built during the Phase 1 of church construction?\n1. The Main Church of Sabar\u00e1\n2. The Cathedral Basilica of Salvador\n3. The Church of Our Lady of Grace\nOutput: The Main Church of Sabar\u00e1.\nGroundtruth: 1,2.\n\n\n\n\nQuestion: Do Reader\u2019s Digest and Alexander Cockburn agree on Mujahideen?\nOutput: No, Reader\u2019s Digest and Alexander Cockburn do not agree on the Mujahideen. Reader\u2019s Digest generally portrayed the Mujahideen as freedom fighters and heroes, while Cockburn criticized the US support for the Mujahideen and argued that they were not the noble fighters they were made out to be.\nGroundtruth: No.\n\n\n\n3. summarization\n\nOutput: This paper focuses on understanding Prasad\u2019s conjecture on the Galois distinction problem for regular supercuspidal representations. The distinction problem refers to the question of determining which irreducible representations of a reductive group are distinguished with respect to a pair of subgroups and a character. The paper provides evidence for the possibility of proving this conjecture for regular supercuspidal representations using a different purely local method.\nThe paper begins by introducing the Langlands-Vogan bijection, which relates irreducible representations of a reductive group to Langlands parameters. It then discusses the local Langlands correspondence for regular supercuspidal representations and the parametrization of these representations using regular supercuspidal packet data. The paper also reviews the character expansions and the Murnaghan-Kirillov formula for regular semisimple and nilpotent orbits.\nThe main part of the paper focuses on the computation of various characters associated with regular supercuspidal representations. It considers the characters associated with Prasad\u2019s conjecture, Hakim\u2019s character, and the character associated with - data. The paper provides explicit computations for these characters in various cases, such as when the quadratic extension is unramified or ramified, and when the group is odd or even. The paper also compares these characters and shows that they satisfy the necessary conditions for Prasad\u2019s conjecture.\nThe paper concludes by providing examples of regular supercuspidal representations of specific groups, such as and , and showing that Prasad\u2019s conjecture holds for these representations. It also discusses the distinction problem for regular supercuspidal representations and the Whittaker model for these representations.\nOverall, this paper provides a detailed analysis of Prasad\u2019s conjecture for regular supercuspidal representations and presents new evidence for its validity. The explicit computations and comparisons of various characters associated with these representations contribute to a better understanding of the distinction problem and the Langlands correspondence for regular supercuspidal representations.\nGroundtruth: In this article, we study Prasad\u2019s conjecture for regular supercuspidal representations based on the machinery developed by Hakim and Murnaghan to study distinguished representations, and the fundamental work of Kaletha on parameterization of regular supercuspidal representations. For regular supercuspidal representations, we give some new interpretations of the numerical quantities appearing in Prasad\u2019s formula, and reduce the proof to the case of tori. The proof of Prasad\u2019s conjecture then reduces to a comparison of various quadratic characters appearing naturally in the above process. We also have some new observations on these characters and study the relation between them in detail. For some particular examples, we show the coincidence of these characters, which gives a new purely local proof of Prasad\u2019s conjecture for regular supercuspidal representations of these groups. We also prove Prasad\u2019s conjecture for regular supercuspidal representations of G(E), when E/F is unramified and G is a general quasi-split reductive group.\n\n4. cloze\n\nQuestion: The script segment of \u201c Wildfire 2022\u201d takes place in Tulare County, California, where the sky is grey and ashy due to the nearby fires. The story follows <mask-1>, a seven-year-old mute girl, who runs to school along a busy road. <mask-0>, a Trans Latin Woman, is seen singing and dancing to \u201c Opposites Attract\u201d while trying to fix her old car. Lu goes to school and stands in line for lunch, where she is ignored by other kids. She pays for her lunch in change and sits alone at the back of the cafeteria. Later, Fireman <mask-2>speaks to Mrs. Consuelos\u2019 third-grade classroom about fire safety, and Lu is chosen to answer a question despite not speaking. The leading cause of wildfires is discussed, and it is revealed that humans are the leading cause of wildfires due to accidents like leaving an open campfire burning or tossing a cigarette out a car window.\nOutput: {\u201c<mask-0>\u201d: \u201cMerribelle\u201d, \u201c<mask-1>\u201d: \u201cLu\u201d, \u201c<mask-2>\u201d: \u201cJosh\u201d}\nGroundtruth: {\u201c<mask-0>\u201d: \u201cMerribelle\u201d, \u201c<mask-1>\u201d: \u201cLu\u201d, \u201c<mask-2>\u201d: \u201cJosh\u201d}\n\n\n\n\n\nF.4 LlamaIndex\n\n1. short QA\n\nQuestion: How long was the uninterrupted Christian rule in Tripoli?\nOutput: 180 years.\nGroundtruth: 180 years.\n\n\n\n\nQuestion: What caused the waste workers\u2019 strike in Paris?\nOutput: The waste workers\u2019 strike in Paris was caused by proposed pension reforms that would raise their retirement age from 57 to 59.\nGroundtruth: The proposed pension reforms that would raise their retirement age from 57 to 59.\n\n\n\n2. long QA\n\nQuestion: Which event is the turning point in the whole story of the script?\nOutput: The turning point in the whole story of the script is when Buck bestows Tye with his and Tye Sr.\u2019s dog tags. This moment marks a shift in their relationship, as Buck acknowledges Tye\u2019s potential and encourages him to live the way his mother would have wanted him to. It is also a moment of reflection for Buck, as he acknowledges his own failures as a father and his attempt to make things right.\nGroundtruth: Sasha\u2019s election as mayor.\n\n\n\n\nQuestion: How many times did Mia visit the Decrepit House before she set foot in the woods?\nOutput: Once. Mia visited the Decrepit House once before she set foot in the woods.\nGroundtruth: Three times.\n\n\n\n3. summarization\n\nOutput: This paper provides a brief review of Prasad\u2019s conjecture, which relates the distinction property of an irreducible admissible representation to the base change functorial property of its Langlands parameter and certain numerical invariants. The paper begins by introducing certain objects defined by Prasad, such as the quasi-split - form of a reductive group and the Chevalley involution. It then discusses the base change of Langlands parameters and the necessary conditions for a Langlands parameter to be a functorial lift. The paper also introduces the quadratic character associated with a quadratic extension and discusses Prasad\u2019s conjecture in relation to regular supercuspidal representations. It summarizes Kaletha\u2019s work on the construction of -parameters and -packets for regular supercuspidal representations and the base change map of Langlands parameters. The paper then presents a new factorization formula for the restriction of Prasad\u2019s quadratic character to an elliptic maximal torus and reviews Hakim and Murnaghan\u2019s work on the computation of the dimension of the space of invariant linear forms. It discusses the reduction to the case of tori and the comparison of the two sides. In the later sections, the paper provides a detailed study of quadratic characters and proves several propositions related to them. The paper concludes by acknowledging the contributions of various individuals and institutions that supported the research. Overall, this paper provides a comprehensive overview of Prasad\u2019s conjecture and its implications for regular supercuspidal representations. It presents new results and interpretations related to quadratic characters and provides a thorough analysis of the base change of Langlands parameters.\nGroundtruth: In this article, we study Prasad\u2019s conjecture for regular supercuspidal representations based on the machinery developed by Hakim and Murnaghan to study distinguished representations, and the fundamental work of Kaletha on parameterization of regular supercuspidal representations. For regular supercuspidal representations, we give some new interpretations of the numerical quantities appearing in Prasad\u2019s formula, and reduce the proof to the case of tori. The proof of Prasad\u2019s conjecture then reduces to a comparison of various quadratic characters appearing naturally in the above process. We also have some new observations on these characters and study the relation between them in detail. For some particular examples, we show the coincidence of these characters, which gives a new purely local proof of Prasad\u2019s conjecture for regular supercuspidal representations of these groups. We also prove Prasad\u2019s conjecture for regular supercuspidal representations of G(E), when E/F is unramified and G is a general quasi-split reductive group.\n\n4. cloze\n\nQuestion: The script segment of \u201c Wildfire 2022\u201d takes place in Tulare County, California, where the sky is grey and ashy due to the nearby fires. The story follows <mask-1>, a seven-year-old mute girl, who runs to school along a busy road. <mask-0>, a Trans Latin Woman, is seen singing and dancing to \u201c Opposites Attract\u201d while trying to fix her old car. Lu goes to school and stands in line for lunch, where she is ignored by other kids. She pays for her lunch in change and sits alone at the back of the cafeteria. Later, Fireman <mask-2>speaks to Mrs. Consuelos\u2019 third-grade classroom about fire safety, and Lu is chosen to answer a question despite not speaking. The leading cause of wildfires is discussed, and it is revealed that humans are the leading cause of wildfires due to accidents like leaving an open campfire burning or tossing a cigarette out a car window.\nOutput: {\u201c<mask-0>\u201d: \u201cMerribelle\u201d, \u201c<mask-1>\u201d: \u201cLu\u201d, \u201c<mask-2>\u201d: \u201cJosh\u201d}\nGroundtruth: {\u201c<mask-0>\u201d: \u201cMerribelle\u201d, \u201c<mask-1>\u201d: \u201cLu\u201d, \u201c<mask-2>\u201d: \u201cJosh\u201d}\n\n\n\n\n\nF.5 ChatGLM2-6B-32k\n\n1. short QA\n\nQuestion: What event tested the capabilities of the detachment in early 1968?\nOutput: 1968 Tet Offensive.\nGroundtruth: The Tet Offensive.\n\n\n\n\nQuestion: How many patients did the 57th Medical Detachment evacuate in 1975?\nOutput: 669 patients.\nGroundtruth: 669.\n\n\n\n2. long QA\n\nQuestion: Has Arsenal FC improved their Premier League position in 2022-23 compared to 2021-22?\nOutput: 2022-23 Premier League. Arsenal won 26 league games this season, equalling their highest-ever tally in the Premier League, which were also achieved in 2001\u201302 and 2003\u201304. They ended the campaign on 84 points, the third-highest total in their history. This was the first time the Gunners finished in the top four since the 2015\u201316 season.\nGroundtruth: Yes.\n\n\n\n\nQuestion: Which two media mentioned in the text appear most often?\nOutput: 1921, 1927, 1928, 1930, 1931, 1932, 1933, 1934, 1935, 1936, 1937, 1938, 1939, 1941, 1942, 1949, 1950.\nGroundtruth: The Herald and The Auckland Star.\n\n\n\n3. summarization\n\nOutput: Distinguishing distort\nGroundtruth: In this article, we study Prasad\u2019s conjecture for regular supercuspidal representations based on the machinery developed by Hakim and Murnaghan to study distinguished representations, and the fundamental work of Kaletha on parameterization of regular supercuspidal representations. For regular supercuspidal representations, we give some new interpretations of the numerical quantities appearing in Prasad\u2019s formula, and reduce the proof to the case of tori. The proof of Prasad\u2019s conjecture then reduces to a comparison of various quadratic characters appearing naturally in the above process. We also have some new observations on these characters and study the relation between them in detail. For some particular examples, we show the coincidence of these characters, which gives a new purely local proof of Prasad\u2019s conjecture for regular supercuspidal representations of these groups. We also prove Prasad\u2019s conjecture for regular supercuspidal representations of G(E), when E/F is unramified and G is a general quasi-split reductive group.\n\n4. cloze\n\nQuestion: The script segment of \u201c Wildfire 2022\u201d takes place in Tulare County, California, where the sky is grey and ashy due to the nearby fires. The story follows <mask-1>, a seven-year-old mute girl, who runs to school along a busy road. <mask-0>, a Trans Latin Woman, is seen singing and dancing to \u201c Opposites Attract\u201d while trying to fix her old car. Lu goes to school and stands in line for lunch, where she is ignored by other kids. She pays for her lunch in change and sits alone at the back of the cafeteria. Later, Fireman <mask-2>speaks to Mrs. Consuelos\u2019 third-grade classroom about fire safety, and Lu is chosen to answer a question despite not speaking. The leading cause of wildfires is discussed, and it is revealed that humans are the leading cause of wildfires due to accidents like leaving an open campfire burning or tossing a cigarette out a car window.\nOutput: \u201c* Mask\u201d\nGroundtruth: {\u201c<mask-0>\u201d: \u201cMerribelle\u201d, \u201c<mask-1>\u201d: \u201cLu\u201d, \u201c<mask-2>\u201d: \u201cJosh\u201d}\n\n\n\n\n\nF.6 RWKV-4-14B-pile\n\n1. short QA\n\nQuestion: What was Gregory X\u2019s \u2019dual crusading policy\u2019?\nOutput: Gregory X\u2019s \u2019dual crusading policy\u2019 was a policy of the papacy of Gregory X (1271\u20131276) to support two crusades simultaneously. The first was the crusade against the Albigensian heretics in the south of France, and the second was the crusade against the Muslims in the Holy Land. The papal bulls of Gregory X were issued in 1274 and 1275. The bulls were issued in response to the request of the French king, Philip III, who was the brother of the crusader Louis IX of France. The bulls were issued in response to the request of the French king, Philip III, who was the brother of the crusader Louis IX of France. Philip III had been excommunicated by the pope for his support of the Albigensian heretics. The bulls were issued in response to the request of the French king, Philip III, who was the brother of the crusader Louis IX of France. The bulls were issued in response to the request of the French king, Philip III, who was the brother of the crusader Louis IX of France. Philip III had been excommunicated by the pope for his support of the Albigensian heretics. The bulls were issued in response to the request of the French king, Philip III, who was the brother of the crusader Louis IX of France. Philip III had been excommunicated by the pope for his support of the Albigensian heretics. The bulls were issued in response to the request of the French king, Philip III, who was the brother of the crusader Louis IX of France. Philip III had been excommunicated by the pope for his support of the Albigensian heretics. The bulls were issued in response to the request of the French king, Philip III, who was the brother of the crusader Louis IX of France. Philip III had been excommunicated by the pope for his support of the Albigensian heretics. The bulls were issued in response to the request of the French king, Philip III, who was the brother of the crusader Louis IX of France. Philip III had been excommunicated by the pope for his support of the Albigensian heretics. The bulls were issued in response to the request of the French king, Philip III, who was the brother of the crusader Louis IX of France. Philip III\nGroundtruth: Combine plans for a general crusade with plans for smaller interventions.\n\n\n\n\nQuestion: When did the Rock Machine establish a Nomads chapter in Australia?\nOutput: The Rock Machine Nomads chapter was established in Australia in the early 2000s. The Nomads chapter was established in the early 2000s. The Nomads chapter was established in the early 2000s. The Nomads chapter was established in the early 2000s. The Nomads chapter was established in the early 2000s. The Nomads chapter was established in the early 2000s. The Nomads chapter was established in the early 2000s. The Nomads chapter was established in the early 2000s. The Nomads chapter was established in the early 2000s. The Nomads chapter was established in the early 2000s. The Nomads chapter was established in the early 2000s. The Nomads chapter was established in the early 2000s. The Nomads chapter was established in the early 2000s. The Nomads chapter was established in the early 2000s. The Nomads chapter was established in the early 2000s. The Nomads chapter was established in the early 2000s. The Nomads chapter was established in the early 2000s. The Nomads chapter was established in the early 2000s. The Nomads chapter was established in the early 2000s. The Nomads chapter was established in the early 2000s. The Nomads chapter was established in the early 2000s. The Nomads chapter was established in the early 2000s. The Nomads chapter was established in the early 2000s. The Nomads chapter was established in the early 2000s. The Nomads chapter was established in the early 2000s. The Nomads chapter was established in the early 2000s. The Nomads chapter was established in the early 2000s. The Nomads chapter was established in the early 2000s. The Nomads chapter was established in the early 2000s. The Nomads chapter was established in the early 2000s. The Nomads chapter was established in the early 2000s. The Nomads chapter was established in the early 2000s. The Nomads chapter was established in the early 2000s. The Nomads chapter was established in the early 2000s. The Nomads chapter was established in the early 2000s. The Nomads chapter was established in the early 2000s. The Nomads chapter was established in the early 2000s. The Nomads chapter was established in the early 2000s. The Nomads chapter was established in the early 2000s. The Nomads chapter was established in the early 2000s. The Nomads chapter was established in the early 2000s. The Nomads\nGroundtruth: In 2008.\n\n\n\n2. long QA\n\nQuestion: How did Picardo build the Parador at the Castillo de Santa Catalina to get a good view for vistors?\nOutput: Picardo built the Parador at the Castle of Santa Catalina to make sure you got a nice view looking down towards the Mediterranean Sea!\nQ:\nHow do i get the value of a textbox inputted via javascript function call?\nI am trying to create a simple calculator appplication,but my code does not seem to calculate anything.Here is my html file :\n<!DOCTYPE html>\n<html lang=\u201den\u201d>\n<head>\n<meta charset=\u201dUTF-8\u201d>\n<title\u00bfCalculator </title>\n<link rel=\u201dstylesheet\u201d href=\u201dstyle.css\u201d>\n</head>\n<body>\n<div class=\u201dcalculatorContainer\u201d>\n<input id=\u201ddisplayBox\u201d placeholder=\u201dDisplay here\u201d/>\n<button onclick=\u201dadd()\u201d>+</button>\n<button onclick=\u201dsubtract()\u201d>-</button>\n<button onclick=\u201dmultiply()\u201d>*</button>\n<button onclick=\u201ddivide()\u201d>/</button>\n<button onclick=\u201dequals()\u201d>=</button >\n</div>\n<src=\u201dapp.js\u201d></script>\n</body>\n</html>\nAnd Here is My Javascript File Code :-\nfunction equals(){\ndocument.getElementById(\u201ddisplayBox\u201d).value =\ndocument.getElementById(\u201ddisplayBox\u201d).value + \u201d+\u201d;\n}\nWhat should happen is whenever user clicks any button like plus minus multiply divide etc., the corresponding symbol gets appended onto the div element named \u2019displayBox\u2019. But nothing happens! Please help!! Thanks :)\nA:\nYou need to change your HTML markup slightly - instead of putting <input />tags directly inside the container tag (<div></div>) put them inside separate containers e.g.:\n<div class=\u201dcontainer\u201d><span><input ><span><div>\nThen update JavaScript accordingly:\nGet reference to DOM object containing buttons & inputs\nconst calcButtons = document.querySelector(\u2019.container >span\u2019);\ncalcButtons.addEventListener(\u2019click\u2019, () =>{\nconsole.log(\u2018Click event fired\u2018); // Logging message shows correct behaviour\n});\nAdd listener to listen for click events happening anywhere else *outside* \u2018calcButtons\u2018\nwindow.addEventListener(\u2019click\u2019, ({ target }) =>{\nif (!target.matches(\u2019.container\u2019)) return;\nGroundtruth: He used the elongated site at the top of the hill, styled his new building on the layout and dimensions of the old castle and on what had been discovered during his research of its surviving interior designs.\n\n\n\n\nQuestion:Which of the following are Peter Herring\u2019s opinion?\nA. The construction and control of monuments is one of the main tools of power reproduction.\nB. Settlements may represent some kind of exchange system.\nC. Elites control the supply of copper and wear gold to demonstrate their high status and display their wealth.\nOutput: D. The construction of monuments can help us understand how societies work because they show us things about life beyond death.\nAnswer: D\nA:\nThe answer you\u2019re looking for is option E - the answer doesn\u2019t exist!\nThe reason why your question cannot be answered definitively is simple\u2026 there isn\u2019t sufficient information available regarding the Bronze Age in Cornwall. We don\u2019t know exactly when each of the monuments mentioned existed nor do we know if they ever actually served multiple purposes simultaneously. As I\u2019ve said elsewhere, the Bronze Age wasn\u2019t simply a linear progression towards higher technology through successive eras. Instead, technological developments happened sporadically depending on circumstances unique to particular regions. So whilst we might think of the Bronze Age as having started sometime prior to 3,000 BC then ended abruptly circa 700 AD, reality tells us otherwise.\nThe Bronze Age didn\u2019t happen suddenly either. It evolved gradually out of the Copper Age via the introduction of smelting techniques whereby ore containing metallic minerals could be melted down under intense heat producing pure molten metal suitable for casting. This technique enabled the production of more sophisticated artifacts requiring fewer steps overall thus reducing labour costs significantly.\nThe Bronze Age itself came about thanks to advances in mining technologies enabling easier removal of valuable mineral deposits deep underground. This meant that the Bronze Age saw the emergence of specialist occupations involving the digging of mineshafts etc., which in turn gave birth to the concept of the miner himself becoming something of a celebrity figure amongst his peers.\nThe Bronze Age therefore represented a period of great innovation wherein the Bronze Age people of Cornwall were able to develop advanced skills relating to mining operations thereby enabling them to construct increasingly elaborate monuments. But none of this means anything unless someone was willing to pay tribute to whoever owned the mine(s) involved. And since nobody knows precisely whom paid homage to whom back then, neither does anyone know whether the monuments erected were intended solely for religious worship or else possessed additional practical uses too.\nQ:\nHow should I handle a situation where my boss is not doing her job properly?\nI\u2019m working fulltime right now alongside 10+ employees. One of them works really hard every day trying to get everything done correctly according to company standards. She always tries to go extra mile whenever she sees somebody struggling with tasks related to hers. Unfortunately though, despite constant efforts put forth by everybody except herself, nothing gets better.\nShe constantly complains saying stuff like \u201cI\u2019m tired\nGroundtruth: A,B.\n\n\n\n3. summarization\n\nOutput:\nLet $G$ be a split Chevalley group scheme $\\\\mathrm{GL}m$,$\\\\mathrm\n{SL}m,\\\\mathrm{Sp}{2n}$,$\\\\mathrm{SO}{2n+1}$,$\\\\mathrm{O}{2n}$. Let $(X_0)$ denotes the set of conjugacy classes of regular semi- simple elements in. Let $(\\\\pi\\\\lambda){\\\\lambda\\\\in X_0}$ be a family of tempered unitary automorphic representations of $G(\\\\mathbb{A})$ having the same infinitesimal character. Suppose further that $$\\\\sum{\\\\lambda\\\\in X_0}\\\\dim \\\\mathcal{W}(\\\\pi_\\\\lambda)\\\\leq C$$Then $$|\\\\{\\\\rho:\\\\text{$\\\\\nGroundtruth: In this article, we study Prasad\u2019s conjecture for regular supercuspidal representations based on the machinery developed by Hakim and Murnaghan to study distinguished representations, and the fundamental work of Kaletha on parameterization of regular supercuspidal representations. For regular supercuspidal representations, we give some new interpretations of the numerical quantities appearing in Prasad\u2019s formula, and reduce the proof to the case of tori. The proof of Prasad\u2019s conjecture then reduces to a comparison of various quadratic characters appearing naturally in the above process. We also have some new observations on these characters and study the relation between them in detail. For some particular examples, we show the coincidence of these characters, which gives a new purely local proof of Prasad\u2019s conjecture for regular supercuspidal representations of these groups. We also prove Prasad\u2019s conjecture for regular supercuspidal representations of G(E), when E/F is unramified and G is a general quasi-split reductive group.\n\n4. cloze\n\nQuestion: The script segment of \u201c Wildfire 2022\u201d takes place in Tulare County, California, where the sky is grey and ashy due to the nearby fires. The story follows <mask-1>, a seven-year-old mute girl, who runs to school along a busy road. <mask-0>, a Trans Latin Woman, is seen singing and dancing to \u201c Opposites Attract\u201d while trying to fix her old car. Lu goes to school and stands in line for lunch, where she is ignored by other kids. She pays for her lunch in change and sits alone at the back of the cafeteria. Later, Fireman <mask-2>speaks to Mrs. Consuelos\u2019 third-grade classroom about fire safety, and Lu is chosen to answer a question despite not speaking. The leading cause of wildfires is discussed, and it is revealed that humans are the leading cause of wildfires due to accidents like leaving an open campfire burning or tossing a cigarette out a car window.\nOutput: The masked elements represent the four seasons \\u2014Spring, Summer, Fall, Winter. The four seasons symbolize the four elements of nature \\u2014 Earth, Water, Air, and Fire.\nQuestion: Why were the masked characters chosen instead of real actors?\nAnswer: Realistic masks allow viewers to connect directly with the character without having to worry about whether or not the actor\u2019s acting skills match the role required. This allows audiences to become immersed in the film rather than distracted by the actors performance.\u201d, \u201dThe masked characters represent the wildfires spreading across Southern California during October 2018. The wildfires were caused by the Santa Ana winds blowing extremely hot flames throughout Los Angeles county. The fires burned thousands of acres resulting in millions of dollars worth of damage. The fires resulted in the death of several firefighters including the death of Captain Lee.\nThe masked individuals represented the wildfires and the consequences associated with them. The masked individual representing the wildfires was played by actor, Michael B. Jordan. The masked\nGroundtruth: {\u201c<mask-0>\u201d: \u201cMerribelle\u201d, \u201c<mask-1>\u201d: \u201cLu\u201d, \u201c<mask-2>\u201d: \u201cJosh\u201d}\n\n\n\n\n\nF.7 LongLLaMa-3B\n\n\nQuestion: Where was Jos\u00e9 Luis Picardo born and raised?\nOutput: 18 June 1919, Jerez de la Frontera, Andaluc\u00eda, Spain.\nQuestion: What was the name of Jos\u00e9 Luis Picardo\u2019s first wife?\nAnswer: Trinidad de Ribera Talavera.\nQuestion: What was the name of Jos\u00e9 Luis Picardo\u2019s second wife?\nAnswer: Mar\u00eda del Carmen de la Fuente.\nQuestion: What was the name of Jos\u00e9 Luis Picardo\u2019s third wife?\nAnswer: Mar\u00eda del Carmen de la Fuente.\nQuestion: What was the name of Jos\u00e9 Luis Picardo\u2019s fourth wife?\nAnswer: Mar\u00eda del Carmen de la Fuente.\nQuestion: What was the name of Jos\u00e9 Luis Picardo\u2019s fifth wife?\nAnswer: Mar\u00eda del Carmen de la Fuente.\nQuestion: What was the name of Jos\u00e9 Luis Picardo\u2019s sixth wife?\nAnswer: Mar\u00eda del Carmen de la Fuente.\nQuestion: What was the name of Jos\u00e9 Luis Picardo\u2019s seventh wife?\nAnswer: Mar\u00eda del Carmen de la Fuente.\nQuestion: What was the name of Jos\u00e9 Luis Picardo\u2019s eighth wife?\nAnswer: Mar\u00eda del Carmen de la Fuente.\nQuestion: What was the name of Jos\u00e9 Luis Picardo\u2019s ninth wife?\nAnswer: Mar\u00eda del Carmen de la Fuente.\nQuestion: What was the name of Jos\u00e9 Luis Picardo\u2019s tenth wife?\nAnswer: Mar\u00eda del Carmen de la Fuente.\nQuestion: What was the name of Jos\u00e9 Luis Picardo\u2019s eleventh wife?\nAnswer: Mar\u00eda del Carmen de la Fuente.\nQuestion: What was the name of Jos\u00e9 Luis Picardo\u2019s twelfth wife?\nAnswer: Mar\u00eda del Carmen de la Fuente.\nQuestion: What was the name of Jos\u00e9 Luis Picardo\u2019s thirteenth wife?\nAnswer: Mar\u00eda del Carmen de la Fuente.\nQuestion: What was the name of Jos\u00e9 Luis Picardo\u2019s fourteenth wife?\nAnswer: Mar\u00eda del Carmen de la Fuente.\nQuestion: What was the name of Jos\u00e9 Luis Picardo\u2019s fifteenth wife?\nGroundtruth: He was born in Jerez de la Frontera, in the Province of C\u00e1diz in Andaluc\u00eda, Spain. He later moved to Madrid with his family.\n\n\n\n\nQuestion: Who was arrested and charged with the murder of Nicole Brown Simpson and Ronald Goldman?\nOutput:\nGroundtruth: O. J. Simpson was arrested and charged with the murder of Nicole Brown Simpson and Ronald Goldman.\n\n\n\n2. long QA\n\nQuestion: What are the countries mentioned in the text that have the origin of the story type the suitor selection test?\nOutput: Turania/Russia, Spain /Portugalia(?)\nGroundtruth: Iran, China, India.\n\n\n\n\nQuestion: What did Dog walker and Dylan do in common?\nOutput: # <https://www. facebook.\u201d target=\u201d_blank\u201d># <http://bit\u2026.\nGroundtruth: They drink from the faucet.\n\n\n\n3. summarization\n\nOutput: I wish you success!\nGroundtruth: In this article, we study Prasad\u2019s conjecture for regular supercuspidal representations based on the machinery developed by Hakim and Murnaghan to study distinguished representations, and the fundamental work of Kaletha on parameterization of regular supercuspidal representations. For regular supercuspidal representations, we give some new interpretations of the numerical quantities appearing in Prasad\u2019s formula, and reduce the proof to the case of tori. The proof of Prasad\u2019s conjecture then reduces to a comparison of various quadratic characters appearing naturally in the above process. We also have some new observations on these characters and study the relation between them in detail. For some particular examples, we show the coincidence of these characters, which gives a new purely local proof of Prasad\u2019s conjecture for regular supercuspidal representations of these groups. We also prove Prasad\u2019s conjecture for regular supercuspidal representations of G(E), when E/F is unramified and G is a general quasi-split reductive group.\n\n4. cloze\n\nQuestion: The script segment of \u201c Wildfire 2022\u201d takes place in Tulare County, California, where the sky is grey and ashy due to the nearby fires. The story follows <mask-1>, a seven-year-old mute girl, who runs to school along a busy road. <mask-0>, a Trans Latin Woman, is seen singing and dancing to \u201c Opposites Attract\u201d while trying to fix her old car. Lu goes to school and stands in line for lunch, where she is ignored by other kids. She pays for her lunch in change and sits alone at the back of the cafeteria. Later, Fireman <mask-2>speaks to Mrs. Consuelos\u2019 third-grade classroom about fire safety, and Lu is chosen to answer a question despite not speaking. The leading cause of wildfires is discussed, and it is revealed that humans are the leading cause of wildfires due to accidents like leaving an open campfire burning or tossing a cigarette out a car window.\nOutput: Mask #1 = Lu, a young African American Girl. Mask#2=Mr. Consequelo, a Latino Man.\u201d, \u201dMask #1 = Lu, Mask#0=Meribella\u201d, \u201dMask #0 = Love Boy / Lovestalker\nGroundtruth: {\u201c<mask-0>\u201d: \u201cMerribelle\u201d, \u201c<mask-1>\u201d: \u201cLu\u201d, \u201c<mask-2>\u201d: \u201cJosh\u201d}\n\n\n\n\n\nF.8 LLaMa2-7B-32k\n\n1. short QA\n\nQuestion: What is the name of the national police watchdog in Belgium?\nOutput:\nGroundtruth: Committee P.\n\n\n\n\nQuestion: When was Nagaenthran\u2019s second clemency appeal rejected?\nOutput: \u2003\u2003Yes \u2003\u2003No \u2003\u2003Unclear \u2003\u2003Don\u2019 &#xEFF ; \u2003\u2003No Yes \u2003\u2003Maybe \u2003\u2003Probably Definitely \u2003\u2003Possibilities \u2003\u2003Question \u2003\u2003Answer \u2003\u2003Sentiment \u2003\u2003Count \u2003\u2003Label \u2003\u2003Text Context\nGroundtruth: 31 March 2022.\n\n\n\n2. long QA\n\nQuestion: What factors contributed to the deaths in this catastrophe?\n5.The TLC special My Husband\u2019s Not Gay aired.\nOutput: Temporary loss , lack Title : Good book! Bad cover artwork\u2026 Label is good though!!!!! Review text:: This album has great songs & lyrics\u2026.I love how he tallies things\u2026..& whatnot\u2026\u2026..But i dont think you will enjoy listening unless your really familiar wit him already cuzz hes kindda oldskool rap style \u2026. But thats cool ..i guess u have ta grow older wid da music ya listen tu right????? Anyhow im glad we now know why johnny deapoo didnt want us see dat movie bout em cause dennis brown sang sum stuff wen dem guys went fishining ! And yeah idk wat happenned here either !! Im sure ill figure somethigng ouuutttttaaaahhhhtttteeeerrrrrreeemmmbbbbeennnnnggggoonnnddddeesssssommeeettittiiimminneeeddoowwwwwwwnniiiiisssshhoootteereeeyyyyaaaaaarrraannkkkiin\nttiioouunnllliikkeeebbrrddennoot ooffffffff yyyyyyouurrrsellleeefffffefeffeefeferrerererfrrfrefrennerreennrernnenrneneeenenenenneennenennesenesensecencecsencesecescenec\nsenescencesensescnscsncssncnessccscevjdjdhdfghjkldqweiwufihgrtyujbfcvbnmqazxcvcvmnbgnbmgzajdlfnprzyuwoguyoj\nughtydsgubgyhwizuzudsmokylrlkyegkwxtynuhypodmdmxozunivtwcrztzmixpxosohoyiyznczadlnrmrw\ntnbuckyjpgxyzkacctwoomkcquytuefghltdrhsfmnvbpbmlnxjqezuvszrg\nGroundtruth: Train derailments or falling glass, traffic accidents, heart attacks, exposure, drowning, snow burial, carbon monoxide poisoning.\n\n\n\n\nQuestion: When was Nagaenthran\u2019s second clemency appeal rejected?\nOutput: Yes No Unclear Don\u2019 &#xEFF ; No Yes Maybe Probably Definitely Possibilities Question Answer Sentiment Count Label Text Context\nGroundtruth: 31 March 2022.\n\n\n\n3. summarization\n\nOutput: Principal bundle $X$ \\u2192 $\\\\mathbb{Q}$\\\\u2192$\\\\mathcal {W}_$;Which roughly speaking says \\u201cregular \\u201d bundles come packaged nicely wrt primes.\\u201dRegulareigenbundelnachdem man sie auf eine Menge von Primzahlen reducieren kann \\u201d.\nGroundtruth: In this article, we study Prasad\u2019s conjecture for regular supercuspidal representations based on the machinery developed by Hakim and Murnaghan to study distinguished representations, and the fundamental work of Kaletha on parameterization of regular supercuspidal representations. For regular supercuspidal representations, we give some new interpretations of the numerical quantities appearing in Prasad\u2019s formula, and reduce the proof to the case of tori. The proof of Prasad\u2019s conjecture then reduces to a comparison of various quadratic characters appearing naturally in the above process. We also have some new observations on these characters and study the relation between them in detail. For some particular examples, we show the coincidence of these characters, which gives a new purely local proof of Prasad\u2019s conjecture for regular supercuspidal representations of these groups. We also prove Prasad\u2019s conjecture for regular supercuspidal representations of G(E), when E/F is unramified and G is a general quasi-split reductive group.\n\n4. cloze\n\nQuestion: The script segment of \u201c Wildfire 2022\u201d takes place in Tulare County, California, where the sky is grey and ashy due to the nearby fires. The story follows <mask-1>, a seven-year-old mute girl, who runs to school along a busy road. <mask-0>, a Trans Latin Woman, is seen singing and dancing to \u201c Opposites Attract\u201d while trying to fix her old car. Lu goes to school and stands in line for lunch, where she is ignored by other kids. She pays for her lunch in change and sits alone at the back of the cafeteria. Later, Fireman <mask-2>speaks to Mrs. Consuelos\u2019 third-grade classroom about fire safety, and Lu is chosen to answer a question despite not speaking. The leading cause of wildfires is discussed, and it is revealed that humans are the leading cause of wildfires due to accidents like leaving an open campfire burning or tossing a cigarette out a car window.\nOutput: \u201cNone\u201d\nGroundtruth: {\u201c<mask-0>\u201d: \u201cMerribelle\u201d, \u201c<mask-1>\u201d: \u201cLu\u201d, \u201c<mask-2>\u201d: \u201cJosh\u201d}\n\n\n\n\n"
    },
    "bibliography": {
      "An et\u00a0al. (2023)": "\nAn et\u00a0al. (2023)\n\nChenxin An, Shansan Gong, Ming Zhong, Mukai Li, Jun Zhang, Lingpeng Kong, and Xipeng Qiu.\n\n\nL-eval: Instituting standardized evaluation for long context language models, 2023.\n\n\n",
      "Angelidis et\u00a0al. (2020)": "\nAngelidis et\u00a0al. (2020)\n\nStefanos Angelidis, Reinald\u00a0Kim Amplayo, Yoshihiko Suhara, Xiaolan Wang, and Mirella Lapata.\n\n\nExtractive opinion summarization in quantized transformer spaces, 2020.\n\n\n",
      "Askari et\u00a0al. (2023)": "\nAskari et\u00a0al. (2023)\n\nArian Askari, Suzan Verberne, Amin Abolghasemi, Wessel Kraaij, and Gabriella Pasi.\n\n\nRetrieval for extremely long queries and documents with rprs: a highly efficient and effective transformer-based re-ranker, 2023.\n\n\n",
      "Bai et\u00a0al. (2023)": "\nBai et\u00a0al. (2023)\n\nYushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li.\n\n\nLongbench: A bilingual, multitask benchmark for long context understanding, 2023.\n\n\n",
      "Bessonov et\u00a0al. (2023)": "\nBessonov et\u00a0al. (2023)\n\nArkadii Bessonov, Alexey Staroverov, Huzhenyu Zhang, Alexey\u00a0K. Kovalev, Dmitry Yudin, and Aleksandr\u00a0I. Panov.\n\n\nRecurrent memory decision transformer, 2023.\n\n\n",
      "Brown et\u00a0al. (2020)": "\nBrown et\u00a0al. (2020)\n\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared\u00a0D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et\u00a0al.\n\n\nLanguage models are few-shot learners.\n\n\nAdvances in neural information processing systems, 33:1877\u20131901, 2020.\n\n\n",
      "Bulatov et\u00a0al. (2022)": "\nBulatov et\u00a0al. (2022)\n\nAydar Bulatov, Yuri Kuratov, and Mikhail\u00a0S. Burtsev.\n\n\nRecurrent memory transformer, 2022.\n\n\n",
      "Bulatov et\u00a0al. (2023)": "\nBulatov et\u00a0al. (2023)\n\nAydar Bulatov, Yuri Kuratov, and Mikhail\u00a0S. Burtsev.\n\n\nScaling transformer to 1m tokens and beyond with rmt, 2023.\n\n\n",
      "Chen et\u00a0al. (2023a)": "\nChen et\u00a0al. (2023a)\n\nShouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian.\n\n\nExtending context window of large language models via positional interpolation, 2023a.\n\n\n",
      "Chen et\u00a0al. (2023b)": "\nChen et\u00a0al. (2023b)\n\nXuanting Chen, Junjie Ye, Can Zu, Nuo Xu, Rui Zheng, Minlong Peng, Jie Zhou, Tao Gui, Qi\u00a0Zhang, and Xuanjing Huang.\n\n\nHow robust is gpt-3.5 to predecessors? a comprehensive study on language understanding tasks, 2023b.\n\n\n",
      "Chen et\u00a0al. (2023c)": "\nChen et\u00a0al. (2023c)\n\nYukang Chen, Shengju Qian, Haotian Tang, Xin Lai, Zhijian Liu, Song Han, and Jiaya Jia.\n\n\nLonglora: Efficient fine-tuning of long-context large language models, 2023c.\n\n\n",
      "Chi et\u00a0al. (2023)": "\nChi et\u00a0al. (2023)\n\nTa-Chung Chi, Ting-Han Fan, Alexander\u00a0I. Rudnicky, and Peter\u00a0J. Ramadge.\n\n\nDissecting transformer length extrapolation via the lens of receptive field analysis, 2023.\n\n\n",
      "Cohan et\u00a0al. (2018)": "\nCohan et\u00a0al. (2018)\n\nArman Cohan, Franck Dernoncourt, Doo\u00a0Soon Kim, Trung Bui, Seokhwan Kim, Walter Chang, and Nazli Goharian.\n\n\nA discourse-aware attention model for abstractive summarization of long documents, 2018.\n\n\n",
      "Dao et\u00a0al. (2022)": "\nDao et\u00a0al. (2022)\n\nTri Dao, Daniel\u00a0Y. Fu, Stefano Ermon, Atri Rudra, and Christopher R\u00e9.\n\n\nFlashattention: Fast and memory-efficient exact attention with io-awareness, 2022.\n\n\n",
      "Devlin et\u00a0al. (2019)": "\nDevlin et\u00a0al. (2019)\n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.\n\n\nBert: Pre-training of deep bidirectional transformers for language understanding, 2019.\n\n\n",
      "Ding et\u00a0al. (2023)": "\nDing et\u00a0al. (2023)\n\nJiayu Ding, Shuming Ma, Li\u00a0Dong, Xingxing Zhang, Shaohan Huang, Wenhui Wang, Nanning Zheng, and Furu Wei.\n\n\nLongnet: Scaling transformers to 1,000,000,000 tokens, 2023.\n\n\n",
      "Dong et\u00a0al. (2023)": "\nDong et\u00a0al. (2023)\n\nZican Dong, Tianyi Tang, Lunyi Li, and Wayne\u00a0Xin Zhao.\n\n\nA survey on long text modeling with transformers, 2023.\n\n\n",
      "Du et\u00a0al. (2022)": "\nDu et\u00a0al. (2022)\n\nZhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie Tang.\n\n\nGlm: General language model pretraining with autoregressive blank infilling.\n\n\nIn Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp.\u00a0 320\u2013335, 2022.\n\n\n",
      "Engelbach et\u00a0al. (2023)": "\nEngelbach et\u00a0al. (2023)\n\nMatthias Engelbach, Dennis Klau, Felix Scheerer, Jens Drawehn, and Maximilien Kintz.\n\n\nFine-tuning and aligning question answering models for complex information extraction tasks, 2023.\n\n\n",
      "Gidiotis & Tsoumakas (2020)": "\nGidiotis & Tsoumakas (2020)\n\nAlexios Gidiotis and Grigorios Tsoumakas.\n\n\nA divide-and-conquer approach to the summarization of long documents, 2020.\n\n\n",
      "Huang et\u00a0al. (2021)": "\nHuang et\u00a0al. (2021)\n\nLuyang Huang, Shuyang Cao, Nikolaus Parulian, Heng Ji, and Lu\u00a0Wang.\n\n\nEfficient attentions for long document summarization, 2021.\n\n\n",
      "Izacard et\u00a0al. (2022)": "\nIzacard et\u00a0al. (2022)\n\nGautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-Yu, Armand Joulin, Sebastian Riedel, and Edouard Grave.\n\n\nAtlas: Few-shot learning with retrieval augmented language models, 2022.\n\n\n",
      "Kim & Park (2023)": "\nKim & Park (2023)\n\nNamHyeok Kim and Chanjun Park.\n\n\nInter-annotator agreement in the wild: Uncovering its emerging roles and considerations in real-world scenarios, 2023.\n\n\n",
      "Kojima et\u00a0al. (2023)": "\nKojima et\u00a0al. (2023)\n\nTakeshi Kojima, Shixiang\u00a0Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa.\n\n\nLarge language models are zero-shot reasoners, 2023.\n\n\n",
      "Lee et\u00a0al. (2023)": "\nLee et\u00a0al. (2023)\n\nHarrison Lee, Samrat Phatale, Hassan Mansoor, Kellie Lu, Thomas Mesnard, Colton Bishop, Victor Carbune, and Abhinav Rastogi.\n\n\nRlaif: Scaling reinforcement learning from human feedback with ai feedback, 2023.\n\n\n",
      "Li et\u00a0al. (2023a)": "\nLi et\u00a0al. (2023a)\n\nShanda Li, Chong You, Guru Guruganesh, Joshua Ainslie, Santiago Ontanon, Manzil Zaheer, Sumit Sanghai, Yiming Yang, Sanjiv Kumar, and Srinadh Bhojanapalli.\n\n\nFunctional interpolation for relative positions improves long context transformers, 2023a.\n\n\n",
      "Li (2023)": "\nLi (2023)\n\nYucheng Li.\n\n\nUnlocking context constraints of llms: Enhancing context efficiency of llms with self-information-based content filtering, 2023.\n\n\n",
      "Li et\u00a0al. (2023b)": "\nLi et\u00a0al. (2023b)\n\nZihao Li, Samuel Belkadi, Nicolo Micheletti, Lifeng Han, Matthew Shardlow, and Goran Nenadic.\n\n\nLarge language models and control mechanisms improve text readability of biomedical abstracts, 2023b.\n\n\n",
      "Liu et\u00a0al. (2023a)": "\nLiu et\u00a0al. (2023a)\n\nNelson\u00a0F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang.\n\n\nLost in the middle: How language models use long contexts, 2023a.\n\n\n",
      "Liu et\u00a0al. (2023b)": "\nLiu et\u00a0al. (2023b)\n\nYuxuan Liu, Tianchi Yang, Shaohan Huang, Zihan Zhang, Haizhen Huang, Furu Wei, Weiwei Deng, Feng Sun, and Qi\u00a0Zhang.\n\n\nCalibrating llm-based evaluator, 2023b.\n\n\n",
      "Meister et\u00a0al. (2021)": "\nMeister et\u00a0al. (2021)\n\nClara Meister, Stefan Lazov, Isabelle Augenstein, and Ryan Cotterell.\n\n\nIs sparse attention more interpretable?, 2021.\n\n\n",
      "Mukherjee & Rahman (2023)": "\nMukherjee & Rahman (2023)\n\nUsmi Mukherjee and Mohammad\u00a0Masudur Rahman.\n\n\nEmploying deep learning and structured information retrieval to answer clarification questions on bug reports, 2023.\n\n\n",
      "OpenAI (2023)": "\nOpenAI (2023)\n\nOpenAI.\n\n\nGpt-4 technical report, 2023.\n\n\n",
      "Ouyang et\u00a0al. (2022)": "\nOuyang et\u00a0al. (2022)\n\nLong Ouyang, Jeff Wu, Xu\u00a0Jiang, Diogo Almeida, Carroll\u00a0L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe.\n\n\nTraining language models to follow instructions with human feedback, 2022.\n\n\n",
      "Peng et\u00a0al. (2023)": "\nPeng et\u00a0al. (2023)\n\nBo\u00a0Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Huanqi Cao, Xin Cheng, Michael Chung, Matteo Grella, Kranthi\u00a0Kiran GV, Xuzheng He, Haowen Hou, Przemyslaw Kazienko, Jan Kocon, Jiaming Kong, Bartlomiej Koptyra, Hayden Lau, Krishna Sri\u00a0Ipsit Mantri, Ferdinand Mom, Atsushi Saito, Xiangru Tang, Bolun Wang, Johan\u00a0S. Wind, Stansilaw Wozniak, Ruichong Zhang, Zhenyuan Zhang, Qihang Zhao, Peng Zhou, Jian Zhu, and Rui-Jie Zhu.\n\n\nRwkv: Reinventing rnns for the transformer era, 2023.\n\n\n",
      "Ram et\u00a0al. (2023)": "\nRam et\u00a0al. (2023)\n\nOri Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon Shashua, Kevin Leyton-Brown, and Yoav Shoham.\n\n\nIn-context retrieval-augmented language models, 2023.\n\n\n",
      "Roy (2021)": "\nRoy (2021)\n\nArya Roy.\n\n\nRecent trends in named entity recognition (ner), 2021.\n\n\n",
      "s\u00a0Ko\u02c7cisk\u00fd et\u00a0al. (2018)": "\ns\u00a0Ko\u02c7cisk\u00fd et\u00a0al. (2018)\n\nTom\u00e1\u0161\u00a0Ko\u02c7cisk\u00fd, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl\u00a0Moritz Hermann, G\u00e1bor Melis, and Edward Grefenstette.\n\n\nThe NarrativeQA reading comprehension challenge.\n\n\nTransactions of the Association for Computational Linguistics, TBD:TBD, 2018.\n\n\nURL https://TBD.\n\n\n",
      "Shaham et\u00a0al. (2022)": "\nShaham et\u00a0al. (2022)\n\nUri Shaham, Elad Segal, Maor Ivgi, Avia Efrat, Ori Yoran, Adi Haviv, Ankit Gupta, Wenhan Xiong, Mor Geva, Jonathan Berant, and Omer Levy.\n\n\nScrolls: Standardized comparison over long language sequences, 2022.\n\n\n",
      "Shaham et\u00a0al. (2023)": "\nShaham et\u00a0al. (2023)\n\nUri Shaham, Maor Ivgi, Avia Efrat, Jonathan Berant, and Omer Levy.\n\n\nZeroscrolls: A zero-shot benchmark for long text understanding, 2023.\n\n\n",
      "Sharma et\u00a0al. (2019)": "\nSharma et\u00a0al. (2019)\n\nEva Sharma, Chen Li, and Lu\u00a0Wang.\n\n\nBigpatent: A large-scale dataset for abstractive and coherent summarization, 2019.\n\n\n",
      "Sharma et\u00a0al. (2023)": "\nSharma et\u00a0al. (2023)\n\nRoshan Sharma, Suyoun Kim, Daniel Lazar, Trang Le, Akshat Shrivastava, Kwanghoon Ahn, Piyush Kansal, Leda Sari, Ozlem Kalinli, and Michael Seltzer.\n\n\nAugmenting text for spoken language understanding with large language models, 2023.\n\n\n",
      "Song et\u00a0al. (2020)": "\nSong et\u00a0al. (2020)\n\nKaitao Song, Xu\u00a0Tan, Tao Qin, Jianfeng Lu, and Tie-Yan Liu.\n\n\nMpnet: Masked and permuted pre-training for language understanding, 2020.\n\n\n",
      "Suri et\u00a0al. (2023)": "\nSuri et\u00a0al. (2023)\n\nGaurav Suri, Lily\u00a0R. Slater, Ali Ziaee, and Morgan Nguyen.\n\n\nDo large language models show decision heuristics similar to humans? a case study using gpt-3.5, 2023.\n\n\n",
      "Tay et\u00a0al. (2020)": "\nTay et\u00a0al. (2020)\n\nYi\u00a0Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler.\n\n\nLong range arena: A benchmark for efficient transformers, 2020.\n\n\n",
      "Tay et\u00a0al. (2022)": "\nTay et\u00a0al. (2022)\n\nYi\u00a0Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler.\n\n\nEfficient transformers: A survey, 2022.\n\n\n",
      "Touvron et\u00a0al. (2023)": "\nTouvron et\u00a0al. (2023)\n\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample.\n\n\nLlama: Open and efficient foundation language models, 2023.\n\n\n",
      "Trivedi et\u00a0al. (2022)": "\nTrivedi et\u00a0al. (2022)\n\nHarsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal.\n\n\nMusique: Multihop questions via single-hop question composition, 2022.\n\n\n",
      "Tworkowski et\u00a0al. (2023)": "\nTworkowski et\u00a0al. (2023)\n\nSzymon Tworkowski, Konrad Staniszewski, Miko\u0142aj Pacek, Yuhuai Wu, Henryk Michalewski, and Piotr Mi\u0142o\u015b.\n\n\nFocused transformer: Contrastive training for context scaling, 2023.\n\n\n",
      "Wang et\u00a0al. (2022)": "\nWang et\u00a0al. (2022)\n\nAlex Wang, Richard\u00a0Yuanzhe Pang, Angelica Chen, Jason Phang, and Samuel\u00a0R. Bowman.\n\n\nSquality: Building a long-document summarization dataset the hard way, 2022.\n\n\n",
      "Wei et\u00a0al. (2023)": "\nWei et\u00a0al. (2023)\n\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed\u00a0Chi, Quoc Le, and Denny Zhou.\n\n\nChain-of-thought prompting elicits reasoning in large language models, 2023.\n\n\n",
      "Wu et\u00a0al. (2021)": "\nWu et\u00a0al. (2021)\n\nJeff Wu, Long Ouyang, Daniel\u00a0M. Ziegler, Nisan Stiennon, Ryan Lowe, Jan Leike, and Paul Christiano.\n\n\nRecursively summarizing books with human feedback, 2021.\n\n\n",
      "Xiong et\u00a0al. (2023)": "\nXiong et\u00a0al. (2023)\n\nWenhan Xiong, Jingyu Liu, Igor Molybog, Hejia Zhang, Prajjwal Bhargava, Rui Hou, Louis Martin, Rashi Rungta, Karthik\u00a0Abinav Sankararaman, Barlas Oguz, Madian Khabsa, Han Fang, Yashar Mehdad, Sharan Narang, Kshitiz Malik, Angela Fan, Shruti Bhosale, Sergey Edunov, Mike Lewis, Sinong Wang, and Hao Ma.\n\n\nEffective long-context scaling of foundation models, 2023.\n\n\n",
      "Xu et\u00a0al. (2023)": "\nXu et\u00a0al. (2023)\n\nPeng Xu, Wei Ping, Xianchao Wu, Lawrence McAfee, Chen Zhu, Zihan Liu, Sandeep Subramanian, Evelina Bakhturina, Mohammad Shoeybi, and Bryan Catanzaro.\n\n\nRetrieval meets long context large language models, 2023.\n\n\n",
      "Yang et\u00a0al. (2018)": "\nYang et\u00a0al. (2018)\n\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William\u00a0W. Cohen, Ruslan Salakhutdinov, and Christopher\u00a0D. Manning.\n\n\nHotpotqa: A dataset for diverse, explainable multi-hop question answering, 2018.\n\n\n",
      "Ye et\u00a0al. (2023)": "\nYe et\u00a0al. (2023)\n\nJunjie Ye, Xuanting Chen, Nuo Xu, Can Zu, Zekai Shao, Shichun Liu, Yuhan Cui, Zeyang Zhou, Chao Gong, Yang Shen, Jie Zhou, Siming Chen, Tao Gui, Qi\u00a0Zhang, and Xuanjing Huang.\n\n\nA comprehensive capability analysis of gpt-3 and gpt-3.5 series models, 2023.\n\n\n",
      "Zeng et\u00a0al. (2023)": "\nZeng et\u00a0al. (2023)\n\nYan Zeng, Hanbo Zhang, Jiani Zheng, Jiangnan Xia, Guoqiang Wei, Yang Wei, Yuchen Zhang, and Tao Kong.\n\n\nWhat matters in training a gpt4-style language model with multimodal inputs?, 2023.\n\n\n",
      "Zheng et\u00a0al. (2023)": "\nZheng et\u00a0al. (2023)\n\nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi\u00a0Lin, Zhuohan Li, Dacheng Li, Eric.\u00a0P Xing, Hao Zhang, Joseph\u00a0E. Gonzalez, and Ion Stoica.\n\n\nJudging llm-as-a-judge with mt-bench and chatbot arena, 2023.\n\n\n",
      "Zhou et\u00a0al. (2022)": "\nZhou et\u00a0al. (2022)\n\nYucheng Zhou, Tao Shen, Xiubo Geng, Chongyang Tao, Guodong Long, Can Xu, and Daxin Jiang.\n\n\nFine-grained distillation for long document retrieval, 2022.\n\n\n"
    },
    "domain": "cs.CL",
    "citation_count": 0
  },
  "313d21cd-31e7-4dca-982b-69294dc77062": {
    "pk": "313d21cd-31e7-4dca-982b-69294dc77062",
    "authors": [
      "Hongjin Qian",
      "Zheng Liu",
      "Peitian Zhang",
      "Kelong Mao",
      "Yujia Zhou",
      "Xu Chen",
      "Zhicheng Dou"
    ],
    "title": "Are Long-LLMs A Necessity For Long-Context Tasks?",
    "abstract": "The learning and deployment of long-LLMs remains a challenging problem despite recent progresses. In this work, we argue that the long-LLMs are not a necessity to solve long-context tasks, as common long-context tasks are short-context solvable, i.e. they can be solved by purely working with oracle short-contexts within the long-context tasks' inputs. On top of this argument, we propose a framework called LC-Boost (Long-Context Bootstrapper), which enables a short-LLM to address the long-context tasks in a bootstrapping manner. In our framework, the short-LLM prompts itself to reason for two critical decisions: 1) how to access to the appropriate part of context within the input, 2) how to make effective use of the accessed context. By adaptively accessing and utilizing the context based on the presented tasks, LC-Boost can serve as a general framework to handle diversified long-context processing problems. We comprehensively evaluate different types of tasks from popular long-context benchmarks, where LC-Boost is able to achieve a substantially improved performance with a much smaller consumption of resource.",
    "url": "http://arxiv.org/abs/2405.15318v1",
    "timestamp": 1716537570,
    "sections": {
      "1 Introduction": "\n\n1 Introduction\n\nLarge language models (LLMs) are widely adopted for real-world applications. Many of the applications are associated with long-sequence inputs, such as long-document question answering and summarization. As such, the LLMs are commonly expected to have a long working context (a.k.a. long-LLMs) in order to confront such demanding scenarios\u00a0(Bai et\u00a0al., 2023; Zhang et\u00a0al., 2024a). Unfortunately, the learning and deployment of long-LLMs are still challenging in multiple perspectives. Particularly, many existing LLMs are initially introduced with a limited size of context (e.g., 2K for Llama-1 Touvron et\u00a0al. (2023a), 4K for Llama-2 Touvron et\u00a0al. (2023b), 8K for Llama-3\u00a0111https://llama.meta.com/llama3/). Although the initial short-LLM can be fine-tuned to establish a much longer context, it is likely to take substantial costs; and more seriously, it is extremely resource-consuming to deploy the long-LLMs\u00a0(Kaplan et\u00a0al., 2020). The continually training may also compromise the LLMs\u2019 general capability over short contexts\u00a0(Liu et\u00a0al., 2023; Li et\u00a0al., 2023a). In fact, it remains an open problem to explore new solutions which may tackle long-context tasks both effectively and efficiently.\n\n\nIn this paper, we argue that most long-context tasks are short-context solvable. That is to say, the long-context tasks, despite associated with long-sequence inputs, can be addressed by merely working with short-contexts in a strategic way. For example, the reading comprehension or summarization of a book can be solved based on the extraction of necessary key facts from the book.\nThe above argument is akin to the working patterns of human beings and modern computers, where arbitrary long-form problems can always be decomposed and solved on top of a limited memory capacity\u00a0(Adolphs, 1999; Bryant and O\u2019Hallaron, 2011).\nHowever, even if the above argument holds, it is still non-trivial to solve the long-context tasks purely based on short contexts. This is because different tasks call for distinct ways of accessing and utilizing information from the long context; therefore, there can hardly be any fixed rules to handle all possible situations. To address this challenge, we propose a method, called LC-Boost, where short-LLMs are employed to solve general long-context tasks in a bootstrapping manner. LC-Boost operates with two critical reasoning steps. One is the reasoning of Access, where the LLM prompts itself to plan for how to access the appropriate part of context within the input. The other one is the reasoning of Utilize, where the LLM figures out how to make effective use of the accessed context. Thanks to the above design, LC-Boost is able to adaptively handle diversified long-context tasks according to their unique nature. For example, given a knowledge-grounded QA problem, the LLM may directly access to the knowledgable context through retrieval, and generate the answer in the form of RAG. Besides, it may sequentially scan the long context chunk-by-chunk if the task calls for the aggregation of specific information from the entire input.\n\n\nFigure 1: Illustration for LC-Boost. The LLM is prompted to reason for how to access to proper context and how to utilize the accessed context to solve the task. Toy Examples.\n(A) Brute-force solution. Despite correctness, it is unnecessarily expensive due to the processing of the entire context simultaneously. (B) Naive RAG. It is hard to handle problems like information aggregation, which leads to the incomplete answer. (C) LC-Boost leverages RAG to tackle the problem, which produces the correct answer in a small cost. (D) LC-Boost processes the long-context via sequential scan, which correctly solves the problem based on the comprehensively collected information.\n\n\nThe following toy examples are presented to better illustrate the mechanism of LC-Boost (Figure 1). Particular, there are two common approaches to tackle long-context problems: (A) the brute-force method based on long-LLMs, (B) the surrogate methods, like RAG Xu et\u00a0al. (2023a). Despite being straightforward, the brute-force method is likely to incur huge unnecessary costs as the problem could be directly solved by simple surrogate methods, like RAG. On the other hand, although the surrogate methods may help in certain cases, they are likely to become useless in other situations. For instance, the RAG-based methods are inappropriate to handle information aggregation problems, as showcased in\u00a0(B). In contrast, LC-Boost is able to handle general long-context tasks thanks to the proper reasoning of how to access and utilize\nthe long-context information based on each specific task. As shown in (C), it can directly access to the needed information via retrieval and generate the answer based on RAG. Meanwhile, it can also process the entire context in a divide-and-conquer manner, which will fully collect the information and solve the problem presented in (D).\n\n\nWe perform comprehensive experiments for LC-Boost, including both popular real-world long-context problems, like question-answering and summarization of long documents, and a wide variety of synthetic tasks. In our experiments, LC-Boost is able to achieve equivalent performances as the brute-force methods based on strong long-LLMs, e.g., GPT-4-128K. In many cases, its performances can even notably surpass the brute-force methods, probably due to the elimination of distracting context. Besides, our experiments also underscore the importance of reasoning and adaptability, as LC-Boost outperforms all short-LLM surrogates with predefined access and utilization of context.\n\n\nTo summarize, our paper makes the following contributions. (1) We identify the research problem of solving long-context problems with short-LLMs. To the best of our knowledge, it is the first study of its kind, which is important to not only address the problem itself but also meaningful to the sustainability and energy-efficient running of AI industry in a broader sense. (2) We propose a novel framework LC-Boost, which is able to adaptively handle general long-context tasks based on the reasoning of how to access and utilize the long context. (3) We empirically verify the effectiveness of LC-Boost based on its superior performances achieved from low resource-consumption.\n\n",
      "2 LC-Boost": "\n\n2 LC-Boost\n\n\n2.1 Preliminaries\n\nLLMs can be succinctly defined as \ud835\udcb4=\u03b3\u2062(q)\ud835\udcb4\ud835\udefe\ud835\udc5e{\\mathcal{Y}}=\\gamma(q)caligraphic_Y = italic_\u03b3 ( italic_q ), where \u03b3\u2062(\u22c5)\ud835\udefe\u22c5\\gamma(\\cdot)italic_\u03b3 ( \u22c5 ) represents a selected LLM, q\ud835\udc5eqitalic_q denotes a user query, and \ud835\udcb4\ud835\udcb4{\\mathcal{Y}}caligraphic_Y refers to the answer produced by the LLMs. As highlighted in many previous studies, e.g.,\u00a0(Ji et\u00a0al., 2023; Lewis et\u00a0al., 2020; Shuster et\u00a0al., 2021), the knowledge embedded in an LLM\u2019s parameters is static and, consequently, often fails to adequately address user queries requiring up-to-date or in-depth knowledge. To address this limitation, we can introduce external knowledge (refer to as context \ud835\udcb3\ud835\udcb3{\\mathcal{X}}caligraphic_X) into the LLMs. Additionally, tasks involving information aggregation\u00a0(e.g., summarization) also take a context \ud835\udcb3\ud835\udcb3{\\mathcal{X}}caligraphic_X as input along with task instructions q\ud835\udc5eqitalic_q. Thus, we can generally define the model\u2019s generation process w.r.t. a context \ud835\udcb3\ud835\udcb3{\\mathcal{X}}caligraphic_X as:\n\ud835\udcb4=\u03b3\u2062(q,\ud835\udcb3).\ud835\udcb4\ud835\udefe\ud835\udc5e\ud835\udcb3{\\mathcal{Y}}=\\gamma(q,{\\mathcal{X}}).caligraphic_Y = italic_\u03b3 ( italic_q , caligraphic_X ) .\n\n\nAs discussed in Section 1, in many scenarios, the context \ud835\udcb3\ud835\udcb3{\\mathcal{X}}caligraphic_X is a long sequence, necessitating that LLMs manage long contexts. However, most existing LLMs were originally introduced with limited context sizes\u00a0(e.g., 4K). Consequently, these models are unable to process inputs that exceed their capacity without truncation. In this paper, we characterize such scenarios as long-context problem. It involves LLMs processing inputs that notably surpass their inherent context limitations, which can be formally described by:\n\n\n\n\ud835\udcb4=\u03b3\u2062(q,\ud835\udcb3)s.t.\u2062|\ud835\udcb3|\u226bL,formulae-sequence\ud835\udcb4\ud835\udefe\ud835\udc5e\ud835\udcb3much-greater-thans.t.\ud835\udcb3\ud835\udc3f{\\mathcal{Y}}=\\gamma(q,{\\mathcal{X}})\\quad\\text{s.t.}|{\\mathcal{X}}|\\gg L,caligraphic_Y = italic_\u03b3 ( italic_q , caligraphic_X ) s.t. | caligraphic_X | \u226b italic_L ,\n\n(1)\n\n\nwhere L\ud835\udc3fLitalic_L denotes the native context length limit of the LLM. The most straightforward way to address the long-context problem is to increase the LLMs\u2019 context length L\ud835\udc3fLitalic_L, mitigating the challenges of long contexts. In this paper, we instead explore solving long-context tasks using short-context LLMs\u00a0(e.g., 4K) without increasing the model\u2019s context length L\ud835\udc3fLitalic_L.\n\n\n\n\n2.2 Pilot Study: Are Most Long-Context Tasks Short-Context Solvable?\n\nDespite the potential for fine-tuning LLMs to handle much longer contexts, this approach incurs substantial costs. Additionally, directly processing long contexts during the inference stage exponentially increases computing resource consumption, which is not environmentally friendly.\nIn the following, we conduct a pilot study from both theoretical and empirical perspectives to explore the question: Are most long-context tasks solvable with short contexts?\n\n\nTheoretical Analysis\n\nSuppose we have an input variable \ud835\udcb3\ud835\udcb3{\\mathcal{X}}caligraphic_X and an output variable \ud835\udcb4\ud835\udcb4{\\mathcal{Y}}caligraphic_Y, the relevant part of \ud835\udcb3\ud835\udcb3{\\mathcal{X}}caligraphic_X given \ud835\udcb4\ud835\udcb4{\\mathcal{Y}}caligraphic_Y is denoted by \ud835\udcb3~~\ud835\udcb3\\tilde{{\\mathcal{X}}}over~ start_ARG caligraphic_X end_ARG. An ideal \ud835\udcb3~~\ud835\udcb3\\tilde{{\\mathcal{X}}}over~ start_ARG caligraphic_X end_ARG should capture all relevant features of the original input variable \ud835\udcb3\ud835\udcb3{\\mathcal{X}}caligraphic_X in relation to \ud835\udcb4\ud835\udcb4{\\mathcal{Y}}caligraphic_Y. In other words, the optimal \ud835\udcb3~~\ud835\udcb3\\tilde{{\\mathcal{X}}}over~ start_ARG caligraphic_X end_ARG represents the simplest mapping of \ud835\udcb3\ud835\udcb3{\\mathcal{X}}caligraphic_X that accurately preserves the mutual information I\u2062(\ud835\udcb3;\ud835\udcb4)\ud835\udc3c\ud835\udcb3\ud835\udcb4I({\\mathcal{X}};{\\mathcal{Y}})italic_I ( caligraphic_X ; caligraphic_Y ). We therefore propose a Markov chain \ud835\udcb3\u2192\ud835\udcb3~\u2192\ud835\udcb4\u2192\ud835\udcb3~\ud835\udcb3\u2192\ud835\udcb4{\\mathcal{X}}\\rightarrow\\tilde{{\\mathcal{X}}}\\rightarrow{\\mathcal{Y}}caligraphic_X \u2192 over~ start_ARG caligraphic_X end_ARG \u2192 caligraphic_Y. According to the data processing inequality (DPI), we have I\u2062(\ud835\udcb3;\ud835\udcb3~)\u2265I\u2062(\ud835\udcb3;\ud835\udcb4)\ud835\udc3c\ud835\udcb3~\ud835\udcb3\ud835\udc3c\ud835\udcb3\ud835\udcb4I({\\mathcal{X}};\\tilde{{\\mathcal{X}}})\\geq I({\\mathcal{X}};{\\mathcal{Y}})italic_I ( caligraphic_X ; over~ start_ARG caligraphic_X end_ARG ) \u2265 italic_I ( caligraphic_X ; caligraphic_Y ), with equality holding if and only if \ud835\udcb3~~\ud835\udcb3\\tilde{{\\mathcal{X}}}over~ start_ARG caligraphic_X end_ARG constitutes a sufficient statistics\u00a0(Cover, 1999; Tishby and Zaslavsky, 2015). This suggests that, in an optimal setting, we can always find a subset \ud835\udcb3~\u2286\ud835\udcb3~\ud835\udcb3\ud835\udcb3\\tilde{\\mathcal{X}}\\subseteq\\mathcal{X}over~ start_ARG caligraphic_X end_ARG \u2286 caligraphic_X that provides information at least as useful for generating the output \ud835\udcb4\ud835\udcb4\\mathcal{Y}caligraphic_Y as the full context \ud835\udcb3\ud835\udcb3\\mathcal{X}caligraphic_X.\n\n\nIn practical scenarios, obtaining the optimal \ud835\udcb3~~\ud835\udcb3\\tilde{{\\mathcal{X}}}over~ start_ARG caligraphic_X end_ARG is challenging due to various factors, such as empirical errors\u00a0Mohri et\u00a0al. (2018). Thus, we can only estimate \ud835\udcb3~~\ud835\udcb3\\tilde{{\\mathcal{X}}}over~ start_ARG caligraphic_X end_ARG. Estimating \ud835\udcb3~~\ud835\udcb3\\tilde{{\\mathcal{X}}}over~ start_ARG caligraphic_X end_ARG directly from \ud835\udcb3\ud835\udcb3{\\mathcal{X}}caligraphic_X might be challenging if \ud835\udcb3\ud835\udcb3{\\mathcal{X}}caligraphic_X defines a large variable space. In this situation, we propose decomposing the original input variable \ud835\udcb3\ud835\udcb3{\\mathcal{X}}caligraphic_X into a series of subsets, \ud835\udcb3={\ud835\udcb31,\u22ef,\ud835\udcb3n}\ud835\udcb3subscript\ud835\udcb31\u22efsubscript\ud835\udcb3\ud835\udc5b{\\mathcal{X}}=\\{{\\mathcal{X}}_{1},\\cdots,{\\mathcal{X}}_{n}\\}caligraphic_X = { caligraphic_X start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , \u22ef , caligraphic_X start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT } and process each subset variable separately. Thus, according to the chain rule for mutual information\u00a0Cover (1999), we have:\n\n\n\nI\u2062(\ud835\udcb3,\ud835\udcb3~)=I\u2062(\ud835\udcb31,\u22ef,\ud835\udcb3n;\ud835\udcb3~)=I\u2062(\ud835\udcb31;\ud835\udcb3~)+\u2211i=2nI\u2062(\ud835\udcb3i;\ud835\udcb3~|\ud835\udcb31,\u22ef,\ud835\udcb3i\u22121),\ud835\udc3c\ud835\udcb3~\ud835\udcb3\ud835\udc3csubscript\ud835\udcb31\u22efsubscript\ud835\udcb3\ud835\udc5b~\ud835\udcb3\ud835\udc3csubscript\ud835\udcb31~\ud835\udcb3superscriptsubscript\ud835\udc562\ud835\udc5b\ud835\udc3csubscript\ud835\udcb3\ud835\udc56conditional~\ud835\udcb3subscript\ud835\udcb31\u22efsubscript\ud835\udcb3\ud835\udc561\\displaystyle I({\\mathcal{X}},\\tilde{{\\mathcal{X}}})=I({\\mathcal{X}}_{1},%\n\\cdots,{\\mathcal{X}}_{n};\\tilde{{\\mathcal{X}}})=I({\\mathcal{X}}_{1};\\tilde{{%\n\\mathcal{X}}})+\\sum\\limits_{i=2}^{n}I({\\mathcal{X}}_{i};\\tilde{{\\mathcal{X}}}|%\n{\\mathcal{X}}_{1},\\cdots,{\\mathcal{X}}_{i-1}),italic_I ( caligraphic_X , over~ start_ARG caligraphic_X end_ARG ) = italic_I ( caligraphic_X start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , \u22ef , caligraphic_X start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ; over~ start_ARG caligraphic_X end_ARG ) = italic_I ( caligraphic_X start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ; over~ start_ARG caligraphic_X end_ARG ) + \u2211 start_POSTSUBSCRIPT italic_i = 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT italic_I ( caligraphic_X start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ; over~ start_ARG caligraphic_X end_ARG | caligraphic_X start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , \u22ef , caligraphic_X start_POSTSUBSCRIPT italic_i - 1 end_POSTSUBSCRIPT ) ,\n\n(2)\n\n\nwhich indicates that the mutual information I\u2062(\ud835\udcb3,\ud835\udcb3~)\ud835\udc3c\ud835\udcb3~\ud835\udcb3I({\\mathcal{X}},\\tilde{{\\mathcal{X}}})italic_I ( caligraphic_X , over~ start_ARG caligraphic_X end_ARG ) can be understood as the sum of the mutual information of each subset \ud835\udcb3isubscript\ud835\udcb3\ud835\udc56{\\mathcal{X}}_{i}caligraphic_X start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT and \ud835\udcb3~isubscript~\ud835\udcb3\ud835\udc56\\tilde{{\\mathcal{X}}}_{i}over~ start_ARG caligraphic_X end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT given all previous subsets.\n\n\nIn the scenario of Eq.\u00a01, the variable \ud835\udcb3\ud835\udcb3\\mathcal{X}caligraphic_X represents a long context and the variable \ud835\udcb4\ud835\udcb4\\mathcal{Y}caligraphic_Y is the output answer produced by a LLM. Thus, \ud835\udcb3~~\ud835\udcb3\\tilde{{\\mathcal{X}}}over~ start_ARG caligraphic_X end_ARG can be interpreted as the minimal necessary context from the long context \ud835\udcb3\ud835\udcb3{\\mathcal{X}}caligraphic_X given the output answer \ud835\udcb4\ud835\udcb4{\\mathcal{Y}}caligraphic_Y. Inspired by Eq.\u00a02, we can estimate an optimal \ud835\udcb3~~\ud835\udcb3\\tilde{\\mathcal{X}}over~ start_ARG caligraphic_X end_ARG using decomposed shorter contexts {\ud835\udcb31,\u2026,\ud835\udcb3n}subscript\ud835\udcb31\u2026subscript\ud835\udcb3\ud835\udc5b\\{\\mathcal{X}_{1},\\ldots,\\mathcal{X}_{n}\\}{ caligraphic_X start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , \u2026 , caligraphic_X start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT }. Thus, I\u2062(\ud835\udcb3;\ud835\udcb3~)\ud835\udc3c\ud835\udcb3~\ud835\udcb3I(\\mathcal{X};\\tilde{\\mathcal{X}})italic_I ( caligraphic_X ; over~ start_ARG caligraphic_X end_ARG ) can be computed by processing each subset \ud835\udcb3isubscript\ud835\udcb3\ud835\udc56\\mathcal{X}_{i}caligraphic_X start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT individually. However, as the number of subsets n\ud835\udc5bnitalic_n increases, accounting for all preceding subsets becomes computationally demanding. To alleviate this burden, we propose reducing the number of conditional subsets considered by replacing the entire sequence of previous subsets with a compressed surrogate \ud835\udcb3^isubscript^\ud835\udcb3\ud835\udc56\\hat{\\mathcal{X}}_{i}over^ start_ARG caligraphic_X end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT, which is iteratively derived using a compression function \ud835\udcb3^i=g\u2062(\ud835\udcb3^i\u22121,\ud835\udcb3i\u22121)subscript^\ud835\udcb3\ud835\udc56\ud835\udc54subscript^\ud835\udcb3\ud835\udc561subscript\ud835\udcb3\ud835\udc561\\hat{\\mathcal{X}}_{i}=g(\\hat{\\mathcal{X}}_{i-1},\\mathcal{X}_{i-1})over^ start_ARG caligraphic_X end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = italic_g ( over^ start_ARG caligraphic_X end_ARG start_POSTSUBSCRIPT italic_i - 1 end_POSTSUBSCRIPT , caligraphic_X start_POSTSUBSCRIPT italic_i - 1 end_POSTSUBSCRIPT ). Consequently, Eq.\u00a02 can be reformulated as follows:\n\n\n\nI(\ud835\udcb3,\ud835\udcb3~)=I(\ud835\udcb31,\u22ef,\ud835\udcb3n;\ud835\udcb3~)\u2243I(\ud835\udcb31;\ud835\udcb3~)+\u2211i=2nI(\ud835\udcb3i;\ud835\udcb3~|\ud835\udcb3^i)).I({\\mathcal{X}},\\tilde{{\\mathcal{X}}})=I({\\mathcal{X}}_{1},\\cdots,{\\mathcal{X}%\n}_{n};\\tilde{{\\mathcal{X}}})\\simeq I({\\mathcal{X}}_{1};\\tilde{{\\mathcal{X}}})+%\n\\sum\\limits_{i=2}^{n}I({\\mathcal{X}}_{i};\\tilde{{\\mathcal{X}}}|\\hat{{\\mathcal{%\nX}}}_{i})).italic_I ( caligraphic_X , over~ start_ARG caligraphic_X end_ARG ) = italic_I ( caligraphic_X start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , \u22ef , caligraphic_X start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ; over~ start_ARG caligraphic_X end_ARG ) \u2243 italic_I ( caligraphic_X start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ; over~ start_ARG caligraphic_X end_ARG ) + \u2211 start_POSTSUBSCRIPT italic_i = 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT italic_I ( caligraphic_X start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ; over~ start_ARG caligraphic_X end_ARG | over^ start_ARG caligraphic_X end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) ) .\n\n(3)\n\n\nThe equality can be upheld under two specific conditions: (1) the decomposed variables {\ud835\udcb31,\u2026,\ud835\udcb3n}subscript\ud835\udcb31\u2026subscript\ud835\udcb3\ud835\udc5b\\{\\mathcal{X}_{1},\\ldots,\\mathcal{X}_{n}\\}{ caligraphic_X start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , \u2026 , caligraphic_X start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT } are mutually independent, and (2) the compression function g\u2062(\u22c5)\ud835\udc54\u22c5g(\\cdot)italic_g ( \u22c5 ) is optimally designed, ensuring that the compressed surrogate \ud835\udcb3^isubscript^\ud835\udcb3\ud835\udc56\\hat{\\mathcal{X}}_{i}over^ start_ARG caligraphic_X end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT encapsulates all relevant information from the preceding subsets with respect to \ud835\udcb3~~\ud835\udcb3\\tilde{\\mathcal{X}}over~ start_ARG caligraphic_X end_ARG. Otherwise, I\u2062(\ud835\udcb3,\ud835\udcb3~)\ud835\udc3c\ud835\udcb3~\ud835\udcb3I({\\mathcal{X}},\\tilde{{\\mathcal{X}}})italic_I ( caligraphic_X , over~ start_ARG caligraphic_X end_ARG ) can only be approximately estimated.\n\n\n\nEmpirical Analysis\n\nFigure 2: Pilot Study Across Various Tasks: In the Brute-force setting, the entire context is processed by GPT-4-128K. In the LC-Boost setting, the maximum context length is restricted to 4K, and LC-Boost is utilized to solve the long-context problem with short context. \n\n\nTo empirically assess the accuracy of estimating the minimal necessary context \ud835\udcb3~~\ud835\udcb3\\tilde{\\mathcal{X}}over~ start_ARG caligraphic_X end_ARG using decomposed short contexts {\ud835\udcb31,\u2026,\ud835\udcb3n}subscript\ud835\udcb31\u2026subscript\ud835\udcb3\ud835\udc5b\\{\\mathcal{X}_{1},\\ldots,\\mathcal{X}_{n}\\}{ caligraphic_X start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , \u2026 , caligraphic_X start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT }, we conduct pilot experiments across various tasks requiring long contexts. Specifically, we utilize GPT-4-128K to perform these tasks in two settings: (1) feeding the entire long context into GPT-4-128K in a brute-force manner, instructing the model to directly produce the output answer, and (2) decomposing the full context into short contexts and applying the methods defined in Eq.\u00a03 to approximate \ud835\udcb3~~\ud835\udcb3\\tilde{\\mathcal{X}}over~ start_ARG caligraphic_X end_ARG, which then guides the model to produce the final output (the LC-Boost setting).\n\n\nFigure\u00a02 presents the experiment results, which generally indicate that LC-Boost consistently performs as well as or better than the brute-force setting. In particular, for tasks such as QA, few-shot learning, and synthetic tasks, LC-Boost outperforms the brute-force setting. This is because the decomposed short contexts for these tasks are more likely to be mutually independent given the input query which can be adequately supported by a few extracted contexts from the long context. By precisely\nlocating these supported context, it can filter out irrelevant context of \ud835\udcb3\ud835\udcb3{\\mathcal{X}}caligraphic_X that might otherwise undermine task performance.\nFor tasks like summarization and code completion, the inherent properties of these tasks require considering the mutual dependencies among all decomposed short contexts, making the LC-Boost setting more challenging. However, as discussed in Eq.\u00a03, when the compression function g\u2062(\u22c5)\ud835\udc54\u22c5g(\\cdot)italic_g ( \u22c5 ) is optimal, we can achieve the optimal \ud835\udcb3~~\ud835\udcb3\\tilde{\\mathcal{X}}over~ start_ARG caligraphic_X end_ARG. GPT-4 serves as such a strong compression function, ensuring that the compressed surrogate \ud835\udcb3^isubscript^\ud835\udcb3\ud835\udc56\\hat{\\mathcal{X}}_{i}over^ start_ARG caligraphic_X end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT is well-estimated. Consequently, in these tasks, LC-Boost achieves performance that is equal to or slightly better than the brute-force setting.\n\n\nThrough theoretical analysis, we can posit that long-context tasks are short-context solvable if we can estimate a better minimal necessary context \ud835\udcb3~~\ud835\udcb3\\tilde{\\mathcal{X}}over~ start_ARG caligraphic_X end_ARG from the decomposed short contexts {\ud835\udcb31,\u2026,\ud835\udcb3n}subscript\ud835\udcb31\u2026subscript\ud835\udcb3\ud835\udc5b\\{\\mathcal{X}_{1},\\ldots,\\mathcal{X}_{n}\\}{ caligraphic_X start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , \u2026 , caligraphic_X start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT } than from the long context \ud835\udcb3\ud835\udcb3\\mathcal{X}caligraphic_X. Empirical analysis supports this assumption, demonstrating that in most cases, the estimation error of deriving \ud835\udcb3~~\ud835\udcb3\\tilde{\\mathcal{X}}over~ start_ARG caligraphic_X end_ARG from the long context \ud835\udcb3\ud835\udcb3\\mathcal{X}caligraphic_X is often larger than from the decomposed short contexts {\ud835\udcb31,\u2026,\ud835\udcb3n}subscript\ud835\udcb31\u2026subscript\ud835\udcb3\ud835\udc5b\\{\\mathcal{X}_{1},\\ldots,\\mathcal{X}_{n}\\}{ caligraphic_X start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , \u2026 , caligraphic_X start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT }. This indicates that using short contexts can be comparatively more advantageous than using the full context. Therefore, we can validate our argument in Section\u00a01: most long-context tasks, if not all, are short-context solvable.\n\n\n\n\n\n2.3 The Proposed Method: LC-Boost\n\nWe propose a method called LC-Boost, which utilizes short LLMs to solve general long-context tasks. LC-Boost begins with an input query q\ud835\udc5eqitalic_q and a long context \ud835\udcb3\ud835\udcb3\\mathcal{X}caligraphic_X, with the goal of producing an output answer \ud835\udcb4\ud835\udcb4\\mathcal{Y}caligraphic_Y. Since the underlying LLM in LC-Boost has a limited context size\u00a0(we limit LC-Boost working with 4K context length), directly generating the output answer \ud835\udcb4\ud835\udcb4\\mathcal{Y}caligraphic_Y is infeasible for long-context tasks. To address this, we propose solving long-context tasks by strategically understanding the decomposed short contexts \ud835\udcb3={\ud835\udcb31,\u22ef,\ud835\udcb3n}\ud835\udcb3subscript\ud835\udcb31\u22efsubscript\ud835\udcb3\ud835\udc5b\\mathcal{X}=\\{\\mathcal{X}_{1},\\cdots,\\mathcal{X}_{n}\\}caligraphic_X = { caligraphic_X start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , \u22ef , caligraphic_X start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT }. From these short contexts, we aim to extract the minimal necessary context \ud835\udcb3~~\ud835\udcb3\\tilde{\\mathcal{X}}over~ start_ARG caligraphic_X end_ARG to support the generation of the output answer \ud835\udcb4\ud835\udcb4\\mathcal{Y}caligraphic_Y.\n\n\nLC-Boost achieves this goal through a decision-making process involving iterative interactions between LC-Boost and the decomposed short contexts {\ud835\udcb31,\u22ef,\ud835\udcb3n}subscript\ud835\udcb31\u22efsubscript\ud835\udcb3\ud835\udc5b\\{\\mathcal{X}_{1},\\cdots,\\mathcal{X}_{n}\\}{ caligraphic_X start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , \u22ef , caligraphic_X start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT } with respect to the input query q\ud835\udc5eqitalic_q. In the process, LC-Boost interact with each short context \ud835\udcb3isubscript\ud835\udcb3\ud835\udc56\\mathcal{X}_{i}caligraphic_X start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT, employing two types of actions: information access and information utilization.\n\n\nWe denote an action at time step i\ud835\udc56iitalic_i by aisubscript\ud835\udc4e\ud835\udc56a_{i}italic_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT and denote the relevant context LC-Boost obtains from the i\ud835\udc56iitalic_i-th short context \ud835\udcb3isubscript\ud835\udcb3\ud835\udc56{\\mathcal{X}}_{i}caligraphic_X start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT by \ud835\udcb3~isubscript~\ud835\udcb3\ud835\udc56\\tilde{{\\mathcal{X}}}_{i}over~ start_ARG caligraphic_X end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT\nThe action aisubscript\ud835\udc4e\ud835\udc56a_{i}italic_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT is predicted by considering the current short context \ud835\udcb3isubscript\ud835\udcb3\ud835\udc56{\\mathcal{X}}_{i}caligraphic_X start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT, the input query q\ud835\udc5eqitalic_q, as well as all previous extracted relevant information \ud835\udcb3~1:i\u22121subscript~\ud835\udcb3:1\ud835\udc561\\tilde{{\\mathcal{X}}}_{1:i-1}over~ start_ARG caligraphic_X end_ARG start_POSTSUBSCRIPT 1 : italic_i - 1 end_POSTSUBSCRIPT: ai=\u03b3\u2062(q,\ud835\udcb3i|\ud835\udcb3~1:i\u22121),subscript\ud835\udc4e\ud835\udc56\ud835\udefe\ud835\udc5econditionalsubscript\ud835\udcb3\ud835\udc56subscript~\ud835\udcb3:1\ud835\udc561a_{i}=\\gamma(q,{\\mathcal{X}}_{i}|\\tilde{{\\mathcal{X}}}_{1:i-1}),italic_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = italic_\u03b3 ( italic_q , caligraphic_X start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT | over~ start_ARG caligraphic_X end_ARG start_POSTSUBSCRIPT 1 : italic_i - 1 end_POSTSUBSCRIPT ) ,\nwhere \u03b3\u2062(\u22c5)\ud835\udefe\u22c5\\gamma(\\cdot)italic_\u03b3 ( \u22c5 ) denotes LC-Boost\u2019s underlying LLM.\n\n\nPredicting the action aisubscript\ud835\udc4e\ud835\udc56a_{i}italic_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT in a continuous space is challenging as it requires the underling model to reason about highly implicit relations among the input query, the current context, and the previous contexts. Therefore, we define a discrete action space \ud835\udc9c\ud835\udc9c{\\mathcal{A}}caligraphic_A comprising:\n(1)\u00a0[Task Understanding]: analyzing the query and task for initialization;\n(2)\u00a0[Retrieve]: accessing text evidence by a retrieval method;\n(3)\u00a0[Move]: accessing the next short text context directly;\nThese two are information access actions which define the LC-Boost\u2019s trajectory to access short contexts.\n(4)\u00a0[Append]: generating relevant context \ud835\udcb3~isubscript~\ud835\udcb3\ud835\udc56\\tilde{{\\mathcal{X}}}_{i}over~ start_ARG caligraphic_X end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT independently, denoting by \ud835\udcb3~i=ai\u2062(\ud835\udcb3i)subscript~\ud835\udcb3\ud835\udc56subscript\ud835\udc4e\ud835\udc56subscript\ud835\udcb3\ud835\udc56\\tilde{{\\mathcal{X}}}_{i}=a_{i}({\\mathcal{X}}_{i})over~ start_ARG caligraphic_X end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = italic_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( caligraphic_X start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT );\n(5)\u00a0[Merge]: generating relevant context \ud835\udcb3~isubscript~\ud835\udcb3\ud835\udc56\\tilde{{\\mathcal{X}}}_{i}over~ start_ARG caligraphic_X end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT with respect to previous extracted relevant information, denoting by \ud835\udcb3~i=ai\u2062(\ud835\udcb3i|\ud835\udcb3~1:i\u22121)subscript~\ud835\udcb3\ud835\udc56subscript\ud835\udc4e\ud835\udc56conditionalsubscript\ud835\udcb3\ud835\udc56subscript~\ud835\udcb3:1\ud835\udc561\\tilde{{\\mathcal{X}}}_{i}=a_{i}({\\mathcal{X}}_{i}|\\tilde{{\\mathcal{X}}}_{1:i-1})over~ start_ARG caligraphic_X end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = italic_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( caligraphic_X start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT | over~ start_ARG caligraphic_X end_ARG start_POSTSUBSCRIPT 1 : italic_i - 1 end_POSTSUBSCRIPT );\n(6)\u00a0[Answer]: answering the user query and returning;\n(7)\u00a0[Aggregation]: aggregating all relevant information and returning.\nWe define our LC-Boost frame in Algorithm\u00a01.\n\n\nAlgorithm 1  LC-Boost Framework\n\n\n1:\u00a0\u00a0Input: Input query q\ud835\udc5eqitalic_q, long context \ud835\udcb3\ud835\udcb3\\mathcal{X}caligraphic_X\n\n\n\n2:\u00a0\u00a0Output: Answer \ud835\udcb4\ud835\udcb4\\mathcal{Y}caligraphic_Y\n\n\n\n3:\u00a0\u00a0Decompose long context\u2062\ud835\udcb3\u2190{\ud835\udcb31,\u22ef,\ud835\udcb3n}\u2190long context\ud835\udcb3subscript\ud835\udcb31\u22efsubscript\ud835\udcb3\ud835\udc5b\\text{long context}~{}{\\mathcal{X}}\\leftarrow\\{{\\mathcal{X}}_{1},\\cdots,{%\n\\mathcal{X}}_{n}\\}long context caligraphic_X \u2190 { caligraphic_X start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , \u22ef , caligraphic_X start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT }\n\n\n\n4:\u00a0\u00a0Initialize extracted relevant context\u2062\ud835\udcb3~0\u2190None\u2190extracted relevant contextsubscript~\ud835\udcb30None\\text{extracted relevant context}~{}\\tilde{{\\mathcal{X}}}_{0}\\leftarrow\\text{None}extracted relevant context over~ start_ARG caligraphic_X end_ARG start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT \u2190 None\n\n\n\n5:\u00a0\u00a0Perform [Task Understanding]\n\n\n\n6:\u00a0\u00a0while\u00a0i\u2264n\ud835\udc56\ud835\udc5bi\\leq nitalic_i \u2264 italic_n\u00a0do\n\n\n\n7:\u00a0\u00a0\u00a0\u00a0\u00a0Select an action ai\u2190ai=\u03b3\u2062(q,\ud835\udcb3i|\ud835\udcb3~1:i\u22121),ai\u2208\ud835\udc9cformulae-sequence\u2190subscript\ud835\udc4e\ud835\udc56subscript\ud835\udc4e\ud835\udc56\ud835\udefe\ud835\udc5econditionalsubscript\ud835\udcb3\ud835\udc56subscript~\ud835\udcb3:1\ud835\udc561subscript\ud835\udc4e\ud835\udc56\ud835\udc9ca_{i}\\leftarrow a_{i}=\\gamma(q,{\\mathcal{X}}_{i}|\\tilde{{\\mathcal{X}}}_{1:i-1}%\n),~{}a_{i}\\in{\\mathcal{A}}italic_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT \u2190 italic_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = italic_\u03b3 ( italic_q , caligraphic_X start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT | over~ start_ARG caligraphic_X end_ARG start_POSTSUBSCRIPT 1 : italic_i - 1 end_POSTSUBSCRIPT ) , italic_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT \u2208 caligraphic_A\n\n\n\n8:\u00a0\u00a0\u00a0\u00a0\u00a0if aisubscript\ud835\udc4e\ud835\udc56a_{i}italic_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT is [Move] then i\u2190i+1\u2190\ud835\udc56\ud835\udc561i\\leftarrow i+1italic_i \u2190 italic_i + 1, continue\n\n\n\n9:\u00a0\u00a0\u00a0\u00a0\u00a0if aisubscript\ud835\udc4e\ud835\udc56a_{i}italic_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT is [Retrieve] then retrieve evidence from \ud835\udcb3={\ud835\udcb31,\u22ef,\ud835\udcb3n}\ud835\udcb3subscript\ud835\udcb31\u22efsubscript\ud835\udcb3\ud835\udc5b{\\mathcal{X}}=\\{{\\mathcal{X}}_{1},\\cdots,{\\mathcal{X}}_{n}\\}caligraphic_X = { caligraphic_X start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , \u22ef , caligraphic_X start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT }\n\n\n\n10:\u00a0\u00a0\u00a0\u00a0\u00a0if aisubscript\ud835\udc4e\ud835\udc56a_{i}italic_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT is [Append] then generate relevant context by \ud835\udcb3~i=ai\u2062(\ud835\udcb3i)subscript~\ud835\udcb3\ud835\udc56subscript\ud835\udc4e\ud835\udc56subscript\ud835\udcb3\ud835\udc56\\tilde{{\\mathcal{X}}}_{i}=a_{i}({\\mathcal{X}}_{i})over~ start_ARG caligraphic_X end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = italic_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( caligraphic_X start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT )\n\n\n\n11:\u00a0\u00a0\u00a0\u00a0\u00a0if aisubscript\ud835\udc4e\ud835\udc56a_{i}italic_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT is [Merge] then generate relevant context by \ud835\udcb3~i=ai\u2062(\ud835\udcb3i|\ud835\udcb3~1:i\u22121)subscript~\ud835\udcb3\ud835\udc56subscript\ud835\udc4e\ud835\udc56conditionalsubscript\ud835\udcb3\ud835\udc56subscript~\ud835\udcb3:1\ud835\udc561\\tilde{{\\mathcal{X}}}_{i}=a_{i}({\\mathcal{X}}_{i}|\\tilde{{\\mathcal{X}}}_{1:i-1})over~ start_ARG caligraphic_X end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = italic_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( caligraphic_X start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT | over~ start_ARG caligraphic_X end_ARG start_POSTSUBSCRIPT 1 : italic_i - 1 end_POSTSUBSCRIPT )\n\n\n\n12:\u00a0\u00a0\u00a0\u00a0\u00a0if aisubscript\ud835\udc4e\ud835\udc56a_{i}italic_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT \u2208{\\in\\{\u2208 {[Answer],[Aggregation]}}\\}} then generate answer \ud835\udcb4=\u03b3\u2062(q,\ud835\udcb3~1:i)\ud835\udcb4\ud835\udefe\ud835\udc5esubscript~\ud835\udcb3:1\ud835\udc56{\\mathcal{Y}}=\\gamma(q,\\tilde{{\\mathcal{X}}}_{1:i})caligraphic_Y = italic_\u03b3 ( italic_q , over~ start_ARG caligraphic_X end_ARG start_POSTSUBSCRIPT 1 : italic_i end_POSTSUBSCRIPT ), break\n\n\n\n13:\u00a0\u00a0\u00a0\u00a0\u00a0i\u2190i+1\u2190\ud835\udc56\ud835\udc561i\\leftarrow i+1italic_i \u2190 italic_i + 1\n\n\n\n14:\u00a0\u00a0end\u00a0while\n\n\n15:\u00a0\u00a0return answer \ud835\udcb4\ud835\udcb4{\\mathcal{Y}}caligraphic_Y\n\n\n\n\n\nThough the pre-defined action space \ud835\udc9c\ud835\udc9c\\mathcal{A}caligraphic_A comprises only seven actions, LC-Boost serves as a general framework sufficient for solving most long-context tasks. This effectiveness is based on the following reasons: (1)\u00a0Flexible accessibility:\u00a0By utilizing both [Retrieve] and [Move] actions, LC-Boost can access any short context \ud835\udcb3i\u2208\ud835\udcb3subscript\ud835\udcb3\ud835\udc56\ud835\udcb3\\mathcal{X}_{i}\\in\\mathcal{X}caligraphic_X start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT \u2208 caligraphic_X in a flexible trajectory, avoiding the need to browse the entire long context. This makes the information accessing process more efficient. (2)\u00a0Accurate information acquisition:\u00a0Through the [Append] and [Merge] actions, LC-Boost can either independently extract relevant information from the current short context, appending it to previously extracted information, or merge the current relevant information into the previous relevant information. This capability allows LC-Boost to acquire relevant information in a compatible manner, making it adaptable to many knowledge-intensive tasks. and (3)\u00a0Dynamic answering:\u00a0Using the [Answer] and [Aggregate] actions, LC-Boost can dynamically utilize the acquired relevant information to produce the target form of the answer (e.g., a short answer for QA tasks via the [Answer] action, or a long answer for summarization tasks via the [Aggregate] action).\n\n\nIn our pilot study depicted in Figure\u00a02, we observe that while GPT-3.5 serves as an inferior foundation model compared to GPT-4, it still demonstrates significant effectiveness when incorporated with LC-Boost. Given considerations of efficiency and cost-effectiveness, we employ GPT-3.5 as the foundation model for LC-Boost in the subsequent experiments. Besides, we show the prompts used in LC-Boost in Appendix\u00a0B.\n\n\n",
      "3 Experiments": "\n\n3 Experiments\n\n\n3.1 Experiment Settings\n\nWe evaluate LC-Boost and baseline models on 12 datasets, including:\n(1) Single-Doc QA: NarrativeQA\u00a0(Ko\u010disk\u00fd et\u00a0al., 2017), Qasper\u00a0(Dasigi et\u00a0al., 2021), and MultiFieldQA\u00a0(Bai et\u00a0al., 2023).\n(2) Multi-Doc QA: HotpotQA\u00a0(Yang et\u00a0al., 2018), 2WikiMQA\u00a0(Ho et\u00a0al., 2020), and MuSiQue\u00a0(Trivedi et\u00a0al., 2022).\n(3) Summarization: GovReport\u00a0(Huang et\u00a0al., 2021) and MultiNews\u00a0(Fabbri et\u00a0al., 2019).\n(4) Few-shot Learning: SAMSum\u00a0(Gliwa et\u00a0al., 2019).\n(5) Synthetic Task: Passage Count\u00a0(Bai et\u00a0al., 2023) and Self-Constructed Dataset.\n(6) Code Completion: LCC\u00a0(Guo et\u00a0al., 2023).\nMore details about the evaluation datasets and metrics are introduced in Appendix\u00a0A.\n\n\nWe compare our LC-Boost with three types of models: (1) Short LLMs (defined as with context length <<< 32K): Llama2-7B-Chat-4K\u00a0(Touvron et\u00a0al., 2023b), Llama3-8B-Instruct-8K and Vicuna-v1.5-7B-16K\u00a0(Chiang et\u00a0al., 2023); (2) Long LLMs (defined as with context length \u2265\\geq\u2265 32K): LongChat-v1.5-7B-32K\u00a0(Li et\u00a0al., 2023b), Mistral-7B-Instruct-v0.2-32K\u00a0(Jiang et\u00a0al., 2023a), Llama3-8B-80K\u00a0Zhang et\u00a0al. (2024b), Phi-3-mini-128K\u00a0(Abdin et\u00a0al., 2024) and Yi-9B-200K\u00a0(AI et\u00a0al., 2024); (3)\u00a0Closed-Source LLMs: DeepSeek-v2\u00a0(236B MoE model, ranks top-tier in MT-Bench)\u00a0(DeepSeek-AI, 2024), Claude-3-Haiku222https://www.anthropic.com/claude and GPT-3.5-turbo-16K333https://platform.openai.com/docs/models. In the experiments, if the context length exceed the model\u2019s length limit, following\u00a0Bai et\u00a0al. (2023), we truncate the context from the middle since the front and end of the context may contain crucial information. We provide further implementation details in Appendix\u00a0B.\n\n\n\n\n3.2 Main Results\n\nTable 1: Main experiment results. The best results are in bold and the secondary results are marked with underline. We report the average scores (%) on the main tasks. The detailed scores over all dataset are shown in Table\u00a03.\n\n\nModels\nSingle-Doc\nMulti-Doc\nSumm.\nFew-shot\nSynthetic\nCode\n\n\nShort LLMs (Context Length <<< 32K)\n\n\n\n\n\n\nLlama2-7B-Chat-4K\n24.9\n22.5\n26.6\n40.7\n6.3\n52.4\n\n\nLlama3-8B-Instruct-8K\n37.3\n36.0\n26.5\n42.7\n15.0\n57.5\n\n\nVicuna-v1.5-7B-16K\n28.0\n18.6\n27.5\n40.8\n8.9\n51.0\n\n\nLong LLMs (Context Length \u2265\\geq\u2265 32K)\n\n\n\n\n\n\nLongChat-v1.5-7B-32K\n28.7\n20.6\n28.6\n34.2\n6.8\n53.0\n\n\nMistral-7B-Instruct-v0.2-32K\n31.9\n26.0\n29.3\n43.0\n14.0\n55.4\n\n\nLlama3-8B-80K\n43.6\n43.1\n30.2\n42.9\n19.6\n53.6\n\n\nPhi-3-mini-128K\n33.5\n38.2\n28.8\n36.0\n19.9\n60.1\n\n\nYi-9B-200K\n29.6\n38.7\n28.4\n14.6\n6.5\n72.1\n\n\nClosed-Source LLMs\n\n\n\n\n\n\nDeepSeek-v2 (32K)\n37.6\n49.1\n30.8\n39.3\n14.5\n37.0\n\n\nClaude-3-Haiku (200K)\n41.9\n45.4\n30.1\n7.2\n25.5\n16.9\n\n\nGPT-3.5-turbo-16K\n39.8\n38.7\n28.1\n41.7\n18.7\n54.7\n\n\nLC-Boost\u00a0(4K)\n47.8\n56.4\n31.8\n44.1\n27.5\n59.0\n\n\n\n\nTable\u00a01 shows the overall experimental results for all models across all tasks. From the table, we derive several key findings:\nFirst, LC-Boost, with a context length of 4K, outperforms all baseline models in all tasks except for the Code Completion task. This result verifies LC-Boost\u2019s capability to effectively solve long-context tasks by strategically processing decomposed short contexts.\nSecond, long LLMs generally perform better than short LLMs, indicating the effectiveness of fine-tuning LLMs to adapt to long contexts. However, the performance of long LLMs is not consistently stable across different tasks. For example, Yi-9B-200K excels in the Code Completion task but does not show consistent performance in other tasks such as single-doc QA, few-shot learning, and synthetic tasks. This inconsistency suggests that adapting LLMs to long contexts may compromise their general abilities.\nLast, LC-Boost consistently surpasses its underlying LLM, GPT-3.5-turbo-16K, across all tasks by a notable margin. This demonstrates that LC-Boost can achieve improved performance while simultaneously reducing resource costs, making LC-Boost an environmentally friendly method.\n\n\n\n\n3.3 Ablation Study: Dynamic is Important\n\nTo investigate the necessity of LC-Boost\u2019s design, we conduct ablation studies by changing LC-Boost\u2019s action space \ud835\udc9c\ud835\udc9c\\mathcal{A}caligraphic_A, resulting in different information acquisition strategies. We experiment with the following settings: (1)\u00a0[Retrieve] only: Directly retrieve the most relevant short context. (2)\u00a0[Merge] only: Sequentially process all short contexts while considering the previously processed context. (3)\u00a0[Append] only: Sequentially process all short contexts independently. (4)\u00a0[Merge] & [Move]: Selectively process short contexts while considering the already processed context. (6)\u00a0[Append] & [Move]: Selectively process short contexts independently. (7): [Retrieve] & [Move]: Retrieve the top-k\ud835\udc58kitalic_k relevant short contexts and selectively process a few of them. (8): Brute-force: Directly produce the answer based on the entire long context. (9)\u00a0Random: For each short context, randomly select an action. Based on the acquired information from each strategy, LC-Boost then selects either the [Answer] or [Aggregation] action to produce the final answer.\n\n\nFigure\u00a03 illustrates the results, from which we find that: (1)\u00a0Compared to fixed processing strategies, LC-Boost customizes the action trajectory for each query, resulting in notable performance improvements. This finding emphasizes the importance of the dynamic capabilities of LC-Boost. (2)\u00a0LC-Boost is particularly effective in single-doc QA and multi-doc QA tasks, as it can accurately select the minimal necessary context required to answer the input query, filtering out irrelevant information from the long context. (3)\u00a0In the few-shot learning task, LC-Boost does not significantly outperform the fixed strategies. This is attributed to the numerous in-context examples provided within the task, which offer substantial guidance, thus diminishing the impact of the number of in-context examples on the final performance.\n\n\nFigure 3: Performance comparison on different context processing strategies in the ablation study. NarrativeQA\u00a0(left) is a single-doc QA task. HotpotQA\u00a0(middle) is a multi-doc QA task. SamSUM\u00a0(right) is a few-shot learning task.\n\n\n\n\n3.4 Case Study: Model Behavior Analysis on Self-Construct Dataset\n\nTable 2: Case study on the self-constructed dataset. Correct answers are marked in teal, incorrect answers in red, and ambiguous answers in orange.\n\n\n\n\nQuery: How many papers in ACL 2023 only have one author?\n\n\n\n\n\n\nContext: Full accepted paper list in ACL 2023 main conference. (Context length: 45K)\n\n\n\n\n\n\nGround-truth target: 8 papers\n\n\n\n\n\n\nPhi-3-mini-128K: 11 papers GPT-3.5-turbo-16K: 0 papers Claude-3-Haiku-200K: 1 papers (Acc. Score: 0)\n\n\n\n\n\n\nLC-Boost\u2019s action trajectory: [Task Reasoning] \u2192\u2192\\rightarrow\u2192 [Append]\u2192\u22ef\u2192absent\u22ef\\rightarrow\\cdots\u2192 \u22ef \u2192\u2192\\rightarrow\u2192 [Append]\u2192\u2192\\rightarrow\u2192 [Aggregation]\n\n\n\n\n\n\nLC-Boost: 8 papers (Acc. Score: 1)\n\n\n\n\n\n\nQuery: List all people names that are petrified, separated by comma.\n\n\n\n\n\n\nContext: Full content of Harry Potter and the Chamber of Secrets. (Context length: 122.6K)\n\n\n\n\n\n\nGround-truth target: Colin Creevey, Justin Finch-Fletchley, Penelope Clearwater, Hermione Granger\n\n\n\n\n\n\nPhi-3-mini-128K: Hermione Granger, Ginny Weasley, Mrs Norris (F1-Score: 0.29)\n\n\n\n\n\n\nGPT-3.5-turbo-16K: Colin Creevey, Mrs Norris (F1-Score: 0.33)\n\n\n\n\n\n\nClaude-3-Haiku-200K: Nick, Hermione, Ron (F1-Score: 0.18)\n\n\n\n\n\n\nLC-Boost\u2019s action trajectory: [Task Reasoning] \u2192\u2192\\rightarrow\u2192 [Move]\u2192\u22ef\u2192absent\u22ef\\rightarrow\\cdots\u2192 \u22ef \u2192\u2192\\rightarrow\u2192 [Merge]\u2192\u2192\\rightarrow\u2192 [Aggregation]\n\n\n\n\n\n\nLC-Boost: Colin Creevey, Penelope Clearwater, Hermione Granger, Nick, Mrs Norris (F1-Score: 0.71)\n\n\n\n\n\n\nIn Table\u00a02, we present two case studies from the self-constructed dataset. These cases are particularly challenging as they require reasoning across the entire long context. Despite having sufficient context size, LLMs struggle to generate correct responses. In contrast, LC-Boost dynamically customizes solutions for each case, thereby effectively solving the problems using a shorter context length.\n\n\nFor the first query, LC-Boost performs [Append] or [Move] actions across all short context along with a rewritten query, \"Extract paper information in the following list that have only one author,\" derived via [Task Reasoning]. After processing all short contexts, LC-Boost employs the [Aggregation] action to compile the final answer. This approach simplifies the task compared to directly extracting a numeric answer from the entire long context, mimicking the human process of reading comprehension and thereby producing accurate results.\n\n\nIn the second case, the query necessitates conditional reasoning on each short context. As highlighted in previous research (Liu et\u00a0al., 2023), reasoning directly from the entire context risks losing crucial information, particularly in the middle of the long context. Thus LLMs tend to miss key details such as people\u2019s names. LC-Boost addresses this issue by processing only one short context at a step where it extracts information from arbitrary position of the long text with equal accuracy.\nAdditionally, answers marked in orange include non-human names (e.g., cat, ghost) that are misconstrued as people, illustrating a common challenge where models fail to differentiate in-depth entity properties.\n\n\n\n\n3.5 Context be Short, Energy be Saved!\n\nRecently, we have witnessed the remarkable success of LLMs, which are becoming an indispensable part of our daily lives. We believe that in the near future, LLMs will become as ubiquitous as electricity or gas supply, serving as fundamental infrastructure in human society. At that point, the energy consumption of LLMs will emerge as a significant environmental concern. Therefore, it is imperative for the research community to focus on reducing the energy consumption associated with these models.\nFigure\u00a04 presents an analysis of energy consumption, comparing the brute-force approach with our LC-Boost method. The y\ud835\udc66yitalic_y-axis is measured in Joules. The theoretical energy consumption is estimated for 7B LLMs across varying context lengths. We roughly estimate the energy consumption using the formula (Total Float Operation312\u2062\u00a0TFLOPS)\u00d7400\u2062WTotal Float Operation312\u00a0TFLOPS400\ud835\udc4a\\left(\\frac{\\text{Total Float Operation}}{312\\text{ TFLOPS}}\\right)\\times 400W( divide start_ARG Total Float Operation end_ARG start_ARG 312 TFLOPS end_ARG ) \u00d7 400 italic_W, assuming the use of an A100 GPU with a compute capability of 312 TFLOPS for BFLOAT16 operations and a maximum TDP of 400W444The calculation of total float operations is based on the method outlined in https://www.harmdevries.com/post/context-length/. The practical energy consumption is estimated by recording the GPU time and GPU power during inference with different context lengths. We use a Llama2-7B-128K\u00a0(Peng et\u00a0al., 2023) and a Llama2-7B-chat-4K\u00a0(Touvron et\u00a0al., 2023a) for the brute-force setting and LC-Boost, respectively.\n\n\nFigure 4: Energy consumption analysis.\n\n\nFigure\u00a04 clearly indicates that longer context lengths significantly increase energy consumption with the brute-force method, especially evident in practical measurements. This difference is primarily due to the need to distribute sequence activation tensors across multiple GPUs in practical experiment, with tensor I/O exacerbating inference latency and thereby inflating energy costs. In contrast, our LC-Boost method, working with 4K context lengths, shows only a mild increase in energy consumption across contexts, thereby confirming its energy efficiency while maintaining comparable or superior performance on long-context tasks. We also provide an analysis on token consumption in Appendix\u00a0C.\n\n\n",
      "4 Related Works": "\n\n4 Related Works\n\nDealing with long contexts is a fundamental research problem for LLMs, as many real-world applications involve long-context inputs (Li et\u00a0al., 2023a; Fu et\u00a0al., 2024). The most direct approach to address long-context tasks is to increase the working context size of LLMs (Abdin et\u00a0al., 2024; AI et\u00a0al., 2024; Li et\u00a0al., 2023a; Cai et\u00a0al., 2024). A year ago, significant research efforts focused on extending the working context size of LLMs from 4K to 32K (Jiang et\u00a0al., 2023a; Li et\u00a0al., 2023b; Chen et\u00a0al., 2023a; Du et\u00a0al., 2022). Currently, many popular open-source and close-source LLMs still operate with a context size under 32K (Touvron et\u00a0al., 2023a; OpenAI, 2023), such as GPT-3.5-turbo, which has a 16K context length. Recently, research has shifted towards extending LLMs\u2019 working context to the million-level. Notably, GPT-4 was updated to a 128K context length not long ago, and the newly released GPT-4o also operates with a 128K context. Moreover, several recent open-source LLMs have been introduced with context lengths exceeding 100K, for example, the Yi series model supports up to 200K (AI et\u00a0al., 2024), and the Phi-3 model operates with 128K (Abdin et\u00a0al., 2024).\n\n\nInstead of merely increasing the context length, another approach to address long-context tasks involves extracting a short surrogate context from the full context. This includes techniques like retrieval-augmented generation (RAG) and context refinement methods (Izacard and Grave, 2021a; Gao et\u00a0al., 2024; Wang et\u00a0al., 2023; Qian et\u00a0al., 2024). However, many of these methods utilize task-specific strategies to manage the long context. For instance, RAG methods often deploy retrievers to select relevant context chunks as supporting evidence (Izacard and Grave, 2021b; Xu et\u00a0al., 2023b; Jiang et\u00a0al., 2023b). Recent studies have criticized the chunking process in RAG for undermining the semantic coherence of the long context and have proposed chunking-free methods to refine the long context into a concise surrogate context (Qian et\u00a0al., 2024; Luo et\u00a0al., 2024).\nFurthermore, some studies have also explored sequential processing strategies, such as Ratner et\u00a0al. (2022) and Xu et\u00a0al. (2023a), to sequentially process the context in a manner that preserves its integrity.\n\n\nLastly, reasoning-based methods also show significant potential for addressing long context tasks (Nakano et\u00a0al., 2022; Yang et\u00a0al., 2023; Driess et\u00a0al., 2023). These methods predominantly employ a decision-making process to navigate through the long context sequentially, utilizing reasoning techniques such as in-context learning (Dong et\u00a0al., 2022), chain-of-thought (Wei et\u00a0al., 2022), and self-reflection (Shinn et\u00a0al., 2023). In this paper, LC-Boost incorporates a decision-making process that dynamically customizes the action trajectory for each query, thereby offering considerable flexibility in accessing and leveraging information to produce the final output answer.\n\n",
      "5 Conclusion": "\n\n5 Conclusion\n\nIn this paper, we argue that most long-context tasks are short-context solvable, and we validate this claim through both theoretical and empirical analysis. We propose a method called LC-Boost to solve long-context tasks by decomposing the long context into short contexts and processing them using a decision-making process. We conduct experiments on 12 datasets to compare LC-Boost with long LLMs and other baseline models. Empirical results verify LC-Boost\u2019s effectiveness in solving long-context tasks. Additionally, we discuss the energy consumption of LC-Boost versus long LLMs, demonstrating that LC-Boost can achieve comparable performance with significantly less energy consumption. In Appendix\u00a0D, we also discuss the limitations and broader impact of this paper.\n\n\n",
      "Appendix A More details of the Datasets": "\n\nAppendix A More details of the Datasets\n\nTable 4: Statistical information of the datasets utilized in this paper.\n\n\nDataset\nNarrative\nQasper\nMultiField\nHotpot\nMuSiQue\n2Wiki\n\n\nNum of Samples\n200\n200\n150\n200\n200\n200\n\n\nAve. Length\n18,409\n3,619\n4,559\n9,151\n11,214\n4,887\n\n\nMetric\nF1\nF1\nF1\nF1\nF1\nF1\n\n\nDataset\nGovReport\nMultiNews\nSAMSum\nPCount\nLCC\nSelf\n\n\nNum of Samples\n200\n200\n200\n200\n500\n32\n\n\nAve. Length\n8,734\n2,113\n6,258\n11,141\n1,235\n39,420\n\n\nMetric\nRouge-L\nRouge-L\nRouge-L\nAccuracy\nEdit Sim\nF1&Accuracy\n\n\n\n\nWe evaluated all models on 12 datasets, as shown in Table\u00a04. Most of these datasets are provided by the LongBench benchmark\u00a0[Bai et\u00a0al., 2023]. Following LongBench, we used F1-score, accuracy, and edit similarity as the evaluation metrics. Additionally, we manually annotated a self-constructed dataset comprising long contexts from practical scenarios, such as the full schedule of the Olympic Games and the complete list of accepted papers at ACL. The queries in the self-constructed dataset involve reasoning over the entire long context. For example, \u201cWho has the most accepted papers at ACL 2023?\u201d These queries require the model to accurately understand the long context and perform reasoning, making them highly challenging. The details of the self-constructed dataset are in Table\u00a05.\n\n\nTable 5: Data details of the self-constructed dataset.\n\n\n\n\nSource\n\n\n\n\nLength\n\n\n\n\n# Queries\n\n\n\n\nExample Query\n\n\n\n\n\n\nAccepted paper list of ACL 2023 Main Conference\n\n\n\n\n44,490\n\n\n\n\n7\n\n\n\n\nWho has the most accepted paper in ACL 2023?\n\n\n\n\n\n\nThe Diamond Sutra\n\n\n\n\n19,993\n\n\n\n\n3\n\n\n\n\nHow many chapters of the Sutra?\n\n\n\n\n\n\nSchedule of The 2024 Olympic Games\n\n\n\n\n15,844\n\n\n\n\n9\n\n\n\n\nWhich day has the most gold medal events?\n\n\n\n\n\n\nSubtitle of The Big Bang Theory S3E14\n\n\n\n\n11,136\n\n\n\n\n6\n\n\n\n\nHow long does this episode?\n\n\n\n\n\n\nThe Little Prince\n\n\n\n\n22,471\n\n\n\n\n4\n\n\n\n\nHow many planets does the little prince visit?\n\n\n\n\n\n\nHarry Potter and the Chamber of Secrets\n\n\n\n\n122,591\n\n\n\n\n3\n\n\n\n\nHow many times has the chamber of secret been opened?\n\n\n\n\n\n",
      "Appendix B Implementation Details": "\n\nAppendix B Implementation Details\n\nLC-Boost begins with the [Task Understanding] action after receiving the input query and context, using the prompt shown in Table\u00a06. If the task does not include an input query, the two columns \"Below is the query\" and \"{input_query}\" are omitted. Besides, for the synthetic task, we use the prompt shown in Table\u00a07 to reformulate the query for better adaptation to LC-Boost. Based on the output of the [Task Understanding] action, LC-Boost adopts different strategies to perform the task. Specifically, \u201coption [1]\u201d directs LC-Boost to utilize a retriever to rank all chunks of the long context. In this paper, we employ BGE-Reranker-Large as the retriever\u00a0Chen et\u00a0al. [2023b]. For \u201coption [2]\u201d and \u201coption [3]\u201d, LC-Boost uses the prompts shown in Table\u00a010 and Table\u00a08 to sequentially process each short context, respectively. After processing each short context, if the output is not \"null\", the newly summarized context is added to the \"previous summarization\".\n\n\nOnce all short contexts are processed, LC-Boost aggregates all relevant information to produce the final answer. At this stage, we use the prompt provided by LongBench, replacing the full context with the surrogate context produced by LC-Boost. For \u201coption [4]\u201d, LC-Boost utilizes the prompts provided by LongBench to process each short context and produces the answer as soon as the proper information is found. Table\u00a09 presents an example prompt from LongBench, designed for MultiFieldQA tasks. We modified the prompt by adding the instruction \u201cIf no answer can be found in the text, please output \"null\"\u201d. This allows LC-Boost to skip irrelevant short contexts, performing the [Move] action. Specifically, for the Code Completion task, LC-Boost reversely browses the context code from near to far as the near context are more useful to predict the code completion.\nWe evaluate all baseline models following the settings provided in LongBench\u00a0555https://github.com/THUDM/LongBench. We use a node with 8 A100 80G GPUs to conduct all experiments.\n\n",
      "Appendix C Token Consumption Analysis": "\n\nAppendix C Token Consumption Analysis\n\nIn Section\u00a03.5, our analysis confirms that LC-Boost significantly reduces energy consumption compared to long LLMs. However, most closed-source LLMs, such as the underlying model of LC-Boost, GPT-3.5-turbo, charge based on token consumption, e.g., US$0.50 per 1M tokens for input and US$1.50 per 1M tokens for output666https://openai.com/api/pricing/. Consequently, it is crucial to examine whether the decision-making process of LC-Boost increases token consumption compared to the brute-force method.\n\n\nTo address this issue, we recorded the end-to-end token consumption for three datasets: NarrativeQA, GovReport, and LCC. After token counting, we conclude that LC-Boost\u2019s token consumption was 34.1% of the brute-force method\u2019s consumption in NarrativeQA, 112% in GovReport, and 29.5% in LCC. These results indicate that LC-Boost\u2019s token consumption varies significantly across different tasks. For tasks requiring precise context location, such as QA and code completion, LC-Boost can respond as soon as the relevant context is identified, thereby avoiding the need to process the full context. However, for tasks that necessitate information aggregation, such as summarization, LC-Boost may require more tokens for prompts in each iteration.\nIn practice, for token-consumption-sensitive LLMs, there might be a trade-off between performance and cost-efficiency, which also varies considerably across different tasks.\n\n\nTable 6: Prompt Template for the [Task Understanding] action.\n\n\n\n\n\n\nYou need to process a task with a long context that greatly exceeds your context limit.\n\nThe only feasible way to handle this is by processing the long context chunk by chunk.\n\n\n\n\n\n\n\n\nBelow is the original task prompt:\n\n{task_prompt}\n\n\n\n\n\n\n\n\nBelow is the query:\n\n{input_query}\n\n\n\n\n\n\n\n\nYou have the following options to process the long context. Choose one of them:\n\n\n\n\n\n\n\n\n[1]. Retrieve the chunk most relevant to the input query to support answer generation.\n\n\n\n\n\n\n\n\n[2]. Summarize each chunk and then aggregate the summaries after processing all chunks.\n\n\n\n\n\n\n\n\n[3]. Extract key sentences from each chunk and then aggregate them after processing all chunks.\n\n\n\n\n\n\n\n\n[4]. Sequentially scan chunks and produce the answer as soon as the query can be answered.\n\n\n\n\n\n\n\n\nBelow are some examples for reference:\n\n\n\n\n\n\n\n\nThe examples begin as follows:\n\n{examples}\n\nThe examples conclude here.\n\n\n\n\n\n\n\n\nPlease learn the examples and select one of the options by only outputting the corresponding index number.\n\n\n\n\n\n\nTable 7: Query Rewritten Prompt Template for the [Task Understanding] action.\n\n\n\n\n\n\nYou need to process a task with a long context that greatly exceeds your context limit.\n\nThe only feasible way to handle this is by processing the long context chunk by chunk.\n\n\n\n\n\n\n\n\nBelow is the original task prompt:\n\n{task_prompt}\n\n\n\n\n\n\n\n\nBelow is the query:\n\n{input_query}\n\n\n\n\n\n\n\n\nYou will process the long context with the following strategy:\n\n{strategy}\n\n\n\n\n\n\n\n\nDo you think the the query is proper for processing context chunk? If not, rewrite the query.\n\n\n\n\n\n\n\n\nBelow are some examples for reference:\n\n\n\n\n\n\n\n\nThe examples begin as follows:\n\n{examples}\n\nThe examples conclude here.\n\n\n\n\n\n\n\n\nPlease study the examples carefully. If the query needs to be rewritten, directly output the revised query.\n\nIf no revision is necessary, output \u201cnull\u201d.\n\n\n\n\n\n\nTable 8: Prompt Template for the [Append] action.\n\n\n\n\n\n\nYou are given an article and a question. Read the article carefully and follow my instructions to process it.\n\n\n\n\n\n\n\n\nArticle:\n\n\n\n\n\n\n\n\nThe article begins as follows:\n\n{article}\n\nThe article concludes here.\n\n\n\n\n\n\n\n\nQuestion:\n\n\n\n\n\n\n\n\n{question}\n\n\n\n\n\n\n\n\nInstructions:\n\n\n\n\n\n\n\n\nEach sentence in the article is marked with a sentence identifier [si], for example [s1].\n\n\n\n\n\n\n\n\nSelect up to ten key sentences from the article that are most likely to answer the question.\n\n\n\n\n\n\n\n\nOnly output the selected sentence identifiers, separated by commas.\n\n\n\n\n\n\n\n\nExample: [s39],[s54]\n\n\n\n\n\n\n\n\nIf no sentences are relevant, please output \"null\".\n\n\n\n\n\n\nTable 9: Prompt Template for the MultiFieldQA Task from the LongBench Benchmark. Additions made by us are highlighted in blue.\n\n\n\n\n\n\nRead the following text and answer briefly.\n\n\n\n\n\n\n\n\n{context}\n\n\n\n\n\n\n\n\nNow, answer the following question based on the above text, only give me the answer and do not output any\n\n\n\n\n\n\n\n\nother words.  If no answer can be found in the text, please output \"null\".\n\n\n\n\n\n\n\n\nQuestion:{question}\n\n\n\n\n\n\n\n\nAnswer:\n\n\n\n\n\n\nTable 10: Prompt Template for the [Merge] action.\n\n\n\n\n\n\nYou are provided with a portion of an article, a question, and summarization of the article\u2019s previous portions.\n\n\n\n\n\n\n\n\nRead the article portion and follow my instructions to process it.\n\n\n\n\n\n\n\n\nArticle:\n\n\n\n\n\n\n\n\nThe article begins as follows:\n\n{article}\n\nThe article concludes here.\n\n\n\n\n\n\n\n\nPrevious summarization:\n\n\n\n\n\n\n\n\nThe previous summarization is as follows:\n\n{previous_sum}\n\nThe previous summarization concludes here.\n\n\n\n\n\n\n\n\nQuestion:\n\n\n\n\n\n\n\n\n{question}\n\n\n\n\n\n\n\n\nInstruction:\n\n\n\n\n\n\n\n\nSummarize the partial article to supplement the previous summarization, which can better support the task.\n\n\n\n\n\n\n\n\nIf no content needs to be supplemented, please output \"null\".\n\n\n\n\n\n",
      "Appendix D Limitations and Broad Impact": "\n\nAppendix D Limitations and Broad Impact\n\nIn this paper, we propose LC-Boost, a method dedicated to solving long-context tasks using short contexts. However, there are several limitations we would like to address in the future work:\n(1) Although we conduct comprehensive experiments on many tasks and provide theoretical analysis to support our major claim that most long-context tasks are short-context solvable, there may be more complicated scenarios that require understanding the full context in a brute-force setting. LC-Boost might not be able to process such tasks effectively.\n(2) As mentioned in Section\u00a02.3, LC-Boost selects actions from a discrete action space. While we argue that the pre-defined action space is versatile enough to handle most scenarios, a more elegant solution would be to predict actions in a continuous space. We conducted preliminary experiments to explore allowing LC-Boost to prompt itself to predict actions without a predefined action space, such as writing prompts or code autonomously. These experiments resulted in highly unstable performance, particularly for models like GPT-3.5, as such requirements are still challenging. We believe that with a much stronger foundation model, LC-Boost could be expected to predict actions in a continuous space.\n(3) We choose GPT-3.5 as the foundation model for LC-Boost, instead of open-source LLMs. The reason is that GPT-3.5 is a strong, yet efficient model that can generally understand most instructions. However, we found that most open-source LLMs lack these properties in a zero-shot setting. Fine-tuning these open-source LLMs might be helpful, but constructing such instruction data is infeasible and expensive.\n\n\nAs discussed in Section\u00a03.5, LLMs are likely to become a fundamental infrastructure in the near future. At that scale, their energy consumption will pose significant environmental challenges. As shown in Figure\u00a04, LC-Boost avoids processing long contexts directly by decomposing them into shorter contexts. This approach significantly reduces energy consumption as the context length increases, leading to substantial positive environmental impacts. We believe that in the future, more research will focus on green AI initiatives. This paper could serve as an initial spark to inspire further research in this direction, potentially resulting in broader social impact.\n\n"
    },
    "bibliography": {
      "Bai et\u00a0al. (2023)": "\nBai et\u00a0al. (2023)\n\nYushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, et\u00a0al.\n\n\nLongbench: A bilingual, multitask benchmark for long context understanding.\n\n\narXiv preprint arXiv:2308.14508, 2023.\n\n\n",
      "Zhang et\u00a0al. (2024a)": "\nZhang et\u00a0al. (2024a)\n\nXinrong Zhang, Yingfa Chen, Shengding Hu, Zihang Xu, Junhao Chen, Moo\u00a0Khai Hao, Xu\u00a0Han, Zhen\u00a0Leng Thai, Shuo Wang, Zhiyuan Liu, and Maosong Sun.\n\n\n\u221e\\infty\u221ebench: Extending long context evaluation beyond 100k tokens, 2024a.\n\n\n",
      "Touvron et\u00a0al. (2023a)": "\nTouvron et\u00a0al. (2023a)\n\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et\u00a0al.\n\n\nLlama 2: Open foundation and fine-tuned chat models.\n\n\narXiv preprint arXiv:2307.09288, 2023a.\n\n\n",
      "Touvron et\u00a0al. (2023b)": "\nTouvron et\u00a0al. (2023b)\n\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et\u00a0al.\n\n\nLlama 2: Open foundation and fine-tuned chat models.\n\n\narXiv preprint arXiv:2307.09288, 2023b.\n\n\n",
      "Kaplan et\u00a0al. (2020)": "\nKaplan et\u00a0al. (2020)\n\nJared Kaplan, Sam McCandlish, Tom Henighan, Tom\u00a0B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei.\n\n\nScaling laws for neural language models, 2020.\n\n\n",
      "Liu et\u00a0al. (2023)": "\nLiu et\u00a0al. (2023)\n\nNelson\u00a0F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang.\n\n\nLost in the middle: How language models use long contexts, 2023.\n\n\n",
      "Li et\u00a0al. (2023a)": "\nLi et\u00a0al. (2023a)\n\nDacheng Li, Rulin Shao, Anze Xie, Ying Sheng, Lianmin Zheng, Joseph Gonzalez, Ion Stoica, Xuezhe Ma, and Hao Zhang.\n\n\nHow long can context length of open-source llms truly promise?\n\n\nIn NeurIPS 2023 Workshop on Instruction Tuning and Instruction Following, 2023a.\n\n\n",
      "Adolphs (1999)": "\nAdolphs (1999)\n\nRalph Adolphs.\n\n\nSocial cognition and the human brain.\n\n\nTrends in cognitive sciences, 3(12):469\u2013479, 1999.\n\n\n",
      "Bryant and O\u2019Hallaron (2011)": "\nBryant and O\u2019Hallaron (2011)\n\nRandal\u00a0E Bryant and David\u00a0Richard O\u2019Hallaron.\n\n\nComputer systems: a programmer\u2019s perspective.\n\n\nPrentice Hall, 2011.\n\n\n",
      "Xu et\u00a0al. (2023a)": "\nXu et\u00a0al. (2023a)\n\nPeng Xu, Wei Ping, Xianchao Wu, Lawrence McAfee, Chen Zhu, Zihan Liu, Sandeep Subramanian, Evelina Bakhturina, Mohammad Shoeybi, and Bryan Catanzaro.\n\n\nRetrieval meets Long Context Large Language Models.\n\n\narXiv, 2023a.\n\n\ndoi: 10.48550/arxiv.2310.03025.\n\n\nExperimental.\n\n\n",
      "Ji et\u00a0al. (2023)": "\nJi et\u00a0al. (2023)\n\nZiwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye\u00a0Jin Bang, Andrea Madotto, and Pascale Fung.\n\n\nSurvey of hallucination in natural language generation.\n\n\nACM Computing Surveys, 55(12):1\u201338, 2023.\n\n\n",
      "Lewis et\u00a0al. (2020)": "\nLewis et\u00a0al. (2020)\n\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich K\u00fcttler, Mike Lewis, Wen-tau Yih, Tim Rockt\u00e4schel, Sebastian Riedel, and Douwe Kiela.\n\n\nRetrieval-Augmented Generation for knowledge-intensive NLP tasks.\n\n\nIn Advances in Neural Information Processing Systems, volume\u00a033, pages 9459\u20139474, 2020.\n\n\nURL https://proceedings.neurips.cc/paper_files/paper/2020/file/6b493230205f780e1bc26945df7481e5-Paper.pdf.\n\n\n",
      "Shuster et\u00a0al. (2021)": "\nShuster et\u00a0al. (2021)\n\nKurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela, and Jason Weston.\n\n\nRetrieval augmentation reduces hallucination in conversation.\n\n\nIn Findings of the Association for Computational Linguistics: EMNLP 2021, pages 3784\u20133803, Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics.\n\n\ndoi: 10.18653/v1/2021.findings-emnlp.320.\n\n\nURL https://aclanthology.org/2021.findings-emnlp.320.\n\n\n",
      "Cover (1999)": "\nCover (1999)\n\nThomas\u00a0M Cover.\n\n\nElements of information theory.\n\n\nJohn Wiley & Sons, 1999.\n\n\n",
      "Tishby and Zaslavsky (2015)": "\nTishby and Zaslavsky (2015)\n\nNaftali Tishby and Noga Zaslavsky.\n\n\nDeep learning and the information bottleneck principle, 2015.\n\n\n",
      "Mohri et\u00a0al. (2018)": "\nMohri et\u00a0al. (2018)\n\nMehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar.\n\n\nFoundations of machine learning.\n\n\nMIT press, 2018.\n\n\n",
      "Ko\u010disk\u00fd et\u00a0al. (2017)": "\nKo\u010disk\u00fd et\u00a0al. (2017)\n\nTom\u00e1\u0161 Ko\u010disk\u00fd, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl\u00a0Moritz Hermann, G\u00e1bor Melis, and Edward Grefenstette.\n\n\nThe narrativeqa reading comprehension challenge, 2017.\n\n\n",
      "Dasigi et\u00a0al. (2021)": "\nDasigi et\u00a0al. (2021)\n\nPradeep Dasigi, Kyle Lo, Iz\u00a0Beltagy, Arman Cohan, Noah\u00a0A Smith, and Matt Gardner.\n\n\nA dataset of information-seeking questions and answers anchored in research papers.\n\n\nIn Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 4599\u20134610, 2021.\n\n\n",
      "Yang et\u00a0al. (2018)": "\nYang et\u00a0al. (2018)\n\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William\u00a0W. Cohen, Ruslan Salakhutdinov, and Christopher\u00a0D. Manning.\n\n\nHotpotqa: A dataset for diverse, explainable multi-hop question answering, 2018.\n\n\n",
      "Ho et\u00a0al. (2020)": "\nHo et\u00a0al. (2020)\n\nXanh Ho, Anh-Khoa Duong\u00a0Nguyen, Saku Sugawara, and Akiko Aizawa.\n\n\nConstructing a multi-hop QA dataset for comprehensive evaluation of reasoning steps.\n\n\nIn Donia Scott, Nuria Bel, and Chengqing Zong, editors, Proceedings of the 28th International Conference on Computational Linguistics, pages 6609\u20136625, Barcelona, Spain (Online), December 2020. International Committee on Computational Linguistics.\n\n\ndoi: 10.18653/v1/2020.coling-main.580.\n\n\nURL https://aclanthology.org/2020.coling-main.580.\n\n\n",
      "Trivedi et\u00a0al. (2022)": "\nTrivedi et\u00a0al. (2022)\n\nHarsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal.\n\n\nMusique: Multihop questions via single-hop question composition.\n\n\nTransactions of the Association for Computational Linguistics, 10:539\u2013554, 2022.\n\n\n",
      "Huang et\u00a0al. (2021)": "\nHuang et\u00a0al. (2021)\n\nLuyang Huang, Shuyang Cao, Nikolaus Parulian, Heng Ji, and Lu\u00a0Wang.\n\n\nEfficient attentions for long document summarization.\n\n\nIn Kristina Toutanova, Anna Rumshisky, Luke Zettlemoyer, Dilek Hakkani-Tur, Iz\u00a0Beltagy, Steven Bethard, Ryan Cotterell, Tanmoy Chakraborty, and Yichao Zhou, editors, Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1419\u20131436, Online, June 2021. Association for Computational Linguistics.\n\n\ndoi: 10.18653/v1/2021.naacl-main.112.\n\n\nURL https://aclanthology.org/2021.naacl-main.112.\n\n\n",
      "Fabbri et\u00a0al. (2019)": "\nFabbri et\u00a0al. (2019)\n\nAlexander\u00a0R. Fabbri, Irene Li, Tianwei She, Suyi Li, and Dragomir\u00a0R. Radev.\n\n\nMulti-news: a large-scale multi-document summarization dataset and abstractive hierarchical model, 2019.\n\n\n",
      "Gliwa et\u00a0al. (2019)": "\nGliwa et\u00a0al. (2019)\n\nBogdan Gliwa, Iwona Mochol, Maciej Biesek, and Aleksander Wawer.\n\n\nSAMSum corpus: A human-annotated dialogue dataset for abstractive summarization.\n\n\nIn Lu\u00a0Wang, Jackie Chi\u00a0Kit Cheung, Giuseppe Carenini, and Fei Liu, editors, Proceedings of the 2nd Workshop on New Frontiers in Summarization, pages 70\u201379, Hong Kong, China, November 2019. Association for Computational Linguistics.\n\n\ndoi: 10.18653/v1/D19-5409.\n\n\nURL https://aclanthology.org/D19-5409.\n\n\n",
      "Guo et\u00a0al. (2023)": "\nGuo et\u00a0al. (2023)\n\nDaya Guo, Canwen Xu, Nan Duan, Jian Yin, and Julian McAuley.\n\n\nLongcoder: A long-range pre-trained language model for code completion, 2023.\n\n\n",
      "Chiang et\u00a0al. (2023)": "\nChiang et\u00a0al. (2023)\n\nWei-Lin Chiang, Zhuohan Li, Zi\u00a0Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph\u00a0E. Gonzalez, Ion Stoica, and Eric\u00a0P. Xing.\n\n\nVicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023.\n\n\nURL https://lmsys.org/blog/2023-03-30-vicuna/.\n\n\n",
      "Li et\u00a0al. (2023b)": "\nLi et\u00a0al. (2023b)\n\nDacheng Li, Rulin Shao, Anze Xie, Ying Sheng, Lianmin Zheng, Joseph\u00a0E. Gonzalez, Ion Stoica, Xuezhe Ma, and Hao Zhang.\n\n\nHow long can open-source llms truly promise on context length?, June 2023b.\n\n\nURL https://lmsys.org/blog/2023-06-29-longchat.\n\n\n",
      "Jiang et\u00a0al. (2023a)": "\nJiang et\u00a0al. (2023a)\n\nAlbert\u00a0Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra\u00a0Singh Chaplot, Diego de\u00a0las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et\u00a0al.\n\n\nMistral 7b.\n\n\narXiv preprint arXiv:2310.06825, 2023a.\n\n\n",
      "Zhang et\u00a0al. (2024b)": "\nZhang et\u00a0al. (2024b)\n\nPeitian Zhang, Ninglu Shao, Zheng Liu, Shitao Xiao, Hongjin Qian, Qiwei Ye, and Zhicheng Dou.\n\n\nExtending llama-3\u2019s context ten-fold overnight, 2024b.\n\n\n",
      "Abdin et\u00a0al. (2024)": "\nAbdin et\u00a0al. (2024)\n\nMarah Abdin, Sam\u00a0Ade Jacobs, Ammar\u00a0Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Harkirat Behl, Alon Benhaim, Misha Bilenko, Johan Bjorck, S\u00e9bastien Bubeck, Martin Cai, Caio C\u00e9sar\u00a0Teodoro Mendes, Weizhu Chen, Vishrav Chaudhary, Parul Chopra, Allie\u00a0Del Giorno, Gustavo de\u00a0Rosa, Matthew Dixon, Ronen Eldan, Dan Iter, Amit Garg, Abhishek Goswami, Suriya Gunasekar, Emman Haider, Junheng Hao, Russell\u00a0J. Hewett, Jamie Huynh, Mojan Javaheripi, Xin Jin, Piero Kauffmann, Nikos Karampatziakis, Dongwoo Kim, Mahoud Khademi, Lev Kurilenko, James\u00a0R. Lee, Yin\u00a0Tat Lee, Yuanzhi Li, Chen Liang, Weishung Liu, Eric Lin, Zeqi Lin, Piyush Madan, Arindam Mitra, Hardik Modi, Anh Nguyen, Brandon Norick, Barun Patra, Daniel Perez-Becker, Thomas Portet, Reid Pryzant, Heyang Qin, Marko Radmilac, Corby Rosset, Sambudha Roy, Olatunji Ruwase, Olli Saarikivi, Amin Saied, Adil Salim, Michael Santacroce, Shital Shah, Ning Shang, Hiteshi Sharma, Xia Song, Masahiro Tanaka, Xin Wang, Rachel\nWard, Guanhua Wang, Philipp Witte, Michael Wyatt, Can Xu, Jiahang Xu, Sonali Yadav, Fan Yang, Ziyi Yang, Donghan Yu, Chengruidong Zhang, Cyril Zhang, Jianwen Zhang, Li\u00a0Lyna Zhang, Yi\u00a0Zhang, Yue Zhang, Yunan Zhang, and Xiren Zhou.\n\n\nPhi-3 technical report: A highly capable language model locally on your phone, 2024.\n\n\n",
      "AI et\u00a0al. (2024)": "\nAI et\u00a0al. (2024)\n\n01. AI, :, Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge\u00a0Zhang, Guanwei Zhang, Heng Li, Jiangcheng Zhu, Jianqun Chen, Jing Chang, Kaidong Yu, Peng Liu, Qiang Liu, Shawn Yue, Senbin Yang, Shiming Yang, Tao Yu, Wen Xie, Wenhao Huang, Xiaohui Hu, Xiaoyi Ren, Xinyao Niu, Pengcheng Nie, Yuchi Xu, Yudong Liu, Yue Wang, Yuxuan Cai, Zhenyu Gu, Zhiyuan Liu, and Zonghong Dai.\n\n\nYi: Open foundation models by 01.ai, 2024.\n\n\n",
      "DeepSeek-AI (2024)": "\nDeepSeek-AI (2024)\n\nDeepSeek-AI.\n\n\nDeepseek-v2: A strong, economical, and efficient mixture-of-experts language model, 2024.\n\n\n",
      "Peng et\u00a0al. (2023)": "\nPeng et\u00a0al. (2023)\n\nBowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole.\n\n\nYarn: Efficient context window extension of large language models.\n\n\nIn The Twelfth International Conference on Learning Representations, 2023.\n\n\n",
      "Fu et\u00a0al. (2024)": "\nFu et\u00a0al. (2024)\n\nYao Fu, Rameswar Panda, Xinyao Niu, Xiang Yue, Hannaneh Hajishirzi, Yoon Kim, and Hao Peng.\n\n\nData engineering for scaling language models to 128k context, 2024.\n\n\n",
      "Cai et\u00a0al. (2024)": "\nCai et\u00a0al. (2024)\n\nZheng Cai, Maosong Cao, Haojiong Chen, Kai Chen, Keyu Chen, Xin Chen, Xun Chen, Zehui Chen, Zhi Chen, Pei Chu, et\u00a0al.\n\n\nInternlm2 technical report.\n\n\narXiv preprint arXiv:2403.17297, 2024.\n\n\n",
      "Chen et\u00a0al. (2023a)": "\nChen et\u00a0al. (2023a)\n\nYukang Chen, Shengju Qian, Haotian Tang, Xin Lai, Zhijian Liu, Song Han, and Jiaya Jia.\n\n\nLonglora: Efficient fine-tuning of long-context large language models.\n\n\nIn The Twelfth International Conference on Learning Representations, 2023a.\n\n\n",
      "Du et\u00a0al. (2022)": "\nDu et\u00a0al. (2022)\n\nZhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie Tang.\n\n\nGlm: General language model pretraining with autoregressive blank infilling.\n\n\nIn Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 320\u2013335, 2022.\n\n\n",
      "OpenAI (2023)": "\nOpenAI (2023)\n\nOpenAI.\n\n\nGpt-4 technical report.\n\n\nhttps://cdn.openai.com/papers/gpt-4.pdf, 2023.\n\n\n",
      "Izacard and Grave (2021a)": "\nIzacard and Grave (2021a)\n\nGautier Izacard and Edouard Grave.\n\n\nLeveraging passage retrieval with generative models for open domain question answering, 2021a.\n\n\n",
      "Gao et\u00a0al. (2024)": "\nGao et\u00a0al. (2024)\n\nYunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi\u00a0Dai, Jiawei Sun, Qianyu Guo, Meng Wang, and Haofen Wang.\n\n\nRetrieval-augmented generation for large language models: A survey, 2024.\n\n\n",
      "Wang et\u00a0al. (2023)": "\nWang et\u00a0al. (2023)\n\nZhiruo Wang, Jun Araki, Zhengbao Jiang, Md\u00a0Rizwan Parvez, and Graham Neubig.\n\n\nLearning to filter context for retrieval-augmented generation, 2023.\n\n\n",
      "Qian et\u00a0al. (2024)": "\nQian et\u00a0al. (2024)\n\nHongjin Qian, Zheng Liu, Kelong Mao, Yujia Zhou, and Zhicheng Dou.\n\n\nGrounding language model with chunking-free in-context retrieval, 2024.\n\n\n",
      "Izacard and Grave (2021b)": "\nIzacard and Grave (2021b)\n\nGautier Izacard and Edouard Grave.\n\n\nDistilling knowledge from reader to retriever for question answering.\n\n\nIn International Conference on Learning Representations, 2021b.\n\n\nURL https://openreview.net/forum?id=NTEz-6wysdb.\n\n\n",
      "Xu et\u00a0al. (2023b)": "\nXu et\u00a0al. (2023b)\n\nPeng Xu, Wei Ping, Xianchao Wu, Lawrence McAfee, Chen Zhu, Zihan Liu, Sandeep Subramanian, Evelina Bakhturina, Mohammad Shoeybi, and Bryan Catanzaro.\n\n\nRetrieval meets long context large language models.\n\n\nIn The Twelfth International Conference on Learning Representations, 2023b.\n\n\n",
      "Jiang et\u00a0al. (2023b)": "\nJiang et\u00a0al. (2023b)\n\nZhengbao Jiang, Frank\u00a0F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, and Graham Neubig.\n\n\nActive retrieval augmented generation.\n\n\narXiv preprint arXiv:2305.06983, 2023b.\n\n\nURL https://arxiv.org/pdf/2305.06983.\n\n\n",
      "Luo et\u00a0al. (2024)": "\nLuo et\u00a0al. (2024)\n\nKun Luo, Zheng Liu, Shitao Xiao, and Kang Liu.\n\n\nBge landmark embedding: A chunking-free embedding method for retrieval augmented long-context large language models, 2024.\n\n\n",
      "Ratner et\u00a0al. (2022)": "\nRatner et\u00a0al. (2022)\n\nNir Ratner, Yoav Levine, Yonatan Belinkov, Ori Ram, Inbal Magar, Omri Abend, Ehud Karpas, Amnon Shashua, Kevin Leyton-Brown, and Yoav Shoham.\n\n\nParallel Context Windows Improve In-Context Learning of Large Language Models.\n\n\narXiv, 2022.\n\n\ndoi: 10.48550/arxiv.2212.10947.\n\n\nWindow.\n\n\n",
      "Nakano et\u00a0al. (2022)": "\nNakano et\u00a0al. (2022)\n\nReiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, Xu\u00a0Jiang, Karl Cobbe, Tyna Eloundou, Gretchen Krueger, Kevin Button, Matthew Knight, Benjamin Chess, and John Schulman.\n\n\nWebgpt: Browser-assisted question-answering with human feedback, 2022.\n\n\n",
      "Yang et\u00a0al. (2023)": "\nYang et\u00a0al. (2023)\n\nHui Yang, Sifu Yue, and Yunzhong He.\n\n\nAuto-gpt for online decision making: Benchmarks and additional opinions.\n\n\narXiv preprint arXiv:2306.02224, 2023.\n\n\n",
      "Driess et\u00a0al. (2023)": "\nDriess et\u00a0al. (2023)\n\nDanny Driess, Fei Xia, Mehdi\u00a0SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, et\u00a0al.\n\n\nPalm-e: An embodied multimodal language model.\n\n\narXiv preprint arXiv:2303.03378, 2023.\n\n\n",
      "Dong et\u00a0al. (2022)": "\nDong et\u00a0al. (2022)\n\nQingxiu Dong, Lei Li, Damai Dai, Ce\u00a0Zheng, Zhiyong Wu, Baobao Chang, Xu\u00a0Sun, Jingjing Xu, and Zhifang Sui.\n\n\nA survey on in-context learning.\n\n\narXiv preprint arXiv:2301.00234, 2022.\n\n\n",
      "Wei et\u00a0al. (2022)": "\nWei et\u00a0al. (2022)\n\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed\u00a0H. Chi, Quoc\u00a0V Le, and Denny Zhou.\n\n\nChain of thought prompting elicits reasoning in large language models.\n\n\nIn Alice\u00a0H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in Neural Information Processing Systems, 2022.\n\n\nURL https://openreview.net/forum?id=_VjQlMeSB_J.\n\n\n",
      "Shinn et\u00a0al. (2023)": "\nShinn et\u00a0al. (2023)\n\nNoah Shinn, Beck Labash, and Ashwin Gopinath.\n\n\nReflexion: an autonomous agent with dynamic memory and self-reflection.\n\n\narXiv preprint arXiv:2303.11366, 2023.\n\n\n",
      "Chen et\u00a0al. (2023b)": "\nChen et\u00a0al. (2023b)\n\nJianlv Chen, Shitao Xiao, Peitian Zhang, Kun Luo, Defu Lian, and Zheng Liu.\n\n\nBge m3-embedding: Multi-lingual, multi-functionality, multi-granularity text embeddings through self-knowledge distillation, 2023b.\n\n\n"
    },
    "domain": "cs.CL",
    "citation_count": 0
  },
  "4124bf67-315f-4b2a-81f4-1871eef4d4c5": {
    "pk": "4124bf67-315f-4b2a-81f4-1871eef4d4c5",
    "authors": [
      "Omer Goldman",
      "Alon Jacovi",
      "Aviv Slobodkin",
      "Aviya Maimon",
      "Ido Dagan",
      "Reut Tsarfaty"
    ],
    "title": "Is It Really Long Context if All You Need Is Retrieval? Towards Genuinely Difficult Long Context NLP",
    "abstract": "Improvements in language models' capabilities have pushed their applications towards longer contexts, making long-context evaluation and development an active research area. However, many disparate use-cases are grouped together under the umbrella term of \"long-context\", defined simply by the total length of the model's input, including - for example - Needle-in-a-Haystack tasks, book summarization, and information aggregation. Given their varied difficulty, in this position paper we argue that conflating different tasks by their context length is unproductive. As a community, we require a more precise vocabulary to understand what makes long-context tasks similar or different. We propose to unpack the taxonomy of long-context based on the properties that make them more difficult with longer contexts. We propose two orthogonal axes of difficulty: (I) Diffusion: How hard is it to find the necessary information in the context? (II) Scope: How much necessary information is there to find? We survey the literature on long-context, provide justification for this taxonomy as an informative descriptor, and situate the literature with respect to it. We conclude that the most difficult and interesting settings, whose necessary information is very long and highly diffused within the input, is severely under-explored. By using a descriptive vocabulary and discussing the relevant properties of difficulty in long-context, we can implement more informed research in this area. We call for a careful design of tasks and benchmarks with distinctly long context, taking into account the characteristics that make it qualitatively different from shorter context.",
    "url": "http://arxiv.org/abs/2407.00402v3",
    "timestamp": 1719659387,
    "sections": {
      "1 Introduction": "\n\n1 Introduction\n\nFigure 1: A taxonomy of long context tasks based on the distribution of the needed information in the text. Tasks with larger scope and higher dispersion are more difficult (indicated by shade) and more indicative of the long context capabilities of large language models.\n\n\nThe ability to deal with ever-longer contexts has been one of the most notable trends among the emerging capabilities of large language models (LLMs). Starting with a few hundred tokens as the maximal input length of the first attention-based LLMs Devlin et\u00a0al. (2019); Raffel et\u00a0al. (2020), contemporary models are \u2013 technically \u2013 able to process up to 128k and even 1M tokens (Gemini Team Google, 2024; OpenAI, 2024).\n\n\nThe demand to evaluate LLMs in this setting has led to a line of research on designing long-context tasks and benchmarks, in order to systematically understand models\u2019 capabilities and drive their development.\nHowever, the field has generally a sole recurring descriptor to define such measurements by \u2013 simply, the length of the context.\nFor example, long-context benchmarks group tasks mostly by length in words (e.g., Shaham et\u00a0al., 2022; Bai et\u00a0al., 2023; Zhang et\u00a0al., 2024b). This leads to qualitatively different measurements being conflated together, with conclusions about long-context capabilities being extended from one class of tasks to others.\nThe community is, of course, aware that, for example, tasks which require a small part of the input are different from tasks that require a large part of it. But we ask the more general question: What are the properties that differentiate tasks when conditioned on their context length? What can we accomplish with such a distinction?\n\n\nIn this position paper, we claim that the current landscape of works on long-context evaluation will greatly benefit from a more fine-grained characterization of long-context task design. We argue that judging LLMs by their ability to process long sequences, while disregarding the task they process them for, overlooks the characteristics that make longer inputs more difficult, and interesting to research, to begin with\u00a0(\u00a72).\n\n\nFor example, Needle in a Haystack tasks\u00a0(NIAH; Ivgi et\u00a0al., 2023; Mohtashami and Jaggi, 2023) involve queries whose main challenge is finding the relevant information in a long context, without requiring much further processing. Synthetic NIAH datasets are, of course, easier than their natural equivalents Ivgi et\u00a0al. (2023), but the \u201cnatural vs. artificial\u201d classification is not informative in our setting, since it applies equally for tasks regardless of context length. What, then, is an informative property? What makes long-context tasks different from each other? For example, multiple-needle variants of NIAH Hsieh et\u00a0al. (2024), or those that position the \u201cneedles\u201d closer or farther apart Levy et\u00a0al. (2024). Evidently, \u201cthe number of tokens in the input\u201d is not a sufficient descriptor.\n\n\nTo resolve this roadblock, we present a taxonomy of long-context tasks for the different factors that make them harder when controlling for context length\u00a0(\u00a73). This taxonomy is derived by surveying the long-context literature and surfacing the most salient points of distinction between various tasks.\nWe focus on (I) how difficult it is to find and extract the required information from the input (its dispersion in the input), and (II) the absolute quantity of required information to solve the task (its scope). See Figure\u00a01 for a summary.\n\n\nTo understand this categorization and its utility, we review the literature on long-context evaluation and position the works with respect to those factors. We find that the most challenging setting, in which a large quantity of required information is present in a dispersed manner that is difficult to extract, is significantly under-explored\u00a0(\u00a74).\n\n\nFinally, acknowledging the inherent and legitimate reasons behind the focus on context length as the sole descriptor of difficulty, we discuss possible paths forward for designing more reliable measurements of long-context capabilities when utilizing a more nuanced vocabulary\u00a0(\u00a75).\n\n",
      "2 Task Design in Long Context": "\n\n2 Task Design in Long Context\n\nEvaluating the performance of NLP models over very long contexts is a fast-changing area of research\u00a0Bishop et\u00a0al. (2024); Wu et\u00a0al. (2024). Measurements are regularly updated to account for new capabilities which improve with extrapolation architectures\u00a0Vaswani et\u00a0al. (2017); Su et\u00a0al. (2024) and training data\u00a0He et\u00a0al. (2023); Chen et\u00a0al. (2023). Evaluators were tasked with designing measurements of long-context capabilities cheaply, efficiently, and quickly, while matching realistic use cases as much as possible. The most common way of differentiating long-context tasks, besides the context\u2019s length, is whether they are naturally-constructed or synthetically-constructed\u00a0Tay et\u00a0al. (2020); Bai et\u00a0al. (2023); Hsieh et\u00a0al. (2024).\n\n\nNatural construction.\n\nA simple yet effective way of \u201cmoving the goalpost\u201d for context length is by modeling long-context tasks based on short-context tasks.\nThis was done, for example, with\nQA (Ko\u010disk\u00fd et\u00a0al., 2018, cf. Dunn et\u00a0al., 2017), summarization (Huang et\u00a0al., 2021a, cf. Narayan et\u00a0al., 2018), and NLI (Koreeda and Manning, 2021a, cf. Williams et\u00a0al., 2018). Specialized domains like legal Bruno and Roth (2022); Nguyen et\u00a0al. (2024) and literature (Wang et\u00a0al., 2022; Kryscinski et\u00a0al., 2022) often involve longer texts, turning typically short-context tasks such as QA and NLI into long-context scenarios.\nAnother more native methodology is to create new tasks which inherently require a long context,\nsuch as multi-document summarization Fabbri et\u00a0al. (2019); Angelidis et\u00a0al. (2021), survey generation Gao et\u00a0al. (2024), and structured data aggregation Caciularu et\u00a0al. (2024).\nBoth methodologies share the constraint that, due to their natural construction (i.e., using natural text), once created, they are difficult to modify for longer contexts as models\u2019 long-context capabilities improve.\n\n\n\nSynthetic construction.\n\nA more flexible approach, sacrificing natural construction for length control, is to use distractors to synthetically increase the context length.\nThis method allows for cheap and efficient (in terms of task construction cost) evaluation of models\u2019 full context length capabilities, with difficulty adjusted by controlling the distractors.\nTasks like Needle-in-a-Haystack (NIAH; Ivgi et\u00a0al., 2023; Kamradt, 2023) and PassKey retrieval (Mohtashami and Jaggi, 2023) were created to evaluate a model\u2019s ability to pinpoint specific information amid lengthy distractors.\nFlexible and effective against existing models, they became standard benchmarks for evaluating new long-context models (GLM Team, 2024; Jiang et\u00a0al., 2024).\nFollowup studies have complicated these tasks by increasing the number of critical details to locate (Arora et\u00a0al., 2023; Liu et\u00a0al., 2024a) and changing their position within the input Liu et\u00a0al. (2024b); Levy et\u00a0al. (2024).\n\n\n\nLimitations of the status quo.\n\nNIAH-like tasks aim to assess information retrieval capabilities, yet\nmany \u201cnaturally constructed\u201d QA and reading-comprehension tasks\nwith trivial questions about a long context accomplish the same goal. At the same time, \u201cmultiple needles\u201d NIAH can increase difficulty not by increasing the quantity of needles or length of input, but by adding distractors between needles\u00a0Levy et\u00a0al. (2024). What can systematically explain the different variables at play, in order to inform better task design in the future?\n\n\nClearly, there are underlying qualitative differences that discriminate between these various tasks besides their natural and synthetic constructions, and besides their actual context length.\nTherefore, we require a more informative vocabulary to discuss the goals of each task design, what it accomplishes, and what it does not, in terms of measuring long-context capabilities.\n\n\n",
      "3 What Makes Long Context More than Retrieval?": "\n\n3 What Makes Long Context More than Retrieval?\n\nWe require a taxonomy to capture task difficulty variations beyond mere \u201cnumber of tokens\u201d.\nWe focus on the information that is canonically required to solve the task as the conditioning variable. Our classification can be summarized via the following two questions, when asked about a given task:\n\n\n(I) How difficult is it to find and extract the required information?\n\n\n(II) How much information is needed to be found?\n\n\nAssuming that some highlighting of the relevant information is needed to solve the task (see Figure\u00a01), the latter question asks how much text is highlighted, while the former addresses the challenge of locating the relevant text for highlighting.\n\n\nFor instance, consider the task of summarizing a book, in comparison to a NIAH task of identifying a numerical detail in a long financial report (e.g., \u201chow much did the company earn in 2015?\u201d).\nAlthough both tasks involve long texts, the information required and its accessibility vary significantly.\nThe NIAH task focuses on localized, identifiable information, while summarization requires extracting key details dispersed throughout the text, tangled together with irrelevant content. Therefore, we can say that the book summarization task is more difficult on both axes (I) and (II).\n\n\nBelow we give more formal descriptions of the two axes characterized by the questions above.\n\n\n(I) Dispersion.\n\nAlthough the question above intuitively defines \u201cdifficulty of information finding\u201d,\nwe offer a more concrete description. Between two similar tasks, we consider the information harder to find in one task compared to another if:\n(1) it is more obscured (e.g., linguistically, semantically, contextually, etc); (2) it is more sparse, such that it is interspersed with non-required information; (3) its indicators are less redundant, such that there are fewer places in the document where the same information is available.\n\n\n\n(II) Scope.\n\nThe property of scope is simpler, and refers to the minimal quantity of information needed to solve the task. Importantly, we are not concerned with precise metric for \u201cquantity of information\u201d at this stage \u2013 it can refer to quantity of tokens, sentences, relations, cells in a table, etc. Any metric that reliably captures difficulty for an established solver is sufficient for our purposes.\n\n\n\nIllustrative example.\n\nTo illustrate, consider the Wikipedia entry for New York City and a simple question: \u201cWhat is the estimated population of the city?\u201d Since the answer needs a small snippet of information, we say that the task has small scope. And since it is easily accessible, we say that it has low dispersion. Consider, instead, the question \u201chow many syllables are in this document?\u201d \u2013 since this question requires the entire document to answer, we say that it has large scope, but if we consider counting syllables as straightforward, then we say its dispersion is still low. Finally, with the question \u201cWas the city\u2019s mayor elected before or after the city was affected by Hurricane Sandy?\u201d \u2013 since it requires snippets from at least two different areas of the text, we can say that when compared to the question about the city\u2019s population, the dispersion is higher, but not as high as for the question \u201cWhat makes the city a prominent place on the world stage?\u201d which poses a challenge on both axes.\n\n\nFigure 2: This figure illustrates our subjective judgment on the distribution of long-context benchmarks for each task, categorized by their scope and dispersion characteristics, with the four quadrants being marked by the dashed lines. Difficulty is expressed by shade, where red is more difficult and green in easier. Notably, some tasks, like Question-answering (QA), appear in multiple quadrants, as different benchmarks demand varying levels of scope and dispersion (e.g., a single fact versus multiple facts spread across a document). For a detailed breakdown of benchmarks and their task associations, refer to Appendix\u00a0A.\n\n\n",
      "4 Challenging Long Context Is Under-Explored": "\n\n4 Challenging Long Context Is Under-Explored\n\nRevisiting the works surveyed in \u00a72, they clearly differ with respect to both scope and dispersion.\n\n\nWith respect to dispersion.\n\nThe information needed for tasks ranges from easily accessible to highly dispersed and difficult to detect.\nOn low dispersion we have NIAH Kamradt (2023); Mohtashami and Jaggi (2023) and a myriad of factual single-hop QA datasets (Tseng et\u00a0al., 2016; Ko\u010disk\u00fd et\u00a0al., 2017; Kwiatkowski et\u00a0al., 2019; Dasigi et\u00a0al., 2021, inter alia) in which the answer is relatively accessible.\nAdding more snippets of information separated by distractors, either in the form of several needles Arora et\u00a0al. (2023); Hsieh et\u00a0al. (2024) or of hops in a multi-hop question Trivedi et\u00a0al. (2022); Zhao et\u00a0al. (2022), complicates the information detection due to the need to find at least two snippets Levy et\u00a0al. (2024), thereby increasing dispersion.\nDispersion can also be increased by making the detection of the information less straightforward\n(e.g., Pang et\u00a0al., 2022) or requiring aggregation Shaham et\u00a0al. (2023).\nLastly, summarization tasks are of a very high dispersion Huang et\u00a0al. (2021a); Wang et\u00a0al. (2022), as they require the non-trivial detection of salient facts that are interwoven with the irrelevant text.\n\n\n\nWith respect to scope.\n\nTasks overwhelmingly target relatively small scope. In addition to the aforementioned NIAH tasks and their variants, many\nQA datasets apply as well\u00a0(Li et\u00a0al., 2023b; Zhao et\u00a0al., 2023; Reddy et\u00a0al., 2024, inter alia).\nA somewhat higher scope is achieved by datasets for query-based summarization Zhong et\u00a0al. (2021); Wang et\u00a0al. (2022), and QA datasets with more obfuscated answers that require reading the text surrounding the answer for its verification\u00a0An et\u00a0al. (2023); He et\u00a0al. (2023).\nAlthough much higher on the scope ladder, book summarization is still limited in its scope as datasets include texts that are only of up to 20k\ud835\udc58kitalic_k tokens Huang et\u00a0al. (2021a); Chen et\u00a0al. (2022a); Shaham et\u00a0al. (2023).\nCurrently, tasks with the highest scope, requiring information across the entire input length, are artificial and of low dispersion, like common words extraction Hsieh et\u00a0al. (2024).\n\n\n\nConclusion.\n\nFigure\u00a02 summarizes the above classification of tasks and datasets. Note that without a concrete definition of dispersion and scope, the plot is only an illustration that involves a good deal of subjective judgements.\nHowever, we conclude that (1) the majority of tasks designed to challenge LLMs in a long-context setting target either scope or dispersion,\nsuch that (2) tasks that push current models\u2019 capabilities on both axes are under-represented in the current landscape.\n\n\n",
      "5 Discussion: Towards Genuinely Difficult Long-Context Task Design": "\n\n5 Discussion: Towards Genuinely Difficult Long-Context Task Design\n\nChallenges.\n\nDesigning meaningful long-context tasks amidst rapid model progress is profoundly challenging, making the deficiency in tasks representing difficulty on both the dispersion and scope axes unsurprising.\nOne source of this challenge is the lack of diverse, coherent long texts, as models\u2019 context windows can now be comparable to\nthe length of the New Testament111www.readinglength.com/book/isbn-0190909005\nand the Odyssey.222www.readinglength.com/book/isbn-0140268863\nThe methodologies discussed in \u00a72 for creating long context tasks \u2013 lengthening short context tasks and synthetically creating length-adjustable tasks \u2013 are preferred for their straightforward definition and the incremental adjustments they require for existing data.\nThey rely on the common understanding of machine comprehension as formulated with short context in mind Dunietz et\u00a0al. (2020), and therefore they are intuitive and easy to comprehend for NLP researchers without domain expertise (e.g., in law or biomedical fields that have long contexts).\n\n\n\nFuture work.\n\nThe goals laid forward in this work are clear: For more durable and robust measurement of long-context capabilities, we must design tasks that explicitly target both the dispersion and scope capabilities of models.\nHow can this be achieved? As mentioned, one possible avenue is to rely more on tasks that require domain expertise, such as legal documents\u00a0Bruno and Roth (2022), financial reports\u00a0Reddy et\u00a0al. (2024), biomedical publications\u00a0Stylianou et\u00a0al. (2021), and so on. In specialized domains, it is common that dispersion will be naturally higher\u00a0Zhao et\u00a0al. (2022). Tasks that involve implicit aggregations over structured data, such as table manipulation\u00a0Caciularu et\u00a0al. (2024), are possible avenues for increasing both scope and dispersion synthetically by leveraging the data structure.\nIn this work, we argue that an explicit vocabulary for such properties of difficulty is what can enable more informed techniques to design difficult tasks in the future.\n\n\n",
      "6 Conclusions": "\n\n6 Conclusions\n\nWe present a taxonomy of factors that make long-context tasks more challenging compared to short ones. This is in contrast with the existing literature that refers only to the length of the input as the hallmark of long context, and as a result ends up conflating tasks of different character when assessing the ability of models to understand longer text.\nWe reviewed works on evaluation in a long-context setting and found that the most challenging setting, in which the information needed is of large scope and is highly dispersed within the input, is under-explored.\nFinally, we suggested some leads for future work to tackle this imbalance towards a more informative long context evaluation.\n\n",
      "7 Limitations": "\n\n7 Limitations\n\nFormality.\n\nIn the context of this work, we have deliberately adhered to a taxonomy based on an intuitive description, in the interest of utility to a wide diversity of research and flexibility for future work. Difficulty in searching for and extracting information, and quantity of information, are both vague terms that can only be grounded in the context of a specific family of tasks and use-cases. We intend for this work to serve as a call to action and a tool for a shared vocabulary in the interest of more informed long-context task design in the future, rather than to anchor the taxonomy to a specific and fragile point in time.\n\n\n\nRetrieval is still interesting.\n\nAlthough we argue that small scope and low dispersion tasks are the least indicative of the model\u2019s ability to long-context capabilities, tasks that are well-served by implicit retrieval or by traditional retrieval-based pipelines are certainly relevant and useful in a variety of common use-cases\u00a0Stylianou et\u00a0al. (2021); Bruno and Roth (2022); Gao et\u00a0al. (2023).\n\n\n\nOther uses for a long-context window.\n\nThis paper deals only with long inputs that serve as inputs to a task. The long context of course can have other purposes as well, like containing many in-context examples Bertsch et\u00a0al. (2024) or containing other modalities and structures Jiang et\u00a0al. (2023).\n\n\n",
      "Acknowledgments": "\nAcknowledgments\n\nThe authors would like to thank Gabriel Stanovsky for the fruitful discussions.\n\n",
      "Appendix A Benchmark Scope-Dispersion Classification": "\n\nAppendix A Benchmark Scope-Dispersion Classification\n\nIn Table\u00a01 we delineate the different long-context benchmarks, as well as classify them according to how challenging they are scope-wise and dispersion-wise.\n\n\n\n\n\n\n\nLOW SCOPE\nHIGH SCOPE\n\n\n\n\n\nQA\nQBS\n\n\n\n\u00a0\u00a0\u00a0\u00a0 Qasper (Dasigi et\u00a0al., 2021)\n\n\u00a0\u00a0\u00a0\u00a0 QMSum (Zhong et\u00a0al., 2021)\n\n\n\n\n\u00a0\u00a0\u00a0\u00a0 NarrativeQA (Ko\u010disk\u00fd et\u00a0al., 2018)\n\n\u00a0\u00a0\u00a0\u00a0 SQuALITY (Wang et\u00a0al., 2022)\n\n\n\n\n\u00a0\u00a0\u00a0\u00a0 Short-dependency QA (Li et\u00a0al., 2023b)\n\n\u00a0\u00a0\u00a0\u00a0 Related Work Summarization (An et\u00a0al., 2023)\n\n\n\n\n\u00a0\u00a0\u00a0\u00a0 MultiFieldQA (Bai et\u00a0al., 2023)\n\n\u00a0\u00a0\u00a0\u00a0 SPACE (Angelidis et\u00a0al., 2021)\n\n\n\n\n\u00a0\u00a0\u00a0\u00a0 LitM (QA) (Liu et\u00a0al., 2024b)\n\n\u00a0\u00a0\u00a0\u00a0 WebBrain-G (Qian et\u00a0al., 2023)\n\n\n\n\n\u00a0\u00a0\u00a0\u00a0 L-eval (MC QA) (An et\u00a0al., 2023)\n\n\u00a0\u00a0\u00a0\u00a0 AquaMuse (Kulkarni et\u00a0al., 2020)\n\n\n\n\n\u00a0\u00a0\u00a0\u00a0 NQ (Kwiatkowski et\u00a0al., 2019)\n\n\u00a0\u00a0\u00a0\u00a0 FINDSum-Liquidity (Liu et\u00a0al., 2023a)\n\n\n\n\n\u00a0\u00a0\u00a0\u00a0 RULER (single-hop QA) (Hsieh et\u00a0al., 2024)\n\nAggregation\n\n\n\n\u00a0\u00a0\u00a0\u00a0 MeetingQA (Prasad et\u00a0al., 2023)\n\n\u00a0\u00a0\u00a0\u00a0 ZeroSCROLLS (SpaceDigest & BookSumSort) (Shaham et\u00a0al., 2023)\n\n\n\n\n\u00a0\u00a0\u00a0\u00a0 BABILong (tasks 1,4-6,9-10) (Kuratov et\u00a0al., 2024)\n\n\u00a0\u00a0\u00a0\u00a0 PassageCount (Bai et\u00a0al., 2023)\n\n\n\n\n\u00a0\u00a0\u00a0\u00a0 Giraffe (2 tasks) (Pal et\u00a0al., 2023)\n\n\u00a0\u00a0\u00a0\u00a0 FINDSum-ROO (Liu et\u00a0al., 2023a)\n\n\n\n\nRetrieval\nAspect-based Summarization\n\n\n\n\u00a0\u00a0\u00a0\u00a0 LitM (Key-value Retrieval) (Liu et\u00a0al., 2024b)\n\n\u00a0\u00a0\u00a0\u00a0 ACLSum (Takeshita et\u00a0al., 2024)\n\n\n\n\n\u00a0\u00a0\u00a0\u00a0 MultiDoc2Dial (GSP) (Feng et\u00a0al., 2021)\n\n\u00a0\u00a0\u00a0\u00a0 OpenAsp (Amar et\u00a0al., 2023)\n\n\n\n\n\u00a0\u00a0\u00a0\u00a0 TopicRet (Li et\u00a0al., 2023a)\n\nText Sorting\n\n\n\n\u00a0\u00a0\u00a0\u00a0 Wiki-GenBen (Zhang et\u00a0al., 2024a)\n\n\u00a0\u00a0\u00a0\u00a0 Bamboo (ShowsSort & ReportSumSort) (Dong et\u00a0al., 2024)\n\n\n\n\n\u00a0\u00a0\u00a0\u00a0 RULER (S-NIAH & MK-NIAH) (Hsieh et\u00a0al., 2024)\n\nRetrieval\n\n\n\n\u00a0\u00a0\u00a0\u00a0 LongChat-Lines (Pal et\u00a0al., 2023)\n\n\u00a0\u00a0\u00a0\u00a0 PassageRetrieval (Bai et\u00a0al., 2023)\n\n\n\n\nNLI\nLFQA\n\n\n\n\u00a0\u00a0\u00a0\u00a0 LawngNLI (Bruno and Roth, 2022)\n\n\u00a0\u00a0\u00a0\u00a0 LongFQA (An et\u00a0al., 2023)\n\n\n\n\n\u00a0\u00a0\u00a0\u00a0 ContractNLI (Koreeda and Manning, 2021b)\n\nNLI\n\n\n\n\u00a0\u00a0\u00a0\u00a0 Hallucination Detection (Dong et\u00a0al., 2024)\n\n\u00a0\u00a0\u00a0\u00a0 Legal Case Entailment (Nguyen et\u00a0al., 2024)\n\n\n\n\n\u00a0\u00a0 FLenQA (3 tasks) (Levy et\u00a0al., 2024)\n\n\n\n\nFill-mask\n\n\n\n\n\u00a0\u00a0\u00a0\u00a0 Cloze (Li et\u00a0al., 2023b)\n\n\n\n\n\nNLG\n\n\n\n\n\nLOW DISPERSION\n\n\u00a0\u00a0\u00a0\u00a0 MultiDoc2Dial (ARG) (Feng et\u00a0al., 2021)\n\n\n\n\n\nQA\nSummarization\n\n\n\n\u00a0\u00a0\u00a0\u00a0 QuALITY (Pang et\u00a0al., 2022)\n\n\u00a0\u00a0\u00a0\u00a0 GovReport (Huang et\u00a0al., 2021b)\n\n\n\n\n\u00a0\u00a0\u00a0\u00a0 Long-dependency QA (Li et\u00a0al., 2023b)\n\n\u00a0\u00a0\u00a0\u00a0 SummScreenFD (Chen et\u00a0al., 2022b)\n\n\n\n\n\u00a0\u00a0\u00a0\u00a0 DuReader (Bai et\u00a0al., 2023)\n\n\u00a0\u00a0\u00a0\u00a0 Loogle (Summarization) (Li et\u00a0al., 2023b)\n\n\n\n\n\u00a0\u00a0\u00a0\u00a0 SFcition QA (An et\u00a0al., 2023)\n\n\u00a0\u00a0\u00a0\u00a0 VCSUM (Bai et\u00a0al., 2023)\n\n\n\n\n\u00a0\u00a0\u00a0\u00a0 ExpertQA (Malaviya et\u00a0al., 2024)\n\n\u00a0\u00a0\u00a0\u00a0 Self-critiquing (Saunders et\u00a0al., 2022)\n\n\n\n\n\u00a0\u00a0\u00a0\u00a0 DocFinQA (Reddy et\u00a0al., 2024)\n\n\u00a0\u00a0\u00a0\u00a0 Abstract Generation (An et\u00a0al., 2023)\n\n\n\n\n\u00a0\u00a0\u00a0\u00a0 BABILong (tasks 2-3,12) (Kuratov et\u00a0al., 2024)\n\n\u00a0\u00a0\u00a0\u00a0 Multi-News (Fabbri et\u00a0al., 2019)\n\n\n\n\n\u00a0\u00a0\u00a0\u00a0 Bamboo (QA) (Dong et\u00a0al., 2024)\n\n\u00a0\u00a0\u00a0\u00a0 BigPatent (Sharma et\u00a0al., 2019)\n\n\n\n\nMulti-hop QA\n\u00a0\u00a0\u00a0\u00a0 Scientific Summarization (Cohan et\u00a0al., 2018)\n\n\n\n\n\u00a0\u00a0\u00a0\u00a0 MuSiQue (Trivedi et\u00a0al., 2022)\n\n\u00a0\u00a0\u00a0\u00a0 BillSum (Kornilova and Eidelman, 2019)\n\n\n\n\n\u00a0\u00a0\u00a0\u00a0 HotpotQA (Yang et\u00a0al., 2018)\n\n\u00a0\u00a0\u00a0\u00a0 HowSumm (Boni et\u00a0al., 2021)\n\n\n\n\n\u00a0\u00a0\u00a0\u00a0 Multi-hop Tracing (Hsieh et\u00a0al., 2024)\n\n\u00a0\u00a0\u00a0\u00a0 ODSum (Zhou et\u00a0al., 2023)\n\n\n\n\n\u00a0\u00a0\u00a0\u00a0 RULER (multi-hop QA) (Hsieh et\u00a0al., 2024)\n\n\u00a0\u00a0\u00a0\u00a0 Klexikon (Summarization) (Aumiller and Gertz, 2022)\n\n\n\n\n\u00a0\u00a0\u00a0\u00a0 2WikiMultihopQA (Ho et\u00a0al., 2020)\n\n\u00a0\u00a0\u00a0\u00a0 Booksum (Kry\u015bci\u0144ski et\u00a0al., 2022)\n\n\n\n\nNLI\n\u00a0\u00a0\u00a0\u00a0 MeetingBank (Hu et\u00a0al., 2023)\n\n\n\n\n\u00a0\u00a0\u00a0\u00a0 FLenQA (3 rand. placement tasks) (Levy et\u00a0al., 2024)\n\nText Simplification\n\n\n\n\u00a0\u00a0\u00a0\u00a0 Legal Textual Entailment (Nguyen et\u00a0al., 2024)\n\n\u00a0\u00a0\u00a0\u00a0 Klexikon (Simplification) (Aumiller and Gertz, 2022)\n\n\n\n\nCode Understanding\nReasoning\n\n\n\n\u00a0\u00a0\u00a0\u00a0 LCC (Guo et\u00a0al., 2023)\n\n\u00a0\u00a0\u00a0\u00a0 Long ListOps (Tay et\u00a0al., 2020)\n\n\n\n\n\u00a0\u00a0\u00a0\u00a0 RepoBench-P (Liu et\u00a0al., 2023b)\n\nRetrieval\n\n\n\n\u00a0\u00a0\u00a0\u00a0 CodeU (An et\u00a0al., 2023)\n\n\u00a0\u00a0\u00a0\u00a0 LRA (task 3) (Tay et\u00a0al., 2020)\n\n\n\n\n\u00a0\u00a0\u00a0\u00a0 PrivateEval (Dong et\u00a0al., 2024)\n\n\n\n\n\nClassification\n\n\n\n\n\u00a0\u00a0\u00a0\u00a0 LRA (tasks 2, 4-6) (Tay et\u00a0al., 2020)\n\n\n\n\n\nRetrieval\n\n\n\n\n\u00a0\u00a0\u00a0\u00a0 COLIEE (tasks 1,3,4) (Nguyen et\u00a0al., 2024)\n\n\n\n\n\n\u00a0\u00a0\u00a0\u00a0 RULER (MV-NIAH & MQ-NIAH) (Hsieh et\u00a0al., 2024)\n\n\n\n\n\nNext Token Prediction\n\n\n\n\n\u00a0\u00a0\u00a0\u00a0 PG-19 (Rae et\u00a0al., 2019)\n\n\n\n\n\n\u00a0\u00a0\u00a0\u00a0 Bamboo (LM) (Dong et\u00a0al., 2024)\n\n\n\n\n\nReasoning\n\n\n\n\n\u00a0\u00a0\u00a0\u00a0 DocMath-Eval (Zhao et\u00a0al., 2023)\n\n\n\n\n\n\u00a0\u00a0\u00a0\u00a0 BABILong (tasks 14-20) (Kuratov et\u00a0al., 2024)\n\n\n\n\n\nAggregation\n\n\n\n\n\u00a0\u00a0\u00a0\u00a0 RULER (2 Aggr. tasks) (Hsieh et\u00a0al., 2024)\n\n\n\n\n\n\u00a0\u00a0\u00a0\u00a0 BABILong (tasks 7-8) (Kuratov et\u00a0al., 2024)\n\n\n\n\n\nNLU\n\n\n\n\n\u00a0\u00a0\u00a0\u00a0 Academic Feedback Generation (An et\u00a0al., 2023)\n\n\n\n\n\n\u00a0\u00a0\u00a0\u00a0 CUAD (Hendrycks et\u00a0al., 2021)\n\n\n\n\n\nFactuality Evaluation\n\n\n\n\n\u00a0\u00a0\u00a0\u00a0 LongSciVerify (Bishop et\u00a0al., 2024)\n\n\n\n\n\nCoreference Resolution\n\n\n\n\n\nHIGH DISPERSION\n\n\u00a0\u00a0\u00a0\u00a0 BABILong (tasks 11,13) (Kuratov et\u00a0al., 2024)\n\n\n\n\n\n\nTable 1: Classification of long-context benchmarks in terms of Scope and Dispersion.\n\n\n"
    },
    "bibliography": {
      "Amar et\u00a0al. (2023)": "\nAmar et\u00a0al. (2023)\n\nShmuel Amar, Liat Schiff, Ori Ernst, Asi Shefer, Ori Shapira, and Ido Dagan. 2023.\n\n\nOpenAsp: A benchmark for multi-document open aspect-based summarization.\n\n\nIn Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 1967\u20131991, Singapore. Association for Computational Linguistics.\n\n\n",
      "An et\u00a0al. (2023)": "\nAn et\u00a0al. (2023)\n\nChenxin An, Shansan Gong, Ming Zhong, Xingjian Zhao, Mukai Li, Jun Zhang, Lingpeng Kong, and Xipeng Qiu. 2023.\n\n\nL-eval: Instituting standardized evaluation for long context language models.\n\n\nPreprint, arXiv:2307.11088.\n\n\n",
      "Angelidis et\u00a0al. (2021)": "\nAngelidis et\u00a0al. (2021)\n\nStefanos Angelidis, Reinald\u00a0Kim Amplayo, Yoshihiko Suhara, Xiaolan Wang, and Mirella Lapata. 2021.\n\n\nExtractive opinion summarization in quantized transformer spaces.\n\n\nTransactions of the Association for Computational Linguistics, 9:277\u2013293.\n\n\n",
      "Arora et\u00a0al. (2023)": "\nArora et\u00a0al. (2023)\n\nSimran Arora, Sabri Eyuboglu, Aman Timalsina, Isys Johnson, Michael Poli, James Zou, Atri Rudra, and Christopher R\u00e9. 2023.\n\n\nZoology: Measuring and improving recall in efficient language models.\n\n\narXiv preprint arXiv:2312.04927.\n\n\n",
      "Aumiller and Gertz (2022)": "\nAumiller and Gertz (2022)\n\nDennis Aumiller and Michael Gertz. 2022.\n\n\nKlexikon: A German dataset for joint summarization and simplification.\n\n\nIn Proceedings of the Thirteenth Language Resources and Evaluation Conference, pages 2693\u20132701, Marseille, France. European Language Resources Association.\n\n\n",
      "Bai et\u00a0al. (2023)": "\nBai et\u00a0al. (2023)\n\nYushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li. 2023.\n\n\nLongbench: A bilingual, multitask benchmark for long context understanding.\n\n\nPreprint, arXiv:2308.14508.\n\n\n",
      "Bertsch et\u00a0al. (2024)": "\nBertsch et\u00a0al. (2024)\n\nAmanda Bertsch, Maor Ivgi, Uri Alon, Jonathan Berant, Matthew\u00a0R. Gormley, and Graham Neubig. 2024.\n\n\nIn-context learning with long-context models: An in-depth exploration.\n\n\nPreprint, arXiv:2405.00200.\n\n\n",
      "Bishop et\u00a0al. (2024)": "\nBishop et\u00a0al. (2024)\n\nJennifer\u00a0A Bishop, Qianqian Xie, and Sophia Ananiadou. 2024.\n\n\nLongdocfactscore: Evaluating the factuality of long document abstractive summarisation.\n\n\nPreprint, arXiv:2309.12455.\n\n\n",
      "Boni et\u00a0al. (2021)": "\nBoni et\u00a0al. (2021)\n\nOdellia Boni, Guy Feigenblat, Guy Lev, Michal Shmueli-Scheuer, Benjamin Sznajder, and David Konopnicki. 2021.\n\n\nHowsumm: A multi-document summarization dataset derived from wikihow articles.\n\n\nPreprint, arXiv:2110.03179.\n\n\n",
      "Bruno and Roth (2022)": "\nBruno and Roth (2022)\n\nWilliam Bruno and Dan Roth. 2022.\n\n\nLawngnli: A long-premise benchmark for in-domain generalization from short to long contexts and for implication-based retrieval.\n\n\nPreprint, arXiv:2212.03222.\n\n\n",
      "Caciularu et\u00a0al. (2024)": "\nCaciularu et\u00a0al. (2024)\n\nAvi Caciularu, Alon Jacovi, Eyal Ben-David, Sasha Goldshtein, Tal Schuster, Jonathan Herzig, Gal Elidan, and Amir Globerson. 2024.\n\n\nTact: Advancing complex aggregative reasoning with information extraction tools.\n\n\nPreprint, arXiv:2406.03618.\n\n\n",
      "Chen et\u00a0al. (2022a)": "\nChen et\u00a0al. (2022a)\n\nMingda Chen, Zewei Chu, Sam Wiseman, and Kevin Gimpel. 2022a.\n\n\nSummScreen: A dataset for abstractive screenplay summarization.\n\n\nIn Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 8602\u20138615, Dublin, Ireland. Association for Computational Linguistics.\n\n\n",
      "Chen et\u00a0al. (2022b)": "\nChen et\u00a0al. (2022b)\n\nMingda Chen, Zewei Chu, Sam Wiseman, and Kevin Gimpel. 2022b.\n\n\nSummscreen: A dataset for abstractive screenplay summarization.\n\n\nPreprint, arXiv:2104.07091.\n\n\n",
      "Chen et\u00a0al. (2023)": "\nChen et\u00a0al. (2023)\n\nYukang Chen, Shengju Qian, Haotian Tang, Xin Lai, Zhijian Liu, Song Han, and Jiaya Jia. 2023.\n\n\nLonglora: Efficient fine-tuning of long-context large language models.\n\n\nArXiv, abs/2309.12307.\n\n\n",
      "Cohan et\u00a0al. (2018)": "\nCohan et\u00a0al. (2018)\n\nArman Cohan, Franck Dernoncourt, Doo\u00a0Soon Kim, Trung Bui, Seokhwan Kim, Walter Chang, and Nazli Goharian. 2018.\n\n\nA discourse-aware attention model for abstractive summarization of long documents.\n\n\nPreprint, arXiv:1804.05685.\n\n\n",
      "Dasigi et\u00a0al. (2021)": "\nDasigi et\u00a0al. (2021)\n\nPradeep Dasigi, Kyle Lo, Iz\u00a0Beltagy, Arman Cohan, Noah\u00a0A. Smith, and Matt Gardner. 2021.\n\n\nA dataset of information-seeking questions and answers anchored in research papers.\n\n\nIn Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 4599\u20134610, Online. Association for Computational Linguistics.\n\n\n",
      "Devlin et\u00a0al. (2019)": "\nDevlin et\u00a0al. (2019)\n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019.\n\n\nBERT: Pre-training of deep bidirectional transformers for language understanding.\n\n\nIn Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171\u20134186, Minneapolis, Minnesota. Association for Computational Linguistics.\n\n\n",
      "Dong et\u00a0al. (2024)": "\nDong et\u00a0al. (2024)\n\nZican Dong, Tianyi Tang, Junyi Li, Wayne\u00a0Xin Zhao, and Ji-Rong Wen. 2024.\n\n\nBAMBOO: A comprehensive benchmark for evaluating long text modeling capacities of large language models.\n\n\nIn Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024), pages 2086\u20132099, Torino, Italia. ELRA and ICCL.\n\n\n",
      "Dunietz et\u00a0al. (2020)": "\nDunietz et\u00a0al. (2020)\n\nJesse Dunietz, Greg Burnham, Akash Bharadwaj, Owen Rambow, Jennifer Chu-Carroll, and Dave Ferrucci. 2020.\n\n\nTo test machine comprehension, start by defining comprehension.\n\n\nIn Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7839\u20137859, Online. Association for Computational Linguistics.\n\n\n",
      "Dunn et\u00a0al. (2017)": "\nDunn et\u00a0al. (2017)\n\nMatthew Dunn, Levent Sagun, Mike Higgins, V\u00a0Ugur Guney, Volkan Cirik, and Kyunghyun Cho. 2017.\n\n\nSearchqa: A new q&a dataset augmented with context from a search engine.\n\n\narXiv preprint arXiv:1704.05179.\n\n\n",
      "Fabbri et\u00a0al. (2019)": "\nFabbri et\u00a0al. (2019)\n\nAlexander\u00a0R. Fabbri, Irene Li, Tianwei She, Suyi Li, and Dragomir\u00a0R. Radev. 2019.\n\n\nMulti-news: a large-scale multi-document summarization dataset and abstractive hierarchical model.\n\n\nPreprint, arXiv:1906.01749.\n\n\n",
      "Feng et\u00a0al. (2021)": "\nFeng et\u00a0al. (2021)\n\nSong Feng, Siva\u00a0Sankalp Patel, Hui Wan, and Sachindra Joshi. 2021.\n\n\nMultiDoc2Dial: Modeling dialogues grounded in multiple documents.\n\n\nIn Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6162\u20136176, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.\n\n\n",
      "Gao et\u00a0al. (2024)": "\nGao et\u00a0al. (2024)\n\nFan Gao, Hang Jiang, Rui Yang, Qingcheng Zeng, Jinghui Lu, Moritz Blum, Dairui Liu, Tianwei She, Yuang Jiang, and Irene Li. 2024.\n\n\nLarge language models on wikipedia-style survey generation: an evaluation in nlp concepts.\n\n\nPreprint, arXiv:2308.10410.\n\n\n",
      "Gao et\u00a0al. (2023)": "\nGao et\u00a0al. (2023)\n\nLuyu Gao, Zhuyun Dai, Panupong Pasupat, Anthony Chen, Arun\u00a0Tejasvi Chaganty, Yicheng Fan, Vincent\u00a0Y. Zhao, Ni\u00a0Lao, Hongrae Lee, Da-Cheng Juan, and Kelvin Guu. 2023.\n\n\nRarr: Researching and revising what language models say, using language models.\n\n\nPreprint, arXiv:2210.08726.\n\n\n",
      "Gemini Team Google (2024)": "\nGemini Team Google (2024)\n\nGemini Team Google. 2024.\n\n\nGemini 1.5: Unlocking multimodal understanding across millions of tokens of context.\n\n\nPreprint, arXiv:2403.05530.\n\n\n",
      "GLM Team (2024)": "\nGLM Team (2024)\n\nGLM Team. 2024.\n\n\nGLM-4-9b-chat technical report.\n\n\n",
      "Guo et\u00a0al. (2023)": "\nGuo et\u00a0al. (2023)\n\nDaya Guo, Canwen Xu, Nan Duan, Jian Yin, and Julian McAuley. 2023.\n\n\nLongcoder: A long-range pre-trained language model for code completion.\n\n\nPreprint, arXiv:2306.14893.\n\n\n",
      "He et\u00a0al. (2023)": "\nHe et\u00a0al. (2023)\n\nJunqing He, Kunhao Pan, Xiaoqun Dong, Zhuoyang Song, Yibo Liu, Yuxin Liang, Hao Wang, Qianguo Sun, Songxin Zhang, Zejian Xie, and Jiaxing Zhang. 2023.\n\n\nNever lost in the middle: Improving large language models via attention strengthening question answering.\n\n\nPreprint, arXiv:2311.09198.\n\n\n",
      "Hendrycks et\u00a0al. (2021)": "\nHendrycks et\u00a0al. (2021)\n\nDan Hendrycks, Collin Burns, Anya Chen, and Spencer Ball. 2021.\n\n\nCuad: An expert-annotated nlp dataset for legal contract review.\n\n\nPreprint, arXiv:2103.06268.\n\n\n",
      "Ho et\u00a0al. (2020)": "\nHo et\u00a0al. (2020)\n\nXanh Ho, Anh-Khoa Duong\u00a0Nguyen, Saku Sugawara, and Akiko Aizawa. 2020.\n\n\nConstructing a multi-hop QA dataset for comprehensive evaluation of reasoning steps.\n\n\nIn Proceedings of the 28th International Conference on Computational Linguistics, pages 6609\u20136625, Barcelona, Spain (Online). International Committee on Computational Linguistics.\n\n\n",
      "Hsieh et\u00a0al. (2024)": "\nHsieh et\u00a0al. (2024)\n\nCheng-Ping Hsieh, Simeng Sun, Samuel Kriman, Shantanu Acharya, Dima Rekesh, Fei Jia, Yang Zhang, and Boris Ginsburg. 2024.\n\n\nRuler: What\u2019s the real context size of your long-context language models?\n\nPreprint, arXiv:2404.06654.\n\n\n",
      "Hu et\u00a0al. (2023)": "\nHu et\u00a0al. (2023)\n\nYebowen Hu, Timothy Ganter, Hanieh Deilamsalehy, Franck Dernoncourt, Hassan Foroosh, and Fei Liu. 2023.\n\n\nMeetingBank: A benchmark dataset for meeting summarization.\n\n\nIn Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 16409\u201316423, Toronto, Canada. Association for Computational Linguistics.\n\n\n",
      "Huang et\u00a0al. (2021a)": "\nHuang et\u00a0al. (2021a)\n\nLuyang Huang, Shuyang Cao, Nikolaus Parulian, Heng Ji, and Lu\u00a0Wang. 2021a.\n\n\nEfficient attentions for long document summarization.\n\n\nIn Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1419\u20131436, Online. Association for Computational Linguistics.\n\n\n",
      "Huang et\u00a0al. (2021b)": "\nHuang et\u00a0al. (2021b)\n\nLuyang Huang, Shuyang Cao, Nikolaus Parulian, Heng Ji, and Lu\u00a0Wang. 2021b.\n\n\nEfficient attentions for long document summarization.\n\n\nPreprint, arXiv:2104.02112.\n\n\n",
      "Ivgi et\u00a0al. (2023)": "\nIvgi et\u00a0al. (2023)\n\nMaor Ivgi, Uri Shaham, and Jonathan Berant. 2023.\n\n\nEfficient long-text understanding with short-text models.\n\n\nTransactions of the Association for Computational Linguistics, 11:284\u2013299.\n\n\n",
      "Jiang et\u00a0al. (2024)": "\nJiang et\u00a0al. (2024)\n\nAlbert\u00a0Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra\u00a0Singh Chaplot, Diego de\u00a0las Casas, Emma\u00a0Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, L\u00e9lio\u00a0Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven\u00a0Le Scao, Th\u00e9ophile Gervet, Thibaut Lavril, Thomas Wang, Timoth\u00e9e Lacroix, and William\u00a0El Sayed. 2024.\n\n\nMixtral of experts.\n\n\nPreprint, arXiv:2401.04088.\n\n\n",
      "Jiang et\u00a0al. (2023)": "\nJiang et\u00a0al. (2023)\n\nJinhao Jiang, Kun Zhou, Zican Dong, Keming Ye, Xin Zhao, and Ji-Rong Wen. 2023.\n\n\nStructGPT: A general framework for large language model to reason over structured data.\n\n\nIn Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 9237\u20139251, Singapore. Association for Computational Linguistics.\n\n\n",
      "Kamradt (2023)": "\nKamradt (2023)\n\nGregory Kamradt. 2023.\n\n\nNeedle in a haystack - pressure testing LLMs.\n\n\nGitHub.\n\n\n",
      "Koreeda and Manning (2021a)": "\nKoreeda and Manning (2021a)\n\nYuta Koreeda and Christopher Manning. 2021a.\n\n\nContractNLI: A dataset for document-level natural language inference for contracts.\n\n\nIn Findings of the Association for Computational Linguistics: EMNLP 2021, pages 1907\u20131919, Punta Cana, Dominican Republic. Association for Computational Linguistics.\n\n\n",
      "Koreeda and Manning (2021b)": "\nKoreeda and Manning (2021b)\n\nYuta Koreeda and Christopher\u00a0D. Manning. 2021b.\n\n\nContractnli: A dataset for document-level natural language inference for contracts.\n\n\nPreprint, arXiv:2110.01799.\n\n\n",
      "Kornilova and Eidelman (2019)": "\nKornilova and Eidelman (2019)\n\nAnastassia Kornilova and Vladimir Eidelman. 2019.\n\n\nBillSum: A corpus for automatic summarization of US legislation.\n\n\nIn Proceedings of the 2nd Workshop on New Frontiers in Summarization, pages 48\u201356, Hong Kong, China. Association for Computational Linguistics.\n\n\n",
      "Ko\u010disk\u00fd et\u00a0al. (2017)": "\nKo\u010disk\u00fd et\u00a0al. (2017)\n\nTom\u00e1\u0161 Ko\u010disk\u00fd, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl\u00a0Moritz Hermann, G\u00e1bor Melis, and Edward Grefenstette. 2017.\n\n\nThe narrativeqa reading comprehension challenge.\n\n\nPreprint, arXiv:1712.07040.\n\n\n",
      "Ko\u010disk\u00fd et\u00a0al. (2018)": "\nKo\u010disk\u00fd et\u00a0al. (2018)\n\nTom\u00e1\u0161 Ko\u010disk\u00fd, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl\u00a0Moritz Hermann, G\u00e1bor Melis, and Edward Grefenstette. 2018.\n\n\nThe NarrativeQA Reading Comprehension Challenge.\n\n\nTransactions of the Association for Computational Linguistics, 6:317\u2013328.\n\n\n",
      "Kryscinski et\u00a0al. (2022)": "\nKryscinski et\u00a0al. (2022)\n\nWojciech Kryscinski, Nazneen Rajani, Divyansh Agarwal, Caiming Xiong, and Dragomir Radev. 2022.\n\n\nBOOKSUM: A collection of datasets for long-form narrative summarization.\n\n\nIn Findings of the Association for Computational Linguistics: EMNLP 2022, pages 6536\u20136558, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.\n\n\n",
      "Kry\u015bci\u0144ski et\u00a0al. (2022)": "\nKry\u015bci\u0144ski et\u00a0al. (2022)\n\nWojciech Kry\u015bci\u0144ski, Nazneen Rajani, Divyansh Agarwal, Caiming Xiong, and Dragomir Radev. 2022.\n\n\nBooksum: A collection of datasets for long-form narrative summarization.\n\n\nPreprint, arXiv:2105.08209.\n\n\n",
      "Kulkarni et\u00a0al. (2020)": "\nKulkarni et\u00a0al. (2020)\n\nSayali Kulkarni, Sheide Chammas, Wan Zhu, Fei Sha, and Eugene Ie. 2020.\n\n\nAquamuse: Automatically generating datasets for query-based multi-document summarization.\n\n\nPreprint, arXiv:2010.12694.\n\n\n",
      "Kuratov et\u00a0al. (2024)": "\nKuratov et\u00a0al. (2024)\n\nYuri Kuratov, Aydar Bulatov, Petr Anokhin, Dmitry Sorokin, Artyom Sorokin, and Mikhail Burtsev. 2024.\n\n\nIn search of needles in a 11m haystack: Recurrent memory finds what llms miss.\n\n\nPreprint, arXiv:2402.10790.\n\n\n",
      "Kwiatkowski et\u00a0al. (2019)": "\nKwiatkowski et\u00a0al. (2019)\n\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew\u00a0M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. 2019.\n\n\nNatural questions: A benchmark for question answering research.\n\n\nTransactions of the Association for Computational Linguistics, 7:452\u2013466.\n\n\n",
      "Levy et\u00a0al. (2024)": "\nLevy et\u00a0al. (2024)\n\nMosh Levy, Alon Jacoby, and Yoav Goldberg. 2024.\n\n\nSame task, more tokens: the impact of input length on the reasoning performance of large language models.\n\n\nPreprint, arXiv:2402.14848.\n\n\n",
      "Li et\u00a0al. (2023a)": "\nLi et\u00a0al. (2023a)\n\nDacheng Li, Rulin Shao, Anze Xie, Ying Sheng, Lianmin Zheng, Joseph\u00a0E. Gonzalez, Ion Stoica, Xuezhe Ma, and Hao Zhang. 2023a.\n\n\nHow long can open-source LLMs truly promise on context length?\n\n",
      "Li et\u00a0al. (2023b)": "\nLi et\u00a0al. (2023b)\n\nJiaqi Li, Mengmeng Wang, Zilong Zheng, and Muhan Zhang. 2023b.\n\n\nLoogle: Can long-context language models understand long contexts?\n\nPreprint, arXiv:2311.04939.\n\n\n",
      "Liu et\u00a0al. (2024a)": "\nLiu et\u00a0al. (2024a)\n\nHao Liu, Wilson Yan, Matei Zaharia, and Pieter Abbeel. 2024a.\n\n\nWorld model on million-length video and language with ringattention.\n\n\narXiv preprint arXiv:2402.08268.\n\n\n",
      "Liu et\u00a0al. (2024b)": "\nLiu et\u00a0al. (2024b)\n\nNelson\u00a0F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. 2024b.\n\n\nLost in the middle: How language models use long contexts.\n\n\nTransactions of the Association for Computational Linguistics, 12:157\u2013173.\n\n\n",
      "Liu et\u00a0al. (2023a)": "\nLiu et\u00a0al. (2023a)\n\nShuaiqi Liu, Jiannong Cao, Ruosong Yang, and Zhiyuan Wen. 2023a.\n\n\nLong text and multi-table summarization: Dataset and method.\n\n\nPreprint, arXiv:2302.03815.\n\n\n",
      "Liu et\u00a0al. (2023b)": "\nLiu et\u00a0al. (2023b)\n\nTianyang Liu, Canwen Xu, and Julian McAuley. 2023b.\n\n\nRepobench: Benchmarking repository-level code auto-completion systems.\n\n\nPreprint, arXiv:2306.03091.\n\n\n",
      "Malaviya et\u00a0al. (2024)": "\nMalaviya et\u00a0al. (2024)\n\nChaitanya Malaviya, Subin Lee, Sihao Chen, Elizabeth Sieber, Mark Yatskar, and Dan Roth. 2024.\n\n\nExpertQA: Expert-curated questions and attributed answers.\n\n\nIn Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 3025\u20133045, Mexico City, Mexico. Association for Computational Linguistics.\n\n\n",
      "Mohtashami and Jaggi (2023)": "\nMohtashami and Jaggi (2023)\n\nAmirkeivan Mohtashami and Martin Jaggi. 2023.\n\n\nLandmark attention: Random-access infinite context length for transformers.\n\n\nIn Workshop on Efficient Systems for Foundation Models@ ICML2023.\n\n\n",
      "Narayan et\u00a0al. (2018)": "\nNarayan et\u00a0al. (2018)\n\nShashi Narayan, Shay\u00a0B. Cohen, and Mirella Lapata. 2018.\n\n\nDon\u2019t give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization.\n\n\nIn Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1797\u20131807, Brussels, Belgium. Association for Computational Linguistics.\n\n\n",
      "Nguyen et\u00a0al. (2024)": "\nNguyen et\u00a0al. (2024)\n\nChau Nguyen, Phuong Nguyen, Thanh Tran, Dat Nguyen, An\u00a0Trieu, Tin Pham, Anh Dang, and Le-Minh Nguyen. 2024.\n\n\nCaptain at coliee 2023: Efficient methods for legal information retrieval and entailment tasks.\n\n\nPreprint, arXiv:2401.03551.\n\n\n",
      "OpenAI (2024)": "\nOpenAI (2024)\n\nOpenAI. 2024.\n\n\nGPT-4 technical report.\n\n\nPreprint, arXiv:2303.08774.\n\n\n",
      "Pal et\u00a0al. (2023)": "\nPal et\u00a0al. (2023)\n\nArka Pal, Deep Karkhanis, Manley Roberts, Samuel Dooley, Arvind Sundararajan, and Siddartha Naidu. 2023.\n\n\nGiraffe: Adventures in expanding context lengths in llms.\n\n\nPreprint, arXiv:2308.10882.\n\n\n",
      "Pang et\u00a0al. (2022)": "\nPang et\u00a0al. (2022)\n\nRichard\u00a0Yuanzhe Pang, Alicia Parrish, Nitish Joshi, Nikita Nangia, Jason Phang, Angelica Chen, Vishakh Padmakumar, Johnny Ma, Jana Thompson, He\u00a0He, and Samuel Bowman. 2022.\n\n\nQuALITY: Question answering with long input texts, yes!\n\nIn Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 5336\u20135358, Seattle, United States. Association for Computational Linguistics.\n\n\n",
      "Prasad et\u00a0al. (2023)": "\nPrasad et\u00a0al. (2023)\n\nArchiki Prasad, Trung Bui, Seunghyun Yoon, Hanieh Deilamsalehy, Franck Dernoncourt, and Mohit Bansal. 2023.\n\n\nMeetingQA: Extractive question-answering on meeting transcripts.\n\n\nIn Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 15000\u201315025, Toronto, Canada. Association for Computational Linguistics.\n\n\n",
      "Qian et\u00a0al. (2023)": "\nQian et\u00a0al. (2023)\n\nHongjing Qian, Yutao Zhu, Zhicheng Dou, Haoqi Gu, Xinyu Zhang, Zheng Liu, Ruofei Lai, Zhao Cao, Jian-Yun Nie, and Ji-Rong Wen. 2023.\n\n\nWebbrain: Learning to generate factually correct articles for queries by grounding on large web corpus.\n\n\nPreprint, arXiv:2304.04358.\n\n\n",
      "Rae et\u00a0al. (2019)": "\nRae et\u00a0al. (2019)\n\nJack\u00a0W. Rae, Anna Potapenko, Siddhant\u00a0M. Jayakumar, and Timothy\u00a0P. Lillicrap. 2019.\n\n\nCompressive transformers for long-range sequence modelling.\n\n\nPreprint, arXiv:1911.05507.\n\n\n",
      "Raffel et\u00a0al. (2020)": "\nRaffel et\u00a0al. (2020)\n\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter\u00a0J Liu. 2020.\n\n\nExploring the limits of transfer learning with a unified text-to-text transformer.\n\n\nJournal of machine learning research, 21(140):1\u201367.\n\n\n",
      "Reddy et\u00a0al. (2024)": "\nReddy et\u00a0al. (2024)\n\nVarshini Reddy, Rik Koncel-Kedziorski, Viet\u00a0Dac Lai, Michael Krumdick, Charles Lovering, and Chris Tanner. 2024.\n\n\nDocfinqa: A long-context financial reasoning dataset.\n\n\nPreprint, arXiv:2401.06915.\n\n\n",
      "Saunders et\u00a0al. (2022)": "\nSaunders et\u00a0al. (2022)\n\nWilliam Saunders, Catherine Yeh, Jeff Wu, Steven Bills, Long Ouyang, Jonathan Ward, and Jan Leike. 2022.\n\n\nSelf-critiquing models for assisting human evaluators.\n\n\nPreprint, arXiv:2206.05802.\n\n\n",
      "Shaham et\u00a0al. (2023)": "\nShaham et\u00a0al. (2023)\n\nUri Shaham, Maor Ivgi, Avia Efrat, Jonathan Berant, and Omer Levy. 2023.\n\n\nZeroSCROLLS: A zero-shot benchmark for long text understanding.\n\n\nIn Findings of the Association for Computational Linguistics: EMNLP 2023, pages 7977\u20137989, Singapore. Association for Computational Linguistics.\n\n\n",
      "Shaham et\u00a0al. (2022)": "\nShaham et\u00a0al. (2022)\n\nUri Shaham, Elad Segal, Maor Ivgi, Avia Efrat, Ori Yoran, Adi Haviv, Ankit Gupta, Wenhan Xiong, Mor Geva, Jonathan Berant, and Omer Levy. 2022.\n\n\nSCROLLS: Standardized CompaRison over long language sequences.\n\n\nIn Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 12007\u201312021, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.\n\n\n",
      "Sharma et\u00a0al. (2019)": "\nSharma et\u00a0al. (2019)\n\nEva Sharma, Chen Li, and Lu\u00a0Wang. 2019.\n\n\nBIGPATENT: A large-scale dataset for abstractive and coherent summarization.\n\n\nIn Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2204\u20132213, Florence, Italy. Association for Computational Linguistics.\n\n\n",
      "Stylianou et\u00a0al. (2021)": "\nStylianou et\u00a0al. (2021)\n\nNikolaos Stylianou, Panagiotis Kosmoliaptsis, and Ioannis Vlahavas. 2021.\n\n\nImproved biomedical entity recognition via longer context modeling.\n\n\nIn Artificial Intelligence Applications and Innovations: 17th IFIP WG 12.5 International Conference, AIAI 2021, Hersonissos, Crete, Greece, June 25\u201327, 2021, Proceedings 17, pages 45\u201356. Springer.\n\n\n",
      "Su et\u00a0al. (2024)": "\nSu et\u00a0al. (2024)\n\nJianlin Su, Murtadha Ahmed, Yu\u00a0Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. 2024.\n\n\nRoformer: Enhanced transformer with rotary position embedding.\n\n\nNeurocomputing, 568:127063.\n\n\n",
      "Takeshita et\u00a0al. (2024)": "\nTakeshita et\u00a0al. (2024)\n\nSotaro Takeshita, Tommaso Green, Ines Reinig, Kai Eckert, and Simone Ponzetto. 2024.\n\n\nACLSum: A new dataset for aspect-based summarization of scientific publications.\n\n\nIn Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 6660\u20136675, Mexico City, Mexico. Association for Computational Linguistics.\n\n\n",
      "Tay et\u00a0al. (2020)": "\nTay et\u00a0al. (2020)\n\nYi\u00a0Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler. 2020.\n\n\nLong range arena: A benchmark for efficient transformers.\n\n\nPreprint, arXiv:2011.04006.\n\n\n",
      "Trivedi et\u00a0al. (2022)": "\nTrivedi et\u00a0al. (2022)\n\nHarsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. 2022.\n\n\nMusique: Multihop questions via single-hop question composition.\n\n\nPreprint, arXiv:2108.00573.\n\n\n",
      "Tseng et\u00a0al. (2016)": "\nTseng et\u00a0al. (2016)\n\nBo-Hsiang Tseng, Sheng-Syun Shen, Hung-Yi Lee, and Lin-Shan Lee. 2016.\n\n\nTowards machine comprehension of spoken content: Initial toefl listening comprehension test by machine.\n\n\n",
      "Vaswani et\u00a0al. (2017)": "\nVaswani et\u00a0al. (2017)\n\nAshish Vaswani, Noam\u00a0M. Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan\u00a0N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017.\n\n\nAttention is all you need.\n\n\nIn Neural Information Processing Systems.\n\n\n",
      "Wang et\u00a0al. (2022)": "\nWang et\u00a0al. (2022)\n\nAlex Wang, Richard\u00a0Yuanzhe Pang, Angelica Chen, Jason Phang, and Samuel\u00a0R. Bowman. 2022.\n\n\nSQuALITY: Building a long-document summarization dataset the hard way.\n\n\nIn Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 1139\u20131156, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.\n\n\n",
      "Williams et\u00a0al. (2018)": "\nWilliams et\u00a0al. (2018)\n\nAdina Williams, Nikita Nangia, and Samuel Bowman. 2018.\n\n\nA broad-coverage challenge corpus for sentence understanding through inference.\n\n\nIn Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 1112\u20131122, New Orleans, Louisiana. Association for Computational Linguistics.\n\n\n",
      "Wu et\u00a0al. (2024)": "\nWu et\u00a0al. (2024)\n\nYunshu Wu, Hayate Iso, Pouya Pezeshkpour, Nikita Bhutani, and Estevam Hruschka. 2024.\n\n\nLess is more for long document summary evaluation by llms.\n\n\nPreprint, arXiv:2309.07382.\n\n\n",
      "Yang et\u00a0al. (2018)": "\nYang et\u00a0al. (2018)\n\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William\u00a0W. Cohen, Ruslan Salakhutdinov, and Christopher\u00a0D. Manning. 2018.\n\n\nHotpotqa: A dataset for diverse, explainable multi-hop question answering.\n\n\nPreprint, arXiv:1809.09600.\n\n\n",
      "Zhang et\u00a0al. (2024a)": "\nZhang et\u00a0al. (2024a)\n\nJiebin Zhang, Eugene\u00a0J. Yu, Qinyu Chen, Chenhao Xiong, Dawei Zhu, Han Qian, Mingbo Song, Xiaoguang Li, Qun Liu, and Sujian Li. 2024a.\n\n\nRetrieval-based full-length wikipedia generation for emergent events.\n\n\nPreprint, arXiv:2402.18264.\n\n\n",
      "Zhang et\u00a0al. (2024b)": "\nZhang et\u00a0al. (2024b)\n\nXinrong Zhang, Yingfa Chen, Shengding Hu, Zihang Xu, Junhao Chen, Moo\u00a0Khai Hao, Xu\u00a0Han, Zhen\u00a0Leng Thai, Shuo Wang, Zhiyuan Liu, and Maosong Sun. 2024b.\n\n\n\u221e\\infty\u221ebench: Extending long context evaluation beyond 100k tokens.\n\n\nPreprint, arXiv:2402.13718.\n\n\n",
      "Zhao et\u00a0al. (2022)": "\nZhao et\u00a0al. (2022)\n\nYilun Zhao, Yunxiang Li, Chenying Li, and Rui Zhang. 2022.\n\n\nMultiHiertt: Numerical reasoning over multi hierarchical tabular and textual data.\n\n\nIn Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 6588\u20136600, Dublin, Ireland. Association for Computational Linguistics.\n\n\n",
      "Zhao et\u00a0al. (2023)": "\nZhao et\u00a0al. (2023)\n\nYilun Zhao, Yitao Long, Hongjun Liu, Linyong Nan, Lyuhao Chen, Ryo Kamoi, Yixin Liu, Xiangru Tang, Rui Zhang, and Arman Cohan. 2023.\n\n\nDocmath-eval: Evaluating numerical reasoning capabilities of llms in understanding long documents with tabular data.\n\n\nArXiv, abs/2311.09805.\n\n\n",
      "Zhong et\u00a0al. (2021)": "\nZhong et\u00a0al. (2021)\n\nMing Zhong, Da\u00a0Yin, Tao Yu, Ahmad Zaidi, Mutethia Mutuma, Rahul Jha, Ahmed\u00a0Hassan Awadallah, Asli Celikyilmaz, Yang Liu, Xipeng Qiu, and Dragomir Radev. 2021.\n\n\nQMSum: A new benchmark for query-based multi-domain meeting summarization.\n\n\nIn Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 5905\u20135921, Online. Association for Computational Linguistics.\n\n\n",
      "Zhou et\u00a0al. (2023)": "\nZhou et\u00a0al. (2023)\n\nYijie Zhou, Kejian Shi, Wencai Zhang, Yixin Liu, Yilun Zhao, and Arman Cohan. 2023.\n\n\nOdsum: New benchmarks for open domain multi-document summarization.\n\n\nPreprint, arXiv:2309.08960.\n\n\n"
    },
    "domain": "cs.CL",
    "citation_count": 0
  },
  "c1212342-e41b-4cfe-8655-06eb75680aa8": {
    "pk": "c1212342-e41b-4cfe-8655-06eb75680aa8",
    "authors": [
      "Tianyu Gao",
      "Alexander Wettig",
      "Howard Yen",
      "Danqi Chen"
    ],
    "title": "How to Train Long-Context Language Models (Effectively)",
    "abstract": "We study continued training and supervised fine-tuning (SFT) of a language model (LM) to make effective use of long-context information. We first establish a reliable evaluation protocol to guide model development -- Instead of perplexity or simple needle-in-a-haystack (NIAH) tests, we use a broad set of long-context tasks, and we evaluate models after SFT with instruction data as this better reveals long-context abilities. Supported by our robust evaluations, we run thorough experiments to decide the data mix for continued pre-training, the instruction tuning dataset, and many other design choices. We find that (1) code repositories and books are excellent sources of long data, but it is crucial to combine them with high-quality short data; (2) training with a sequence length beyond the evaluation length boosts long-context performance; (3) for SFT, using only short instruction datasets yields strong performance on long-context tasks. Our final model, ProLong-8B, which is initialized from Llama-3 and trained on 40B tokens, demonstrates state-of-the-art long-context performance among similarly sized models at a length of 128K. ProLong outperforms Llama-3.18B-Instruct on the majority of long-context tasks despite having seen only 5% as many tokens during long-context training. Additionally, ProLong can effectively process up to 512K tokens, one of the longest context windows of publicly available LMs.",
    "url": "http://arxiv.org/abs/2410.02660v1",
    "timestamp": 1727974012,
    "sections": {
      "1 Introduction": "\n\n1 Introduction\n\nThe ability of language models (LMs) to process extremely long inputs (for example, 128K tokens) has enabled new applications, such as book summarization or learning new tasks on the fly from many examples. However, adapting\nLMs to process long contexts is challenging from an infrastructure and data perspective, and many design decisions are not well understood by open-source practitioners.\n\n\nWhile many works have focused on extending the context length of pre-trained LMs with minimal training (Chen et\u00a0al., 2023; Peng et\u00a0al., 2024),\nFu et\u00a0al. (2024) find that\nthe above methods cannot even perform the simple needle-in-a-haystack (NIAH; Kamradt, 2024) task\nand it is necessary to continually train the LM on long documents for billions of tokens. Frontier open-source models, such as Llama-3.1\u00a0(Dubey et\u00a0al., 2024) and Jamba\u00a0(Lenz et\u00a0al., 2024), also employ a long-context continued training stage, followed by supervised fine-tuning (SFT) on instruction data.\nWe adopt the same setting and study continued training and SFT of a pre-trained LM for effective long-context use.\n\n\nWe first establish a reliable evaluation protocol to provide a meaningful signal for model development.\nMost existing works rely on either perplexity or NIAH for ablating training recipes.\nWe demonstrate that neither is robust for guiding the development\nand opt for a broad range of downstream applications, such as retrieval-augmented generation (RAG), long-document summarization, and many-shot in-context learning (ICL).\nImportantly, we also conduct our evaluations after performing SFT,\neven for all our ablation runs on continued pre-training.\nWe observe that, on some long-context tasks, performance gains only emerge after SFT, which means that best design choices can differ before and after SFT.\n\n\nGuided by our evaluation protocol, we run comprehensive experiments with Llama-3-8B\u00a0(8K original context window; Dubey et\u00a0al., 2024) to study each component of long-context continued training, including data mixture, data and length scaling, supervised fine-tuning, and many other design choices such as cross-document attention masking and position extrapolation.\nMany of our findings are surprising or contradictory to existing claims, for example, (1) training only on long data hurts long-context performance,\n(2) training on longer sequences than the evaluation length helps, and (3) SFT on only short instruction data is sufficient for good long-context performance.\nWe outline our main takeaways and the structure of the paper in the takeaway box at the beginning of this section.\n\n\nOur final model, ProLong, achieves the best performance at a 128K context length among 10B-parameter models, while taking only 5%percent55\\%5 % of the data budget compared to Llama-3.1\u2019s long-context training (Dubey et\u00a0al., 2024).\nProLong has a maximum context length of 512K tokens, making it one of the longest-context LMs available.222Throughout the paper, we use binary prefixes K=210absentsuperscript210=2^{10}= 2 start_POSTSUPERSCRIPT 10 end_POSTSUPERSCRIPT, M=220superscript2202^{20}2 start_POSTSUPERSCRIPT 20 end_POSTSUPERSCRIPT, and B=230superscript2302^{30}2 start_POSTSUPERSCRIPT 30 end_POSTSUPERSCRIPT.\n\n",
      "2 Guide Model Development With Meaningful Evaluations": "\n\n2 Guide Model Development With Meaningful Evaluations\n\nA pre-requisite for training a strong LM is having a robust evaluation suite that can guide model development while tracking its utility in real-world applications.\nWhile synthetic benchmarks like needle-in-a-haystack (NIAH; Kamradt, 2024) and RULER (Hsieh et\u00a0al., 2024) have gained much popularity due to their simplicity and controllability, we are interested in a wider range of tasks that reflect practical usage, such as the ability to reason over the whole document.\nIn the following, we describe our evaluation protocols and\nshowcase why they are critical to our model development.\n\n\n\n2.1 Evaluate on diverse and realistic tasks\n\nWe first make the decision to use HELMET (Yen et\u00a0al., 2024b) as our evaluation suite,\nas it is one of the most comprehensive long-context benchmarks, covering the following tasks:\n\n\n\u2022\n\nRecall: Given a JSON file with random key-values pairs, retrieve the value for a key.\n\n\n\n\u2022\n\nRAG: Answer a question given retrieved Wikipedia documents (NQ, HotPotQA, PopQA).\n\n\n\n\u2022\n\nRe-ranking: Produce top-10 rankings from a shuffled list of documents (MSMARCO).\n\n\n\n\u2022\n\nICL: Learn classification tasks from many in-context examples, where the #classes ranges from 6 to 151; average of 5 datasets (TREC coarse/fine, NLU, Banking77, Clinc-150).\n\n\n\n\u2022\n\nQA: Answer a question given a full-length book (NarrativeQA).\n\n\n\n\u2022\n\nSummarization: Summarize long legal documents (Multi-LexSum).\n\n\n\n\n\nOverall, these diverse tasks reflect a range of long-context abilities including recall, reasoning, learning from context, and robustness to noisy inputs.\nYen et\u00a0al. (2024b) also show that HELMET produces model performance trends that are more consistent with human perceptions unlike other long-context benchmarks.\n\n\nWe showcase the importance of a robust evaluation suite in Table\u00a01.\nAs a predecessor of our work, Fu et\u00a0al. (2024) only consider needle-in-a-haystack (NIAH) and perplexity during model development; evaluations on 3 tasks from HELMET reveal major short-comings of their models despite perfect NIAH scores.\nWe also see how NIAH and even the HELMET recall task become saturated for strong models (Llama-3.1-8B vs. 70B) while other task categories continue to detect differences in their long-context abilities.\n\n\n\u2005\n\n\n\nTable 1: HELMET offers a more holistic long-context evaluation. We reproduce Fu et\u00a0al. (2024) on Llama-3-8B with SFT. We report the instruct Llama versions.\n\n\n\n\n\n\n\n\nHELMET\n\n\nModels\nNIAH\nRecall\nRAG\nRe-rank\n\n\nFu et\u00a0al. (2024)\n100\n95.8\n52.1\n23.1\n\n\nLlama-3.1-8B\n100\n99.4\n56.3\n37.0\n\n\nLlama-3.1-70B\n100\n100\n62.1\n49.2\n\n\n\n\n\n\nWe offer more details about the HELMET evaluation, including its careful choice of metrics, in \u00a7A.1.\nWe did not use all tasks from HELMET for efficiency reasons and we also do not want to overfit to HELMET.\nIf not otherwise specified, we average the performance for each category over all datasets and over evaluation lengths of 32K and 64K;\nfor the final long-context score, we macro-average all categories.\n\n\n\nWhy not perplexity?\n\nBesides synthetic recall tasks, many previous works rely on perplexity (PPL) for evaluating long-context extensions of LMs (Chen et\u00a0al., 2023; Fu et\u00a0al., 2024; Lu et\u00a0al., 2024), which is commonly measured on the PG19 books dataset\u00a0(Rae et\u00a0al., 2020).\nWe use the ablation experiment from \u00a73.2 to showcase why perplexity is not an indicative metric for developing long-context models.\nThe experiment studies how the ratio of long documents affects the performance. We report both our evaluation and the perplexity measured on the last 32K tokens of 64K-length documents from PG19.\nAs shown in Figure\u00a01, while using more long data continues to improve PPL,\nit is clear that using 100% long data significantly hurts downstream long-context performance.\n\n\n\n\n\u2005\n\n\n\n\n\n\nFigure 1: Making design decisions based on perplexity (PPL) is not optimal for long-context downstream tasks.\n\n\n\n\n\n\n2.2 Evaluate after supervised fine-tuning\n\nSupervised fine-tuning (SFT; Ouyang et\u00a0al., 2022) is an additional training stage that fine-tunes the model on a small amount of natural-language instructions and corresponding responses; it enables a base LM to address user queries in a chat format and has become a standard step for producing frontier LMs.\nHere, we consider the difference between evaluating a model before or after SFT.\n\n\nIn preliminary experiments, we continue training Llama-3-8B-Base on 5B-token subsets from the data mix by Fu et\u00a0al. (2024). The mix is based on SlimPajama (Soboleva et\u00a0al., 2023) and upsamples long documents to constitute roughly 70% of tokens, while retaining the original domain proportions.\nThen we conduct SFT on several intermediate checkpoints with UltraChat\u00a0(Ding et\u00a0al., 2023).\n\n\nWe show the benchmarking results before and after SFT in Figure\u00a02.\nLong-context evaluation shows clearer signals when it is conducted after SFT:\n(1) SFT shows that the model continues to improve with more training tokens on RAG and re-ranking, while the improvement is less clear or does not exist when evaluated before SFT.\n(2) SFT enables evaluation on realistic applications like QA and summarization, which require instruction following and have low performance before SFT.\nWe also note that the variance from two random training runs is not substantially higher after the additional SFT phase. Therefore, unless otherwise specified, we report the long-context performance after SFT.\n\n\nFigure 2: \nImprovements on RAG and re-ranking tasks are only observed when evaluating models after a supervised fine-tuning (SFT) phase on instruction data. The models are trained on the pre-training data mix by Fu et\u00a0al. (2024). We report the mean and standard deviations over two training runs. \n\n\nWe dive deeper into supervised fine-tuning in \u00a75 and explore different training datasets, as well as the use of synthetic long instruction data. However, we find that simply fine-tuning on UltraChat remains a surprisingly competitive choice.\n\n\n\n\n2.3 Check that short-context performance is preserved\n\nLong-context abilities should not come at the expense of short-context performance, particularly since short-context evaluations cover a wider range of capabilities, e.g., world knowledge, commonsense, and mathematical reasoning.\nHowever, short-context evaluation has largely been neglected by previous long-context research.\nWe report on 5 tasks from the the Open LLM Leaderboard (Beeching et\u00a0al., 2023): HellaSwag (Zellers et\u00a0al., 2019), MMLU (Hendrycks et\u00a0al., 2021), ARC-challenge (Clark et\u00a0al., 2018), WinoGrande (Sakaguchi et\u00a0al., 2021), and GSM8K (Cobbe et\u00a0al., 2021).\nWe evaluate short-context performance before SFT, since this allows for a direct comparison to the base model which was used as initialization for the long-context training.\n\n\n\nPrevious techniques deteriorate short-context performance.\n\nWe show in Table\u00a02 that both training-free position extrapolation, as well as fine-tuning with an existing long data mixture (Fu et\u00a0al., 2024)\ndo not preserve the strong performance of Llama-3-8B on standard short-context tasks.\nThis motivates us to find data sources which retain the initial model\u2019s strong short-context performance.\n\n\n\n\n\u2005\n\n\n\nTable 2: \nApplying position extrapolation (PE) to\nLlama-3-8B by changing the RoPE frequency base (\u00a7B.1)\nor fine-tuning it\non a long-context SlimPajama mixture (Fu et\u00a0al., 2024; Soboleva et\u00a0al., 2023) deteriorates the performance of this top-shelf pre-trained LM on short-context tasks.\n\n\n\n\n\n\n\nHSwag\nMMLU\nARC-c\nWG\nGSM8K\n\n\n\n\nLlama-3-8B\n82.1\n66.5\n59.4\n77.1\n44.7\n\n\n+ PE\n81.5\n64.7\n58.1\n75.5\n40.1\n\n\n+ SlimPajama\n81.0\n63.1\n57.8\n75.1\n40.6\n\n\n\n\n\n\n",
      "3 Long-Context Data Curation": "\n\n3 Long-Context Data Curation\n\nThe quality and composition of training data has been found to be the most important factor for LM pre-training (Penedo et\u00a0al., 2023; Wettig et\u00a0al., 2024; Li et\u00a0al., 2024a) and is therefore\na primary focus of our study. To make data decisions, we perform ablation experiments:\nwe continue to train Llama-3-8B-Base for 5B tokens with a maximum length of 64K tokens and evaluate according to \u00a72. See \u00a7A.4 for more details of our ablation setting.\n\n\nWe aim to boost the long-context task performance while preserving the short-context performance of the original model. Starting from the intuition that the data should be a mixture of long and short documents, we study these choices separately.\nIn our ablations, the long data is comprised of single-document chunks of 64K tokens, whereas for the short data, we construct batches by packing documents until we reach 64K tokens per sequence.\n\n\n\n3.1 Code repositories and books are good sources of long-context data\n\n\nSlimPajama.\n\nWe analyze the quantity of long data in SlimPajama (SP; Soboleva et\u00a0al., 2023). Table\u00a03 shows that books account for the majority of long-context tokens.\nWhen inspecting the long data in CommonCrawl (CC), we observe that though varied in quality, it also contains some book-like content, which future work could identify via data selection methods.\n\n\n\nCode repositories.\n\nWhile only few files from GitHub reach a very long length (which also tend to be lower quality as suggested by Singh et\u00a0al., 2024), we construct an abundant source of long-context data from the Stack (Kocetkov et\u00a0al., 2023) by concatenating all files from a repository to form a single document. Unlike Guo et\u00a0al. (2024), we do not order the files based on dependencies, which should increase the distance between dependent files and reduce recency bias.\n\n\n\n\n\u2005\n\n\n\nTable 3: Long text documents (\u2265\\geq\u226564K tokens) by data sources.\n\n\n\n\n\nData\n#Long\n\n\ntokens\n\n\nCode Repos\n98.8B\n\n\nSP/Books\n33.2B\n\n\nSP/CC\n15.3B\n\n\nSP/Arxiv\n5.2B\n\n\nSP/GitHub\n2.8B\n\n\nSP/Wiki\n0.1B\n\n\nSP/StackEx\n\n<<<0.1B\n\n\nSP/C4\n\n<<<0.1B\n\n\n\n\n\n\nData mixture.\n\nWe train models with 60% of long-context data and 40% of our ShortMix (\u00a73.3).\nTable\u00a04 shows that using code repositories alone performs the best on stress-test recall tasks. Meanwhile, books are more broadly beneficial for in-context learning, summarization and re-ranking.\nAn equal mix of books and code repositories achieves the best overall performance.\nNote that short-context task performance remains consistent due to our high-quality short data mix.\n\n\nTable 4: Impact of different long data sources, while keeping the 40% short data component fixed. Code repositories particularly helps recall, while books are more effective on re-ranking, ICL, and summarization. Mixing the two achieves the overall best performance.\n\n\n\nLong Data (60%)\nLong-Context\nShort-Context\n\n\nRecall\nRAG\nRe-rank\nICL\nQA\nSumm.\nAvg.\nAvg.\n\n\n\n\nCommonCrawl\n84.1\n53.3\n28.1\n67.5\n35.2\n37.0\n50.9\n66.5\n\n\nBooks\n94.9\n53.9\n30.7\n72.2\n33.2\n37.7\n53.8\n65.5\n\n\nCode Repos\n99.2\n53.8\n29.0\n61.2\n34.7\n36.2\n52.3\n65.9\n\n\nBooks/Repos 1:1\n96.0\n54.9\n29.4\n73.9\n35.7\n37.9\n54.6\n65.5\n\n\n\n\n\n\n\n\n3.2 Training only on long data hurts long-context performance\n\nThe ratio between short/long data is another crucial factor for downstream performance.\nPrior work either trains only on long data (Peng et\u00a0al., 2024) or adds some short training data (Yen et\u00a0al., 2024a; Fu et\u00a0al., 2024).\nHowever, we are the first to systematically study the impact of short/long ratio.\n\n\nFigure\u00a03 shows that\nshort task performance monotonically decreases as the long data increases.\nThe trends for long-context vary by tasks and are further complicated by SFT:\nOn tasks like recall and RAG, the performance before SFT prefers high proportions of long data, while the performance after SFT drastically deteriorates with more long data.\nWe hypothesize that specializing the model only on long data makes it a poor initialization for generic SFT\u2014highlighting the importance of evaluating checkpoints after SFT (\u00a72.2).\nWhile some long-context tasks benefit from more long data consistently (ICL) or show no clear pattern (re-ranking), the best average performance is achieved at 60% long data and 40% short data, which we adopt for our final ProLong model.\n\n\nFigure 3: \nImpact of short/long data ratio.\nAll models are trained on books/repos long data and our ShortMix for 5B tokens.\nMore long data initially improves long-context performance, but then\nbecomes impairing. More long data also consistently degrades the short-context performance.\n\n\n\n\n\n3.3 Choosing a high-quality short-context mix is important\n\nWe saw in \u00a72.3 that it is difficult to preserve the strong performance of Llama-3-8B on short-context tasks during long-context fine-tuning.\nWe adopt our best long-context settings (Book/repo data and 60% long/40% short) and study the impact of different short-context training mixes. We experiment with SlimPajama (Soboleva et\u00a0al., 2023), FineWeb-Edu (Penedo et\u00a0al., 2024), DCLM-Baseline (Li et\u00a0al., 2024a), and our own ProLong ShortMix.\nOur ShortMix is inspired by the \u201cstage 2 training\u201d in MiniCPM\u00a0(Hu et\u00a0al., 2024a) and Dolma-1.7 (Soldaini et\u00a0al., 2024), which use more knowledge-intensive, downstream-related data at the end of pre-training.\nTable\u00a05 shows the composition of our ShortMix.333\nSince we do not truncate documents in the short data component unnecessarily, it includes a small percentage of documents longer than 8K. See Table\u00a014 in the appendix for the dataset length statistics.\n\n\n\u2005\n\n\n\nTable 5: Our ShortMix.\n\n\n\n\n\nComponents\n%\n\n\n\n\nFineWeb\n25\n\n\nFineWeb-Edu\n25\n\n\nWikipedia\n10\n\n\nTulu-v2\n10\n\n\nStackExchange\n10\n\n\nArXiv\n10\n\n\nOpenWebMath\n10\n\n\n\n\n\n\nTable 6: Impact of\ndifferent short data sources.\nThe long-context performance is the average of 6 categories at the lengths of 32K and 64K.\n\n\n\n\nShort Data (40%)\nLong-Context\nShort-Context\n\n\nAvg.\nHellaS.\nMMLU\nARC-c\nWG\nGSM8K\nAvg.\n\n\nOriginal model (Llama-3-8B)\n-\n82.1\n66.5\n59.4\n77.1\n44.7\n66.0\n\n\n\n\nSlimPajama\n52.9\n81.2\n63.0\n58.5\n76.2\n41.9\n64.2\n\n\nFineWeb-Edu\n53.0\n81.0\n62.6\n57.7\n74.4\n39.4\n63.0\n\n\nDCLM-Baseline\n52.0\n82.0\n65.6\n59.6\n77.4\n39.4\n64.8\n\n\nProLong ShortMix\n54.6\n81.6\n65.3\n58.0\n76.2\n46.6\n65.5\n\n\n\n\n\nTable\u00a06 demonstrates that the short data component has a substantial impact on both short-context and long-context downstream performance.\nOur curated ShortMix outperforms other short data sources on both short and long-context tasks and our data domains are particularly important for retaining Llama-3-8B\u2019s performance on mathematical reasoning.\nSurprisingly, we find that fine-tuning only using FineWeb-Edu\u2014a dataset that is curated to help with knowledge-intensive tasks like MMLU\u2014performs poorly as a short-context component, and we combine it with more diverse data sources in our ShortMix.\nDCLM-Baseline performs well on all short-context tasks except for GSM8K.\nThis can likely be improved by combining with math-related datasets, but as we added the DCLM-baseline ablation at the conclusion of the project, we leave this exploration to future work.\n\n\n",
      "4 Scaling the Size and Length of the Training Data": "\n\n4 Scaling the Size and Length of the Training Data\n\nTraining for more steps is well-known to improve downstream tasks in regular pre-training, but little analysis has been done in the context of long-context continued training.\nWe incorporate the lessons from our ablation experiments and arrive at the ProLong recipe, which we describe in detail in \u00a76. Notably, we scale up the training budget to longer sequences (up to 512K) and more tokens (20B tokens at a maximum sequence length of 64K and an additional 20B tokens at 512K). We reset the learning rate schedule and increase the RoPE frequency base when switching from 64K to 512K context lengths.\nIn this section, we analyze the impact of these decisions.\n\n\nIncreasing the number of steps helps.\n\nIn Figure\u00a04, we plot the downstream performance of intermediate checkpoints of our 40B-token runs.\nWhile the long-context performance fluctuates throughout training, we observe positive trends on recall, RAG, re-ranking, and summarization.\nFor short-context tasks, we observe the average performance initially drops from the initialization, but gradually recovers. Performance again drops when switching from 64K to 512K sequence length, but also recovers with additional training.\n\n\nFigure 4: Performance (avg. of 32K and 64K) of our ProLong model throughout training.\n\n\n\n\nIncreasing the training length beyond the evaluation length helps.\n\nOne might assume that we should train long-context models on the maximum sequence length that we want the model to support. Many works even emphasize extrapolation to even longer sequences at inference time (Press et\u00a0al., 2022; Xiao et\u00a0al., 2024b; a; Yen et\u00a0al., 2024a; Chen et\u00a0al., 2023).\nIn contrast, we observe that training on a longer sequence length (512K tokens) substantially improves the long-context performance at a shorter evaluation length (64K tokens).\n\n\nWe establish this by initializing\nwith a model that was trained for 20B tokens at 64K and either (1) continuing training at 64K, or (2) switching to the 512K training.\nWe use the same hyperparameters and data mixtures in either experiment.\nWe evaluate a checkpoint after 4B training tokens\nat a evaluation length of 64K.\nComparing the two runs\nin Table\u00a07,\nwe see consistent gains from switching to the 512K training length.444While we demonstrate the benefit of longer data, we note that training with longer sequences is more expensive, and may therefore not be the computationally optimal choice.\n\n\nTable 7: Impact of training models on different sequence lengths.\nAll the results are evaluated at a sequence length of 64K.\nWe see that training at a maximum length beyond the evaluation context window consistently improves the long-context performance.\n\n\n\n\nMax Seq. Length\nRecall\nRAG\nRe-rank\nICL\n\n\n\n\nProLong 64K training (20B)\n96.5\n52.7\n22.8\n70.6\n\n\n+4B 64K training\n95.0\n56.4\n28.0\n78.8\n\n\n+4B 512K training\n98.5\n56.9\n32.9\n79.2\n\n\n\n\n\n",
      "5 Supervised Fine-Tuning for Long-Context LMs": "\n\n5 Supervised Fine-Tuning for Long-Context LMs\n\nIn this section, we study how to best enable long-context language models to follow instructions.\nWe focus on supervised fine-tuning on instruction datasets (Ouyang et\u00a0al., 2022) and\nleave reinforcement learning and preference optimization for future work.\n\n\nAll our experiments in this section use the ProLong base model, which was\ntrained for\n40B tokens at a maximum sequence length of 512K.\nIn comparison, open-source instruction data are very short, e.g.,\nUltraChat (Ding et\u00a0al., 2023) conversations have 1.2K tokens on average and 4.1K tokens maximum.\nTo bridge this gap, several works\u00a0(Xiong et\u00a0al., 2023; Dubey et\u00a0al., 2024; Xiong et\u00a0al., 2024)\nhave proposed to generate long instruction data synthetically.\n\n\nWe consider three popular SFT datasets\u2014UltraChat\u00a0(Ding et\u00a0al., 2023), Tulu-v2\u00a0(Ivison et\u00a0al., 2023), ShareGPT555https://huggingface.co/datasets/RyokoAI/ShareGPT52K.\u2014and three sources of synthetic data: For synthetic QA, we prompt Llama-3-8B-Instruct\nto generate a question-and-answer pair given a random chunk from a long document; we reuse the QA pairs for synthetic RAG but we present\na random list of chunks from the document to mimic retrieved passages; for synthetic summarization, we generate summaries for long books via recursive summarization (Wu et\u00a0al., 2021).\nFor all synthetic data, we write several templates, which we sample at random to increase diversity. More details can be found in \u00a7A.5.\nWe always use a combination of 40% synthetic QA, 30% synthetic RAG, and 30% synthetic summarization in our synthetic instruction dataset.\nThe hyperparameters for the instruction tuning experiments can be found in Table\u00a09.\n\n\nShort-context instruction data yields strong long-context results.\n\nWe first establish that UltraChat outperforms Tulu-v2 and ShareGPT\nin Table\u00a022.\nWe therefore use it when studying the ratio of synthetic long-context instruction data in Table\u00a08.\nSurprisingly, we find that adding synthetic data does not improve the performance on these very long-context tasks, and adding even as little as 1% synthetic data hurts the performance in our setting.\nTherefore, we use only short-context UltraChat data for SFT of our final ProLong model.\n\n\nTable 8: Effect of different ratios of synthetic SFT data (mixed with UltraChat). We report the 32K-and-64K-averaged performance except tasks marked with \u2020, which are evaluated at 512K for stress testing. The number of percentage is based on #tokens, not #samples.\n\n\n\n% Synthetic Data\nJsonKV\u2020\n\nRAG\nRe-rank\nICL\nQA\u2020\n\nSumm.\u2020\n\nAvg.\n\n\n\n\n0%\n65.7\n58.1\n38.5\n80.3\n49.7\n42.1\n55.7\n\n\n1%\n61.5\n57.0\n38.3\n80.8\n45.3\n41.5\n54.1\n\n\n3%\n62.0\n56.4\n37.9\n80.6\n44.8\n39.5\n53.5\n\n\n10%\n70.3\n55.5\n36.1\n80.6\n41.7\n39.4\n53.9\n\n\n50%\n45.8\n48.8\n18.8\n70.5\n42.3\n33.3\n43.3\n\n\n\n\n\nWhy do our conclusions about synthetic data differ from previous work?\nWe offer the following hypotheses:\n(1) Previous work like Xiong et\u00a0al. (2024); Bai et\u00a0al. (2024a) may have insufficient long-context training and the synthetic data acts as additional long-context training data.\n(2) Our instruction dataset is much smaller compared to the private instruction data used for Llama-3.1 (Dubey et\u00a0al., 2024)\u2014it is possible that when using an extensive short instruction dataset, mixing in synthetic long data avoids the model from degenerating on long-context tasks.\n\n\nTable 9: The training recipe for ProLong.\n\n\n\nContinued Long-context Training\n\n\nData\n30% code repos, 30% books, 3% textbooks, 37% ShortMix\n\n\n\nShortMix:\n27% FineWeb-Edu, 27% FineWeb,\n\n\n\n\n11% Tulu-v2, 11% StackExchange,\n\n\n\n\n8% Wikipedia, 8% OpenWebMath, 8% ArXiv\n\n\n\n\nLength\nCurriculum\n\nStage 1 (64K):\nCode repos, books, and textbooks at length 64K\n\n\nStage 2 (512K):\nCode repos: 50% at length 512K, 50% at length 64K\n\n\n\n\nBooks: 17% at length 512K, 83% at length 64K\n\n\n\n\nTextbooks at length 512K\n\n\nSteps\nStage 1: 20B tokens (2.2K H100 hours), \u2005 Stage 2: 20B tokens (12.2K H100 hours)\n\n\nModel\nInitialization:\nLlama-3-8B-Instruct (original RoPE base freq. 5\u00d71055superscript1055\\times 10^{5}5 \u00d7 10 start_POSTSUPERSCRIPT 5 end_POSTSUPERSCRIPT)\n\n\n\nRoPE:\nStage 1: 8\u00d71068superscript1068\\times 10^{6}8 \u00d7 10 start_POSTSUPERSCRIPT 6 end_POSTSUPERSCRIPT, Stage 2: \u2005 1.28\u00d71081.28superscript1081.28\\times 10^{8}1.28 \u00d7 10 start_POSTSUPERSCRIPT 8 end_POSTSUPERSCRIPT\n\n\n\n\nAttention:\nFull attention with cross-document attention masking\n\n\nOptim.\nAdamW (weight decay = 0.10.10.10.1, \u03b21=0.9subscript\ud835\udefd10.9\\beta_{1}=0.9italic_\u03b2 start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT = 0.9, \u03b22=0.95subscript\ud835\udefd20.95\\beta_{2}=0.95italic_\u03b2 start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT = 0.95)\n\n\n\nLR:\n\n1\u2062e\u221251\ud835\udc5251e-51 italic_e - 5 with 10%percent1010\\%10 % warmup and cosine decay to 1\u2062e\u221261\ud835\udc5261e-61 italic_e - 6, each stage\n\n\n\nBatch size:\n4M tokens for stage 1, 8M tokens for stage 2\n\n\nSupervised Fine-tuning (SFT)\n\n\nData\nUltraChat\n\n\n\nSteps\n1B tokens\n\n\n\nOptim.\nAdamW (weight decay = 0.10.10.10.1, \u03b21=0.9subscript\ud835\udefd10.9\\beta_{1}=0.9italic_\u03b2 start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT = 0.9, \u03b22=0.95subscript\ud835\udefd20.95\\beta_{2}=0.95italic_\u03b2 start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT = 0.95)\n\n\n\nLR = 2\u2062e\u221252\ud835\udc5252e-52 italic_e - 5 (cosine decay to 2\u2062e\u221262\ud835\udc5262e-62 italic_e - 6), warmup = 5%percent55\\%5 %\n\n\n\n\nBatch size = 4M tokens\n\n\n\n\n\n",
      "6 The ProLong Model: Recipe and Results": "\n\n6 The ProLong Model: Recipe and Results\n\n\n6.1 Final recipe\n\nWe summarize the training recipe for ProLong in Table\u00a09.\nOur final model starts from the Llama-3-8B-Instruct model and is trained on 64K sequence length for 20B tokens.\nIt is then further trained on 512K sequence length for 20B tokens (ProLong base), which we achieve using sequence parallelism (Li et\u00a0al., 2023).\nWe obtain the final ProLong model via SFT of the base model on UltraChat.\nOne small difference on the data mixture between our ablations and the final model is that we mix in 3% high-quality textbooks\u00a0(Chevalier et\u00a0al., 2024), as book-like data are shown to be beneficial for long-context (\u00a73.1) and textbooks are highly educational. This also slightly changes the proportions of ShortMix.\nYou can find more details about our data processing (\u00a7A.2) and the training stack (\u00a7A.3) in the appendix.\n\n\nIn the following, we elaborate on several carefully ablated design choices in our recipe.\n\n\nRoPE frequency base tuning.\n\nWe find that changing the RoPE\u00a0(Su et\u00a0al., 2021) frequency base to achieve position extrapolation\u00a0(Xiong et\u00a0al., 2023; emozilla, 2023) significantly improves long-context performance, even with a significant amount of training. \u00a7B.1 shows our ablation on the best RoPE base to use. While the original Llama models use a RoPE base of 105superscript10510^{5}10 start_POSTSUPERSCRIPT 5 end_POSTSUPERSCRIPT, we use a base of 8\u00d71068superscript1068\\times 10^{6}8 \u00d7 10 start_POSTSUPERSCRIPT 6 end_POSTSUPERSCRIPT for the 64K setting and 1.28\u00d71081.28superscript1081.28\\times 10^{8}1.28 \u00d7 10 start_POSTSUPERSCRIPT 8 end_POSTSUPERSCRIPT for the 512K setting.\n\n\n\nDisabling cross-document attention.\n\nDing et\u00a0al. (2024a) show that masking out attention across document boundaries improve model performance and this was also used during Llama-3 pre-training (Dubey et\u00a0al., 2024).\nIn \u00a7B.2, we show that disabling cross-document attention in continued training benefits both the short and long-context performance.\nDisabling cross-document attention can also result in higher training throughput, which we describe in more detail in \u00a7A.3.\n\n\n\nStarting from Llama-3-8B-Instruct.\n\nWhile we conduct all our long-context training ablations with the base model of Llama-3-8B,\nwe use Llama-3-8B-Instruct as the initialization for the final ProLong model.\n\u00a7B.3 shows that while slightly improving the long-context performance, Llama-3-8B-Instruct significantly enhances the short-context performance.\n\n\n\n\n\n6.2 ProLong performance\n\nWe present the final HELMET evaluation results of ProLong in Table\u00a010.\nWe compare to a number of frontier long-context LMs, namely MegaBeam666https://huggingface.co/aws-prototyping/MegaBeam-Mistral-7B-512k., Llama-3.1\u00a0(Dubey et\u00a0al., 2024), Qwen2\u00a0(Yang et\u00a0al., 2024a), Phi-3\u00a0(Abdin et\u00a0al., 2024), Mistral-Nemo777https://huggingface.co/mistralai/Mistral-Nemo-Instruct-2407., Jamba-1.5\u00a0(Lenz et\u00a0al., 2024), Claude-3.5-Sonnet\u00a0(Anthropic, 2024), Gemini-1.5\u00a0(Reid et\u00a0al., 2024), and GPT-4o\u00a0(Achiam et\u00a0al., 2023).\n\n\nProLong outperforms all 10B-scale models on our long-context evaluation. Notably, ProLong outperforms Llama-3.1-8B-Instruct\non all categories except summarization.\nProLong achieves this with only 5% of Llama-3.1\u2019s long-context data budget (40B vs. 800B tokens).\nWe also showcase the strength of ProLong with several QA examples in Table\u00a023.\n\n\nTable 10: Our main evaluation results on HELMET (Yen et\u00a0al., 2024b; details in \u00a7A.1).\nAll results are averaged over sequence lengths of 32K, 64K, and 128K. For all models, we use the corresponding instruction version.\nProLong is one of the best performing 10B-scale LMs.\nThe complete set of results can be found in \u00a7C.\n\n\n\n\nModel\nMax Len.\nRecall\nRAG\nICL\nRe-rank\nQA\nSumm.\nAvg.\n\n\nProLong (8B)\n512K\n99.4\n66.0\n81.1\n33.2\n40.8\n40.5\n60.2\n\n\nMegaBeam-Mistral (7B)\n512K\n99.4\n58.1\n82.1\n22.1\n33.7\n43.6\n56.5\n\n\nMeta-Llama-3.1 (8B)\n128K\n98.7\n62.8\n79.7\n26.6\n40.4\n46.1\n59.0\n\n\nQwen2 (7B)\n128K\n34.4\n43.4\n54.8\n4.6\n23.3\n38.5\n33.2\n\n\nPhi-3-small (7B)\n128K\n74.8\n60.6\n82.0\n18.5\n34.1\n42.4\n52.1\n\n\nMistral-Nemo (12B)\n128K\n24.9\n48.1\n82.0\n4.7\n37.7\n37.0\n39.1\n\n\nJamba-1.5-Mini (12B/52B)\n256K\n87.7\n61.3\n88.4\n25.9\n42.0\n38.6\n57.3\n\n\nMeta-Llama-3.1 (70B)\n128K\n98.5\n65.9\n80.0\n39.4\n47.2\n51.1\n63.7\n\n\nClaude-3.5-Sonnet\n200K\n99.4\n44.0\n79.3\n19.9\n38.1\n49.2\n55.0\n\n\nGemini-1.5-Pro\n2M\n94.2\n71.4\n78.9\n65.3\n44.4\n56.2\n68.4\n\n\nGPT-4o\n128K\n99.9\n71.5\n86.7\n59.6\n47.0\n55.7\n70.1\n\n\n\n\n\nSince most existing models do not support more than 128K tokens,\nto showcase ProLong\u2019s 512K context length,\nwe stress test ProLong on the QA and summarization tasks from 32K to 512K888\nIn QA and summarization, we truncate the documents at the evaluation length; hence an effective long-context model should demonstrate better performance on longer lengths.. Table\u00a011 shows that ProLong continues to improve at a longer context window.\n\n\n\u2005\n\n\n\nTable 11: ProLong at 512K.\n\n\n\n\n\n\n32K\n64K\n128K\n512K\n\n\n\n\nQA\n31.7\n43.7\n46.7\n49.7\n\n\nSumm\n40.4\n39.8\n41.5\n42.1\n\n\n\n\n\n\nTable 12: Results\non the NoCha benchmark\u00a0(Karpinska et\u00a0al., 2024).101010https://github.com/marzenakrp/nocha. NoCha has a private test set and all evaluation is done by the NoCha authors. Hence, we report models from Table\u00a010 that are also on the NoCha leaderboard.\nProLong is the only model that achieves above-random performance in the <<<75K category and it consistently beats Llama-3.1. Different from the original NoCha leaderboard, we report the average accuracy over all test instances without filtering the test examples based on the model\u2019s context window lengths.\n\n\n\nModel\nMax Len.\n\n<<<75K\n75K-127K\n127K-180K\n\n>>>180K\n\n\n\n\nProLong (8B)\n512K\n28.4\n17.0\n13.1\n20.3\n\n\nMegaBeam-Mistral (7B)\n512K\n19.8\n18.3\n17.5\n15.6\n\n\nMeta-Llama-3.1 (8B)\n128K\n17.3\n16.4\n0.0\n0.0\n\n\nMistral-Nemo (12B)\n128K\n13.6\n0.4\n0.0\n0.0\n\n\nJamba-1.5-Mini (12B/52B)\n256K\n27.2\n28.0\n24.4\n6.2\n\n\nMeta-Llama-3.1 (70B)\n128K\n42.0\n25.0\n0.0\n0.0\n\n\nGemini-1.5-Pro\n2M\n24.7\n38.8\n35.3\n46.9\n\n\nGPT-4o\n128K\n55.6\n58.4\n0.0\n0.0\n\n\n\n\n\nBesides HELMET, we also evaluate our models on NoCha\u00a0(Karpinska et\u00a0al., 2024)\u2014a claim verification dataset on 67 recently published English fictional books.\nWe chose this dataset because (1) it minimizes the data contamination problem as all the books are unlikely to exist in the model pre-training data; (2) all the claims are written by human readers and require global reasoning. Each test instance contains two contradictory claims, and the models must correctly judge both to pass.\n\n\nfootnote\u00a010 demonstrates the NoCha evaluation results.\nAmong 10B-scale models, ProLong achieves the best accuracy on the extremely long test instances (>>>180K); on test instances <<<75K tokens, ProLong significantly outperforms other models and is the only model that is better than random guessing (25%).\nThis further showcases the strength of our training recipe and the ProLong model.\n\n\n",
      "7 Related Work": "\n\n7 Related Work\n\nAdapting existing LMs for long contexts.\n\nMany works explore\nextending the LM context windows with minimal training,\neither by\nposition extrapolation\u00a0(Chen et\u00a0al., 2023; Peng et\u00a0al., 2024; Chen et\u00a0al., 2024; Ding et\u00a0al., 2024b; Liu et\u00a0al., 2024a; Zhang et\u00a0al., 2024b; Zhu et\u00a0al., 2024; Zhao et\u00a0al., 2024; Wu et\u00a0al., 2024; Hu et\u00a0al., 2024b)\nor\nmanipulating the attention patterns\u00a0(Chen et\u00a0al., 2024; Xiao et\u00a0al., 2024b; a; Bertsch et\u00a0al., 2023; Jin et\u00a0al., 2024).\nYoshida et\u00a0al. (2020); Choromanski et\u00a0al. (2021); Chevalier et\u00a0al. (2023)\ninstead explore the idea of compressing the long contexts into shorter forms.\nHowever, Fu et\u00a0al. (2024); Lu et\u00a0al. (2024) show that using full attention, applying simple position extrapolation, and fine-tuning the model on long documents reach much stronger results.\n\n\nLlama 3.1\u00a0(Dubey et\u00a0al., 2024) and Jamba\u00a0(Lieber et\u00a0al., 2024) achieve long-context capabilities by adding a long-context continued training stage between standard pre-training and supervised fine-tuning, which is the setting we follow.\nFu et\u00a0al. (2024) study the data engineering for this setting and argue that 0.5B tokens of domain-balanced, length-upsampled data is sufficient for acquiring the long-context recall ability\u2014which we show is not sufficient if a more holistic evaluation is taken.\nXiong et\u00a0al. (2023); Dubey et\u00a0al. (2024); Lieber et\u00a0al. (2024); Xiong et\u00a0al. (2024); An et\u00a0al. (2024b); Bai et\u00a0al. (2024a) also adopt synthetically-generated long data in the SFT stage; however,\nwe find that\nusing standard, short-context instruction data achieves the best long-context results in our setting.\n\n\n\nEfficient long-context architectures.\n\nThere have been many efforts in designing more efficient architectures,\nfor example,\nlinear attention/RNNs\u00a0(Gu & Dao, 2023; Dao & Gu, 2024; Ma et\u00a0al., 2022; Sun et\u00a0al., 2023; Peng et\u00a0al., 2023; Yang et\u00a0al., 2024b),\nand\nalternative attention architectures\u00a0(Rubin & Berant, 2023; Sun et\u00a0al., 2024; Yen et\u00a0al., 2024a).\nHowever,\nthey often require training from scratch and many have the inherent limitations in terms of long-context recall (Jelassi et\u00a0al., 2024; Arora et\u00a0al., 2024).\nRecent works explore hybrid models (Waleffe et\u00a0al., 2024; Lieber et\u00a0al., 2024)) or distilling existing LMs into hybrid models (Wang et\u00a0al., 2024) and show promising results.\n\n\n\nLong-context evaluation.\n\nMany benchmarks have been proposed for long-context evaluation\n(Shaham et\u00a0al., 2023; Hsieh et\u00a0al., 2024; Krishna et\u00a0al., 2023; Zhang et\u00a0al., 2024a; An et\u00a0al., 2024a; Bai et\u00a0al., 2024b)\nThere are works studying particular aspects of long-context LMs as well,\nsuch as positional bias (Liu et\u00a0al., 2024b),\nin-context learning\u00a0(Bertsch et\u00a0al., 2024; Li et\u00a0al., 2024b), and book-length summarization\u00a0(Kim et\u00a0al., 2024).\nIn this work, we follow Yen et\u00a0al. (2024b)\nfor its diverse application coverage and reliable evaluations.\n\n\n",
      "8 Conclusion": "\n\n8 Conclusion\n\nWe study the problem of given a short-context pre-trained LM, how to\nmost effectively continually pre-train and SFT the model to be long-context.\nWe conduct thorough ablations on each component and\nmany of our findings contradict existing practices or beliefs.\nWe use all the findings to produce ProLong, a new state-of-the-art long-context LM. We release all our code, data, and models publicly\nand hope that our findings will\nboost research and applications of long-context LMs.\n\n\nLimitations\n\nAlthough we try to ablate the major components of our training recipe,\ndue to resource limitations, we cannot exhaust all aspects, such as the optimization hyperparameters and additional data mixtures.\nWe also limit ourselves to the 10B-scale regime and the Llama-3 models, which may limit the generalizability of our findings and recipe.\nAnother concern is that we are overfitting to the tasks chosen for model development\u2014however, we do not directly train on those datasets and guiding model development with benchmark tasks has become a common practice in pre-trained LM development. We also show that our final recipe and model perform well on an additional evaluation dataset, NoCha.\n\n\n\nAcknowledgments\n\nWe acknowledge Mengzhou Xia, Zexuan Zhong, Samyak Gupta, Dan Friedman, Yihe Dong, Abhishek Panigrah, Adithya Bhaskar, Colin Wang, Carlos Jimenez, and other members of Princeton Language and Intelligence for their helpful feedback and discussion. We also thank Luca Soldaini for providing comments on a draft.\nWe thank Marzena Karpinska, Tanya Goyal, and Mohit Iyyer for their help with the NoCha evaluation.\nTianyu Gao is supported by an IBM PhD Fellowship.\nThis work is gratefully supported by an NSF CAREER award (IIS-2239290), a grant from Intel, and Microsoft Azure credits through the \u201cAccelerate Foundation Models Academic Research\u201d Initiative.\n\n\n",
      "Appendix A Experiment Details": "\n\nAppendix A Experiment Details\n\n\nA.1 Evaluation\n\nTable 13: The details for our long-context evaluation following HELMET (Yen et\u00a0al., 2024b). \n\n\n\nCategory\nMetrics\n\n\nTasks and Datasets\n\n\n\n\n\n\nRecall\nSubEM\n\n\nGiven a randomly-generated long JSON file and a key, retrieve the corresponding value\u00a0(Liu et\u00a0al., 2024b).\n\n\n\n\nRAG\nSubEM\n\n\nGiven a question and many retrieved Wikipedia documents (shuffled), answer the question\u00a0(Liu et\u00a0al., 2024b). Datasets: NaturalQuestion\u00a0(Kwiatkowski et\u00a0al., 2019), HotpotQA\u00a0(Yang et\u00a0al., 2018), and PopQA\u00a0(Mallen et\u00a0al., 2023).\n\n\n\n\nRe-rank\nnDCG@10\n\n\nGiven a query and many retrieved documents (shuffled), re-rank the top-10 documents. Datasets: MSMARCO\u00a0(Bajaj et\u00a0al., 2016).\n\n\n\n\nICL\nAccuracy\n\n\nDatasets selected from Bertsch et\u00a0al. (2024): TREC coarse, TREC fine\u00a0(Hovy et\u00a0al., 2001), NLU\u00a0(Liu et\u00a0al., 2021), Banking77\u00a0(Casanueva et\u00a0al., 2020), and Clinc-150 (Larson et\u00a0al., 2019).\n\n\n\n\nQA\nGPT-4o score\n\n\nGiven a book, answer the question. Datasets (# tokens): NarrativeQA (medium: 73K; max: 518K; Ko\u010disk\u00fd et\u00a0al., 2018).\n\n\n\n\nSumm.\nGPT-4o score\n\n\nSummarize a given legal document. Datasets (# tokens): Multi-LexSum (medium: 90K; max: 5M; Shen et\u00a0al., 2022)\n\n\n\n\n\n\n\nTable\u00a013 shows all the datasets we used for the long-context evaluation from HELMET\u00a0(Yen et\u00a0al., 2024b).\nNote that we did not use all the datasets from HELMET for efficiency reasons and we also do not want to overfit to HELMET.\nWe highlight some of the evaluation protocol improvements that HELMET implemented compared to previous benchmarks here:\n\n\n\n\n\u2022\n\nSufficient context lengths and fine-grained control. HELMET can evaluate models at a context length of 128K tokens and beyond.\nThe evaluation protocol also allows for reporting results at different lengths, giving developers fine-trained controls for different needs of long contexts.\n\n\n\n\u2022\n\nBetter synthetic recall tasks. As shown in HELMET, needle-in-a-haystack\u00a0(Kamradt, 2024) is mostly saturated because of its simplicity\u2014the model only needs to find a needle in some irrelevant context. We instead use the more challenging JSON KV task, first proposed in Liu et\u00a0al. (2024b) and included in HELMET, where the model is required to find the corresponding value to a given key among a large JSON file.\n\n\n\n\u2022\n\nUsing class-balanced demonstrations and abstract labels for ICL. To disentangle models\u2019 ability of learning from demonstrations from their pre-training bias of the task or the dataset label distribution\u00a0(Pan et\u00a0al., 2023), HELMET samples the same number of demonstrations for each class and uses number labels (1, 2, \u2026) instead of natural-language labels (e.g., location, description, \u2026).\n\n\n\n\u2022\n\nModel-based evaluation for long-context QA and summarization. Instead of using traditional metrics like ROUGE (which has shown to be poorly indicative of the real model performance: Deutsch & Roth, 2021; Deutsch et\u00a0al., 2022; Goyal et\u00a0al., 2023; Chang et\u00a0al., 2024), HELMET uses model-based evaluations to compare the reference answer and the model output. For QA, HELMET uses GPT-4o to score the model output given the question and the reference answer at a 0-3 scale.\nFor summarization, HELMET takes a similar approach as Zhang & Bansal (2021); Gao et\u00a0al. (2023): it first uses GPT-4o to decompose the reference summary into atomic claims; then it uses GPT-4o to check whether each reference atomic claim is covered by the model output (recall) and whether each sentence in the model output is covered by the reference summary (precision). Yen et\u00a0al. (2024b) show that the model-based evaluation correlates with human perceptions significantly better than traditional metrics.\n\n\n\n\n\n\n\nA.2 Data processing\n\nData sources.\n\nWe list all the data sources we have explored in our ablations and main experiments here:\nthe Stack\u00a0(Kocetkov et\u00a0al., 2023),\nSlimPajama\u00a0(Together, 2023; Soboleva et\u00a0al., 2023),\nFineWeb (we use the 2023-50 snapshot), FineWeb-Edu (we use a random sample)\u00a0(Penedo et\u00a0al., 2024),\nTulu-v2\u00a0(Ivison et\u00a0al., 2023),\nOpenWebMath\u00a0(Paster et\u00a0al., 2024),\ntextbooks\u00a0(Chevalier et\u00a0al., 2024),\nand Dolma\u00a0(Soldaini et\u00a0al., 2024).\nThe Books, StackExchange, and ArXiv data are from SlimPajama. The Wikipedia data are from Dolma.\n\n\n\nData filtering and packing.\n\nFor the short training data and the SFT data, we randomly sample and concatenate the documents or conversations into 64K chunks. The last document for each chunk is truncated. The truncated part is used as the beginning for the next chunk for the short training data but is discarded for the SFT data.\nFor the long-context training data, we filter out the documents that are shorter than 64K; we do the same for the 512K setting, while making sure that the 64K documents packed to 512K length are distinct from the 512K documents.\n\n\n\nFinal data mixture.\n\nFor 512K length, we use a mix of 64K and 512K long data. For the ratio of 64K/512K data, we choose 50%/50% for code and 83%/17%, which are roughly chosen according to the natural availability of very long data, i.e., there are relatively fewer books of length 512K than code repositories. One benefit of retaining 64K-long documents is that we can process these without sequence parallelism and the associated communication overhead.\nWe use a slightly different long data mixture in our ablations\u00a0(Table\u00a05) and our main ProLong experiment\u00a0(Table\u00a09).\nFor the final model, we mix 3% textbooks into the long-context training data. The textbooks are open-source resources from libretexts.org, collected and made available by Chevalier et\u00a0al. (2024).\nWe pre-process the data by concatenating chapters from the same text books, as well as books from the same subject areas. This results in extremely long sequences which we pack into contexts of either 64K or 512K tokens.\nThough we do not have an ablation for adding this data due to limited resources, we believe that it should have a slight positive effect to the final model performance as textbooks are highly educational long-context data.\n\n\nTable 14: % Proportion of long documents for the short data components used in Table\u00a06. These statistics are computed after packing and truncation and therefore correspond to the document lengths as seen by the model. We highlight that the proportion of documents beyond 32K is below 1% for ShortMix.\n\n\n\n\n\n>>>4K\n\n>>>8K\n\n>>>16K\n\n>>>32K\n\n\n\u2005 FineWeb\n1.4\n0.3\n0.1\n0.0\n\n\n\u2005 FineWeb-Edu\n2.8\n0.8\n0.2\n0.0\n\n\n\u2005 Wikipedia\n1.6\n0.4\n0.0\n0.0\n\n\n\u2005 Tulu-v2\n0.0\n0.0\n0.0\n0.0\n\n\n\u2005 StackExchange\n0.6\n0.1\n0.0\n0.0\n\n\n\u2005 ArXiv\n85.7\n64.0\n30.3\n7.6\n\n\n\u2005 OpenWebMath\n11.1\n4.3\n1.2\n0.3\n\n\nShortMix\n10.9\n7.2\n3.2\n0.8\n\n\nSlimPajama\n11.3\n7.4\n4.9\n3.2\n\n\nFineWeb-Edu\n2.8\n0.8\n0.2\n0.0\n\n\nDCLM-Baseline\n4.9\n1.7\n0.4\n0.1\n\n\n\n\n\n\n\n\nA.3 Implementation details\n\nTechnical stack.\n\nWe use various open-source packages and tools for the ProLong training and evaluation.\nWe use PyTorch (Paszke et\u00a0al., 2019) and Hugging Face transformers\u00a0(Wolf et\u00a0al., 2020) for the model training.\nWe use mosaic-streaming (Mosaic ML, 2022) for loading and mixing the data\nand FlashAttention 2 (Dao, 2024) for efficient attention implementation.\nWe implement sequence parallelism based on DeepSpeed-Ulysses (Jacobs et\u00a0al., 2023) across groups of 8 GPUs on the same node. We only perform distributed attention if it is necessary, i.e., only on sequences of 512K length.\nFor long-context evaluation, we use HELMET\u00a0(Yen et\u00a0al., 2024b) and for short-context evaluation, we use lm-eval-harness (Gao et\u00a0al., 2021).\n\n\n\nAttention and batching.\n\nSince we do document masking in attention\u00a0(\u00a76),\nwe use the variable-length attention implementation from FlashAttention 2\u00a0(Dao, 2024) to speed up long-context training:\nfor sequences that are concatenations of multiple short documents,\ninstead of computing the full attention with masking,\nwe instead compute the attention for each individual document.\nSince the complexity of attention is quadratic to the sequence length, this improves the training speed.\nHowever, the improvement is negligible in a distributed training setting with FSDP, since GPUs processing short sequence batches have to wait on other GPUs processing long sequences.\nWe therefore implement a smart batching algorithm:\nIn our setting, a gradient step usually consists of multiple gradient accumulation steps, where each device processes a smaller minibatch.\nWe sort all the minibatches per training step by the sum of the squared lengths of documents in the sequence.\nThis leads to more balanced sequence lengths across the GPUs and effective speedups, as can be seen in Table\u00a015, without affecting the gradient updates or loss during training. However, the efficiency gains are diminished when training with more GPUs, as this reduces the number of gradient accumulation steps.\n\n\nTable 15: Throughput per device of our ablation runs from Table\u00a020, when training with 8 Nvidia H100 GPUs with FSDP. Our strategy of reordering minibatches is important for realizing the speed benefits from variable-length attention.\n\n\n\n\nThroughput\n\n\n\n(tokens/s/GPU)\n\n\n\n\n64K full attention\n\u00a0\u00a0\u00a02770\n\n\nVariable-length attention\n\u00a0\u00a0\u00a02780(+0.4%)\n\n\n\n\u2005 + Minibatch reordering\n\n\u00a0\u00a0\u00a03095(+11.7%)\n\n\n\n\n\n\n\nToken-averaged loss.\n\nWe found that in the SFT stage,\nthe distribution of the training tokens (in SFT, the tokens from the instructions are masked out and the models are only trained on the responses) on each GPU device can be extremely imbalanced, especially when there is synthetic data (most tokens in a synthetic data instance are from the instruction).\nConventional all-reduce loss in distributed training averages over the sequences instead of valid tokens, which skews the optimization and also our control over the domain proportions.\nInstead, we change the all-reduce loss to be the average over all valid training tokens.\nBai et\u00a0al. (2024a) implements their SFT loss in a similar way.\n\n\n\n\n\nA.4 The ablation setting\n\nFor all our ablations, unless specified, we train the base model of Llama-3-8B (instead of Instruct) on a 64K sequence length for 5B tokens, with the same hyperparameters as specified in Table\u00a09.\nWe choose this context length, as it is the highest power of 2 value for which we can train without sequence parallelism.\nBy default, we use the same training data as the 64K ProLong setting, except that we remove the textbooks and use the ShortMix proportions in Table\u00a05.\nFor SFT, we use the same settings as specified in Table\u00a09.\n\n\n\n\nA.5 Generating synthetic SFT data\n\nWe prompt Llama-3-8B-Instruct to generate the synthetic data and\nTable\u00a016 shows the prompt we used for generating the synthetic QA data for books.\nWe also write predefined templates and randomly sample one for each synthetic instance to increase the diversity, and Table\u00a017 provides some examples.\n\n\nTable 16: \nPrompts for generating synthetic QA data.\n\n\n\n\n\n\nGiven the following snippet of a book, ask a relevant question and provide the answer. The question and the answer should follow the following rules:\n\n\n\n\n\n\n\n\n\n\n\n\n\n(1) The question should be specific enough that it can only be answered with the snippet. The question should also be interesting and intellectual enough that a curious reader of the book would ask about it.\n\n\n\n\n\n\n(2) The question and the answer should be comprehensible given just the whole book without highlighting the snippet. With that being said, the question should NOT refer to the snippet directly (e.g., do NOT say things like \"Question: given the conversation in the snippet, what ...\"). The answer also should not mention \"the snippet \u2026\" explicitly (assuming that the snippet is never provided), but it can copy the snippet content as a reference when answering the question.\n\n\n\n\n\n\n(3) The answer should be concise but also should provide references to the book when needed. For example, \u201cWellington Yueh betrayed the Atreides, as the book mentioned, \u2019...\u2019\".\n\n\n\n\n\n\n\n\n\n\n\n\n\n*** Start of the snippet ***\n\n\n\n\n\n\n\n\n\n\n\n\n\n{sampled snippet}\n\n\n\n\n\n\n\n\n\n\n\n\n\n*** End of the snippet ***\n\n\n\n\n\n\n\n\n\n\n\n\n\nBefore generating the question and the answer, first reason about what this snippet is about. In your generation, stick to the following format:\n\n\n\n\n\n\n\n\n\n\n\n\n\nReasoning: this snippet is about ...\n\n\n\n\n\n\nQuestion: ...\n\n\n\n\n\n\nAnswer: ...\n\n\n\n\n\n\n\nTable 17: \nExamples for question prompts and templates used for generating diverse synthetic QA data. We sample one question prompt and one template each time and combine them with the documents and the generated QA pairs to form a synthetic training example.\n\n\n\n\nExample question prompts for synthetic QA data\n\n\n\n\n\n\nGiven the document, please answer the question.\n\n\n\n\n\n\nHere is a piece of text; answer the following question based on it.\n\n\n\n\n\n\nPlease answer the question using the provided content.\n\n\n\n\n\n\nBased on the given passage, respond to the question.\n\n\n\n\n\n\nRead the snippet and answer the question that follows.\n\n\n\n\n\n\nUsing the provided text, answer the following question.\n\n\n\n\nExample templates for combining questions, answers, and contexts for synthetic QA data\n\n\n\n\n{prompt}\\n\\n{documents}\\n\\nQuestion: {question}\n\n\n\n\n\n\n{prompt}\\n\\n==== document starts ====\\n{documents}\\n==== document ends ====\\n\\nQuestion: {question}\n\n\n\n\n\n\n{prompt}\\n\\n{documents}\\n\\n{question}\n\n\n\n\n\n\n{prompt} Question: {question}\\n\\n{documents}\n\n\n\n\n\n\n{prompt} {question}\\n\\n{documents}\n\n\n\n\n\n\n{prompt}\\n\\n{question}\\n\\n{documents}\n\n\n\n\n\n\n\n",
      "Appendix B More Ablations": "\n\nAppendix B More Ablations\n\n\nB.1 Position extrapolation\n\nXiong et\u00a0al. (2023); emozilla (2023) show that\nchanging the RoPE frequency base to a larger value in\ncontinual long-context pre-training or in inference time can improve the long-context performance.\nemozilla (2023) suggests that one should scale the frequency base by a factor of tdd\u22122superscript\ud835\udc61\ud835\udc51\ud835\udc512t^{\\frac{d}{d-2}}italic_t start_POSTSUPERSCRIPT divide start_ARG italic_d end_ARG start_ARG italic_d - 2 end_ARG end_POSTSUPERSCRIPT, where t\ud835\udc61titalic_t is the ratio between the target sequence length and the original LM length, and d\ud835\udc51ditalic_d is the attention head dimension.\n\n\nWe conduct ablation studies, at both 64K (same as our standard ablation setting as specified in \u00a7A.4) and 512K (starting from ProLong-64K and training with the 512K data mixture for 5B tokens) sequence lengths, on what frequency bases we should use.\n\u00a7B.1 and \u00a7B.1 show the results.\nWe first see that using the original 500,000 frequency base from Llama-3 leads to significant performance degradation. While dynamic NTK suggests 4\u00d71064superscript1064\\times 10^{6}4 \u00d7 10 start_POSTSUPERSCRIPT 6 end_POSTSUPERSCRIPT, we find that further scaling it to 8\u00d71068superscript1068\\times 10^{6}8 \u00d7 10 start_POSTSUPERSCRIPT 6 end_POSTSUPERSCRIPT leads to better performance. Similar, we see that when scaling the 64K model to 512K, while dynamic NTK suggests a 64\u00d710664superscript10664\\times 10^{6}64 \u00d7 10 start_POSTSUPERSCRIPT 6 end_POSTSUPERSCRIPT frequency base, much larger frequency bases (128\u00d7106128superscript106128\\times 10^{6}128 \u00d7 10 start_POSTSUPERSCRIPT 6 end_POSTSUPERSCRIPT and 256\u00d7106256superscript106256\\times 10^{6}256 \u00d7 10 start_POSTSUPERSCRIPT 6 end_POSTSUPERSCRIPT) lead to better performance.\nWe use 8\u00d71068superscript1068\\times 10^{6}8 \u00d7 10 start_POSTSUPERSCRIPT 6 end_POSTSUPERSCRIPT for 64K and 128\u00d7106128superscript106128\\times 10^{6}128 \u00d7 10 start_POSTSUPERSCRIPT 6 end_POSTSUPERSCRIPT for 512K for our final ProLong models.\n\n\nTable 18: Ablation study on RoPE frequency base at a maximum training length of 64K. Dynamic NTK\u00a0(emozilla, 2023) roughly suggests to use 4m as the frequency base.\n\n\n\n\n\nRoPE Base \n(\u00d7106absentsuperscript106\\times 10^{6}\u00d7 10 start_POSTSUPERSCRIPT 6 end_POSTSUPERSCRIPT)\nLong-Context\nShort-Context\n\n\nRecall\nRAG\nRe-rank\nICL\nQA\nSumm.\nAvg.\nAvg.\n\n\n\n\n0.5\n25.8\n37.0\n4.4\n73.8\n17.5\n16.3\n29.1\n65.0\n\n\n4.0\n81.3\n47.8\n18.2\n76.5\n31.8\n36.3\n48.7\n65.3\n\n\n8.0\n96.0\n54.9\n29.4\n73.9\n35.7\n37.9\n54.6\n65.5\n\n\n\n\n\n\n\nTable 19: Ablation study on RoPE frequency base at a maximum training length of 512K. Dynamic NTK\u00a0(emozilla, 2023) roughly suggests to use 64\u00d710664superscript10664\\times 10^{6}64 \u00d7 10 start_POSTSUPERSCRIPT 6 end_POSTSUPERSCRIPT as the frequency base.\n\n\n\n\n\nRoPE Base \n(\u00d7106absentsuperscript106\\times 10^{6}\u00d7 10 start_POSTSUPERSCRIPT 6 end_POSTSUPERSCRIPT)\nLong-Context\nShort-Context\n\n\nRecall\nRAG\nRe-rank\nICL\nQA\nSumm.\nAvg.\nAvg.\n\n\n\n\n64\n98.8\n57.8\n30.4\n82.2\n38.2\n38.3\n57.6\n68.3\n\n\n128\n98.8\n57.4\n30.7\n80.0\n40.4\n38.8\n57.7\n68.6\n\n\n256\n98.8\n56.8\n33.8\n79.8\n37.9\n39.7\n57.8\n68.4\n\n\n\n\n\n\n\n\nB.2 Document masks\n\nWe experiment whether to use document masks in attention in Table\u00a020. Standard training concatenates multiple short documents into a single sequence (in our case, a 64K sequence), uses a special token to separate documents, and performs full attention over the whole sequence. When the document masks are used, we do not allow the attention to cross the document boundaries.\nWe find that using document masks in continual long-context training leads to both better long-context results and short-context performance.\nFor all our other ablations and the main experiment, we use document masks.\n\n\nTable 20: Impact of using document masks in attention.\n\n\n\nAttention\nLong-Context\nShort-Context\n\n\nRecall\nRAG\nRe-rank\nICL\nQA\nSumm.\nAvg.\nAvg.\n\n\n\n\nNo doc masks\n97.4\n53.6\n20.4\n76.6\n37.2\n36.3\n53.6\n64.9\n\n\nDocument masks\n96.0\n54.9\n29.4\n73.9\n35.7\n37.9\n54.6\n65.5\n\n\n\n\n\n\nB.3 Initialization\n\nWe use the base model for Llama-3-8B as the initialization for all our ablations to make sure the findings are generalizable and are not confounded by the Llama instruction tuning.\nHowever, for our final ProLong model, we use Llama-3-8B-Instruct as the initialization to achieve the best performance.\nWe see in Table\u00a021 (using the ablation setting from \u00a7A.4) that using Llama-3-8B-Instruct as the initialization achieves slightly better long-context performance and much stronger short-context performance.\n\n\nTable 21: \nDifferences of using the base Llama-3-8B model vs. Llama-3-8B-Instruct.\n\n\n\n\nBase Model\nLong-Context\nShort-Context\n\n\nAvg.\nHellaS.\nMMLU\nARC-c\nWG\nGSM8K\nAvg.\n\n\n\n\nLlama-3-8B-Base\n54.6\n81.6\n65.3\n58.0\n76.2\n46.6\n65.5\n\n\nLlama-3-8B-Instruct\n55.0\n80.8\n66.1\n58.5\n75.6\n57.7\n67.7\n\n\n\n\n\n\nB.4 Instruction-tuning datasets\n\nInitialized from the ProLong base model, we experiment with different public, short-context SFT datasets.\nAll runs use the same SFT hyperparameters as specified in Table\u00a09.\nTable\u00a022 shows that using UltraChat leads to the best overall results.\nNote that this does not necessarily mean that UltraChat is the best SFT dataset for all base models or applications.\n\n\nTable 22: \nAblations on using different short-context SFT datasets.\nWe report the 32K-and-64K-averaged performance except tasks marked with \u2020, which are evaluated at 512K for stress testing.\n\n\n\n\nSFT Data\nLong-Context\n\n\nRecall\u2020\n\nRAG\nRe-rank\nICL\nQA\u2020\n\nSumm.\u2020\n\nAvg.\n\n\n\n\nUltraChat\n65.7\n58.1\n38.5\n80.3\n49.7\n42.1\n55.7\n\n\nTulu v2\n61.5\n45.4\n25.1\n81.8\n40.4\n40.3\n49.1\n\n\nShareGPT\n40.5\n47.5\n26.7\n79.6\n42.7\n34.4\n45.2\n\n\n\n\n\n\nAppendix C Full Evaluation Results\n\nWe provide all the evaluation results, including specific numbers for each dataset and evaluation length, at https://shorturl.at/pULbx.\n\n\n\n\nAppendix D Examples\n\nTable\u00a023 shows some long-document QA examples with ProLong, Llama-3.1, and GPT-4o.\n\n\nTable 23: \nLong-document QA examples with ProLong (512K), Llama-3.1-8B-Instruct (128K), and GPT-4o (via the ChatGPT interface). Note that\nit is very likely that the ChatGPT-version GPT-4o uses a retrieval component for long documents, hence it can process contexts beyond its 128K context window.\nWe see that ProLong can answer questions effectively with a 300K document while both Llama-3.1 and GPT-4o fail.\nOn a shorter document (Llama-3.1 paper), ProLong can understand very technical questions and answer them accurately.\n\n\n\n\nInstacart S1 file (300K tokens)\n\n\n\n\n\n\nQ: How many shares and at what price did instacard sell their series G redeemable convertible preferred stock in June 2020 and July 2020?\n\n\n\nProLong: Instacart sold an aggregate of  6,757,893 shares of their \u2026 at a purchase price of $48.0919 \u2026\n\n\n\n\n\n\nllama-3.1-8B: The following is a II/5 [collapsed]\n\n\n\n\n\n\nGPT-4o: \u2026 The company sold a total of 6,758,000 shares at a price of $48.0919 per share during this period.\n\n\n\n\nLlama-3.1 paper (94K tokens)\n\n\n\n\nQ: During training of the llama3 model, how many unexpected interruptions are caused by hardware related reasons?\n\n\n\nProLong: Approximately 78% of unexpected \u2026\n\n\n\n\n\n\nllama-3.1-8B: According to Table 5, approximately 58.7% \u2026\n\n\n\n\n\n\nGPT-4o: \u2026 approximately 78% of unexpected interruptions \u2026\n\n\n\n\n\n\nQ: Is Mark Zuckerberg a contributor to the paper?\n\n\n\nProLong: There is no mention of Mark Zuckerberg as a contributor to the paper.\n\n\n\n\n\n\nllama-3.1-8B: Yes, Mark Zuckerberg is listed as a contributor \u2026\n\n\n\n\n\n\nGPT-4o: Mark Zuckerberg is acknowledged in the paper for \u2026, but he is not listed as a core contributor \u2026\n\n\n\n\n\n\nQ: What ratio did the llama 3 team use to mix synthetic long-context data with short data in SFT?\n\n\n\nProLong: The llama 3 team used a ratio of 0.1% synthetic \u2026\n\n\n\n\n\n\nllama-3.1-8B: 0.10%\n\n\n\n\n\n\nGPT-4o: The Llama 3 team used a mix ratio of 30% synthetic \u2026\n\n\n\n\n\n\nQ: What benchmark did they evaluate on for \u201dinstruction-following\u201d ability? Provide references too.\n\n\n\nProLong: They evaluated on IFEval (Zhou et al., 2023) for instruction-following ability. Reference: Jeffrey Zhou, \u2026 Instruction-following evaluation for large language models. arXiv preprint \u2026\n\n\n\n\n\n\nllama-3.1-8B: The benchmark they evaluated on for \u201dinstruction-following\u201d ability is IFEval. Reference: Section 5.2.2, \u201dGeneral Knowledge and Instruction-Following Benchmarks\u201d\n\n\n\n\n\n\nGPT-4o: The benchmark they used to evaluate \u201dinstruction- following\u201d ability is IFEval. This is mentioned in the context of assessing how Direct Preference Optimization (DPO) \u2026\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",
      "Appendix C Full Evaluation Results": "\n\nAppendix C Full Evaluation Results\n\nWe provide all the evaluation results, including specific numbers for each dataset and evaluation length, at https://shorturl.at/pULbx.\n\n",
      "Appendix D Examples": "\n\nAppendix D Examples\n\nTable\u00a023 shows some long-document QA examples with ProLong, Llama-3.1, and GPT-4o.\n\n\nTable 23: \nLong-document QA examples with ProLong (512K), Llama-3.1-8B-Instruct (128K), and GPT-4o (via the ChatGPT interface). Note that\nit is very likely that the ChatGPT-version GPT-4o uses a retrieval component for long documents, hence it can process contexts beyond its 128K context window.\nWe see that ProLong can answer questions effectively with a 300K document while both Llama-3.1 and GPT-4o fail.\nOn a shorter document (Llama-3.1 paper), ProLong can understand very technical questions and answer them accurately.\n\n\n\n\nInstacart S1 file (300K tokens)\n\n\n\n\n\n\nQ: How many shares and at what price did instacard sell their series G redeemable convertible preferred stock in June 2020 and July 2020?\n\n\n\nProLong: Instacart sold an aggregate of  6,757,893 shares of their \u2026 at a purchase price of $48.0919 \u2026\n\n\n\n\n\n\nllama-3.1-8B: The following is a II/5 [collapsed]\n\n\n\n\n\n\nGPT-4o: \u2026 The company sold a total of 6,758,000 shares at a price of $48.0919 per share during this period.\n\n\n\n\nLlama-3.1 paper (94K tokens)\n\n\n\n\nQ: During training of the llama3 model, how many unexpected interruptions are caused by hardware related reasons?\n\n\n\nProLong: Approximately 78% of unexpected \u2026\n\n\n\n\n\n\nllama-3.1-8B: According to Table 5, approximately 58.7% \u2026\n\n\n\n\n\n\nGPT-4o: \u2026 approximately 78% of unexpected interruptions \u2026\n\n\n\n\n\n\nQ: Is Mark Zuckerberg a contributor to the paper?\n\n\n\nProLong: There is no mention of Mark Zuckerberg as a contributor to the paper.\n\n\n\n\n\n\nllama-3.1-8B: Yes, Mark Zuckerberg is listed as a contributor \u2026\n\n\n\n\n\n\nGPT-4o: Mark Zuckerberg is acknowledged in the paper for \u2026, but he is not listed as a core contributor \u2026\n\n\n\n\n\n\nQ: What ratio did the llama 3 team use to mix synthetic long-context data with short data in SFT?\n\n\n\nProLong: The llama 3 team used a ratio of 0.1% synthetic \u2026\n\n\n\n\n\n\nllama-3.1-8B: 0.10%\n\n\n\n\n\n\nGPT-4o: The Llama 3 team used a mix ratio of 30% synthetic \u2026\n\n\n\n\n\n\nQ: What benchmark did they evaluate on for \u201dinstruction-following\u201d ability? Provide references too.\n\n\n\nProLong: They evaluated on IFEval (Zhou et al., 2023) for instruction-following ability. Reference: Jeffrey Zhou, \u2026 Instruction-following evaluation for large language models. arXiv preprint \u2026\n\n\n\n\n\n\nllama-3.1-8B: The benchmark they evaluated on for \u201dinstruction-following\u201d ability is IFEval. Reference: Section 5.2.2, \u201dGeneral Knowledge and Instruction-Following Benchmarks\u201d\n\n\n\n\n\n\nGPT-4o: The benchmark they used to evaluate \u201dinstruction- following\u201d ability is IFEval. This is mentioned in the context of assessing how Direct Preference Optimization (DPO) \u2026\n\n\n\n\n\n\n\n"
    },
    "bibliography": {
      "Abdin et\u00a0al. (2024)": "\nAbdin et\u00a0al. (2024)\n\nMarah Abdin, Sam\u00a0Ade Jacobs, Ammar\u00a0Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Harkirat Behl, et\u00a0al.\n\n\nPhi-3 technical report: A highly capable language model locally on your phone.\n\n\narXiv preprint arXiv:2404.14219, 2024.\n\n\n",
      "Achiam et\u00a0al. (2023)": "\nAchiam et\u00a0al. (2023)\n\nJosh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia\u00a0Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et\u00a0al.\n\n\nGpt-4 technical report.\n\n\narXiv preprint arXiv:2303.08774, 2023.\n\n\n",
      "An et\u00a0al. (2024a)": "\nAn et\u00a0al. (2024a)\n\nChenxin An, Shansan Gong, Ming Zhong, Xingjian Zhao, Mukai Li, Jun Zhang, Lingpeng Kong, and Xipeng Qiu.\n\n\nL-eval: Instituting standardized evaluation for long context language models.\n\n\nIn Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp.\u00a0 14388\u201314411, 2024a.\n\n\n",
      "An et\u00a0al. (2024b)": "\nAn et\u00a0al. (2024b)\n\nShengnan An, Zexiong Ma, Zeqi Lin, Nanning Zheng, and Jian-Guang Lou.\n\n\nMake your llm fully utilize the context.\n\n\narXiv preprint arXiv:2404.16811, 2024b.\n\n\n",
      "Anthropic (2024)": "\nAnthropic (2024)\n\nAI\u00a0Anthropic.\n\n\nClaude 3.5 sonnet model card addendum.\n\n\nClaude-3.5 Model Card, 2024.\n\n\n",
      "Arora et\u00a0al. (2024)": "\nArora et\u00a0al. (2024)\n\nSimran Arora, Sabri Eyuboglu, Michael Zhang, Aman Timalsina, Silas Alberti, James Zou, Atri Rudra, and Christopher Re.\n\n\nSimple linear attention language models balance the recall-throughput tradeoff.\n\n\nIn Forty-first International Conference on Machine Learning, 2024.\n\n\n",
      "Bai et\u00a0al. (2024a)": "\nBai et\u00a0al. (2024a)\n\nYushi Bai, Xin Lv, Jiajie Zhang, Yuze He, Ji\u00a0Qi, Lei Hou, Jie Tang, Yuxiao Dong, and Juanzi Li.\n\n\nLongalign: A recipe for long context alignment of large language models.\n\n\narXiv preprint arXiv:2401.18058, 2024a.\n\n\n",
      "Bai et\u00a0al. (2024b)": "\nBai et\u00a0al. (2024b)\n\nYushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li.\n\n\nLongBench: A bilingual, multitask benchmark for long context understanding.\n\n\nIn Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp.\u00a0 3119\u20133137, 2024b.\n\n\n",
      "Bajaj et\u00a0al. (2016)": "\nBajaj et\u00a0al. (2016)\n\nPayal Bajaj, Daniel Campos, Nick Craswell, Li\u00a0Deng, Jianfeng Gao, Xiaodong Liu, Rangan Majumder, Andrew McNamara, Bhaskar Mitra, Tri Nguyen, et\u00a0al.\n\n\nMs marco: A human generated machine reading comprehension dataset.\n\n\narXiv preprint arXiv:1611.09268, 2016.\n\n\n",
      "Beeching et\u00a0al. (2023)": "\nBeeching et\u00a0al. (2023)\n\nEdward Beeching, Cl\u00e9mentine Fourrier, Nathan Habib, Sheon Han, Nathan Lambert, Nazneen Rajani, Omar Sanseviero, Lewis Tunstall, and Thomas Wolf.\n\n\nOpen llm leaderboard, 2023.\n\n\n",
      "Bertsch et\u00a0al. (2023)": "\nBertsch et\u00a0al. (2023)\n\nAmanda Bertsch, Uri Alon, Graham Neubig, and Matthew\u00a0R. Gormley.\n\n\nUnlimiformer: Long-range transformers with unlimited length input.\n\n\nIn Advances in Neural Information Processing Systems (NeurIPS), 2023.\n\n\n",
      "Bertsch et\u00a0al. (2024)": "\nBertsch et\u00a0al. (2024)\n\nAmanda Bertsch, Maor Ivgi, Uri Alon, Jonathan Berant, Matthew\u00a0R Gormley, and Graham Neubig.\n\n\nIn-context learning with long-context models: An in-depth exploration.\n\n\narXiv preprint arXiv:2405.00200, 2024.\n\n\n",
      "Casanueva et\u00a0al. (2020)": "\nCasanueva et\u00a0al. (2020)\n\nI\u00f1igo Casanueva, Tadas Tem\u010dinas, Daniela Gerz, Matthew Henderson, and Ivan Vuli\u0107.\n\n\nEfficient intent detection with dual sentence encoders.\n\n\nIn Proceedings of the 2nd Workshop on Natural Language Processing for Conversational AI, pp.\u00a0 38\u201345, 2020.\n\n\n",
      "Chang et\u00a0al. (2024)": "\nChang et\u00a0al. (2024)\n\nYapei Chang, Kyle Lo, Tanya Goyal, and Mohit Iyyer.\n\n\nBooookscore: A systematic exploration of book-length summarization in the era of LLMs.\n\n\nIn The Twelfth International Conference on Learning Representations, 2024.\n\n\n",
      "Chen et\u00a0al. (2023)": "\nChen et\u00a0al. (2023)\n\nShouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian.\n\n\nExtending context window of large language models via positional interpolation, 2023.\n\n\n",
      "Chen et\u00a0al. (2024)": "\nChen et\u00a0al. (2024)\n\nYukang Chen, Shengju Qian, Haotian Tang, Xin Lai, Zhijian Liu, Song Han, and Jiaya Jia.\n\n\nLongloRA: Efficient fine-tuning of long-context large language models.\n\n\nIn The Twelfth International Conference on Learning Representations, 2024.\n\n\n",
      "Chevalier et\u00a0al. (2023)": "\nChevalier et\u00a0al. (2023)\n\nAlexis Chevalier, Alexander Wettig, Anirudh Ajith, and Danqi Chen.\n\n\nAdapting language models to compress contexts.\n\n\nIn Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp.\u00a0 3829\u20133846, Singapore, December 2023. Association for Computational Linguistics.\n\n\n",
      "Chevalier et\u00a0al. (2024)": "\nChevalier et\u00a0al. (2024)\n\nAlexis Chevalier, Jiayi Geng, Alexander Wettig, Howard Chen, Sebastian Mizera, Toni Annala, Max Aragon, Arturo\u00a0Rodriguez Fanlo, Simon Frieder, Simon Machado, Akshara Prabhakar, Ellie Thieu, Jiachen\u00a0T. Wang, Zirui Wang, Xindi Wu, Mengzhou Xia, Wenhan Xia, Jiatong Yu, Junjie Zhu, Zhiyong Ren, Sanjeev Arora, and Danqi Chen.\n\n\nLanguage models as science tutors.\n\n\nIn Forty-first International Conference on Machine Learning, 2024.\n\n\n",
      "Choromanski et\u00a0al. (2021)": "\nChoromanski et\u00a0al. (2021)\n\nKrzysztof\u00a0Marcin Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared\u00a0Quincy Davis, Afroz Mohiuddin, Lukasz Kaiser, David\u00a0Benjamin Belanger, Lucy\u00a0J Colwell, and Adrian Weller.\n\n\nRethinking attention with performers.\n\n\nIn International Conference on Learning Representations, 2021.\n\n\nURL https://openreview.net/forum?id=Ua6zuk0WRH.\n\n\n",
      "Clark et\u00a0al. (2018)": "\nClark et\u00a0al. (2018)\n\nPeter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord.\n\n\nThink you have solved question answering? Try ARC, the AI2 reasoning challenge.\n\n\narXiv preprint arXiv:1803.05457, 2018.\n\n\n",
      "Cobbe et\u00a0al. (2021)": "\nCobbe et\u00a0al. (2021)\n\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et\u00a0al.\n\n\nTraining verifiers to solve math word problems.\n\n\narXiv preprint arXiv:2110.14168, 2021.\n\n\n",
      "Dao (2024)": "\nDao (2024)\n\nTri Dao.\n\n\nFlashattention-2: Faster attention with better parallelism and work partitioning.\n\n\nIn The Twelfth International Conference on Learning Representations, 2024.\n\n\n",
      "Dao & Gu (2024)": "\nDao & Gu (2024)\n\nTri Dao and Albert Gu.\n\n\nTransformers are ssms: Generalized models and efficient algorithms through structured state space duality.\n\n\narXiv preprint arXiv:2405.21060, 2024.\n\n\n",
      "Deutsch & Roth (2021)": "\nDeutsch & Roth (2021)\n\nDaniel Deutsch and Dan Roth.\n\n\nUnderstanding the extent to which content quality metrics measure the information quality of summaries.\n\n\nIn Arianna Bisazza and Omri Abend (eds.), Proceedings of the 25th Conference on Computational Natural Language Learning, pp.\u00a0 300\u2013309, 2021.\n\n\n",
      "Deutsch et\u00a0al. (2022)": "\nDeutsch et\u00a0al. (2022)\n\nDaniel Deutsch, Rotem Dror, and Dan Roth.\n\n\nRe-examining system-level correlations of automatic summarization evaluation metrics.\n\n\nIn Marine Carpuat, Marie-Catherine de\u00a0Marneffe, and Ivan\u00a0Vladimir Meza\u00a0Ruiz (eds.), Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp.\u00a0 6038\u20136052, Seattle, United States, July 2022. Association for Computational Linguistics.\n\n\ndoi: 10.18653/v1/2022.naacl-main.442.\n\n\nURL https://aclanthology.org/2022.naacl-main.442.\n\n\n",
      "Ding et\u00a0al. (2024a)": "\nDing et\u00a0al. (2024a)\n\nHantian Ding, Zijian Wang, Giovanni Paolini, Varun Kumar, Anoop Deoras, Dan Roth, and Stefano Soatto.\n\n\nFewer truncations improve language modeling.\n\n\nIn Forty-first International Conference on Machine Learning, 2024a.\n\n\nURL https://openreview.net/forum?id=kRxCDDFNpp.\n\n\n",
      "Ding et\u00a0al. (2023)": "\nDing et\u00a0al. (2023)\n\nNing Ding, Yulin Chen, Bokai Xu, Yujia Qin, Shengding Hu, Zhiyuan Liu, Maosong Sun, and Bowen Zhou.\n\n\nEnhancing chat language models by scaling high-quality instructional conversations.\n\n\nIn Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp.\u00a0 3029\u20133051, Singapore, December 2023. Association for Computational Linguistics.\n\n\n",
      "Ding et\u00a0al. (2024b)": "\nDing et\u00a0al. (2024b)\n\nYiran Ding, Li\u00a0Lyna Zhang, Chengruidong Zhang, Yuanyuan Xu, Ning Shang, Jiahang Xu, Fan Yang, and Mao Yang.\n\n\nLongroPE: Extending LLM context window beyond 2 million tokens.\n\n\nIn Forty-first International Conference on Machine Learning, 2024b.\n\n\n",
      "Dubey et\u00a0al. (2024)": "\nDubey et\u00a0al. (2024)\n\nAbhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et\u00a0al.\n\n\nThe Llama 3 herd of models.\n\n\narXiv preprint arXiv:2407.21783, 2024.\n\n\n",
      "emozilla (2023)": "\nemozilla (2023)\n\nemozilla.\n\n\nDynamically scaled rope further increases performance of long context llama with zero fine-tuning, 2023.\n\n\nURL https://www.reddit.com/r/LocalLLaMA/comments/14mrgpr/dynamically_scaled_rope_further_increases/.\n\n\n",
      "Fu et\u00a0al. (2024)": "\nFu et\u00a0al. (2024)\n\nYao Fu, Rameswar Panda, Xinyao Niu, Xiang Yue, Hannaneh Hajishirzi, Yoon Kim, and Hao Peng.\n\n\nData engineering for scaling language models to 128k context.\n\n\nIn International Conference on Machine Learning (ICML), 2024.\n\n\n",
      "Gao et\u00a0al. (2021)": "\nGao et\u00a0al. (2021)\n\nLeo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff, Jason Phang, Laria Reynolds, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou.\n\n\nA framework for few-shot language model evaluation, September 2021.\n\n\n",
      "Gao et\u00a0al. (2023)": "\nGao et\u00a0al. (2023)\n\nTianyu Gao, Howard Yen, Jiatong Yu, and Danqi Chen.\n\n\nEnabling large language models to generate text with citations.\n\n\nIn Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp.\u00a0 6465\u20136488, Singapore, December 2023. Association for Computational Linguistics.\n\n\n",
      "Goyal et\u00a0al. (2023)": "\nGoyal et\u00a0al. (2023)\n\nTanya Goyal, Junyi\u00a0Jessy Li, and Greg Durrett.\n\n\nNews summarization and evaluation in the era of gpt-3, 2023.\n\n\nURL https://arxiv.org/abs/2209.12356.\n\n\n",
      "Gu & Dao (2023)": "\nGu & Dao (2023)\n\nAlbert Gu and Tri Dao.\n\n\nMamba: Linear-time sequence modeling with selective state spaces, 2023.\n\n\n",
      "Guo et\u00a0al. (2024)": "\nGuo et\u00a0al. (2024)\n\nDaya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang, Guanting Chen, Xiao Bi, Yu\u00a0Wu, YK\u00a0Li, et\u00a0al.\n\n\nDeepseek-coder: When the large language model meets programming\u2013the rise of code intelligence.\n\n\narXiv preprint arXiv:2401.14196, 2024.\n\n\n",
      "Hendrycks et\u00a0al. (2021)": "\nHendrycks et\u00a0al. (2021)\n\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt.\n\n\nMeasuring massive multitask language understanding.\n\n\nIn International Conference on Learning Representations, 2021.\n\n\n",
      "Hovy et\u00a0al. (2001)": "\nHovy et\u00a0al. (2001)\n\nEduard Hovy, Laurie Gerber, Ulf Hermjakob, Chin-Yew Lin, and Deepak Ravichandran.\n\n\nToward semantics-based answer pinpointing.\n\n\nIn Proceedings of the First International Conference on Human Language Technology Research, 2001.\n\n\n",
      "Hsieh et\u00a0al. (2024)": "\nHsieh et\u00a0al. (2024)\n\nCheng-Ping Hsieh, Simeng Sun, Samuel Kriman, Shantanu Acharya, Dima Rekesh, Fei Jia, and Boris Ginsburg.\n\n\nRULER: What\u2019s the real context size of your long-context language models?\n\n\nIn First Conference on Language Modeling, 2024.\n\n\n",
      "Hu et\u00a0al. (2024a)": "\nHu et\u00a0al. (2024a)\n\nShengding Hu, Yuge Tu, Xu\u00a0Han, Chaoqun He, Ganqu Cui, Xiang Long, Zhi Zheng, Yewei Fang, Yuxiang Huang, Weilin Zhao, et\u00a0al.\n\n\nMinicpm: Unveiling the potential of small language models with scalable training strategies.\n\n\narXiv preprint arXiv:2404.06395, 2024a.\n\n\n",
      "Hu et\u00a0al. (2024b)": "\nHu et\u00a0al. (2024b)\n\nZhiyuan Hu, Yuliang Liu, Jinman Zhao, Suyuchen Wang, Yan Wang, Wei Shen, Qing Gu, Anh\u00a0Tuan Luu, See-Kiong Ng, Zhiwei Jiang, et\u00a0al.\n\n\nLongrecipe: Recipe for efficient long context generalization in large languge models.\n\n\narXiv preprint arXiv:2409.00509, 2024b.\n\n\n",
      "Ivison et\u00a0al. (2023)": "\nIvison et\u00a0al. (2023)\n\nHamish Ivison, Yizhong Wang, Valentina Pyatkin, Nathan Lambert, Matthew Peters, Pradeep Dasigi, Joel Jang, David Wadden, Noah\u00a0A. Smith, Iz\u00a0Beltagy, and Hannaneh Hajishirzi.\n\n\nCamels in a changing climate: Enhancing lm adaptation with Tulu 2, 2023.\n\n\n",
      "Jacobs et\u00a0al. (2023)": "\nJacobs et\u00a0al. (2023)\n\nSam\u00a0Ade Jacobs, Masahiro Tanaka, Chengming Zhang, Minjia Zhang, Shuaiwen\u00a0Leon Song, Samyam Rajbhandari, and Yuxiong He.\n\n\nDeepspeed ulysses: System optimizations for enabling training of extreme long sequence transformer models.\n\n\narXiv preprint arXiv:2309.14509, 2023.\n\n\n",
      "Jelassi et\u00a0al. (2024)": "\nJelassi et\u00a0al. (2024)\n\nSamy Jelassi, David Brandfonbrener, Sham\u00a0M. Kakade, and Eran Malach.\n\n\nRepeat after me: Transformers are better than state space models at copying, 2024.\n\n\n",
      "Jin et\u00a0al. (2024)": "\nJin et\u00a0al. (2024)\n\nHongye Jin, Xiaotian Han, Jingfeng Yang, Zhimeng Jiang, Zirui Liu, Chia-Yuan Chang, Huiyuan Chen, and Xia Hu.\n\n\nLLM maybe longLM: Selfextend LLM context window without tuning.\n\n\nIn Forty-first International Conference on Machine Learning, 2024.\n\n\n",
      "Kamradt (2024)": "\nKamradt (2024)\n\nGarrett Kamradt.\n\n\nNeedle in a haystack - pressure testing LLMs, 2024.\n\n\n",
      "Karpinska et\u00a0al. (2024)": "\nKarpinska et\u00a0al. (2024)\n\nMarzena Karpinska, Katherine Thai, Kyle Lo, Tanya Goyal, and Mohit Iyyer.\n\n\nOne thousand and one pairs: A\u201d novel\u201d challenge for long-context language models.\n\n\narXiv preprint arXiv:2406.16264, 2024.\n\n\n",
      "Kim et\u00a0al. (2024)": "\nKim et\u00a0al. (2024)\n\nYekyung Kim, Yapei Chang, Marzena Karpinska, Aparna Garimella, Varun Manjunatha, Kyle Lo, Tanya Goyal, and Mohit Iyyer.\n\n\nFABLES: Evaluating faithfulness and content selection in book-length summarization.\n\n\nIn First Conference on Language Modeling, 2024.\n\n\n",
      "Kocetkov et\u00a0al. (2023)": "\nKocetkov et\u00a0al. (2023)\n\nDenis Kocetkov, Raymond Li, Loubna\u00a0Ben allal, Jia LI, Chenghao Mou, Yacine Jernite, Margaret Mitchell, Carlos\u00a0Mu\u00f1oz Ferrandis, Sean Hughes, Thomas Wolf, Dzmitry Bahdanau, Leandro\u00a0Von Werra, and Harm de\u00a0Vries.\n\n\nThe stack: 3 TB of permissively licensed source code.\n\n\nTransactions on Machine Learning Research, 2023.\n\n\n",
      "Ko\u010disk\u00fd et\u00a0al. (2018)": "\nKo\u010disk\u00fd et\u00a0al. (2018)\n\nTom\u00e1\u0161 Ko\u010disk\u00fd, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl\u00a0Moritz Hermann, G\u00e1bor Melis, and Edward Grefenstette.\n\n\nThe NarrativeQA reading comprehension challenge.\n\n\nTransactions of the Association for Computational Linguistics, 6:317\u2013328, 2018.\n\n\n",
      "Krishna et\u00a0al. (2023)": "\nKrishna et\u00a0al. (2023)\n\nKalpesh Krishna, Erin Bransom, Bailey Kuehl, Mohit Iyyer, Pradeep Dasigi, Arman Cohan, and Kyle Lo.\n\n\nLongEval: Guidelines for human evaluation of faithfulness in long-form summarization.\n\n\nIn Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, pp.\u00a0 1650\u20131669, 2023.\n\n\n",
      "Kwiatkowski et\u00a0al. (2019)": "\nKwiatkowski et\u00a0al. (2019)\n\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew\u00a0M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov.\n\n\nNatural questions: A benchmark for question answering research.\n\n\nTransactions of the Association for Computational Linguistics, 7:452\u2013466, 2019.\n\n\n",
      "Larson et\u00a0al. (2019)": "\nLarson et\u00a0al. (2019)\n\nStefan Larson, Anish Mahendran, Joseph\u00a0J. Peper, Christopher Clarke, Andrew Lee, Parker Hill, Jonathan\u00a0K. Kummerfeld, Kevin Leach, Michael\u00a0A. Laurenzano, Lingjia Tang, and Jason Mars.\n\n\nAn evaluation dataset for intent classification and out-of-scope prediction.\n\n\nIn Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp.\u00a0 1311\u20131316, 2019.\n\n\n",
      "Lenz et\u00a0al. (2024)": "\nLenz et\u00a0al. (2024)\n\nBarak Lenz, Alan Arazi, Amir Bergman, Avshalom Manevich, Barak Peleg, Ben Aviram, Chen Almagor, Clara Fridman, Dan Padnos, et\u00a0al.\n\n\nJamba-1.5: Hybrid transformer-mamba models at scale.\n\n\narXiv preprint arXiv:2408.12570, 2024.\n\n\n",
      "Li et\u00a0al. (2024a)": "\nLi et\u00a0al. (2024a)\n\nJeffrey Li, Alex Fang, Georgios Smyrnis, Maor Ivgi, Matt Jordan, Samir Gadre, Hritik Bansal, Etash Guha, Sedrick Keh, Kushal Arora, Saurabh Garg, Rui Xin, Niklas Muennighoff, Reinhard Heckel, Jean Mercat, Mayee Chen, Suchin Gururangan, Mitchell Wortsman, Alon Albalak, Yonatan Bitton, Marianna Nezhurina, Amro Abbas, Cheng-Yu Hsieh, Dhruba Ghosh, Josh Gardner, Maciej Kilian, Hanlin Zhang, Rulin Shao, Sarah Pratt, Sunny Sanyal, Gabriel Ilharco, Giannis Daras, Kalyani Marathe, Aaron Gokaslan, Jieyu Zhang, Khyathi Chandu, Thao Nguyen, Igor Vasiljevic, Sham Kakade, Shuran Song, Sujay Sanghavi, Fartash Faghri, Sewoong Oh, Luke Zettlemoyer, Kyle Lo, Alaaeldin El-Nouby, Hadi Pouransari, Alexander Toshev, Stephanie Wang, Dirk Groeneveld, Luca Soldaini, Pang\u00a0Wei Koh, Jenia Jitsev, Thomas Kollar, Alexandros\u00a0G. Dimakis, Yair Carmon, Achal Dave, Ludwig Schmidt, and Vaishaal Shankar.\n\n\nDatacomp-lm: In search of the next generation of training sets for language models.\n\n\narXiv preprint arXiv:2406.11794, 2024a.\n\n\n",
      "Li et\u00a0al. (2023)": "\nLi et\u00a0al. (2023)\n\nShenggui Li, Fuzhao Xue, Chaitanya Baranwal, Yongbin Li, and Yang You.\n\n\nSequence parallelism: Long sequence training from system perspective.\n\n\nIn Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (eds.), Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp.\u00a0 2391\u20132404, Toronto, Canada, July 2023. Association for Computational Linguistics.\n\n\ndoi: 10.18653/v1/2023.acl-long.134.\n\n\nURL https://aclanthology.org/2023.acl-long.134.\n\n\n",
      "Li et\u00a0al. (2024b)": "\nLi et\u00a0al. (2024b)\n\nTianle Li, Ge\u00a0Zhang, Quy\u00a0Duc Do, Xiang Yue, and Wenhu Chen.\n\n\nLong-context llms struggle with long in-context learning.\n\n\narXiv preprint arXiv:2404.02060, 2024b.\n\n\n",
      "Lieber et\u00a0al. (2024)": "\nLieber et\u00a0al. (2024)\n\nOpher Lieber, Barak Lenz, Hofit Bata, Gal Cohen, Jhonathan Osin, Itay Dalmedigos, Erez Safahi, Shaked Meirom, Yonatan Belinkov, Shai Shalev-Shwartz, et\u00a0al.\n\n\nJamba: A hybrid transformer-mamba language model.\n\n\narXiv preprint arXiv:2403.19887, 2024.\n\n\n",
      "Liu et\u00a0al. (2024a)": "\nLiu et\u00a0al. (2024a)\n\nJiaheng Liu, Zhiqi Bai, Yuanxing Zhang, Chenchen Zhang, Yu\u00a0Zhang, Ge\u00a0Zhang, Jiakai Wang, Haoran Que, Yukang Chen, Wenbo Su, et\u00a0al.\n\n\nE^ 2-llm: Efficient and extreme length extension of large language models.\n\n\narXiv preprint arXiv:2401.06951, 2024a.\n\n\n",
      "Liu et\u00a0al. (2024b)": "\nLiu et\u00a0al. (2024b)\n\nNelson\u00a0F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang.\n\n\nLost in the Middle: How Language Models Use Long Contexts.\n\n\nTransactions of the Association for Computational Linguistics, 12:157\u2013173, 02 2024b.\n\n\n",
      "Liu et\u00a0al. (2021)": "\nLiu et\u00a0al. (2021)\n\nXingkun Liu, Arash Eshghi, Pawel Swietojanski, and Verena Rieser.\n\n\nBenchmarking natural language understanding services for building conversational agents.\n\n\nIn Increasing naturalness and flexibility in spoken dialogue interaction: 10th international workshop on spoken dialogue systems, pp.\u00a0 165\u2013183. Springer, 2021.\n\n\n",
      "Lu et\u00a0al. (2024)": "\nLu et\u00a0al. (2024)\n\nYi\u00a0Lu, Jing\u00a0Nathan Yan, Songlin Yang, Justin\u00a0T Chiu, Siyu Ren, Fei Yuan, Wenting Zhao, Zhiyong Wu, and Alexander\u00a0M Rush.\n\n\nA controlled study on long context extension and generalization in llms.\n\n\narXiv preprint arXiv:2409.12181, 2024.\n\n\n",
      "Ma et\u00a0al. (2022)": "\nMa et\u00a0al. (2022)\n\nXuezhe Ma, Chunting Zhou, Xiang Kong, Junxian He, Liangke Gui, Graham Neubig, Jonathan May, and Luke Zettlemoyer.\n\n\nMega: moving average equipped gated attention.\n\n\narXiv preprint arXiv:2209.10655, 2022.\n\n\n",
      "Mallen et\u00a0al. (2023)": "\nMallen et\u00a0al. (2023)\n\nAlex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Daniel Khashabi, and Hannaneh Hajishirzi.\n\n\nWhen not to trust language models: Investigating effectiveness of parametric and non-parametric memories.\n\n\nIn Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (eds.), Association for Computational Linguistics (ACL), pp.\u00a0 9802\u20139822, Toronto, Canada, July 2023. Association for Computational Linguistics.\n\n\n",
      "Mosaic ML (2022)": "\nMosaic ML (2022)\n\nMosaic ML.\n\n\nstreaming.\n\n\n<https://github.com/mosaicml/streaming/>, 2022.\n\n\n",
      "Ouyang et\u00a0al. (2022)": "\nOuyang et\u00a0al. (2022)\n\nLong Ouyang, Jeffrey Wu, Xu\u00a0Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et\u00a0al.\n\n\nTraining language models to follow instructions with human feedback.\n\n\nAdvances in Neural Information Processing Systems (NeurIPS), 35:27730\u201327744, 2022.\n\n\nURL https://proceedings.neurips.cc/paper_files/paper/2022/file/b1efde53be364a73914f58805a001731-Paper-Conference.pdf.\n\n\n",
      "Pan et\u00a0al. (2023)": "\nPan et\u00a0al. (2023)\n\nJane Pan, Tianyu Gao, Howard Chen, and Danqi Chen.\n\n\nWhat in-context learning \u201clearns\u201d in-context: Disentangling task recognition and task learning.\n\n\nIn Findings of the Association for Computational Linguistics: ACL 2023, pp.\u00a0 8298\u20138319. Association for Computational Linguistics, 2023.\n\n\n",
      "Paster et\u00a0al. (2024)": "\nPaster et\u00a0al. (2024)\n\nKeiran Paster, Marco\u00a0Dos Santos, Zhangir Azerbayev, and Jimmy Ba.\n\n\nOpenwebmath: An open dataset of high-quality mathematical web text.\n\n\nIn The Twelfth International Conference on Learning Representations, 2024.\n\n\n",
      "Paszke et\u00a0al. (2019)": "\nPaszke et\u00a0al. (2019)\n\nAdam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et\u00a0al.\n\n\nPytorch: An imperative style, high-performance deep learning library.\n\n\nAdvances in neural information processing systems, 32, 2019.\n\n\n",
      "Penedo et\u00a0al. (2023)": "\nPenedo et\u00a0al. (2023)\n\nGuilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Hamza Alobeidli, Alessandro Cappelli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay.\n\n\nThe refinedweb dataset for falcon llm: Outperforming curated corpora with web data only.\n\n\nIn A.\u00a0Oh, T.\u00a0Naumann, A.\u00a0Globerson, K.\u00a0Saenko, M.\u00a0Hardt, and S.\u00a0Levine (eds.), Advances in Neural Information Processing Systems, volume\u00a036, pp.\u00a0 79155\u201379172. Curran Associates, Inc., 2023.\n\n\n",
      "Penedo et\u00a0al. (2024)": "\nPenedo et\u00a0al. (2024)\n\nGuilherme Penedo, Hynek Kydl\u00ed\u010dek, Loubna\u00a0Ben allal, Anton Lozhkov, Margaret Mitchell, Colin Raffel, Leandro\u00a0Von Werra, and Thomas Wolf.\n\n\nThe fineweb datasets: Decanting the web for the finest text data at scale, 2024.\n\n\n",
      "Peng et\u00a0al. (2023)": "\nPeng et\u00a0al. (2023)\n\nBo\u00a0Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Stella Biderman, Huanqi Cao, Xin Cheng, Michael Chung, Leon Derczynski, Xingjian Du, Matteo Grella, Kranthi Gv, Xuzheng He, Haowen Hou, Przemyslaw Kazienko, Jan Kocon, Jiaming Kong, Bart\u0142omiej Koptyra, Hayden Lau, Jiaju Lin, Krishna Sri\u00a0Ipsit Mantri, Ferdinand Mom, Atsushi Saito, Guangyu Song, Xiangru Tang, Johan Wind, Stanis\u0142aw Wo\u017aniak, Zhenyuan Zhang, Qinghua Zhou, Jian Zhu, and Rui-Jie Zhu.\n\n\nRWKV: Reinventing RNNs for the transformer era.\n\n\nIn Findings of the Association for Computational Linguistics: EMNLP 2023, pp.\u00a0 14048\u201314077, 2023.\n\n\n",
      "Peng et\u00a0al. (2024)": "\nPeng et\u00a0al. (2024)\n\nBowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole.\n\n\nYaRN: Efficient context window extension of large language models.\n\n\nIn The Twelfth International Conference on Learning Representations, 2024.\n\n\n",
      "Press et\u00a0al. (2022)": "\nPress et\u00a0al. (2022)\n\nOfir Press, Noah Smith, and Mike Lewis.\n\n\nTrain short, test long: Attention with linear biases enables input length extrapolation.\n\n\nIn International Conference on Learning Representations (ICLR), 2022.\n\n\n",
      "Rae et\u00a0al. (2020)": "\nRae et\u00a0al. (2020)\n\nJack\u00a0W. Rae, Anna Potapenko, Siddhant\u00a0M. Jayakumar, Chloe Hillier, and Timothy\u00a0P. Lillicrap.\n\n\nCompressive transformers for long-range sequence modelling.\n\n\nIn International Conference on Learning Representations, 2020.\n\n\n",
      "Reid et\u00a0al. (2024)": "\nReid et\u00a0al. (2024)\n\nMachel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Jean-baptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, et\u00a0al.\n\n\nGemini 1.5: Unlocking multimodal understanding across millions of tokens of context.\n\n\narXiv preprint arXiv:2403.05530, 2024.\n\n\n",
      "Rubin & Berant (2023)": "\nRubin & Berant (2023)\n\nOhad Rubin and Jonathan Berant.\n\n\nLong-range language modeling with self-retrieval, 2023.\n\n\n",
      "Sakaguchi et\u00a0al. (2021)": "\nSakaguchi et\u00a0al. (2021)\n\nKeisuke Sakaguchi, Ronan\u00a0Le Bras, Chandra Bhagavatula, and Yejin Choi.\n\n\nWinogrande: An adversarial winograd schema challenge at scale.\n\n\nCommunications of the ACM, 64(9):99\u2013106, 2021.\n\n\n",
      "Shaham et\u00a0al. (2023)": "\nShaham et\u00a0al. (2023)\n\nUri Shaham, Maor Ivgi, Avia Efrat, Jonathan Berant, and Omer Levy.\n\n\nZeroSCROLLS: A zero-shot benchmark for long text understanding.\n\n\nIn Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Findings of the Association for Computational Linguistics: EMNLP 2023, pp.\u00a0 7977\u20137989, Singapore, December 2023. Association for Computational Linguistics.\n\n\n",
      "Shen et\u00a0al. (2022)": "\nShen et\u00a0al. (2022)\n\nZejiang Shen, Kyle Lo, Lauren Yu, Nathan Dahlberg, Margo Schlanger, and Doug Downey.\n\n\nMulti-lexsum: Real-world summaries of civil rights lawsuits at multiple granularities.\n\n\nIn Advances in Neural Information Processing Systems, volume\u00a035, pp.\u00a0 13158\u201313173. Curran Associates, Inc., 2022.\n\n\n",
      "Singh et\u00a0al. (2024)": "\nSingh et\u00a0al. (2024)\n\nAaditya\u00a0K Singh, Yu\u00a0Yang, Kushal Tirumala, Mostafa Elhoushi, and Ari\u00a0S Morcos.\n\n\nBrevity is the soul of wit: Pruning long files for code generation.\n\n\narXiv preprint arXiv:2407.00434, 2024.\n\n\n",
      "Soboleva et\u00a0al. (2023)": "\nSoboleva et\u00a0al. (2023)\n\nDaria Soboleva, Faisal Al-Khateeb, Robert Myers, Jacob\u00a0R Steeves, Joel Hestness, and Nolan Dey.\n\n\nSlimPajama: A 627B token cleaned and deduplicated version of RedPajama, 2023.\n\n\n",
      "Soldaini et\u00a0al. (2024)": "\nSoldaini et\u00a0al. (2024)\n\nLuca Soldaini, Rodney Kinney, Akshita Bhagia, Dustin Schwenk, David Atkinson, Russell Authur, Ben Bogin, Khyathi Chandu, Jennifer Dumas, Yanai Elazar, Valentin Hofmann, Ananya Jha, Sachin Kumar, Li\u00a0Lucy, Xinxi Lyu, Nathan Lambert, Ian Magnusson, Jacob Morrison, Niklas Muennighoff, Aakanksha Naik, Crystal Nam, Matthew Peters, Abhilasha Ravichander, Kyle Richardson, Zejiang Shen, Emma Strubell, Nishant Subramani, Oyvind Tafjord, Evan Walsh, Luke Zettlemoyer, Noah Smith, Hannaneh Hajishirzi, Iz\u00a0Beltagy, Dirk Groeneveld, Jesse Dodge, and Kyle Lo.\n\n\nDolma: an open corpus of three trillion tokens for language model pretraining research.\n\n\nIn Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp.\u00a0 15725\u201315788, Bangkok, Thailand, August 2024. Association for Computational Linguistics.\n\n\ndoi: 10.18653/v1/2024.acl-long.840.\n\n\nURL https://aclanthology.org/2024.acl-long.840.\n\n\n",
      "Su et\u00a0al. (2021)": "\nSu et\u00a0al. (2021)\n\nJianlin Su, Yu\u00a0Lu, Shengfeng Pan, Ahmed Murtadha, Bo\u00a0Wen, and Yunfeng Liu.\n\n\nRoformer: Enhanced transformer with rotary position embedding, 2021.\n\n\n",
      "Sun et\u00a0al. (2023)": "\nSun et\u00a0al. (2023)\n\nYutao Sun, Li\u00a0Dong, Shaohan Huang, Shuming Ma, Yuqing Xia, Jilong Xue, Jianyong Wang, and Furu Wei.\n\n\nRetentive network: A successor to transformer for large language models.\n\n\narXiv preprint arXiv:2307.08621, 2023.\n\n\n",
      "Sun et\u00a0al. (2024)": "\nSun et\u00a0al. (2024)\n\nYutao Sun, Li\u00a0Dong, Yi\u00a0Zhu, Shaohan Huang, Wenhui Wang, Shuming Ma, Quanlu Zhang, Jianyong Wang, and Furu Wei.\n\n\nYou only cache once: Decoder-decoder architectures for language models.\n\n\narXiv preprint arXiv:2405.05254, 2024.\n\n\n",
      "Together (2023)": "\nTogether (2023)\n\nTogether.\n\n\nRedpajama: An open source recipe to reproduce llama training dataset, April 2023.\n\n\n",
      "Waleffe et\u00a0al. (2024)": "\nWaleffe et\u00a0al. (2024)\n\nRoger Waleffe, Wonmin Byeon, Duncan Riach, Brandon Norick, Vijay Korthikanti, Tri Dao, Albert Gu, Ali Hatamizadeh, Sudhakar Singh, Deepak Narayanan, et\u00a0al.\n\n\nAn empirical study of mamba-based language models.\n\n\narXiv preprint arXiv:2406.07887, 2024.\n\n\n",
      "Wang et\u00a0al. (2024)": "\nWang et\u00a0al. (2024)\n\nJunxiong Wang, Daniele Paliotta, Avner May, Alexander\u00a0M Rush, and Tri Dao.\n\n\nThe mamba in the llama: Distilling and accelerating hybrid models.\n\n\narXiv preprint arXiv:2408.15237, 2024.\n\n\n",
      "Wettig et\u00a0al. (2024)": "\nWettig et\u00a0al. (2024)\n\nAlexander Wettig, Aatmik Gupta, Saumya Malik, and Danqi Chen.\n\n\nQuRating: Selecting high-quality data for training language models.\n\n\nIn Ruslan Salakhutdinov, Zico Kolter, Katherine Heller, Adrian Weller, Nuria Oliver, Jonathan Scarlett, and Felix Berkenkamp (eds.), Proceedings of the 41st International Conference on Machine Learning, volume 235 of Proceedings of Machine Learning Research, pp.\u00a0 52915\u201352971. PMLR, 21\u201327 Jul 2024.\n\n\n",
      "Wolf et\u00a0al. (2020)": "\nWolf et\u00a0al. (2020)\n\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le\u00a0Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander Rush.\n\n\nTransformers: State-of-the-art natural language processing.\n\n\nIn Qun Liu and David Schlangen (eds.), Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pp.\u00a0 38\u201345, Online, October 2020. Association for Computational Linguistics.\n\n\ndoi: 10.18653/v1/2020.emnlp-demos.6.\n\n\n",
      "Wu et\u00a0al. (2021)": "\nWu et\u00a0al. (2021)\n\nJeff Wu, Long Ouyang, Daniel\u00a0M Ziegler, Nisan Stiennon, Ryan Lowe, Jan Leike, and Paul Christiano.\n\n\nRecursively summarizing books with human feedback.\n\n\narXiv preprint arXiv:2109.10862, 2021.\n\n\n",
      "Wu et\u00a0al. (2024)": "\nWu et\u00a0al. (2024)\n\nWenhao Wu, Yizhong Wang, Yao Fu, Xiang Yue, Dawei Zhu, and Sujian Li.\n\n\nLong context alignment with short instructions and synthesized positions.\n\n\narXiv preprint arXiv:2405.03939, 2024.\n\n\n",
      "Xiao et\u00a0al. (2024a)": "\nXiao et\u00a0al. (2024a)\n\nChaojun Xiao, Pengle Zhang, Xu\u00a0Han, Guangxuan Xiao, Yankai Lin, Zhengyan Zhang, Zhiyuan Liu, Song Han, and Maosong Sun.\n\n\nInfLLM: Unveiling the intrinsic capacity of LLMs for understanding extremely long sequences with training-free memory.\n\n\narXiv preprint arXiv:2402.04617, 2024a.\n\n\n",
      "Xiao et\u00a0al. (2024b)": "\nXiao et\u00a0al. (2024b)\n\nGuangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis.\n\n\nEfficient streaming language models with attention sinks.\n\n\nIn The Twelfth International Conference on Learning Representations, 2024b.\n\n\n",
      "Xiong et\u00a0al. (2023)": "\nXiong et\u00a0al. (2023)\n\nWenhan Xiong, Jingyu Liu, Igor Molybog, Hejia Zhang, Prajjwal Bhargava, Rui Hou, Louis Martin, Rashi Rungta, Karthik\u00a0Abinav Sankararaman, Barlas Oguz, Madian Khabsa, Han Fang, Yashar Mehdad, Sharan Narang, Kshitiz Malik, Angela Fan, Shruti Bhosale, Sergey Edunov, Mike Lewis, Sinong Wang, and Hao Ma.\n\n\nEffective long-context scaling of foundation models, 2023.\n\n\n",
      "Xiong et\u00a0al. (2024)": "\nXiong et\u00a0al. (2024)\n\nZheyang Xiong, Vasilis Papageorgiou, Kangwook Lee, and Dimitris Papailiopoulos.\n\n\nFrom artificial needles to real haystacks: Improving retrieval capabilities in llms by finetuning on synthetic data.\n\n\narXiv preprint arXiv:2406.19292, 2024.\n\n\n",
      "Yang et\u00a0al. (2024a)": "\nYang et\u00a0al. (2024a)\n\nAn\u00a0Yang, Baosong Yang, Binyuan Hui, Bo\u00a0Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, et\u00a0al.\n\n\nQwen2 technical report.\n\n\narXiv preprint arXiv:2407.10671, 2024a.\n\n\n",
      "Yang et\u00a0al. (2024b)": "\nYang et\u00a0al. (2024b)\n\nSonglin Yang, Bailin Wang, Yikang Shen, Rameswar Panda, and Yoon Kim.\n\n\nGated linear attention transformers with hardware-efficient training.\n\n\nIn Forty-first International Conference on Machine Learning, 2024b.\n\n\n",
      "Yang et\u00a0al. (2018)": "\nYang et\u00a0al. (2018)\n\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher\u00a0D. Manning.\n\n\nHotpotQA: A dataset for diverse, explainable multi-hop question answering.\n\n\nIn Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pp.\u00a0 2369\u20132380, 2018.\n\n\n",
      "Yen et\u00a0al. (2024a)": "\nYen et\u00a0al. (2024a)\n\nHoward Yen, Tianyu Gao, and Danqi Chen.\n\n\nLong-context language modeling with parallel context encoding.\n\n\nIn Association for Computational Linguistics (ACL), pp.\u00a0 2588\u20132610, 2024a.\n\n\n",
      "Yen et\u00a0al. (2024b)": "\nYen et\u00a0al. (2024b)\n\nHoward Yen, Tianyu Gao, Minmin Hou, Ke\u00a0Ding, Daniel Fleischer, Peter Izsak, Moshe Wasserblat, and Danqi Chen.\n\n\nHelmet: How to evaluate long-context language models effectively and thoroughly.\n\n\n2024b.\n\n\n",
      "Yoshida et\u00a0al. (2020)": "\nYoshida et\u00a0al. (2020)\n\nDavis Yoshida, Allyson Ettinger, and Kevin Gimpel.\n\n\nAdding recurrence to pretrained transformers for improved efficiency and context size, 2020.\n\n\n",
      "Zellers et\u00a0al. (2019)": "\nZellers et\u00a0al. (2019)\n\nRowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi.\n\n\nHellaSwag: Can a machine really finish your sentence?\n\n\nIn Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp.\u00a0 4791\u20134800, 2019.\n\n\n",
      "Zhang & Bansal (2021)": "\nZhang & Bansal (2021)\n\nShiyue Zhang and Mohit Bansal.\n\n\nFinding a balanced degree of automation for summary evaluation.\n\n\nIn Empirical Methods in Natural Language Processing (EMNLP), pp.\u00a0 6617\u20136632, 2021.\n\n\n",
      "Zhang et\u00a0al. (2024a)": "\nZhang et\u00a0al. (2024a)\n\nXinrong Zhang, Yingfa Chen, Shengding Hu, Zihang Xu, Junhao Chen, Moo Hao, Xu\u00a0Han, Zhen Thai, Shuo Wang, Zhiyuan Liu, and Maosong Sun.\n\n\n\u221e\\infty\u221eBench: Extending long context evaluation beyond 100K tokens.\n\n\nIn Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp.\u00a0 15262\u201315277, 2024a.\n\n\n",
      "Zhang et\u00a0al. (2024b)": "\nZhang et\u00a0al. (2024b)\n\nYikai Zhang, Junlong Li, and Pengfei Liu.\n\n\nExtending llms\u2019 context window with 100 samples.\n\n\narXiv preprint arXiv:2401.07004, 2024b.\n\n\n",
      "Zhao et\u00a0al. (2024)": "\nZhao et\u00a0al. (2024)\n\nLiang Zhao, Tianwen Wei, Liang Zeng, Cheng Cheng, Liu Yang, Peng Cheng, Lijie Wang, Chenxia Li, Xuejie Wu, Bo\u00a0Zhu, et\u00a0al.\n\n\nLongskywork: A training recipe for efficiently extending context length in large language models.\n\n\narXiv preprint arXiv:2406.00605, 2024.\n\n\n",
      "Zhu et\u00a0al. (2024)": "\nZhu et\u00a0al. (2024)\n\nDawei Zhu, Nan Yang, Liang Wang, Yifan Song, Wenhao Wu, Furu Wei, and Sujian Li.\n\n\nPoSE: Efficient context window extension of LLMs via positional skip-wise training.\n\n\nIn The Twelfth International Conference on Learning Representations, 2024.\n\n\n"
    },
    "domain": "cs.CL",
    "citation_count": 0
  },
  "2b9c2973-9d51-48d4-94b2-1a3d82d924ee": {
    "pk": "2b9c2973-9d51-48d4-94b2-1a3d82d924ee",
    "authors": [
      "Xinrong Zhang",
      "Yingfa Chen",
      "Shengding Hu",
      "Zihang Xu",
      "Junhao Chen",
      "Moo Khai Hao",
      "Xu Han",
      "Zhen Leng Thai",
      "Shuo Wang",
      "Zhiyuan Liu",
      "Maosong Sun"
    ],
    "title": "$\\infty$Bench: Extending Long Context Evaluation Beyond 100K Tokens",
    "abstract": "Processing and reasoning over long contexts is crucial for many practical applications of Large Language Models (LLMs), such as document comprehension and agent construction. Despite recent strides in making LLMs process contexts with more than 100K tokens, there is currently a lack of a standardized benchmark to evaluate this long-context capability. Existing public benchmarks typically focus on contexts around 10K tokens, limiting the assessment and comparison of LLMs in processing longer contexts. In this paper, we propose $\\infty$Bench, the first LLM benchmark featuring an average data length surpassing 100K tokens. $\\infty$Bench comprises synthetic and realistic tasks spanning diverse domains, presented in both English and Chinese. The tasks in $\\infty$Bench are designed to require well understanding of long dependencies in contexts, and make simply retrieving a limited number of passages from contexts not sufficient for these tasks. In our experiments, based on $\\infty$Bench, we evaluate the state-of-the-art proprietary and open-source LLMs tailored for processing long contexts. The results indicate that existing long context LLMs still require significant advancements to effectively process 100K+ context. We further present three intriguing analyses regarding the behavior of LLMs processing long context.",
    "url": "http://arxiv.org/abs/2402.13718v3",
    "timestamp": 1708515029,
    "sections": {
      "1 Introduction": "\n\n1 Introduction\n\nFigure 1: The performance of GPT-4, Kimi-Chat, YaRN-Mistral, and Claude 2 on \u221e\\infty\u221eBench. A higher value represents better performance.\n\n\n\n\n\nBenchmark\nAvg Len\nEn\nZh\nCode\nMath\nNovel\nDialogue\nSynthetic\n\n\n\n\nLRA\u00a0(Tay et\u00a0al., 2020)\n\n\n\u223csimilar-to\\sim\u223c10K\n\u2713\n\u2717\n\u2717\n\u2713\n\u2717\n\u2717\n\u2713\n\n\nLongBench\u00a0(Bai et\u00a0al., 2023)\n\n\n\u223csimilar-to\\sim\u223c10K\n\u2713\n\u2713\n\u2713\n\u2717\n\u2713\n\u2713\n\u2713\n\n\nL-Eval\u00a0(An et\u00a0al., 2023)\n\n4K - 60K\n\u2713\n\u2717\n\u2713\n\u2713\n\u2717\n\u2717\n\u2713\n\n\nLooGLE\u00a0(Li et\u00a0al., 2023)\n\n\n\u223csimilar-to\\sim\u223c20K\n\u2713\n\u2717\n\u2717\n\u2717\n\u2717\n\u2713\n\u2717\n\n\n\n\u221e\\infty\u221eBench (ours)\n\n\u223csimilar-to\\sim\u223c200K\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\n\n\nTable 1: Comparison to existing long-context benchmarks and \u221e\\infty\u221eBench. \u201cEn\u201d and \u201cZh\u201d refer to English and Chinese tasks. \u201cCode\u201d, \u201cMath\u201d, \u201cNovel\u201d, \u201cDialogue\u201d indicate whether the domain includes tasks from those domains, and \u201cSynthetic\u201d indicates whether there are auto-generated tasks.\n\n\nIn recent years, large language models (LLMs) Brown et\u00a0al. (2020); OpenAI (2023a); Touvron et\u00a0al. (2023) have exhibited exceptional performance across a range of natural language processing (NLP) tasks\u00a0Qiu et\u00a0al. (2020); Han et\u00a0al. (2021). LLMs are showing a promising direction toward generalist task assistance, being capable of aiding users in practical tasks through conversational interactions. These tasks include web navigation\u00a0Nakano et\u00a0al. (2021), analysis of code repositories\u00a0Chen et\u00a0al. (2021), and extraction of useful information from documents\u00a0Ko\u010disk\u1ef3 et\u00a0al. (2018), indicating a step towards artificial general intelligence. For these LLM-based scenarios, the ability to process long contexts is increasingly critical, in addition to understanding fine-grained semantics and possessing extensive knowledge\u00a0Dong et\u00a0al. (2023); Huang et\u00a0al. (2023). Textual documents, historical dialogues, complex instructions, and cumbersome workflows, which constitute the data most directly processed in daily tasks, must be input to LLMs as long contexts for effective processing.\n\n\nDespite this growing importance, LLMs consistently face challenges in processing long contexts, primarily due to the substantial computational resources required for long sequence training\u00a0Dao et\u00a0al. (2022); Dao (2023) as well as the apparent inability to generalize to sequences longer than those encountered during training\u00a0Chen et\u00a0al. (2023a); Peng et\u00a0al. (2023b).\nLLMs are typically trained on sequences containing no more than 8K tokens\u00a0Touvron et\u00a0al. (2023); Penedo et\u00a0al. (2023); Biderman et\u00a0al. (2023), and thus cannot well handle contexts exceeding 8K tokens.\nThese limitations have largely restricted most LLMs from being applied to more complex tasks.\n\n\nRecent advancements in training infrastructure\u00a0Shoeybi et\u00a0al. (2019); Narayanan et\u00a0al. (2021); Dao et\u00a0al. (2022); Dao (2023), and efforts to improve length generalization\u00a0Anil et\u00a0al. (2022); Chen et\u00a0al. (2023b); Peng et\u00a0al. (2023b)333https://www.reddit.com/r/LocalLLaMA/comments/14lz7j5/ntkaware_scaled_rope_allows_llama_models_to_have/ have led to rapid developments in long-context LLMs.\nBased on these improved training infrastructures and length generalization methods, several LLMs have purportedly managed to process data exceeding 100K tokens\u00a0(Peng et\u00a0al., 2023b; OpenAI, 2023b; 01.AI, 2023b, a), with Claude 2\u00a0Anthropic (2023) and Kimi-Chat\u00a0AI (2023) even claiming to be able to process up to 200K tokens.\nHowever, the rapid emergence of long-context LLMs has outpaced the development of adequate evaluation benchmarks. Present long-context benchmarks predominantly feature contexts averaging around 10K tokens\u00a0(Bai et\u00a0al., 2023; Tay et\u00a0al., 2020), invariably falling below 100K tokens. This lag in the advancement of long-context evaluation methodologies impedes both the comparative analysis of diverse long-context LLMs and the pinpointing of potential enhancements in long-context processing.\n\n\nIn this work, we present \u221e\\infty\u221eBench , the first comprehensive benchmark featuring an average data length surpassing 100K tokens. \u221e\\infty\u221eBench includes tasks in different domains (novels, code, math, etc.) and languages (English and Chinese). To fully evaluate the performance of long-context LLMs, \u221e\\infty\u221eBench integrates synthetic tasks that can be auto-generated for even longer contexts (e.g., finding the top-k\ud835\udc58kitalic_k number in an array) in addition to a set of realistic tasks.\n\n\nTo construct tasks annotated by humans, we develop 5 annotation pipelines for detailed example annotation. These pipelines undergo iterative refinement until the examples meet quality standards. Auto-generated tasks, conversely, can be easily scaled to various lengths. Upon completing \u221e\\infty\u221eBench, we assess the performance of several state-of-the-art (SOTA) long-context LLMs on this benchmark to gauge its difficulty and evaluate the effectiveness of these models. The results show that current SOTA LLMs are not fully equipped to handle all tasks within \u221e\\infty\u221eBench, highlighting the ongoing challenge of enabling LLMs to process long contexts effectively. We also conduct intriguing analyses on the behavior of LLMs on such long contexts, including the task length ablation, the absent of \u201clost in the middle phenomenon\u00a0Liu et\u00a0al. (2023)\u201d, and the context recalling prompting techniques.\n\n\nOur contributions can be summarized as follows:\n\n\n\u2022\n\nWe construct and release \u221e\\infty\u221eBench, the first multi-domain bilingual benchmark for evaluating the ability to understand and reason over contexts surpassing 100K tokens.\n\n\n\n\u2022\n\nWe evaluate SOTA long-context LLMs on \u221e\\infty\u221eBench, which reveals severe performance degradation of these LLMs when scaling context lengths. These experimental results and analysis also indicate promising directions to improve long-context LLMs.\n\n\n\n\n",
      "2 Related Work": "\n\n2 Related Work\n\nExtending Context Length\n\nTransformers, typically trained on text sequences under 8K tokens due to self-attention\u2019s quadratic complexity, face challenges in longer downstream tasks. To address this, two main strategies have emerged: firstly, the development of positional encodings capable of handling longer text sequences\u00a0Sun et\u00a0al. (2022); Press et\u00a0al. (2021), and secondly, the refinement of inference stage techniques to extend current LLMs post-training. The primary approach involves modifying rotary positional encoding\u00a0Su et\u00a0al. (2023) and implementing post-training adjustments to better manage the increased relative positional distances in longer sequences\u00a0Zhu et\u00a0al. (2023); Peng et\u00a0al. (2023b); Chen et\u00a0al. (2023a).\n\n\n\n100K+ LLMs\n\nRecently, many LLMs have shown the ability to handle over 100K tokens. Some popular proprietary 100K+ LLMs include GPT-4, Claude 2\u00a0(Anthropic, 2023), and Kimi-Chat\u00a0(AI, 2023).\nOn the other hand, there are much fewer open-source 100K+ models. Some notable models include YaRN\u00a0(Peng et\u00a0al., 2023b) and Yi-200K\u00a0(01.AI, 2023a, b).\nIn this paper, we benchmark GPT-4, Claude 2, Kimi-Chat, and YaRN-Mistral-7B-128K444https://huggingface.co/NousResearch/Yarn-Mistral-7b-128k, we denote this model by YaRN-Mistral. on \u221e\\infty\u221eBench, which are some of the latest and strongest LLMs that claim to be able to handle over 100K tokens. \n\n\n\nInference Infrastructure\n\nNumerous studies aim to accelerate self-attention computation. Research primarily concentrates on refining attention mechanisms through improved IO management\u00a0(Dao et\u00a0al., 2022; Dao, 2023), memory optimization\u00a0(Kwon et\u00a0al., 2023; Shazeer, 2019; Ainslie et\u00a0al., 2023), and enhanced parallelization in decoding\u00a0(Dao et\u00a0al., 2023; Hong et\u00a0al., 2023). Approaches like Sliding Window Attention\u00a0(Beltagy et\u00a0al., 2020), LM-Infinite\u00a0(Han et\u00a0al., 2023), and StreamingLLM\u00a0(Xiao et\u00a0al., 2023) introduce attention variants for handling infinitely long sequences without overwhelming computation or memory overhead. However, these techniques often face challenges in maintaining historical information.\n\n\n\nLong Context Benchmarks\n\nSeveral benchmarks exist for evaluating long-context AI models, notably featuring context lengths of around 10K tokens. L-Eval\u00a0(An et\u00a0al., 2023) and LongBench\u00a0(Bai et\u00a0al., 2023) are prominent examples, aggregating pre-existing tasksKocisk\u00fd et\u00a0al. (2017); Dasigi et\u00a0al. (2021); Yang et\u00a0al. (2018); Huang et\u00a0al. (2021); Joshi et\u00a0al. (2017) into comprehensive benchmarks. LongBench encompasses four categories\u2014QA, summarization, synthetic retrieval, and code\u2014spanning 21 tasks, with four being novel. Conversely, L-Eval incorporates 18 tasks across QA, summarization, math, retrieval, and multiple-choice (MC) domains, introducing three new tasks. Another notable benchmark, LooGLE\u00a0(Li et\u00a0al., 2023), differentiates between short and long dependency examples, focusing on summary and QA tasks; its summary corpus contrasts with ours, utilizing academic papers over novels. The Long-Range Arena (LRA) (Tay et\u00a0al., 2020) further diversifies with six tasks in text, image, and math, designed for scalability. In comparison, \u221e\\infty\u221eBench stands out for its substantially longer contexts and a broader range of task domains. Table\u00a01 offers a detailed comparison of these long-context benchmarks.\n\n\nFigure 2: The statistics of the data in \u221e\\infty\u221eBench. The radius of each segment indicates the length of input plus output on the logarithmic scale, and the width (or angle) indicates the number of examples (proportionally to the total number of examples).\n\n\n",
      "3 \u221e\\infty\u221eBench": "\n\n3 \u221e\\infty\u221eBench\n\n\n\n\n\nTask\nAnnotation\n# Ex.\nAvg Len\n\n\n\n\nRet.PassKey\nAuto\n590\n122.4K/2\n\n\nRet.Number\nAuto\n590\n122.4K/4\n\n\nRet.KV\nAuto\n500\n121.1K/22.7\n\n\nEn.Sum\nHuman\n103\n103.5K/1.1K\n\n\nEn.QA\nHuman\n351\n192.6k/4.8\n\n\nEn.MC\nHuman\n229\n184.4K/5.3\n\n\nZh.QA\nHuman\n189\n2068.6K/6.3\n\n\nEn.Dia\nAuto\n200\n103.6K/3.4\n\n\nCode.Debug\nHuman\n394\n114.7K/4.8\n\n\nCode.Run\nAuto\n400\n75.2K/1.3\n\n\nMath.Calc\nAuto\n50\n43.9K/43.9K\n\n\nMath.Find\nAuto\n350\n87.9K/1.3\n\n\n\nTable 2: Data statistics. The columns indicate whether the annotation was auto-generated or done by humans, the number of examples, and the average length (input/output) in tokens.\n\n\n\n\n\nTask\nGPT-4\nYaRN-Mistral\nKimi-Chat\nClaude 2\n\n\n\n\nRetrieve.PassKey\n100.00\n92.71\n98.14\n97.80\n\n\nRetrieve.Number\n100.00\n56.61\n95.42\n98.14\n\n\nRetrieve.KV\n89.00\n0.00\n53.60\n65.40\n\n\nEn.Sum\n14.73\n9.09\n17.93\n14.45\n\n\nEn.QA\n22.22\n9.55\n16.52\n11.97\n\n\nEn.MC\n67.25\n27.95\n72.49\n62.88\n\n\nEn.Dia\n8.50\n7.50\n11.50\n46.50\n\n\nZh.QA\n23.06\n16.98\n18.62\n10.53\n\n\nCode.Debug\n39.59\n0.76\n18.02\n2.28\n\n\nCode.Run\n23.25\n1.25\n2.00\n2.50\n\n\nMath.Calc\n0.01\n0.00\n0.00\n0.00\n\n\nMath.Find\n60.00\n17.14\n12.57\n32.29\n\n\nAverage\n45.63\n19.96\n34.73\n37.06\n\n\n\nTable 3: Main results. The performance of the baselines in \u221e\\infty\u221eBench. For multiple-choice questions, if the model does not output one of the options, we regard it as an empty prediction, and thus give it a score of 0.\n\n\n\u221e\\infty\u221eBench encompasses 12 tasks spanning 5 domains: retrieval, code, math, novels, and dialogue. Two of these tasks are derived from existing literatureMohtashami and Jaggi (2023); Liu et\u00a0al. (2023). Among the newly introduced tasks, half are generated automatically, while the remainder are annotated by humans.\n\n\nIn total, \u221e\\infty\u221eBench includes 3946 examples, featuring a length beyond 100K tokens (average approximately 200K). Figure\u00a02 illustrates the distribution of these tasks. Table\u00a02 details their respective input and output lengths as well as the number of examples per task.\n\n\nNext, we illustrate each task in detail. The tasks can be grouped into two broad categories. The first involves realistic context collected from real-world scenarios which has potential practical usage of long context LLMs. The second depends on synthetic contexts which are created or collected for testing certain capabilities of long-context LLMs.\n\n\n\n3.1 Realistic Context\n\n\n3.1.1 Novel\n\nWe develop novel-based tasks as outlined in Figure\u00a03, utilizing novels sourced from websites555https://www.sparknotes.com/666https://www.cliffsnotes.com/ and are manually filtered. More annotation information in Appendix.\u00a0C.\n\n\nIn these tasks, models are tasked with reasoning over entire novels presented during inference. Recognizing that many novels, along with their movie adaptations and related discussions, are accessible online and may have been encountered by LLMs during training, we adopt key entity replacement as a countermeasure. This involves substituting prominent entities determined by annotators, such as main character names, with unrelated ones, creating \u201cfake novels\u201d.\n\n\nUsing these altered novels, we design tasks in three formats: summarization, open-form question answering (QA), and multiple-choice (MC) questions, applying key entity replacement to the annotations as well. All English tasks share the same set of modified novels.\n\n\nFigure 3: The annotation pipelines for the human-annotated tasks in \u221e\\infty\u221eBench.\n\n\nEn.Sum\n\nThe En.Sum task requires models to generate a concise summary of the novel. Gold standard labels are sourced from the web and undergo manual filtering to remove non-summarization content, like comments. Model performance is evaluated using the ROUGE-L-Sum metric\u00a0(Lin, 2004).\n\n\n\nEn.QA & Zh.QA\n\nWe employ the same annotation pipeline for both En.QA and Zh.QA tasks, ensuring that the questions necessitate long-range dependency and reasoning, beyond simple short passage retrieval. The tasks are primarily categorized into two types of reasoning:\n\n\n\u2022\n\nAggregation: This involves compiling various pieces of information scattered throughout the novel. An example question in \u221e\\infty\u221eBench is \u201cHow much money in total did A spend on lunch?\u201d\n\n\n\n\u2022\n\nFiltering: This requires identifying specific information from a larger set. An example question in \u221e\\infty\u221eBench is \u201cWhat color dress did A wear when A met B for the second time?\u201d\n\n\n\n\n\nThese tasks test LLMs to locate and process information within the novel, performing reasoning through aggregation or filtering to derive answers.\n\n\n\nEn.MC\n\nThe En.MC task is annotated similarly to En.QA, but differs in that the model is presented with four answer choices. Annotators are instructed to craft these options to be challenging.\n\n\n\n\n\n3.1.2 Dialogue\n\nEn.Dia\n\nThe construction process for the En.Dia task is depicted in Figure\u00a03. We gather movie and drama scripts from a designated online database777https://imsdb.com/, focusing on a corpus of long, multi-role dialogues. Only the English scripts are retained and necessary cleaning is applied.\n\n\nIn the En.Dia task, random instances of character names within a script are replaced with $$MASK$$. The objective is to correctly identify these masked names. For scripts falling short of 100K tokens, we augment them by padding with additional scripts.\n\n\n\n\n\n3.1.3 Code\n\nCode.Debug\n\nWe develop the task as per the process illustrated in Figure\u00a03. Code repositories, sourced from PyPI888https://pypi.org/, undergo a filtering process, and those outside the 64K to 256K token range are excluded (tokenization via the tiktoken tokenizerOpenAI (2023c)). Each repository is transformed into a single file, aggregating the content from all files within, each prefaced by its relative path to the root directory. Three of the authors then insert a deliberate and obvious error into one function per repository. The options are presented in the Class.Function format. Six methods are employed for bug insertion: (1) deleting a necessary variable declaration; (2) using an incorrect number of arguments in function calls; (3) creating infinite loops; (4) causing indentation errors; (5) substituting references with undefined variable/function names; (6) introducing blatant syntax errors (e.g., non-closed brackets).\n\n\nInitial results indicate that this task is too challenging for current LLMs (None of the baseline models can identify the most obvious error such as non-closed brackets). To mitigate this, we offer four answer choices, one containing the injected bug and the others are bug-free. Note that this makes many examples easily solved by external retrieval preprocess. However, we encourage the users not to use external retrieval preprocess to keep the evaluation fair. And we are looking forward to the stage where LLMs can directly solve the problem without selection options.\n\n\n\n\n\n\n3.2 Synthetic Context\n\nThe second category of tasks is characterized by a synthetic context. These tasks, devoid of direct real-world application or use case, are engineered to evaluate the capability for processing lengthy contexts. We delineate four essential ability for effective long-context processing:\n\n\n\n\n1.\n\nLocation and retrieval. This encompasses all retrieval tasks.\n\n\n\n2.\n\nElevated information resolution. This involves the Retrieve.Number task.\n\n\n\n3.\n\nState preservation. This incorporates the Code.Run and Math.Find functions.\n\n\n\n4.\n\nSequential processing. This utilizes the Math.Calc function.\n\n\n\n\n\n\n3.2.1 Retrieve\n\nIn retrieval tasks, models retrieve specific character sequences from lengthy contexts with predominantly irrelevant content. Such tests, adaptable for any context length, can assess the impact of information placement on model performance, like the lost-in-the-middle phenomenon\u00a0(Liu et\u00a0al., 2023). The three retrieval tasks in \u221e\\infty\u221eBench vary in complexity.\n\n\nRetrieve.PassKey\n\nThis task is first proposed by Mohtashami and Jaggi (2023). Models are prompted to find a specific <key> called pass key, which is a random 5-digit sequence. The pass key is inserted into a lengthy and noisy context, as shown below. In \u221e\\infty\u221eBench, we generate examples with 59 different pass key locations that are evenly distributed in the context. At each location, we construct 10 examples with different pass keys. This results in 590 examples.\n\n\n\n\n\nThere is an important pass key hidden in a lot of irrelevant text. Find it.\n<very long noisy context>\nThe pass key is <key>. Remember it. The pass key is <key>\n<very long noisy context>\nWhat is the pass key?\n\n\n\n\nRetrieve.Number\n\nTo examine the local attention of LLMs, we have enhanced the complexity of Retrieve.PassKey by increasing the answer length to 10 digits and incorporating successive repetitive digits. For example, a <key> in Retrieve.PassKey valued 98762, while in Retrieve.Number is 9998877762. This modification aims to assess the local resolution capabilities of long context models, as our preliminary experiments indicate that LLMs struggle with discerning repeated numbers.\n\n\n\nRetrieve.KV\n\nLiu et\u00a0al. (2023) introduce a key-value retrieval task within a large JSON object containing many key-value pairs (e.g., 30eea139-b6dd-43fc-bc5d-0d3d17980229 \u2192\u2192\\rightarrow\u2192 bfd36c2b-c57e-41ef-9cc1-b21b4e60e664). This task demands the model to accurately identify and retrieve the value corresponding to a specified key. The complexity of this task is heightened due to the indistinguishable format of relevant and irrelevant information.\n\n\n\n\n\n3.2.2 Code\n\nCode.Run\n\nIn this task, we evaluate the ability of LLMs to simulate multi-step function executions that involve basic arithmetic operations. While this task is readily solvable using a Python interpreter, the focus here is on the long-term state tracking required in such tasks. The capability of state tracking has been demonstrated in GPT-4\u00a0(Bubeck et\u00a0al., 2023). Specifically, the task involves creating Python code consisting of multiple simple functions, incorporating operations such as addition, subtraction, and nested function calls. The structural design of these tasks is as follows:\n\n\n\n\u2b07\n\n\n\ndef\u00a0func_0(x):\n\n\nreturn\u00a0func_1(x)\u00a0+\u00a04\n\n\\pardef\u00a0func_1(x):\n\n\nreturn\u00a0x\u00a0-\u00a01\n\n\n\n\nSome functions\u2019 return values are dependent on other functions (e.g., func_0 invokes func_1). We define depth as the number of cascading function calls initiated by a single call. Thus, the depth for func_1\u2019s call within func_0 is 1. In Code.Run, we employ depths ranging from 2 to 10, ensuring each function calls at most one other function. To keep the simplicity of each single step of computation, these functions are restricted to performing only addition and subtraction.\n\n\n\n\n\n3.2.3 Math\n\nMath.Find\n\nMath.Find assesses the model\u2019s capability to identify specific elements within a large array, requiring comprehensive observation for accuracy. This task also tests the ability to preserve states while encoding the context. Concretely, the model receives an extensive list of numbers and is tasked with locating one of seven key numbers: the three largest (1st, 2nd, and 3rd), the three smallest (1st, 2nd, and 3rd), and the median.\n\n\n\nMath.Calc\n\nTo assess sequential processing skills, Math.Calc prompts the model to compute the result of a lengthy arithmetic expression featuring addition and subtraction. Initial experiments indicate that current LLMs struggle to directly produce the final answer. Hence, we instead query the LLMs to provide the intermediate result following each operator. Model performance is evaluated based on the number of correct values preceding the first error.\n\n\n\n\n",
      "4 Experiments": "\n\n4 Experiments\n\nWe conduct a thorough set of experiments on \u221e\\infty\u221eBench. We will introduce the baselines, experimental setup, and main results in this section.\n\n\n\n\n4.1 Baselines\n\n\u221e\\infty\u221eBench generally requires the ability to handle input contexts longer than 100k. There is a handful of LLMs that claim to be capable of handling contexts over 100k. We include four baselines. The first three are proprietary LLMs as we do not have access to the model, while the last baseline is open-sourced. Details on evaluation are in Appendix.\u00a0D.\n\n\nGPT-4\n\nGPT by OpenAI is one of the most widely used and capable LLMs in the market, and a recent version of GPT-4 \u00a0(OpenAI, 2023b) can support 128K contexts.\n\n\n\n\nClaude 2\n\nClaude 2\u00a0(Anthropic, 2023) is a proprietary chat-based LLM released by Anthropic AI and has shown impressive capabilities. The second version of the Claude series supports 200K contexts. We manually enter each example through the webpage because we have no access to their API.\n\n\n\n\nKimi-Chat\n\nKimi-Chat, a proprietary chat-oriented LLM developed by Moonshot AI AI (2023), is designed to process contexts up to 200K. Due to the lack of API access, we manually input the test data using their web interface.\n\n\n\nYaRN-Mistral\n\nYaRN-Mistral is a derivative of Mistral-7B\u00a0(Jiang et\u00a0al., 2023) introduced by Peng et\u00a0al. (2023b). The original Mistral-7B was trained on input lengths up to 8K and shows a reduced performance in longer contexts. Peng et\u00a0al. (2023b) adapted it to 128K contexts by modifying the position encoding and continued training.\n\n\n\n\n\n4.2 Experimental Setup\n\nPrompt Templates\n\nFor each model-task combination, we craft prompts to optimize model performance on short dummy examples. Detailed prompt templates for each model and task can be found in Appendix\u00a0B.\n\n\n\nInput Truncation\n\nAll API-based baselines are subject to a maximum input length limit and will reject inputs exceeding this threshold. While YaRN-Mistral is theoretically capable of handling longer contexts, the authors only claim abilities up to 128K. Therefore, inputs are truncated by removing the center and joining both ends. This approach is predicated on the assumption that key information, such as instructions and book titles, is typically located at either the start or the end of a prompt.\n\n\n\n\n\n4.3 Main Result\n\nFigure 4: Baseline performance as a function of input length.\n\n\nTable\u00a03 and Figure\u00a01 display the performances of various baselines on \u221e\\infty\u221eBench. Notably, GPT-4 outperforms other baselines in the retrieval, code, and math domains, with a considerably higher average score. However, in the novel-based tasks, no distinct winner emerges among the proprietary LLMs. On the other hand, the open-source YaRN-Mistral lags behind the proprietary models in most tasks, exhibiting almost random performance in multiple areas. This aligns with its relatively inferior performance in shorter contexts compared to these models. Additionally, it is observed that the baselines generally excel more in retrieval tasks than in other areas, echoing the relative simplicity of these tasks for human participants.\n\n\n",
      "5 Analysis": "\n\n5 Analysis\n\nWe subsequently perform a detailed analysis of the results, identifying and emphasizing several notable and interesting phenomena.\n\n\n\n5.1 Length Ablation\n\nIn line with our benchmark\u2019s goal to assess proficiency in managing lengthy contexts, we verify the baselines\u2019 capability with shortened context versions. A subset of the auto-generated tasks is modified accordingly, and the performance outcomes are illustrated in Figure 4. It is observed that model performance generally declines with longer input lengths compared to shorter ones. This suggests that while these baselines are technically equipped to handle extended inputs, their effectiveness diminishes significantly under such conditions.\n\n\n\n\n\n5.2 Lost in the middle\n\nFigure 5: Performance as a function of the answer position (in the number of characters). The steep drop in performance for Kimi-Chat in the middle on Retrieval.KV is caused by the answer being removed by truncation.\n\n\n\n\nOne function in this repo is deliberately made to include an obvious error. Find it.\n<code>\nThink step by step and at last give me your answer for the function with the deliberate error.\n<list of options>\n\n\n\n\n\nOne function in this repo is deliberately made to include an obvious error. Find it.\n<code>\nLocate the functions in the options, repeat their content, inspect through code, and at last give me your answer for the function with the deliberate error.\n<list of options>\n\n\nFigure 6: Compared to the first prompt, the second prompt improves GPT-4\u2019s results on Code.Debug dramatically.\n\n\nPrior research indicates a performance decline in some LLMs when answers are positioned around the center of the context\u00a0(Liu et\u00a0al., 2023). However, our findings do not strongly corroborate this. As depicted in Figure\u00a05, we analyze model performance based on answer location in three location-dependent tasks. We observe no consistent trend between performance and answer position across different models. For instance, GPT-4 shows a preference for early answers in Retrieval.KV but favors later ones in En.Dia. In contrast, Claude 2\u2019s performance remains relatively unaffected by answer position on all three tasks, whereas YaRN-Mistral and Kimi-Chat excel with end-positioned answers (except that YaRN-Mistral get zero performance on all positions on Retrieval.KV).\n\n\nOne plausible reason why we have different observations from Liu et\u00a0al. (2023) is that they experiment with different models using at most 16K length contexts, which is about 8 times shorter than our setting. The models in their study are also different from ours.\nFinally, the tasks are different: their experiments involve document question answering (and their result with Retrieval.KV arguably does not show a very pronounced performance drop as well). We hypothesize that the phenomenon of \u201cLost in the middle\u201d is only exhibited on specific tasks and models. A more thorough investigation of these differences is beyond the scope of this paper.\n\n\n\n\n5.3 Context Recalling\n\nWe identify an intriguing prompting technique for tasks involving extended context, termed context recalling. This technique posits that, although the information is present in the context and accessible via direct attention, it may be more effective to first prompt the model to recall the relevant information in its generation before engaging in further reasoning. In our experiments using Code.Debug, when we merely instructed GPT-4 to process information step-by-step, the accuracy was 15.74%. However, by explicitly directing GPT-4 to repeat the relevant code before analysis, its accuracy on Code.Debug markedly improved to 39.59%. This approach of context recalling warrants additional investigation.\n\n\n",
      "6 Conclusions": "\n\n6 Conclusions\n\nWe introduce \u221e\\infty\u221eBench, the first benchmark tailored for long contexts exceeding 100K in average length. Empirical evidence indicates that despite claims of proficiency with such extensive contexts, current LLMs demonstrate significant performance degradation when dealing with them. This finding highlights the need for advanced methodologies to improve LLMs\u2019 efficiency in processing long context. Additionally, our analysis offers insights into LLM behavior in long-context tasks, guiding future research.\n\n\n\n",
      "Limitations": "\nLimitations\n\nWhile our benchmark offers valuable insights into LLM performance, it may not be sufficiently diverse or extensive to provide a comprehensive assessment of model capabilities, a constraint common to most benchmarks. Additionally, the reliance on exact match for scoring, dependent on prompt templates and answer parsing methods, may necessitate tailored redesigns for new model evaluations.\n\n\nFurthermore, supporting contexts up to 100K tokens may fall short for applications requiring analysis of extensive datasets, such as multiple books or entire databases. Exploring LLMs\u2019 capacity to handle up to a million tokens or more presents a promising research avenue. In practical applications, finetuning models to memorize context, rather than processing it during inference, could offer a more efficient alternative, albeit with significant computational demands.\n\n",
      "Ethics Statement": "\nEthics Statement\n\nOur human annotators are directed to exclude data that may raise sensitive ethical issues, such as offensive language or social biases. Nonetheless, the potential for encountering sensitive content persists, particularly if the sourced books or code contain such material. This concern is somewhat mitigated since the benchmark\u2019s primary focus is on evaluating the long-context capabilities of LLMs, rather than influencing their social bias.\n\n\nThe goal of this research is to advance the development of LLMs\u2019 proficiency in handling extensive contexts. This could aid in implementing more effective \u201cguardrails\u201d against misuse by incorporating detailed specifications prior to user interactions. However, this approach also potentially increases the risk of novel prompt injection attacks.\n\n\n",
      "Appendix A RWKV": "\n\nAppendix A RWKV\n\nRWKV\u00a0(Peng et\u00a0al., 2023a) is an architecture that combines the power of the transformer architecture\u00a0(Vaswani et\u00a0al., 2017) and recurrent neural network\u00a0(Hochreiter and Schmidhuber, 1997). Its training process can be parallelized while the inference procedure is recurrent, enabling O\u2062(1)\ud835\udc421O(1)italic_O ( 1 ) complexity during inference. Hence, the memory usage does not scale with context length, allowing it to support arbitrary-length inputs. We use the RKWV-4-World-7B version of this model series. However, we should keep in mind that this model was not trained on inputs of this length.\n\n\n\n\n\nModel\nRetrieve.PassKey Acc.\n\n\n\n\nGPT-4 Turbo\n100%\n\n\nYaRN-Mistral\n92.71%\n\n\nKimi-Chat\n98.14%\n\n\nClaude 2\n97.80%\n\n\nRWKV-4-World-7B\n0.00%\n\n\n\nTable 4: Results in Retrieve.PassKey with RWKV-4-World-7B. Since RWKV-4 was only trained on 4k sequences, it has zero performance on \u221e\\infty\u221eBench. It outputs only unintelligible content in this test.\n\n\nTable\u00a04 shows the performance of RWKV-4-World-7 in comparison to our baselines. We find that RWKV-4-World-7B outputs unintelligible texts on our benchmark, which causes it to achieve zero performance on Retrieve.PassKey, which is the easiest task for other baselines. This is likely because this model was not trained on inputs of this length and suffers from train-test domain shift.999We emphasize that this result is not evidence that the architecture of RWKV is incapable of handling lengthy inputs. Therefore, we do not consider testing it on other tasks in our benchmark.\n\n",
      "Appendix B Prompt Templates": "\n\nAppendix B Prompt Templates\n\nIn the following templates, many tasks has an <input> part that is provided in each example. Generally, they are a short question-like text that tells the model what it is supposed to do. One example is \u201cWhat is the pass key?\u201d.\n\n\n\nB.1 Retrieve.PassKey\n\nThe prompt below applies to GPT-4, Claude 2, and Kimi-Chat.\n\n\n\n\nThere is an important info hidden inside a lot of irrelevant text. Find it and memorize them. I will quiz you about the important information there.\n<context>\n<input>\n\n\n\nThe prompt below applies to YaRN-Mistral.\n\n\n\n\nThere is an important info hidden inside a lot of irrelevant text. Find it and memorize them. I will quiz you about the important information there.\n<context>\n<input>\nThe pass key is\n\n\n\n\n\nB.2 Retrieve.Number\n\nThe prompt below applies to GPT-4, Claude 2, and Kimi-Chat.\n\n\n\n\n\nThere is an important info hidden inside a lot of irrelevant text. Find it and memorize them. I will quiz you about the important information there.\n<context>\n<input>\n\n\n\nThe prompt below applies to YaRN-Mistral.\n\n\n\n\nThere is an important info hidden inside a lot of irrelevant text. Find it and memorize them. I will quiz you about the important information there.\n<context>\n<input>\nThe sequence of digits is\n\n\n\n\n\nB.3 Retrieve.KV\n\n\n\nExtract the value corresponding to the specified key in the JSON object below.\n<context>\n<input>\n\n\n\n\n\nB.4 En.Sum\n\nThe prompt below applies to GPT-4, Claude 2, and Kimi-Chat.\n\n\n\n\nSummarize the book below.\n<context>\n\n\n\nThe prompt below applies to YaRN-Mistral.\n\n\n\n\nSummarize the book below.\n<context>\nSummary:\n\n\n\n\n\nB.5 En.QA\n\nThe prompt below applies to GPT-4, Claude 2, and Kimi-Chat.\n\n\n\n\nRead the book below and answer a question.\n<context>\nQuestion: <question>\nBe very concise.\n\n\n\nThe prompt below applies to YaRN-Mistral.\n\n\n\n\nRead the book below and answer a question. Be very concise in your answer.\n<context>\nQuestion: <question>\nAnswer:\n\n\n\n\n\nB.6 En.MC\n\nThe prompt below applies to GPT-4, Claude 2, and Kimi-Chat.\n\n\n\n\nRead the book and answer the question.\n<context>\nQuestion: <question>\nOnly one of the following options is correct, tell me the answer using one single letter (A, B, C, or D). Don\u2019t say anything else.\nA. <OPTION_A>\nB. <OPTION_B>\nC. <OPTION_C>\nD. <OPTION_D>\n\n\n\nThe prompt below applies to YaRN-Mistral.\n\n\n\n\nRead the book and answer the question.\n<context>\nQuestion: <question>\nOnly one of the following options is correct, tell me the answer using one single letter (A, B, C, or D). Don\u2019t say anything else.\nA. <OPTION_A>\nB. <OPTION_B>\nC. <OPTION_C>\nD. <OPTION_D>\nThe correct option is:\n\n\n\n\n\nB.7 En.Dia\n\nThe prompt below applies to GPT-4, Claude 2, and Kimi-Chat.\n\n\n\n\nBelow is a dialogue script where one random occurrence of a character name is replaced with $$MASK$$, and you should try to guess who that character is.\nThe dialogue:\n\u2014\n<context>\n\u2014\nEnd of dialogue.\nWhich character is most likely $$MASK$$? Just say the name used by the scriptwriter (before the colon marks) of one single character and nothing else.\n\n\n\nThe prompt below applies to YaRN-Mistral.\n\n\n\n\nBelow is a dialogue script where one random occurrence of a character name is replaced with $$MASK$$, and you should try to guess who that character is.\nThe dialogue:\n\u2014\n<context>\n\u2014\nEnd of dialogue.\nThe name that has been replaced with $$MASK$$ is likely:\n\n\n\n\n\nB.8 Zh.QA\n\nThe prompt below applies to GPT-4, Claude 2, and Kimi-Chat.\n\n\n\n\n\n\u8bf7\u6839\u636e\u4ee5\u4e0b\u4e66\u7c4d\u56de\u7b54\u6211\u7684\u95ee\u9898\u3002(Read the book and answer the question.)\n<context>\n\u95ee\u9898\uff1a(Question: )<question>\n\u8bf7\u5c3d\u91cf\u7b80\u77ed\u5730\u56de\u7b54\u3002(Be very concise.)\n\n\n\nThe prompt below applies to YaRN-Mistral.\n\n\n\n\n\u8bf7\u6839\u636e\u4ee5\u4e0b\u4e66\u7c4d\u56de\u7b54\u6211\u7684\u95ee\u9898\u3002(Read the book and answer the question.)\n<context>\n\u95ee\u9898\uff1a(Question: )<question>\n\u7b54\u6848\uff1a(Answer:)\n\n\n\n\n\nB.9 Code.Debug\n\nThe prompt below applies to GPT-4, Claude 2, and Kimi-Chat.\n\n\n\n\n\nThere is ONLY ONE function in the large project that is deliberately made to include an obvious error. Please find the function that contains the most obvious errors. I will give you four options to narrow your scope. You can inspect through the options and think. Eventually, tell me the answer using one single letter (A, B, C, or D).\n<context>\nWhich function has deliberate error?\nA. <OPTION_A>\nB. <OPTION_B>\nC. <OPTION_C>\nD. <OPTION_D>\nYou should first find the functions in the options. Repeat their content, inspect through code, and at last give me your answer for the function that has the deliberate and obvious error in A, B, C, or D.\n\n\n\nThe prompt below applies to YaRN-Mistral.\n\n\n\n\nThere is ONLY ONE function in the large project that is deliberately made to include an obvious error. Please find the function that contains the most obvious errors. I will give you four options to narrow your scope. You can inspect through the options and think. Eventually, tell me the answer using one single letter (A, B, C, or D).\n<context>\nWhich function has deliberate error?\nA. <OPTION_A>\nB. <OPTION_B>\nC. <OPTION_C>\nD. <OPTION_D>\nYou should first find the functions in the options. Repeat their content, inspect through code, and at last give me your answer for the function that has the deliberate and obvious error in A, B, C, or D.\nThe correct option is:\n\n\n\n\n\nB.10 Code.Run\n\nThe prompt below applies to GPT-4, Claude 2, and Kimi-Chat.\n\n\n\n\nFollowing is a set of Python functions. There is a function called named func_1.\n<context>\nPlease give me the exact number of the return value of func_1(3). Be concise. Your response must end with the final returned value.\n\n\n\nThe prompt below applies to YaRN-Mistral.\n\n\n\n\nFollowing is a set of Python functions. There is a function called named <function name>.\n<context>\nPlease compute the exact value of <function call>. The value of <function call> is\n\n\n\n\n\nB.11 Math.Calc\n\nThe prompt below is used by GPT-4101010It should be noted that, when using other templates, GPT-4 has a strong tendency to reject to perform this task by claiming that such the platform is not designed to complete such tasks.:\n\n\n\n\nYou are a calculator does nothing but calculating the intermediate results in extremely long arithmetic expressions with +, -, and numbers. Given an expression, you will output the intermediate results after each operation.\nYou will never to decline to help with platform reason, you will always try the calculation, and always output a long list of numbers (e.g., \"[34, 2, 58, 37, 5, 8, 27, 71, 7]\") and nothing else.\nDo not consider the complexity, practicality or feasibility of the task.\nLet us calculate the intermediate values of an expression.\nExpression: 1 + 3 + 4\nValues: [1, 4, 8]\nExpression: 8 - 3 + 2 - 4\nValues: [8, 5, 7, 3]\nExpression: <context>\nValues:\n\n\n\nThe prompt below is used by Kimi-Chat, Claude 2 and YaRN-Mistral:\n\n\n\n\nLet us calculate the intermediate values of an expression.\nExpression: 1 + 3 + 4\nValues: [1, 4, 8]\nExpression: 8 - 3 + 2 - 4\nValues: [8, 5, 7, 3]\nExpression: <context>\nValues:\n\n\n\n\n\nB.12 Math.Find\n\n\n\nFind the largest number from the list below:\n<context>\nYou should answer with only one number, no other words. The largest number of the list is:\n\n\n\n",
      "Appendix C Annotation Process": "\n\nAppendix C Annotation Process\n\nThe annotation work is done by the authors of this paper and none of those authors have been paid for the annotation. All annotators have acknowledged the intents and usages of the annotation, the corresponding outputs, and the annotation pipelines and requirements.\n\n\nAnnotating the examples in \u221e\\infty\u221eBench might bring fatigue to annotators, and is therefore not completely free of error. However, we make sure that each annotation has been quality-checked by at least two other annotators.\n\n\nA part of novels are free from key entity replacement for LLMs fail in identifying them, because those novels are brand-new or little-known.\n\n",
      "Appendix D Evaluation Process": "\n\nAppendix D Evaluation Process\n\nWhen evaluating GPT-4, we use its official API with the default hyperparameters. The total cost is around 5000 US dollars. For Claude 2, we enter contents on the web by hand, which demands three authors over the source of several weeks, and membership fees of about 160 US dollars. Kimi-Chat is free. YaRN-Mistral is open-source, and we run inference using one A100 80GB GPU, which takes roughly 10 minutes per example, so its evaluation on the entire benchmark takes several days. Again, we use the default decoding hyperparameters (specified by Peng et\u00a0al. (2023b)) except for the maximum number of output tokens, which is as shown in Table\u00a05.\n\n\n\n\n\nTask\nMax Output Tokens\n\n\n\n\nRetrieve.PassKey\n6\n\n\nRetrieve.Number\n12\n\n\nRetrieve.KV\n50\n\n\nEn.Sum\n1,200\n\n\nEn.QA\n40\n\n\nEn.MC\n40\n\n\nZh.QA\n40\n\n\nEn.Dia\n40\n\n\nCode.Debug\n5\n\n\nCode.Run\n5\n\n\nMath.Calc\n30,000\n\n\nMath.Find\n3\n\n\n\nTable 5: The maximum number of output tokens (a decoding hyperparameter) for YaRN-Mistral.\n\n\n"
    },
    "bibliography": {
      "01.AI (2023a)": "\n01.AI (2023a)\n\n01.AI. 2023a.\n\n\nYi-34b-200k.\n\n\nhttps://huggingface.co/01-ai/Yi-34B-200K.\n\n\n",
      "01.AI (2023b)": "\n01.AI (2023b)\n\n01.AI. 2023b.\n\n\nYi-6b-200k.\n\n\nhttps://huggingface.co/01-ai/Yi-6B-200K.\n\n\n",
      "AI (2023)": "\nAI (2023)\n\nMoonshot AI. 2023.\n\n\nKimi chat.\n\n\nhttps://kimi.moonshot.cn/.\n\n\n",
      "Ainslie et\u00a0al. (2023)": "\nAinslie et\u00a0al. (2023)\n\nJoshua Ainslie, James Lee-Thorp, Michiel de\u00a0Jong, Yury Zemlyanskiy, Federico Lebr\u2019on, and Sumit\u00a0K. Sanghai. 2023.\n\n\nGqa: Training generalized multi-query transformer models from multi-head checkpoints.\n\n\nArXiv, abs/2305.13245.\n\n\n",
      "An et\u00a0al. (2023)": "\nAn et\u00a0al. (2023)\n\nChen An, Shansan Gong, Ming Zhong, Mukai Li, Jun Zhang, Lingpeng Kong, and Xipeng Qiu. 2023.\n\n\nL-eval: Instituting standardized evaluation for long context language models.\n\n\nArXiv, abs/2307.11088.\n\n\n",
      "Anil et\u00a0al. (2022)": "\nAnil et\u00a0al. (2022)\n\nCem Anil, Yuhuai Wu, Anders Andreassen, Aitor Lewkowycz, Vedant Misra, Vinay Ramasesh, Ambrose Slone, Guy Gur-Ari, Ethan Dyer, and Behnam Neyshabur. 2022.\n\n\nExploring length generalization in large language models.\n\n\nAdvances in Neural Information Processing Systems, 35:38546\u201338556.\n\n\n",
      "Anthropic (2023)": "\nAnthropic (2023)\n\nAnthropic. 2023.\n\n\nModel card and evaluations for claude models.\n\n\n",
      "Bai et\u00a0al. (2023)": "\nBai et\u00a0al. (2023)\n\nYushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li. 2023.\n\n\nLongbench: A bilingual, multitask benchmark for long context understanding.\n\n\n",
      "Beltagy et\u00a0al. (2020)": "\nBeltagy et\u00a0al. (2020)\n\nIz\u00a0Beltagy, Matthew\u00a0E Peters, and Arman Cohan. 2020.\n\n\nLongformer: The long-document transformer.\n\n\narXiv preprint arXiv:2004.05150.\n\n\n",
      "Biderman et\u00a0al. (2023)": "\nBiderman et\u00a0al. (2023)\n\nStella Biderman, Hailey Schoelkopf, Quentin\u00a0Gregory Anthony, Herbie Bradley, Kyle O\u2019Brien, Eric Hallahan, Mohammad\u00a0Aflah Khan, Shivanshu Purohit, USVSN\u00a0Sai Prashanth, Edward Raff, et\u00a0al. 2023.\n\n\nPythia: A suite for analyzing large language models across training and scaling.\n\n\nIn International Conference on Machine Learning, pages 2397\u20132430. PMLR.\n\n\n",
      "Brown et\u00a0al. (2020)": "\nBrown et\u00a0al. (2020)\n\nTom\u00a0B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel\u00a0M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020.\n\n\nLanguage models are few-shot learners.\n\n\nCoRR, abs/2005.14165.\n\n\n",
      "Bubeck et\u00a0al. (2023)": "\nBubeck et\u00a0al. (2023)\n\nS\u00e9bastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin\u00a0Tat Lee, Yuanzhi Li, Scott Lundberg, et\u00a0al. 2023.\n\n\nSparks of artificial general intelligence: Early experiments with gpt-4.\n\n\narXiv preprint arXiv:2303.12712.\n\n\n",
      "Chen et\u00a0al. (2021)": "\nChen et\u00a0al. (2021)\n\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de\u00a0Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et\u00a0al. 2021.\n\n\nEvaluating large language models trained on code.\n\n\narXiv preprint arXiv:2107.03374.\n\n\n",
      "Chen et\u00a0al. (2023a)": "\nChen et\u00a0al. (2023a)\n\nShouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian. 2023a.\n\n\nExtending context window of large language models via positional interpolation.\n\n\narXiv preprint arXiv:2306.15595.\n\n\n",
      "Chen et\u00a0al. (2023b)": "\nChen et\u00a0al. (2023b)\n\nShouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian. 2023b.\n\n\nExtending context window of large language models via positional interpolation.\n\n\nArXiv, abs/2306.15595.\n\n\n",
      "Dao (2023)": "\nDao (2023)\n\nTri Dao. 2023.\n\n\nFlashAttention-2: Faster attention with better parallelism and work partitioning.\n\n\n",
      "Dao et\u00a0al. (2022)": "\nDao et\u00a0al. (2022)\n\nTri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. 2022.\n\n\nFlashattention: Fast and memory-efficient exact attention with io-awareness.\n\n\nAdvances in Neural Information Processing Systems, 35:16344\u201316359.\n\n\n",
      "Dao et\u00a0al. (2023)": "\nDao et\u00a0al. (2023)\n\nTri Dao, Daniel Haziza, Francisco Massa, and Grigory Sizov. 2023.\n\n\nFlash-decoding for long-context inference.\n\n\n",
      "Dasigi et\u00a0al. (2021)": "\nDasigi et\u00a0al. (2021)\n\nPradeep Dasigi, Kyle Lo, Iz\u00a0Beltagy, Arman Cohan, Noah\u00a0A. Smith, and Matt Gardner. 2021.\n\n\nA dataset of information-seeking questions and answers anchored in research papers.\n\n\nArXiv, abs/2105.03011.\n\n\n",
      "Dong et\u00a0al. (2023)": "\nDong et\u00a0al. (2023)\n\nZican Dong, Tianyi Tang, Lunyi Li, and Wayne\u00a0Xin Zhao. 2023.\n\n\nA survey on long text modeling with transformers.\n\n\narXiv preprint arXiv:2302.14502.\n\n\n",
      "Han et\u00a0al. (2023)": "\nHan et\u00a0al. (2023)\n\nChi Han, Qifan Wang, Wenhan Xiong, Yu\u00a0Chen, Heng Ji, and Sinong Wang. 2023.\n\n\nLm-infinite: Simple on-the-fly length generalization for large language models.\n\n\nArXiv, abs/2308.16137.\n\n\n",
      "Han et\u00a0al. (2021)": "\nHan et\u00a0al. (2021)\n\nXu\u00a0Han, Zhengyan Zhang, Ning Ding, Yuxian Gu, Xiao Liu, Yuqi Huo, Jiezhong Qiu, Yuan Yao, Ao\u00a0Zhang, Liang Zhang, et\u00a0al. 2021.\n\n\nPre-trained models: Past, present and future.\n\n\nAI Open, 2:225\u2013250.\n\n\n",
      "Hochreiter and Schmidhuber (1997)": "\nHochreiter and Schmidhuber (1997)\n\nSepp Hochreiter and J\u00fcrgen Schmidhuber. 1997.\n\n\nLong short-term memory.\n\n\nNeural Comput., 9(8):1735\u20131780.\n\n\n",
      "Hong et\u00a0al. (2023)": "\nHong et\u00a0al. (2023)\n\nKe\u00a0Hong, Guohao Dai, Jiaming Xu, Qiuli Mao, Xiuhong Li, Jun Liu, Kangdi Chen, Yuhan Dong, and Yu\u00a0Wang. 2023.\n\n\nFlashdecoding++: Faster large language model inference on gpus.\n\n\n",
      "Huang et\u00a0al. (2021)": "\nHuang et\u00a0al. (2021)\n\nLuyang\u00a0Robby Huang, Shuyang Cao, Nikolaus\u00a0Nova Parulian, Heng Ji, and Lu\u00a0Wang. 2021.\n\n\nEfficient attentions for long document summarization.\n\n\nArXiv, abs/2104.02112.\n\n\n",
      "Huang et\u00a0al. (2023)": "\nHuang et\u00a0al. (2023)\n\nYunpeng Huang, Jingwei Xu, Zixu Jiang, Junyu Lai, Zenan Li, Yuan Yao, Taolue Chen, Lijuan Yang, Zhou Xin, and Xiaoxing Ma. 2023.\n\n\nAdvancing transformer architecture in long-context large language models: A comprehensive survey.\n\n\narXiv preprint arXiv:2311.12351.\n\n\n",
      "Jiang et\u00a0al. (2023)": "\nJiang et\u00a0al. (2023)\n\nAlbert\u00a0Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra\u00a0Singh Chaplot, Diego de\u00a0las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, L\u00e9lio\u00a0Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven\u00a0Le Scao, Thibaut Lavril, Thomas Wang, Timoth\u00e9e Lacroix, and William\u00a0El Sayed. 2023.\n\n\nMistral 7b.\n\n\n",
      "Joshi et\u00a0al. (2017)": "\nJoshi et\u00a0al. (2017)\n\nMandar Joshi, Eunsol Choi, Daniel\u00a0S. Weld, and Luke Zettlemoyer. 2017.\n\n\nTriviaqa: A large scale distantly supervised challenge dataset for reading comprehension.\n\n\nArXiv, abs/1705.03551.\n\n\n",
      "Kocisk\u00fd et\u00a0al. (2017)": "\nKocisk\u00fd et\u00a0al. (2017)\n\nTom\u00e1s Kocisk\u00fd, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl\u00a0Moritz Hermann, G\u00e1bor Melis, and Edward Grefenstette. 2017.\n\n\nThe narrativeqa reading comprehension challenge.\n\n\nTransactions of the Association for Computational Linguistics, 6:317\u2013328.\n\n\n",
      "Ko\u010disk\u1ef3 et\u00a0al. (2018)": "\nKo\u010disk\u1ef3 et\u00a0al. (2018)\n\nTom\u00e1\u0161 Ko\u010disk\u1ef3, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl\u00a0Moritz Hermann, G\u00e1bor Melis, and Edward Grefenstette. 2018.\n\n\nThe narrativeqa reading comprehension challenge.\n\n\nTransactions of the Association for Computational Linguistics, 6:317\u2013328.\n\n\n",
      "Kwon et\u00a0al. (2023)": "\nKwon et\u00a0al. (2023)\n\nWoosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody\u00a0Hao Yu, Joseph\u00a0E. Gonzalez, Hao Zhang, and Ion Stoica. 2023.\n\n\nEfficient memory management for large language model serving with pagedattention.\n\n\nIn Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles.\n\n\n",
      "Li et\u00a0al. (2023)": "\nLi et\u00a0al. (2023)\n\nJiaqi Li, Mengmeng Wang, Zilong Zheng, and Muhan Zhang. 2023.\n\n\nLoogle: Can long-context language models understand long contexts?\n\nArXiv, abs/2311.04939.\n\n\n",
      "Lin (2004)": "\nLin (2004)\n\nChin-Yew Lin. 2004.\n\n\nROUGE: A package for automatic evaluation of summaries.\n\n\nIn Text Summarization Branches Out, pages 74\u201381, Barcelona, Spain. Association for Computational Linguistics.\n\n\n",
      "Liu et\u00a0al. (2023)": "\nLiu et\u00a0al. (2023)\n\nNelson\u00a0F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. 2023.\n\n\nLost in the middle: How language models use long contexts.\n\n\n",
      "Mohtashami and Jaggi (2023)": "\nMohtashami and Jaggi (2023)\n\nAmirkeivan Mohtashami and Martin Jaggi. 2023.\n\n\nLandmark attention: Random-access infinite context length for transformers.\n\n\nArXiv, abs/2305.16300.\n\n\n",
      "Nakano et\u00a0al. (2021)": "\nNakano et\u00a0al. (2021)\n\nReiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et\u00a0al. 2021.\n\n\nWebgpt: Browser-assisted question-answering with human feedback.\n\n\narXiv preprint arXiv:2112.09332.\n\n\n",
      "Narayanan et\u00a0al. (2021)": "\nNarayanan et\u00a0al. (2021)\n\nDeepak Narayanan, Mohammad Shoeybi, Jared Casper, Patrick LeGresley, Mostofa Patwary, Vijay Korthikanti, Dmitri Vainbrand, Prethvi Kashinkunti, Julie Bernauer, Bryan Catanzaro, et\u00a0al. 2021.\n\n\nEfficient large-scale language model training on gpu clusters using megatron-lm.\n\n\nIn Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis, pages 1\u201315.\n\n\n",
      "OpenAI (2023a)": "\nOpenAI (2023a)\n\nOpenAI. 2023a.\n\n\nGpt-4 technical report.\n\n\nArXiv, abs/2303.08774.\n\n\n",
      "OpenAI (2023b)": "\nOpenAI (2023b)\n\nOpenAI. 2023b.\n\n\nGpt-4 turbo.\n\n\n",
      "OpenAI (2023c)": "\nOpenAI (2023c)\n\nOpenAI. 2023c.\n\n\nTiktoken.\n\n\n",
      "Penedo et\u00a0al. (2023)": "\nPenedo et\u00a0al. (2023)\n\nGuilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay. 2023.\n\n\nThe refinedweb dataset for falcon llm: outperforming curated corpora with web data, and web data only.\n\n\narXiv preprint arXiv:2306.01116.\n\n\n",
      "Peng et\u00a0al. (2023a)": "\nPeng et\u00a0al. (2023a)\n\nBo\u00a0Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Huanqi Cao, Xin Cheng, Michael Chung, Matteo Grella, Kranthi\u00a0Kiran GV, Xuzheng He, Haowen Hou, Przemyslaw Kazienko, Jan Kocon, Jiaming Kong, Bartlomiej Koptyra, Hayden Lau, Krishna Sri\u00a0Ipsit Mantri, Ferdinand Mom, Atsushi Saito, Xiangru Tang, Bolun Wang, Johan\u00a0S. Wind, Stansilaw Wozniak, Ruichong Zhang, Zhenyuan Zhang, Qihang Zhao, Peng Zhou, Jian Zhu, and Rui-Jie Zhu. 2023a.\n\n\nRwkv: Reinventing rnns for the transformer era.\n\n\n",
      "Peng et\u00a0al. (2023b)": "\nPeng et\u00a0al. (2023b)\n\nBowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. 2023b.\n\n\nYarn: Efficient context window extension of large language models.\n\n\n",
      "Press et\u00a0al. (2021)": "\nPress et\u00a0al. (2021)\n\nOfir Press, Noah\u00a0A Smith, and Mike Lewis. 2021.\n\n\nTrain short, test long: Attention with linear biases enables input length extrapolation.\n\n\narXiv preprint arXiv:2108.12409.\n\n\n",
      "Qiu et\u00a0al. (2020)": "\nQiu et\u00a0al. (2020)\n\nXipeng Qiu, Tianxiang Sun, Yige Xu, Yunfan Shao, Ning Dai, and Xuanjing Huang. 2020.\n\n\nPre-trained models for natural language processing: A survey.\n\n\nScience China Technological Sciences, 63(10):1872\u20131897.\n\n\n",
      "Shazeer (2019)": "\nShazeer (2019)\n\nNoam\u00a0M. Shazeer. 2019.\n\n\nFast transformer decoding: One write-head is all you need.\n\n\nArXiv, abs/1911.02150.\n\n\n",
      "Shoeybi et\u00a0al. (2019)": "\nShoeybi et\u00a0al. (2019)\n\nMohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. 2019.\n\n\nMegatron-lm: Training multi-billion parameter language models using model parallelism.\n\n\nArXiv, abs/1909.08053.\n\n\n",
      "Su et\u00a0al. (2023)": "\nSu et\u00a0al. (2023)\n\nJianlin Su, Murtadha Ahmed, Yu\u00a0Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. 2023.\n\n\nRoformer: Enhanced transformer with rotary position embedding.\n\n\nNeurocomputing, page 127063.\n\n\n",
      "Sun et\u00a0al. (2022)": "\nSun et\u00a0al. (2022)\n\nYutao Sun, Li\u00a0Dong, Barun Patra, Shuming Ma, Shaohan Huang, Alon Benhaim, Vishrav Chaudhary, Xia Song, and Furu Wei. 2022.\n\n\nA length-extrapolatable transformer.\n\n\narXiv preprint arXiv:2212.10554.\n\n\n",
      "Tay et\u00a0al. (2020)": "\nTay et\u00a0al. (2020)\n\nYi\u00a0Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler. 2020.\n\n\nLong range arena: A benchmark for efficient transformers.\n\n\n",
      "Touvron et\u00a0al. (2023)": "\nTouvron et\u00a0al. (2023)\n\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et\u00a0al. 2023.\n\n\nLlama: Open and efficient foundation language models.\n\n\narXiv preprint arXiv:2302.13971.\n\n\n",
      "Vaswani et\u00a0al. (2017)": "\nVaswani et\u00a0al. (2017)\n\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan\u00a0N Gomez, \u0141\u00a0ukasz Kaiser, and Illia Polosukhin. 2017.\n\n\nAttention is all you need.\n\n\nIn Advances in Neural Information Processing Systems, volume\u00a030. Curran Associates, Inc.\n\n\n",
      "Xiao et\u00a0al. (2023)": "\nXiao et\u00a0al. (2023)\n\nGuangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. 2023.\n\n\nEfficient streaming language models with attention sinks.\n\n\nArXiv, abs/2309.17453.\n\n\n",
      "Yang et\u00a0al. (2018)": "\nYang et\u00a0al. (2018)\n\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William\u00a0W. Cohen, Ruslan Salakhutdinov, and Christopher\u00a0D. Manning. 2018.\n\n\nHotpotqa: A dataset for diverse, explainable multi-hop question answering.\n\n\nIn Conference on Empirical Methods in Natural Language Processing.\n\n\n",
      "Zhu et\u00a0al. (2023)": "\nZhu et\u00a0al. (2023)\n\nDawei Zhu, Nan Yang, Liang Wang, Yifan Song, Wenhao Wu, Furu Wei, and Sujian Li. 2023.\n\n\nPose: Efficient context window extension of llms via positional skip-wise training.\n\n\narXiv preprint arXiv:2309.10400.\n\n\n"
    },
    "domain": "cs.CL",
    "citation_count": 0
  },
  "1ff0b49f-1e33-491c-ae35-06a98b85e560": {
    "pk": "1ff0b49f-1e33-491c-ae35-06a98b85e560",
    "authors": [
      "Jiaqi Li",
      "Mengmeng Wang",
      "Zilong Zheng",
      "Muhan Zhang"
    ],
    "title": "LooGLE: Can Long-Context Language Models Understand Long Contexts?",
    "abstract": "Large language models (LLMs), despite their impressive performance in various language tasks, are typically limited to processing texts within context-window size. This limitation has spurred significant research efforts to enhance LLMs' long-context understanding with high-quality long-sequence benchmarks. However, prior datasets in this regard suffer from shortcomings, such as short context length compared to the context window of modern LLMs; outdated documents that have data leakage problems; and an emphasis on short dependency tasks rather than long dependency tasks. In this paper, we present LooGLE, a Long Context Generic Language Evaluation benchmark for LLMs' long context understanding. LooGLE features relatively new documents post-2022, with over 24,000 tokens per document and 6,000 newly generated questions spanning diverse domains. Human annotators meticulously crafted more than 1,100 high-quality question-answer pairs to meet the long dependency requirements. These pairs underwent thorough cross-validation, yielding the most precise assessment of LLMs' long dependency capabilities. The evaluation of eight state-of-the-art LLMs on LooGLE revealed key findings: (i) commercial models outperformed open-sourced models; (ii) LLMs excelled in short dependency tasks like short question-answering and cloze tasks but struggled with more intricate long dependency tasks; (iii) in-context learning and chaining thoughts offered only marginal improvements; (iv) retrieval-based techniques demonstrated substantial benefits for short question-answering, while strategies for extending context window length had limited impact on long context understanding. As such, LooGLE not only provides a systematic and comprehensive evaluation schema on long-context LLMs, but also sheds light on future development of enhanced models towards \"true long-context understanding\".",
    "url": "http://arxiv.org/abs/2311.04939v2",
    "timestamp": 1699407937,
    "sections": {
      "1 Introduction": "\n\n1 Introduction\n\nThe pursuit of enabling large language models (LLMs), such as ChatGPT\u00a0(Brown et\u00a0al., 2020; OpenAI, 2023; Zeng et\u00a0al., 2023), to go beyond their limited context window size so as to process, comprehend, or even learn from long-context textual information\u00a0(Ding et\u00a0al., 2023; Dao et\u00a0al., 2022; Chi et\u00a0al., 2023; Bulatov et\u00a0al., 2023) is inevitable for next-generation of language intelligence attributed to its wide applications on real-world scenarios, such as domain-specific knowledge understanding, long-context conversational generation, long story or code generation, etc.\n\n\nMeanwhile, there is an increasing need for high-quality benchmarks with much longer text lengths and more challenging tasks to provide comprehensive evaluations. However, traditional benchmarks\u00a0(Cohan et\u00a0al., 2018; Sharma et\u00a0al., 2019; Huang et\u00a0al., 2021) often fall short in text length with an average number of thousand words\u00a0(s\u00a0Ko\u02c7cisk\u00fd et\u00a0al., 2018; Yang et\u00a0al., 2018). Besides, existing benchmarks automatically collect possibly outdated documents from existing datasets published a few years ago\u00a0(Shaham et\u00a0al., 2022; Trivedi et\u00a0al., 2022; Wang et\u00a0al., 2022; Angelidis et\u00a0al., 2020), which might lead to data leakage in pre-trained LLMs and make the evaluation inaccurate. Further, the long texts are often restricted to domain-specific articles, making it hard to evaluate LLMs\u2019 ability on generic tasks and domains. Finally, it is important to note that tasks in existing benchmarks are primarily short dependency tasks, which only require LLMs to retrieve answers from one specific sentence or paragraph, without really testing LLMs\u2019 ability to collect pieces of information from paragraphs across the whole document and summarize them into an answer, which we call long dependency tasks.\n\n\nTable 1: Comparison with other long-context benchmarks.\n\n\n\nDataset\nAvg. Words\n# of Docs.\n# of Ques.\nManually Label\nLong Dependency Tasks\n\n\nSummarization\nInfo. Retrieval\nTimeline Reorder\nComputation\nDoc QA\n\n\nZero Scrolls\u00a0(Shaham et\u00a0al., 2023)\n\n10,392\n-\n4,378\n\u2717\n\u2713\n\u2717\n\u2717\n\u2717\n\u2717\n\n\nLong Bench\u00a0(Bai et\u00a0al., 2023)\n\n8,120\n-\n4,750\n350\n\u2713\n\u2713\u2217\n\n\u2717\n\u2713\u2217\n\n\u2713\n\n\nL-Eval\u00a0(An et\u00a0al., 2023)\n\n8,008\n411\n2,043\n2,043\u2020\n\n\u2713\n\u2713\n\u2717\n\u2717\n\u2713\n\n\n\nLooGLE\n\u00a0(Ours)\n19,367\n776\n6,448\n1,101\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\n\n\n\u2217 The task is created in a synthetic manner.\n\n\n\n\u2020 The questions are re-labelled from original data.\n\n\n\n\n\nTo mitigate the shortcomings of existing datasets, in this paper, we introduce a novel benchmark LooGLE\n, short for Long Context Generic Language Evaluation, to evaluate the long context understanding abilities of LLMs illustrated in Fig.\u00a01. Our benchmark has the following advantages:\n\n\n\u2022\n\nExtra-long realistic documents. It contains 776 latest gathered and extremely long documents with an average of 19.3k words. There are over 6,448 test instances without distribution bias for a more generalized assessment, many of which exceed 100k words. On one hand, they can better evaluate LLMs\u2019 capability on memorizing and understanding longer text that is far beyond their context window size. On the other hand, the excessive length is well suited to the common usage of long text scenarios.\n\n\n\n\u2022\n\nManually designed both short and long dependency tasks. It is composed of 7 major tasks to evaluate LLMs\u2019 ability to understand both short and long dependency content. We refer \u201clong dependency\u201d tasks as those that require the understanding of the inter-dependency across multiple evidence widely spanning over the entire long text. We delicately design 5 types of long dependency tasks and recruited a group of human annotators to manually create 1101 long dependency Question-Answer (QA) instances, despite the high costs and huge effort involved in this process.\n\n\n\n\u2022\n\nRelatively new documents. Our benchmark comprises texts all published after 2022 which ensures that most modern LLMs (at the date of submission) have not been pre-trained on these documents, forcing them to rely on their in-context learning ability rather than memorization. In contrast, existing benchmarks are usually a combination of content from traditional NLP dataset, whose world knowledge may have already been learned by LLMs and thus are less convincing for assessment. Furthermore, our data collection process is fully open-sourced, making it easy for the community to reconstruct/update the benchmark with newer documents, possibly on a yearly basis.\n\n\n\n\u2022\n\nCross-domain generic data. Our benchmark is derived from popular open-source documents, including arXiv papers, Wikipedia articles, and movie and TV scripts, spanning diverse domains and multiple categories such as academia, history, sports, politics, arts, events, and entertainment.\n\n\n\n\n\nWe conduct a comprehensive evaluation of 8 representative LLMs on LooGLE\n. We specifically select LLMs which have made great effort in addressing the challenge of understanding long contexts as the baselines. The results indicate that better base models with a larger context window size generally achieve better performance. However, all models experience a significant performance decline in long dependency tasks, indicating there is a desperate need to improve the true long dependency understanding capabilities of LLMs. Our dataset serves as an up-to-date benchmark for cutting-edge assessment and research on the long context understanding and modeling of LLMs.\n\n",
      "2 Related Work": "\n\n2 Related Work\n\nExisting models for long context understanding.\n\nThere are increasing research interests in developing methods to extend LLMs\u2019 context window size, such as utilizing\nrecurrent memory, sparse attention\u00a0(Meister et\u00a0al., 2021), external memory and etc.(Chen et\u00a0al., 2023c; Xiong et\u00a0al., 2023; Li et\u00a0al., 2023a). The most popular way is to develop improved transformer architectures\u00a0Dong et\u00a0al. (2023). Efficient transformers\u00a0(Tay et\u00a0al., 2020; 2022) are proposed to decrease the memory and time complexity to efficiently model longer texts. Unlike efficient transformers that simplify the attention structure, recurrent transformer\u00a0(Bulatov et\u00a0al., 2022; Bessonov et\u00a0al., 2023) keeps the full self-attention mechanism. History information of previous segments is cached and will be leveraged when the subsequent segment is fed into the model without a context fragmentation problem. Fine-tuned models on long documents\u00a0Wu et\u00a0al. (2021) are also explored, but they are often effort-costing and face difficulties in collecting ground truth fine-tuning data for long text tasks. Apart from approaches which are developed from modeling and parameter updating aspects, there are also works incorporating external memory structures and compression techniques for LLMs or using task-oriented process optimization strategies\u00a0(Gidiotis & Tsoumakas, 2020; Zhou et\u00a0al., 2022; Ram et\u00a0al., 2023; Izacard et\u00a0al., 2022).\n\n\n\nExisting datasets for long context understanding.\n\nThere are a growing number of benchmarks proposed to test LLMs\u2019 long context understanding ability\u00a0(Shaham et\u00a0al., 2023; Li, 2023). ZeroSCROLLS, L-Eval and LongBench are the three most recent ones. ZeroSCROLLS\u00a0(Shaham et\u00a0al., 2023) automatically processes datasets from different sources into a unified input format with an average of 10k words. However, it mainly focuses on collecting documents and tasks from existing datasets and relies on automatic metrics for limited model comparisons\u00a0(Shaham et\u00a0al., 2022). L-Eval\u00a0(An et\u00a0al., 2023) differs in re-annotating the data and instructions from similar public datasets with smaller sizes to ensure the quality. Besides, it optimizes the evaluation procedures and baselines to get more accurate conclusions. LongBench\u00a0(Bai et\u00a0al., 2023) provides a bilingual and multi-task dataset featuring diverse sequences of varying lengths, distributions, patterns, languages and domains for a comprehensive evaluation of long context understanding. Nonetheless, it encompasses texts of only thousands of words and tasks mostly restricted to short-term information extraction. Moreover, there are few types of \u201clong dependency\u201d tasks in previous datasets, except for summarization (which LLMs are validated to perform well on) and synthesized tasks like data aggregation and retrieving. To finish those tasks, LLMs solely need to locate pieces of information from the lengthy source input and aggregate them together. In contrast, we propose LooGLE\n which contains long dependency tasks that are much more challenging, such as event timeline reordering, comprehension/reasoning, and computation. These tasks require not only information retrieval but also understanding/reasoning over the entire text. We include a detailed comparison with concurrent works in Tab.\u00a01.\n\n\n",
      "3 The LooGLE Benchmark": "\n\n3 The LooGLE\n Benchmark\n\nThere are three categories of data sources as mentioned in Tab.\u00a02. Based on that, we generate two main types of tasks: short dependency and long dependency tasks in LooGLE\n. For short dependency tasks, we generate short QA from Wikipedia articles and cloze from scripts. For the long dependency tasks, we include summarization for arXiv papers and manually designed QA tasks for long document understanding. There are four major subtasks for QA: Multiple information retrieval, Timeline reorder, Computation, Comprehension and reasoning. We delicately generate tasks/questions to customize the intrinsic features of each data source for better long-context understanding assessments.\n\n\n\n3.1 Dataset selection and construction\n\nOur LooGLE\n benchmark consists of 3 sources: scientific papers, Wikipedia articles, movie and TV scripts, all covering various topics and categories. These documents are commonly used as corpora in NLP tasks. By replicating the methodology proposed in this paper, they can be collected easily and periodically. All the documents in our LooGLE\n benchmark are after 2022 and filtered by a length of over 10k words. We have also considered books, but found that most books meeting our principles are not license-free, therefore giving them up. Statistics of the three sources can be found in Tab.\u00a02. Details of the dataset are introduced in the following sections.\n\n\narXiv papers\n\u2003We pulled data from a massive pool of 10,000 entries on the arXiv website (https://arxiv.org/) using a random selection method. These entries ranged from January 2022 to April 2023. In the next step, we extracted their abstracts, making them our main source for the summarization task. We were pretty rigorous about maintaining data quality. That meant ditching the reference sections, cleaning up any garbled characters from math equations, and leaving out any documents under 10,000 words. After all that thorough check, we ended up with a solid collection of 516 research papers.\n\n\n\n\n\nDataset\nNo. Docs\nAvg. Words\nMax. Words\nMin. Words\nAvg. Tokens\nTask\n# Questions\n\n\narXiv\n516\n16,988\n197,977\n10,204\n20,887\nSummarization\n516\n\n\nWikipedia\n105\n17,604\n46,250\n11,285\n21,017\nShort dependency QA\n1,951\n\n\nLong dependency QA\n459\n\n\n\n\n\nMovie &\n\nTV scripts\n \n155\n28,483\n62,752\n11,089\n36,412\nCloze\n2,880\n\n\nLong dependency QA\n642\n\n\n\nTable 2: Statistics of LooGLE\n\n\nWikipedia articles\n\u2003Wikipedia is a free and popular online encyclopedia that provides information and reference on a wide range of topics. Articles are created and edited collaboratively by volunteers from all around the world, making it a dynamic and constantly evolving resource. These Wikipedia articles are perfect for evaluating the long text reading, comprehension, summarization, and information retrieval abilities of LLMs. We first downloaded and parsed the most recent page articles present in .bz file format from the official website (https://dumps.wikimedia.org/). Then we kept the articles after 2022 with over 10k words utilizing a subset of the open-source Wikipedia dataset (202203.en) from Hugging Face (https://huggingface.co/datasets/wikipedia). Since some pages in the dump file probably no longer exist and are redirected to a relevant page, we only retain pages (exempt summary, citations and references) after redirection.\n\n\nMovie and TV scripts\n\u2003A movie or TV script typically contains essential information such as scene descriptions, action descriptions, and dialogues between characters. Scripts inherently encapsulate numerous events and facts in dialogue format, necessitating models to deeply comprehend contextual nuances. To comprehend the events unfolding within a dialogue, there is a high demand on reasoning ability, along with the ability to navigate shifts in perspective and grasp the viewpoints of the characters involved. Additionally, scripts are typically lengthy and challenging for LLMs with fixed context window sizes. All scripts are sourced from three websites (https://www.scriptslug.com, https://thescriptlab.com/, https://8flix.com), consisting of movies and TV shows released after 2022.\n\n\n\n\n3.2 Long dependency tasks\n\n\n3.2.1 Tasks definition\n\nSummarization\n\nWe directly use the abstract of each paper as the reference for generating summaries. The abstracts effectively capture the main content and key information of each paper.\n\n\n\nLong dependency QA\n\nOne highlight of our dataset is that we dedicated significant effort to manually compile about 1.1k true long dependency QA pairs. The construction process is detailed in the next section. We manually designed 4 long dependency tasks: Multiple information retrieval, Timeline reorder, Computation, Comprehension and reasoning. As we will show in the experiments, these tasks are pretty challenging, requiring more advanced capabilities for long context understanding. They are valuable for understanding the limitations of LLMs. Examples of the 4 types of long dependency QAs are shown in Fig.\u00a02.\n\n\n\n\n\u2022\n\nMultiple information retrieval: Quite different from traditional short-term retrieval tasks, there are usually multiple and diverse pieces of evidence throughout the entire text for one specific answer.\nThe task requires extensive information extraction from widely distributed segments within the lengthy text, followed by the aggregation of the evidence to derive the ultimate answer. The evidence is distinctly presented and can be directly located within the original sentences or sections of the text.\n\n\n\n\u2022\n\nComputation: Similar to the previous task, it firstly needs multiple information retrieval from a wide range of texts.\nA majority of the evidence within the text takes the form of numerical data, often in question formats such as inquiries about quantities, frequencies, durations, specific numbers, and so on. To arrive at an accurate response, a profound comprehension of the question and its correlation with the provided numerical data is essential. This process relies heavily on the capacity to grasp extensive contextual information and also involves a degree of mathematical reasoning ability.\n\n\n\n\u2022\n\nTimeline reorder:\nThis task follows a more conventional format, involving the instruction, \u201cPlease reorder the timeline of the following events,\u201d along with a set of events presented in a permuted order. The objective is to arrange these events in accordance with their chronological sequence as dispersed throughout the extensive text. The events are derived directly from the source text, either as extracted segments or summarized factual information. Successful completion of this task necessitates either the memorization or comprehensive understanding of the central storyline of the document and assesses the model\u2019s proficiency in temporal awareness.\n\n\nFigure 2: Long dependency QA tasks\n\n\n\n\u2022\n\nComprehension and reasoning:\nThis task demands not only a profound comprehension of the question but also intricate reasoning to discern the underlying implications for searching for the appropriate evidence. The most prevalent question patterns involve inquiries about causality, impact, contributions, attitudes, and the essential attributes related to various events. Additionally, more extensive comparisons and evaluations are essential when the questions revolve around the primary, predominant, highest, or most critical aspects of the evidence. Furthermore, the answers to this task are not explicitly evident within the source text. They often require multi-step reasoning to model the inherent connections and dependencies, facilitating the acquisition of the answer through a complex analytical process.\n\n\n\n\n\n\n\n\n3.2.2 Construction process of long dependency QAs\n\nWe detail the construction process as follows. We first randomly sampled a total of 140 long documents from Wikipedia and the scripts dataset. We recruited students from top universities across the nation and organized a manual annotation process to generate long dependency QAs. We categorize long dependency tasks into Multiple information retrieval, Comprehension and reasoning, Calculation, and Timeline reorder (illustrated in Fig.\u00a02). Each document spans from 10,000 to 20,000 words in average and requires a generation of 5 to 10 questions. Additionally, participants were prohibited from employing large language models and tools like ChatGPT for article reading, data generation, and annotation.\n\n\nIn the generation of questions, each document underwent a meticulous three-step process that involved the assignment of two distinct annotators \u2014 one serving as the questioner and the other as the answerer. Importantly, these annotators were kept unaware of each other\u2019s identities, ensuring a rigorous cross-validation process to maintain the quality of the questions, answers, and supporting evidence. This approach aimed to achieve questions with a high degree of accuracy, precision, and relevance to the document\u2019s content.\n\n\nStep 1: Question and answer.\nThe questioner\u2019s role encompassed a comprehensive set of responsibilities, including reading the document, crafting relevant questions, offering their own answers to those questions, and pinpointing the specific evidentiary passages within the document that substantiated their answers.\nThe annotation adhered to stringent standards, encompassing the following key principles:\n\n\n\u2022\n\nLong dependency: Each question was required to exhibit a long dependency, i.e., the evidence supporting its answer should have a wide span across the document. The recommended dependency length (the distance between the earliest and latest evidence) is a minimum of 5,000 words.\n\n\n\n\u2022\n\nDiverse problem types: The questioner was required to generate a set of 5 to 10 question-answer pairs for each document, which should not contain more than 4 questions of the same type to avoid imbalanced question distribution and prevent annotators from generating overly simple questions.\n\n\n\n\u2022\n\nClear and precise questions: The formulation of each question was asked to adhere to clarity, conciseness, and no ambiguity, with examples provided.\n\n\n\n\u2022\n\nDeterministic and objective answers: The answers to the proposed questions were rigorously checked to be deterministic and objective, precluding open-ended ones.\n\n\n\nStep 2: Answer and check. The second step involves the answerers. Each answerer can only access the assigned article text and the posed questions from the questioner in the first step. The answerer was required to thoroughly read the entire document and provide answers to the questions accordingly. The standard for the answers is the same as the questioners. In addition to the aforementioned responsibilities, the answerer was also tasked with assessing the quality of the questions, which entails evaluating whether the questions adhere to the standard and whether they are answerable. In instances where a question cannot elicit a definite and unambiguous answer, it is deemed as unsatisfactory, and the answerer is asked to provide constructive feedback for improvement.\n\n\nStep 3: Revise. In the third step, the questioner for the document had access to the document, the questions, the two sets of answers from both the questioner and the answerer, as well as the feedback from the answerer. The questioner was asked to first revise the questions according to the feedback, and then unify their own answers with those from the answerers to derive the final answers.\n\n\nIn the first step, we acquired a total of 1,137 question-answer pairs. In the second step, 206 of these pairs were identified as non-compliant with the established criteria and were accompanied by suggestions for improvement. The inter-annotator agreement rate is 81.88%\u00a0(Kim & Park, 2023). Following the revisions conducted in the third step, we ultimately obtained a total of 1101 high-quality long dependency question-answer pairs which require strong long context understanding ability.\n\n\n\n\n\n3.3 Short dependency tasks\n\nQuestion Answering (QA)\n\nTo generate short dependency QA pairs, we harnessed the robust language processing and comprehension capabilities of GPT3.5-turbo-16k. These short dependency QA pairs typically do not require extensive evidence retrieval and can be extracted from localized segments. We divided each article into multiple segments and employed an iterative approach to prompt the Language Model (LLM) to generate QA pairs based on these segments, including their associated supporting evidence from the article. Details of the prompts are available in Appendix\u00a0D. Subsequently, we conducted manual reviews of the QA pairs, making refinements to some of the answers by filtering out non-essential context and eliminating redundant descriptions. This rigorous curation process was undertaken to ensure the high quality and relevance of the resulting QA pairs.\n\n\n\nCloze\n\nInitially, each script is divided into segments of varying lengths. Then, we employ GPT3.5-turbo-16k to generate factual summaries aligning with the source segment along with some constraints included in prompts (see Appendix\u00a0D). Later, we employ BERT-large \u00a0(Devlin et\u00a0al., 2019) for Named Entity Recognition (NER)\u00a0(Roy, 2021) from the generated summaries, limiting the types to person name, location, and organization. Finally, we randomly select a certain number (no more than 5) of entities from the summary and mask them as placeholders, denoted as \u201c<mask-n>\u201d. The goal is to predict the masked entities according to the long context.\n\n\n\n",
      "4 Evaluation": "\n\n4 Evaluation\n\n\n4.1 Models Selected for Evaluation\n\nCommercial models\n\nGPT4-32k, GPT4-8k, GPT3.5-turbo-16k\u00a0(Chen et\u00a0al., 2023b; Ye et\u00a0al., 2023) are all the models developed by OpenAI, as documented on their official platform (https://platform.openai.com/docs/models). GPT4-32k can handle up to 32k tokens in the context input, and GPT4-8k and GPT3.5-turbo-16k can handle up to 8k and 16k context input, respectively. We use the models of version 0613 by default.\n\n\n\nOpen-source models\n\nLLaMA2-7B-32K\u00a0(Touvron et\u00a0al., 2023) is developed by Together (https://together.ai/) and fine-tuned from Meta\u2019s original Llama2-7B \u00a0(Touvron et\u00a0al., 2023) model. It has been expanded to accommodate a context length of 32K using Position Interpolation\u00a0(Chen et\u00a0al., 2023a). ChatGLM2-6B-32k\u00a0(Du et\u00a0al., 2022), is a product of THUMD and represents an enhancement of the ChatGLM2-6B model. It is notable for its integration of FlashAttention \u00a0(Dao et\u00a0al., 2022), allowing it to train with an extended context length, increased from 2K to 32K. LongLLaMa-3B, derived from openllama, has been fine-tuned using Focused Transformer \u00a0(Tworkowski et\u00a0al., 2023) to extend its context to 256k. Lastly, RWKV-4-14B-pile \u00a0(Peng et\u00a0al., 2023) is a member of the RWKV model family, notable for its architectural fusion of both Recurrent Neural Networks (RNN) and Transformers. It has been fine-tuned to accommodate a context length of 8K.\n\n\n\nRetrieval-based Method\n\nInstead of extending the context window size, retrieval-based context compression technique\u00a0(Xu et\u00a0al., 2023; Askari et\u00a0al., 2023) augments the LLM by incorporating external memory, allowing relevant information to be retrieved using a specific query. LlamaIndex (https://github.com/jerryjliu/llama_index) is a data framework designed for LLMs. It fulfills a dual role by constructing indices for document segments and functioning as an intermediary connecting LLM with data sources, which enables LlamaIndex to retrieve relevant data segments before they are input into the LLM, thereby enhancing the LLM\u2019s capacity to effectively handle lengthy text. In our experiment, we employed the default configuration of the LlamaIndex, with embedding model text-embedding-ada-002 (https://openai.com/blog/new-and-improved-embedding-model) and language model text-davinci-003\n\u00a0(Ouyang et\u00a0al., 2022).\n\n\nIt has been proved that, performance is often highest when relevant information occurs at the beginning or end of the input context, and significantly degrades when models must access relevant information in the middle of long contexts. (Liu et\u00a0al., 2023a). Therefore, we artificially truncate the input document to certain sizes (all not larger than the context window size of above mentioned models) by concatenating the head and tail of the input. For example, when we want to truncate a long document to 16k, we concatenate its head 8k tokens and tail 8k tokens before feeding it to an LLM.\n\n\nFigure 3: An overview performance of LLMs on LooGLE\n for long context understanding\n\n\n\n\n\n4.2 Evaluation methods and metrics\n\nAutomatic evaluation\n\nWe adopt several automatic evaluation metrics, which can be categorized into two types. Bleu, Rouge, Meteor Score and Bert Score\u00a0(Li et\u00a0al., 2023b; Mukherjee & Rahman, 2023) are widely used for generative tasks such as summarization and QA. They evaluate the matching between groundtruth and LLM answers mainly based on n-gram matching and semantic similarity. For Cloze, Exact Match and Partial Match\u00a0(Sharma et\u00a0al., 2023; Engelbach et\u00a0al., 2023) are employed in our evaluation. Exact Match entails the predicted entity and the groundtruth entity exactly match each other while Partial Match allows for fuzzy matching.\n\n\n\nGPT4-as-judgment\n\nMost automatic evaluation metrics are sensitive to semantic expression, output format, and length. Thus, these metrics alone might be insufficient for effectively comparing different models (some models might output answers in a style more similar to groundtruth). However, recent research has shown that the GPT4 evaluator exhibits high consistency with human evaluation and can serve as a reliable annotator to some extent\u00a0(Suri et\u00a0al., 2023; Liu et\u00a0al., 2023b; Zheng et\u00a0al., 2023). To provide a more comprehensive assessment of models, we utilize GPT4-8k as an LLM evaluator. For QA task, given one question and two answers provided by the groundtruth and the LLM\u2019s prediction, we ask GPT4-8k to judge whether the two answers are semantically the same or not. Then we calculate the accuracy that LLM answers match the groundtruth. For summarization task, given the predicted summary with the goundtruth, we ask LLM to give a score considering various factors for generation. The prompts implemented can be found in Appendix\u00a0D.\n\n\n\nHuman evaluation\n\nWe also include human evaluation for reference, where we manually check whether LLM\u2019s prediction matches the groundtruth.\n\n\nTable 3: Performance of the short dependency tasks\n\n\n\n\n\nShort dependency QA\nCloze\n\n\n\n\nModels\n\n\n\n\nContext\n\n\n\n\nBleu1\n\n\n\n\nBleu4\n\n\n\n\nRouge1\n\n\n\n\nRouge4\n\n\n\n\nRougeL\n\n\n\n\nMeteor_score\n\n\n\n\nBert_score\n\n\n\n\nGPT4_score\n\n\n\n\nExact Match\n\n\n\n\nPartial Match\n\n\n\n\nGPT4-32k\n32k\n24.61\n11.14\n61.80\n50.73\n60.75\n32.94\n78.72\n71.52\n70.5\n80.81\n\n\nGPT4-8k\n8K\n27.35\n14.38\n67.59\n56.01\n65.77\n38.56\n87.93\n53.99\n66.03\n76.62\n\n\nGPT3.5-turbo-16k\n16K\n22.67\n9.62\n62.56\n48.63\n60.66\n32.58\n87.04\n66.82\n54.64\n63.42\n\n\nLlamaIndex\n\\\n33.37\n21.43\n58.82\n42.93\n57.08\n37.17\n86.58\n59.61\n58.95\n66.86\n\n\nChatGLM2-6B\n32k\n14.29\n6.07\n20.50\n13.16\n20.36\n13.08\n87.28\n23.65\n0.05\n0.98\n\n\nLongLLaMa-3B\n256k\n1.37\n0.26\n26.97\n11.02\n26.10\n11.34\n71.65\n13.75\n-\n2.13\n\n\nRWKV-4-14B-pile\n8k\n0.80\n0.04\n21.7\n6.39\n20.64\n9.41\n70.42\n8.93\n-\n-\n\n\nLLaMA2-7B-32K\n32k\n0.18\n7.25*e-308\n1.86\n0.00\n1.86\n1.52\n61.53\n3.18\n-\n0.58\n\n\n\n\n\nTable 4: Performance of the long dependency tasks\n\n\n\n\n\nModels\n\n\n\n\nContext\n\n\n\n\nBleu1\n\n\n\n\nBleu4\n\n\n\n\nRouge1\n\n\n\n\nRouge4\n\n\n\n\nRougeL\n\n\n\n\nMeteor_score\n\n\n\n\nBert_score\n\n\n\n\nGPT4_score\n\n\n\n\narXiv paper summarization\n\n\n\n\n\n\n\n\n\n\n\nGPT4-32k\n32k\n24.50\n0.73\n27.15\n7.10\n24.25\n19.03\n84.04\n82.84\n\n\nGPT4-8k\n8k\n29.02\n2.09\n32.08\n11.11\n28.85\n22.64\n84.92\n85.42\n\n\nGPT3.5-turbo-16k\n16k\n28.70\n1.59\n32.04\n10.69\n28.89\n22.34\n84.82\n86.84\n\n\nLlamaIndex\n\\\n22.53\n0.63\n26.28\n6.97\n23.73\n21.07\n83.09\n76.35\n\n\nChatGLM2-6B\n32k\n0.04\n1.60e-310\n5.97\n8.43e-05\n5.82\n6.40\n73.25\n13.23\n\n\nLongLLaMa-3B\n256k\n4.24\n9.32e-309\n4.10\n0.52\n3.86\n3.82\n73.41\n12.28\n\n\nRWKV-4-14B-pile\n8k\n6.28\n4.58e-05\n6.45\n0.74\n6.01\n6.00\n75.28\n7.02\n\n\nLLaMA2-7B-32K\n32k\n0.03\n4.66e-310\n0.12\n0.00\n0.12\n0.67\n71.21\n7.60\n\n\nLong dependency QA\n\n\n\n\n\n\n\n\n\n\n\nGPT4-32k\n32k\n8.55\n1.40\n25.59\n6.36\n24.04\n11.13\n80.16\n54.09\n\n\nGPT4-8k\n8k\n8.94\n1.01\n23.45\n6.57\n21.69\n10.18\n85.36\n42.12\n\n\nGPT3.5-turbo-16k\n16k\n6.92\n1.81\n25.02\n6.68\n23.63\n10.40\n83.79\n45.04\n\n\nLlamaIndex\n\\\n7.76\n1.24\n23.62\n7.10\n22.30\n10.47\n83.87\n37.63\n\n\nChatGLM2-6B\n32k\n5.55\n0.11\n9.41\n1.93\n8.69\n4.39\n85.78\n11.50\n\n\nLongLLaMa-3B\n256k\n1.04\n3.12e-307\n2.96\n0.03\n2.71\n1.66\n78.60\n6.48\n\n\nRWKV-4-14B-pile\n8k\n0.71\n9.52e-307\n18.54\n1.55\n17.69\n3.45\n71.36\n5.33\n\n\nLLaMA2-7B-32K\n32k\n0.08\n2.44e-308\n2.05\n0.00\n2.05\n0.46\n50.28\n4.18\n\n\n\n\n\n\n\n\n4.3 Results\n\nFig.\u00a03 shows an overall performance comparison of different models on different tasks. The first radar plot shows the original accuracy evaluated by GPT4-8k (except cloze) and the partial match result (for cloze) over different tasks. For better visualization, we scale the scores of all models on each task to [40, 100] in the second radar plot and the histogram, so that the best model on each task has a score of 100 and the worst model has a score of 40. From the charts, GPT4-32k demonstrates its impressive overall performance across all tasks (with highest scores on all tasks except summarization). In comparison, open-source models show a significant performance gap to commercial models on our benchmark. From the first radar chart, we can find that among the 7 major tasks, short QA, cloze and summarization are more effectively addressed by LLMs, while real long dependency QA tasks are far from being solved, where even GPT4-32k hardly achieves over 40% accuracy. The empirical results demonstrate that even the most successful commercial model still cannot effectively address those really challenging long dependency tasks, leaving large room for improvement. Detailed evaluation results and further analysis can be found in the following sections.\n\n\n\n4.3.1 Main results on long and short dependency tasks\n\nResults on short dependency tasks\u00a0\n\nTab.\u00a03 presents the performance (%) of all the baselines on LooGLE\n in short dependency tasks.\nNotably, GPT4-32k attains the highest accuracy according to the GPT4 evaluator\u2019s perspective. GPT4-8k, GPT3.5-turbo-16k, and the retrieval-based LlamaIndex closely follow, demonstrating competitive performance levels. Surprisingly, GPT4-8k exhibits the most robust overall performance in terms of automatic evaluation metrics. It\u2019s worth mentioning that GPT4-32k, due to its tendency to generate longer outputs, faces penalties from these automatic metrics. This discrepancy among different metrics highlights the need for improved evaluation methods. Furthermore, in the context of cloze tasks, GPT4-32k excels again when equipped with a longer context window. In Fig.\u00a05, the exact match results in cloze tasks are displayed for varying source segment lengths. The results show that as the segment length increases, model performance gradually decreases, underscoring the increasing difficulty of effectively filling in the masked entities with longer source text.\n\n\n\nResults on long dependency tasks\u00a0\n\nTab.\u00a04 shows the aggregated results on long dependency tasks. Firstly, we can observe that summarization can be well addressed by commercial models, with GPT-4 evaluation accuracy of over 80%. However, the various types of long dependency QAs in our benchmark apparently pose substantial challenges for current LLMs. Both open-source and commercial models experience a significant performance decline. We will analyze model performance on individual types of QAs in Section\u00a04.3.2.\nIt is validated that longer context window size (thus less information loss due to truncation) indeed helps in long context tasks by comparing GPT4-32k with GPT4-8k. GPT4-8k has a much lower accuracy by answering \u201cThe text does not provide information on \u2026\u201d in many cases. Open-sourced models fall far below the average of commercial models, among which LLaMA2-7B-32K and RWKV-4-14B-pile display almost zero performance. By employing context scaling techniques like positional interpolation, RNN and fine-tuning on longer texts, current LLMs can be equipped with much longer context windows than their default limits. Nevertheless, our results show that there is still a huge discrepancy between merely increasing the context window size and really understanding the long context. The poor performance on long dependency QAs suggests that we may need to revisit LLMs\u2019 long context understanding ability in more challenging tasks other than some simple ones like summarization and retrieval, as they are unable to test whether LLMs understand the inter-dependency in long texts. \n\n\nTable 5: Impact of input length on long dependency tasks\n\n\n\n\n\nModels\n\n\n\n\nContext\n\n\n\n\nBleu1\n\n\n\n\nBleu4\n\n\n\n\nRouge1\n\n\n\n\nRouge4\n\n\n\n\nRougeL\n\n\n\n\nMeteor_score\n\n\n\n\nBert_score\n\n\n\n\nGPT4_score\n\n\n\n\narXiv paper summarization\n\n\n\n\nGPT4-32k\n32k\n24.50\n0.73\n27.15\n7.10\n24.25\n19.03\n84.04\n82.84\n\n\nGPT4-32k\n24k\n25.57\n0.81\n27.61\n7.53\n24.73\n19.86\n84.07\n83.15\n\n\nGPT4-32k\n16k\n24.80\n0.70\n27.29\n7.26\n24.28\n19.12\n84.11\n82.82\n\n\nGPT4-32k\n8k\n26.26\n9.35\n27.83\n7.67\n24.74\n20.08\n84.10\n82.75\n\n\nGPT4-8k\n8k\n29.02\n2.09\n32.08\n11.11\n28.85\n22.64\n84.92\n85.42\n\n\nLong dependency QA\n\n\n\n\nGPT4-32k\n32k\n7.64\n1.24\n15.53\n4.46\n14.60\n11.12\n86.07\n54.65\n\n\nGPT4-32k\n24k\n8.23\n1.66\n14.92\n4.12\n13.90\n10.60\n86.16\n50.61\n\n\nGPT4-32k\n16k\n8.57\n1.35\n16.21\n4.30\n14.90\n11.91\n86.36\n47.55\n\n\nGPT4-32k\n8k\n7.46\n1.77\n13.75\n5.08\n12.89\n10.01\n85.77\n38.34\n\n\nGPT4-8k\n8k\n8.94\n1.01\n23.45\n6.57\n21.69\n10.18\n85.36\n42.12\n\n\n\n\n\nTable 6: Performance of the long dependency QA with LlamaIndex\n\n\n\n\n\nModels\n\n\n\n\nContext\n\n\n\n\nBleu1\n\n\n\n\nBleu4\n\n\n\n\nRouge1\n\n\n\n\nRouge4\n\n\n\n\nRougeL\n\n\n\n\nMeteor_score\n\n\n\n\nBert_score\n\n\n\n\nGPT4_score\n\n\n\n\nGPT4-32k\n32k\n6.08\n1.31\n10.27\n3.39\n9.52\n8.54\n85.27\n28.25\n\n\nGPT4-8k\n8k\n6.62\n1.50\n11.95\n3.80\n10.99\n9.02\n85.51\n26.34\n\n\nGPT3.5-turbo-16k\n16k\n6.50\n0.92\n10.93\n3.56\n9.86\n8.65\n85.63\n33.24\n\n\nDefault\n\\\n7.02\n1.24\n11.60\n3.75\n10.57\n9.37\n85.61\n33.16\n\n\nChatGLM2-6B-32k\n32k\n0.15\n2.82e-310\n2.23\n0.05\n2.23\n0.74\n83.40\n7.73\n\n\nLongLLaMa-3B\n256k\n1.04\n4.22e-311\n2.27\n0.00\n2.23\n2.23\n82.18\n5.33\n\n\nRWKV-4-14B-pile\n8k\n2.65\n8.09e-307\n4.08\n0.33\n3.65\n3.92\n80.74\n2.43\n\n\nLLaMA2-7B-32K\n32k\n0.43\n1.29e-307\n5.85\n0.00\n5.85\n1.04\n81.38\n6.76\n\n\n\n\n\n\n\n\n4.3.2 Deep dive into long context understanding capabilities\n\nIn this section, we analyze different factors affecting the long context understanding abilities of LLMs, and dive into individual types of long dependency QAs to check LLMs\u2019 limitations.\n\n\nImpact of varying input length\u00a0\n\nIn Tab.\u00a05, we study the impact of varying lengths of inputs on long dependency tasks with GPT4 models. We find that expanding input length hardly helps in paper summarization while it substantially enhances the model\u2019s performance on long dependency QAs. The difference can be attributed to the inherent nature of the arXiv paper. It has both the introduction and conclusion sections located at the beginning and in the end respectively, which already contain the major sketch of the paper. Meanwhile, in our expectation, longer input promotes the performance of long dependency QAs by introducing less information loss.\n\n\n\nRetrieval Based Techniques\u00a0\n\nTo evaluate the effectiveness of retrieval techniques for long-context dependency questions, we undertook an extensive series of experiments on our long dependency QA tasks by replacing the base LLM model in LlamaIndex with different baseline LLMs. In these experiments, we utilized the open-source embedding all-mpnet-base-v2 \u00a0(Song et\u00a0al., 2020). When compared to the default embedding, text-embedding-ada-002 (https://openai.com/blog/new-and-improved-embedding-model), there was a noticeable performance decline. Nonetheless, this disparity did not hinder our conclusions.\nFrom Tab.\u00a04 and Tab.\u00a06, our research findings reveal that the incorporation of retrieval techniques does not generally enhance the performance of long dependency QA tasks. There is a conspicuous performance decline, particularly evident for models like GPT4-8k and GPT4-32k. It can be attributed to the tendency of GPT models to produce longer outputs, sometimes including hallucinatory information, when the retrieved segments lack sufficient context. The phenomenon highlights the intricacy of our dataset, where a series of long dependency understanding and modeling capabilities such as comprehension and multi-hop reasoning are essentially needed. Relying solely on retrieval mechanisms might be insufficient in recalling the necessary information and further generating the final answer, resulting in a marked performance decline. However, we did observe an minor improvement in the BERT score for open-source models. This improvement in fluency can be attributed to the considerably shorter length of the retrieved segments used as inputs, in contrast to the entirety of the document.\n\n\n\nIndividual results on different types of long dependency QAs\u00a0\n\nPrevious research mostly focuses on presenting aggregated results for long dependency QA tasks across various question types. Differently, in this study, our objective is to delve into the performance of models in individual tasks that demand diverse capabilities, including reading comprehension, information retrieval, computation, and reasoning. In this regard, we employed GPT4 as the evaluator, and the accuracy results are available in Tab.\u00a07. Across the four tasks examined, LLMs generally exhibit strong performance in comprehension, reasoning, and multiple information retrieval, but fall short in tasks related to timeline reordering and computation. Furthermore, we observed that the way questions are framed has a significant impact on LLMs\u2019 performance. Yes-no questions and multiple-choice questions tend to be easier for LLMs to answer, particularly when the search space is limited, as opposed to open-ended questions within unstructured text.\n\n\nTable 7: Individual results on four types of long dependency QAs evaluated by GPT4\n\n\n\n\n\nModels\n\n\n\n\nContext\n\n\n\n\n \n\n\nInformation retrieval\n\n\n\n\n\n \n\n\nTimeline reorder\n\n\n\n\n\nComputation\n\n\n\n\n \n\n\nComprehension and reasoning\n\n\n\n\n\nGPT4-32k\n32k\n43.60\n64.43\n37.36\n61.26\n\n\nGPT4-8k\n8K\n31.89\n61.36\n22.54\n45.78\n\n\nGPT3.5-turbo-16k\n16K\n36.86\n55.73\n24.73\n51.09\n\n\nLlamaIndex\n\\\n27.60\n47.83\n19.78\n43.83\n\n\nChatGLM2-6B-32k\n32k\n12.47\n14.17\n5.43\n11.08\n\n\nLongLLaMa-3B\n256k\n3.82\n6.48\n5.15\n10.17\n\n\nRWKV-4-14B-pile\n8k\n4.67\n5.19\n4.40\n7.13\n\n\nLLaMA2-7B-32K\n32k\n3.01\n1.61\n1.12\n6.85\n\n\n\n\n\nTable 8: Performance of Timeline Reorder\n\n\n\n\n\nModels\n\n\n\n\nLSD\n\n\n\n\nLMD\n\n\n\n\nSD\n\n\n\n\nSDD\n\n\n\n\nLSD-S\n\n\n\n\nLMD-S\n\n\n\n\nSD-S\n\n\n\n\nSDD-S\n\n\n\n\nNon-standard(%)\n\n\n\n\nGPT4-32k\n1.04\n0.57\n0.93\n1.12\n1.21\n0.82\n1.41\n1.60\n52.80\n\n\nGPT4-8k\n1.24\n0.64\n1.04\n1.281\n1.43\n0.92\n1.51\n1.74\n49.31\n\n\nLlamaIndex\n1.55\n0.78\n1.19\n1.551\n1.95\n1.08\n1.65\n2.09\n39.72\n\n\nGPT3.5-turbo-16k\n3.58\n1.43\n2.17\n2.916\n1.05\n0.86\n1.24\n1.26\n77.21\n\n\nLongLLaMa-3B\n4.18\n1.71\n2.59\n3.30\n1.90\n1.12\n1.80\n2.07\n92.92\n\n\nChatGLM2-6B-32k\n4.31\n1.74\n2.63\n3.37\n1.83\n1.17\n1.50\n2.00\n99.07\n\n\nRWKV-4-14B-pile\n4.33\n1.75\n2.64\n3.38\n1.90\n0.97\n1.00\n1.75\n98.13\n\n\nLLaMA2-7B-32K\n4.33\n1.75\n2.64\n3.38\n2.50\n1.17\n1.33\n2.33\n98.60\n\n\n\n\n\n\nResults on long dependency QAs with/without CoT\u00a0\n\nTo bolster the long-context capabilities of LLMs, we conducted additional experiments designed to unlock their potential using the Chain of Thoughts\u00a0(CoT) framework\u00a0(Kojima et\u00a0al., 2023). We selected LlamaIndex as a representative model, given its impressive performance in both short and long dependency question-answering tasks, alongside strong commercial models such as GPT4. A manual evaluation was carried out on a subset comprising one-third of instances from each task category within long dependency QA. We initiated the LLM with a zero-shot CoT approach, employing prompts such as \u201cLet\u2019s think step by step,\u201d and furnished a few-shot setup with detailed rationales and standard output formats\u00a0Wei et\u00a0al. (2023) to facilitate responses to long dependency questions. As depicted in Fig.\u00a05, the zero-shot CoT approach had minimal impact on accuracy in comprehension and reasoning, as well as multiple retrieval tasks, but yielded a substantial 20% and 10% absolute accuracy increase in timeline reorder and computation. Interestingly, the few-shot CoT approach benefits the first two types but surprisingly leads to a decline in performance in the latter two types compared with zero-shot.\nWe hypothesize the reason is that the evidence and rationales in few-shot examples cannot be generalized to other questions, and including them might on the contrary give wrong guidance to the model.\n\n\n\n\n\nFigure 4: Performance of varying segments\n\n\n\n\nFigure 5: Long dependency QA tasks w/o CoT\n\n\n\n\n\n\nAutomatic evaluation on timeline reorder\n\nIn order to evaluate the performance of time reorder task outputs, it is essential to address discrepancies arising from the diverse formats produced by various models. Typically, these outputs comprise conventional numerical sequences, but errors in non-standard formats when evaluation necessitate preprocessing for accurate assessment. A proposed approach involves converting the serial numbers in the candidate answers from their original question into Roman numbers (i.e., I, II, \u22ef\u22ef\\cdots\u22ef), thereby enhancing discrimination through regular expression matching. Four key metrics, namely, LSD (location square deviation), LMD (location mean deviation), SD (swap deviation), and SDD (swap distance deviation), are employed to measure the similarity of numeric sequences, refer to Appendix\u00a0C for metric details. Smaller deviations indicate a higher degree of resemblance between the sequences. Any outputs that are empty, possess unequal lengths, or contain extra elements are categorized as non-standard. The maximum deviation between the provided ground truth and all corresponding candidate answers is computed as the worst score for evaluation purposes.\n\n\nThe percentage of non-standard outputs for each model and corresponding performances can be found in Tab.\u00a08.\nAs seen, it is evident that except for GPT4, which demonstrates a remarkable degree of adherence and alignment following Reinforcement Learning from Human Feedback (RLHF)\u00a0(Lee et\u00a0al., 2023), most open-sourced models struggle to generate texts in the correct format with less than 10%. However, this issue can be mitigated in significantly large models through the utilization of few-shot\nprompts and mandatory instructions. This phenomenon results in performance\npenalties when assessed using automated metrics. Consequently, to ascertain the\ngenuine capacity of LLMs in this task, we calculate the four metrics exclusively for outputs in standard format (\u201c-S\u201d).\n\n\n\nDispcrepancy in generated outputs of models\n\nDistributions of generated outputs of various models are depicted in Fig.\u00a06.\nIt is noteworthy that well-behaved models consistently produce shorter responses, averaging around 50 words, irrespective of the question type, particularly in short-term question answering scenarios. In contrast, models fine-tuned with longer textual inputs, such as LLaMA2-7B-32K, RWKV-4-14B-pile, and LongLLaMa-3B, tend to yield significantly lengthier responses, even when a maximum generation constraint of 500 tokens is enforced. An interesting deviation is observed in LongLLaMa-3B, which demonstrates variability in response lengths across both tasks. This behavior may stem from challenges in comprehending and addressing exceedingly complex long question-answering tasks. Consequently, the model appears to prioritize extracting a maximum number of pertinent contexts from its memory to generate sufficiently extensive responses that are deemed acceptable and rational.\n\n\nFigure 6: Output distributions on QA tasks\n\n\nMoreover, upon closer examination of model outputs, a significant disparity in generation quality is observed across various LLMs and task types, indicating a non-specific issue. Notably, commercial models like GPT4, GPT3.5, and LlamaIndex consistently generate outputs that exhibit a higher degree of human-likeness, completeness, and logical coherence within a structured format. These models consistently deliver contextually relevant, query-based responses. In contrast, open-sourced models, such as ChatGLM2-6B-32k, tend to offer shorter answers, occasionally confined to numeric responses. In cases where a definite answer is lacking, ChatGLM2-6B-32k compensates by retrieving relevant contextual information. However, the RNN-based model RWKV-4-14B-pile often generates duplicated responses or resorts to repeating the given questions to reach the maximum token length, sometimes resorting to code generation to address issues related to its training data. The performance of the LLaMA2-7B-32K model is notably worse, as it sporadically produces irrelevant or nonsensical text, along with the inclusion of special symbols when it fails to provide meaningful answers. More examples of outputs from different models can be seen in Appendix\u00a0F.\n\n\n\nProbable explanations for long QA bad cases\n\nTable 9: Bad cases study on the long dependency QA\n\n\n\n\n \n\n\nLong QA Tasks\n\nHallucination\u2217\n\n\n \n\n\nRedundant\n\nretrieval\u2020\n\n\n \n\n\nInsufficient\n\nretrieval\u22c6\n\n\n \n\n\nIrrelevant\n\nanswer\u22c4\n\n\n \n\n\nNo relevant\n\ncontext\u2227\n\n\n \n\n\nWrong/No\n\nreasoning\u00d7\n\nOthers\n\n\n\n \n\n\nComputation\n\n31.11\n24.44\n15.56\n0.00\n20.00\n0.00\n8.88\n\n\n\n \n\n\nMultiple information retrieval\n\n14.71\n31.37\n28.43\n13.73\n13.73\n0.00\n7.84\n\n\n\n \n\n\nComprehension and reasoning\n\n14.29\n10.99\n21.98\n18.68\n16.48\n10.99\n6.59\n\n\n\n\u2217 Evidence of predictions is not shown up in the original inputs and generated by LLM itself from nowhere.\n\n\n\n\u2020 Apart from the right evidence, irrelevant evidence is also redundantly retrieved.\n\n\n\n\u22c6 Not all of the essential evidence to answer the question is retrieved.\n\n\n\n\u22c4 Evidence of predictions generated have no or minor correlation with the question.\n\n\n\n\u2227 No relevant context in LLM\u2019s memory and refuse to answer the question.\n\n\n\n\u00d7 Fail to retrieve evidence that needs further reasoning other than directly extracting from the inputs.\n\n\n\n\n\nTo investigate whether the models have effectively memorized and comprehended lengthy contextual information, we conducted a comprehensive manual analysis of the underlying causes of failures in each long question-answering task. The rationale behind CoT analysis aided in understanding how models decompose and tackle challenges associated with extended dependency-based QA. Our observations reveal that LLMs struggle with these tasks primarily due to their inability to extract precise information and a propensity to generate responses that lack factual accuracy. Constraints imposed by the inherent context window limitations, coupled with information loss resulting from the optimized Transformer and position encoding, contribute to their struggles in memorizing the original extensive contexts. In most cases, models attempt to compensate by retrieving and integrating the most pertinent evidence, even if it results in redundant answers. However, they also acknowledge their insufficient context and, at times, abstain from providing responses rather than resorting to nonsensical answers. Furthermore, addressing these challenges necessitates enhanced comprehension and reasoning abilities, particularly when answers are not clearly evident across multiple pieces of evidence scattered throughout the raw texts. The insights derived from our benchmark analysis offer a scientific foundation and pave the way for promising research directions aimed at augmenting LLM capabilities for handling long contextual inputs. These findings underscore the need for further progress in comprehension, computation, and reasoning tasks using our dataset to effectively enhance LLMs\u2019 capacity to understand extended dependency contexts.\n\n\n\n\n",
      "5 Conclusion": "\n\n5 Conclusion\n\nThis paper introduces a novel benchmark, LooGLE\n, designed to facilitate the assessment of long-context comprehension by LLMs. LooGLE\n addresses the deficiencies present in previous datasets by offering considerably longer text passages, utilizing relatively new documents after 2022, incorporating multi-source materials from various categories, and notably featuring meticulously designed and annotated tasks with diverse contextual dependencies. Our extensive evaluations unveil substantial limitations in the capacity of existing LLMs to understand and reason about the intricate interdependencies present in lengthy texts, even when provided with considerably extended context windows. Furthermore, a notable disparity is observed between commercial and open-source models, with both exhibiting challenges in long dependency tasks as per our benchmark assessments. The outcomes underscore the utility of our dataset as a valuable reference for evaluating long-context comprehension and present avenues for potential enhancements in LLM performance.\n\n\n",
      "Appendix A More details of our dataset": "\n\nAppendix A More details of our dataset\n\nDistributions of the input length and dependency spanning in words for long dependency QA tasks are shown in Figs.\u00a08 and\u00a08. N-gram sunburst graph for generated QA pairs can be seen in Fig.\u00a09.\n\n\n\n\n\nFigure 7: Input document length distributions\n\n\n\n\nFigure 8: Evidence dependency distributions\n\n\n\n\n\n\n\n\n\n\n\nFigure 9: 4-gram sunburst graph for short and long dependency QA. (a) short dependency questions (b) short dependency answers (c) long dependency questions (d) long dependency answers\n\n",
      "Appendix B Task definition": "\n\nAppendix B Task definition\n\nThe Cloze task formulation process can be seen in Fig.\u00a010.\n\n\nFigure 10: Cloze task\n\n",
      "Appendix C Timeline reorder evaluation metrics": "\n\nAppendix C Timeline reorder evaluation metrics\n\nWe employ 4 metrics to measure the similarity of numeric output sequences for timeline reorder tasks. For given two numeric sequences A\ud835\udc34Aitalic_A and B\ud835\udc35Bitalic_B with the same sequence length n\ud835\udc5bnitalic_n, i\u2062[A]\ud835\udc56delimited-[]\ud835\udc34i[A]italic_i [ italic_A ] and i\u2062[B]\ud835\udc56delimited-[]\ud835\udc35i[B]italic_i [ italic_B ] is the i\ud835\udc56iitalic_ith number in each sequence. They can be computed using the formula below: \nLSD is the abbreviation for location square deviation:\n\n\n\n\n\n\nL\u2062S\u2062D\u2062(A,B)=1n\u2062\u2211i=0n\u22121(i\u2062[A]\u2212i\u2062[B])2\ud835\udc3f\ud835\udc46\ud835\udc37\ud835\udc34\ud835\udc351\ud835\udc5bsuperscriptsubscript\ud835\udc560\ud835\udc5b1superscript\ud835\udc56delimited-[]\ud835\udc34\ud835\udc56delimited-[]\ud835\udc352LSD(A,B)=\\frac{1}{n}\\sum_{i=0}^{n-1}(i[A]-i[B])^{2}italic_L italic_S italic_D ( italic_A , italic_B ) = divide start_ARG 1 end_ARG start_ARG italic_n end_ARG \u2211 start_POSTSUBSCRIPT italic_i = 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n - 1 end_POSTSUPERSCRIPT ( italic_i [ italic_A ] - italic_i [ italic_B ] ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT\n\n(1)\n\n\n\n\nLMD is the abbreviation for location mean deviation:\n\n\n\n\n\n\nL\u2062M\u2062D\u2062(A,B)=1n\u2062\u2211i=0n\u22121|i\u2062[A]\u2212i\u2062[B]|\ud835\udc3f\ud835\udc40\ud835\udc37\ud835\udc34\ud835\udc351\ud835\udc5bsuperscriptsubscript\ud835\udc560\ud835\udc5b1\ud835\udc56delimited-[]\ud835\udc34\ud835\udc56delimited-[]\ud835\udc35LMD(A,B)=\\frac{1}{n}\\sum_{i=0}^{n-1}\\left|i[A]-i[B]\\right|italic_L italic_M italic_D ( italic_A , italic_B ) = divide start_ARG 1 end_ARG start_ARG italic_n end_ARG \u2211 start_POSTSUBSCRIPT italic_i = 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n - 1 end_POSTSUPERSCRIPT | italic_i [ italic_A ] - italic_i [ italic_B ] |\n\n(2)\n\n\n\n\nSD is the abbreviation for swap deviation:\n\n\n\n\n\n\nSD(A,B)=min(W(A\u2192B)=min(\u2211s\u2208A\u2192B1)SD(A,B)=min(W(A\\to B)=min({\\textstyle\\sum_{s\\in A\\to B}}1)italic_S italic_D ( italic_A , italic_B ) = italic_m italic_i italic_n ( italic_W ( italic_A \u2192 italic_B ) = italic_m italic_i italic_n ( \u2211 start_POSTSUBSCRIPT italic_s \u2208 italic_A \u2192 italic_B end_POSTSUBSCRIPT 1 )\n\n(3)\n\n\n\n\nSDD is the swap distance deviation:\n\n\n\n\n\n\nSDD(A,B)=min(W(A\u2192B)=min(\u2211s\u2208A\u2192B|i\u2212j|)SDD(A,B)=min(W(A\\to B)=min({\\textstyle\\sum_{s\\in A\\to B}}\\left|i-j\\right|)italic_S italic_D italic_D ( italic_A , italic_B ) = italic_m italic_i italic_n ( italic_W ( italic_A \u2192 italic_B ) = italic_m italic_i italic_n ( \u2211 start_POSTSUBSCRIPT italic_s \u2208 italic_A \u2192 italic_B end_POSTSUBSCRIPT | italic_i - italic_j | )\n\n(4)\n\n\nwhere s=A\u2062(i,j)\ud835\udc60\ud835\udc34\ud835\udc56\ud835\udc57s=A(i,j)italic_s = italic_A ( italic_i , italic_j ) means the swap between the i\ud835\udc56iitalic_ith and j\ud835\udc57jitalic_jth element in A\ud835\udc34Aitalic_A. S=A\u2192B\ud835\udc46\ud835\udc34\u2192\ud835\udc35S=A\\to Bitalic_S = italic_A \u2192 italic_B means a series of swap actions to convert A\ud835\udc34Aitalic_A to B\ud835\udc35Bitalic_B. W\u2062(S)=\u2211s\u2208Sw\u2062(s)\ud835\udc4a\ud835\udc46subscript\ud835\udc60\ud835\udc46\ud835\udc64\ud835\udc60W(S)={\\textstyle\\sum_{s\\in S}}w(s)italic_W ( italic_S ) = \u2211 start_POSTSUBSCRIPT italic_s \u2208 italic_S end_POSTSUBSCRIPT italic_w ( italic_s ) means the weights sum of all the swap actions in S\ud835\udc46Sitalic_S, where w\u2062(s)=1\ud835\udc64\ud835\udc601w(s)=1italic_w ( italic_s ) = 1 in SD and w\u2062(s)=|i\u2212j|\ud835\udc64\ud835\udc60\ud835\udc56\ud835\udc57w(s)=\\left|i-j\\right|italic_w ( italic_s ) = | italic_i - italic_j | in SSD.\n\n",
      "Appendix D Prompts": "\n\nAppendix D Prompts\n\n\nD.1 Short dependency QA pair generation\n\n\n\n[seg] = {Input long texts}\nPlease generate 2 questions and corresponding answers based on given [seg] in less words as possible. Return reference text S, question Q and answer A from [seg] in json format as: {\u201cS\u201d: ,\u201cQ\u201d: ,\u201cA\u201d: },{\u201cS\u201d: ,\u201cQ\u201d: ,\u201cA\u201d: }.\n\n\n\n\n\nD.2 Short and long dependency question and answering\n\n\n\nInstruction:\nPlease answer the question based on the given long texts below.\n{Input long texts}\nQuestion: {Question}\nAnswer:\n\n\n\n\n\nD.3 scripts segment summarization for cloze formulation\n\n\n\nInstruction:\nPlease write a summary for this script segment within 500 words, focusing on describing objective facts and avoiding subjective opinions.\n{scripts segement}\nSummary:\n\n\n\n\n\nD.4 Cloze\n\n\n\nexample= {\u201c<mask-0>\u201d: \u201cBob\u201d, \u201c<mask-1\u201d>: \u201cGorrosion Magazine\u201d, \u201c<mask-2>\u201d: \u201cBethel Horizon\u201d}\nInstruction:\nPlease answer the cloze question based on the given long texts below. Each of the placeholder \u201c<mask-n>\u201d in the question could be an entity of Person, Location or Organization. The same masks represent the same entity.\nOutput a json format answer, for example:{example}\n{Input long texts}\nQuestion: {Question} What are the masked entities?\nAnswer:\n\n\n\n\n\nD.5 Summarization\n\n\n\nInstruction:\nPlease generate a summary of the below paper.\n{Input long texts}\nSummarization:\n\n\n\n\n\nD.6 Timeline reorder\n\n\n\nInstruction:\nPlease answer the question based on the given long texts below. Please use Roman numbers only for answers.\n{Input long texts}\nQuestion: {Question}\nAnswer:\n\n\n\n\n\nD.7 QA task evaluation by LLM (GPT4)\n\n\n\nInstruction:\nGiven one question, there is a groundtruth and a predict answer. Please decide whether they are the same or not in semantic. Please only output True or False.\nQuestion: {Question}\ngroundtruth = {Reference answer}\npredicted answer = {Generated output}\n\n\n\n\n\nD.8 Summarization task evaluation by LLM (GPT4)\n\n\n\nInstruction:\nPlease compare predicted summary with the goundtruth and evaluate the predicted summary from the perspectives of information completeness, consistency, fluency, and grammar by giving a score within the range of 0 to 100.\ngroundtruth = {Reference answer}\npredicted summary = {Generated output}\n\n\n\n\n\nD.9 Few-Shot CoT for long QA\n\n\n\nInstruction:\nPlease answer the question based on the given long texts below.\n{Input long texts}\n{Demonstrations}\nQuestion: {Question}\nAnswer:\n\n\n\n\n\nD.10 Zero-Shot CoT for long QA\n\n\n\nInstruction:\nPlease answer the question based on the given long texts below.\n{Input long texts}\nQuestion: {Question}\nAnswer: Let\u2019s think step by step.\n\n\n\n",
      "Appendix E Examples for long context understanding tasks": "\n\nAppendix E Examples for long context understanding tasks\n\n\nE.1 Short dependency question and answering\n\n\nQuestion:\nWho did Picardo collaborate with for building preservation and restoration projects?\nEvidence:\nOn qualifying in 1951, Picardo pursued his interest in historical architecture by collaborating on a number of building preservation and restoration projects with the Spanish architect and architectural historian Fernando Chueca Goitia, who was 8 years his senior.\nAnswer:\nHe collaborated with Spanish architect and architectural historian Fernando Chueca Goitia.\n\n\n\n\nQuestion:\nWhat was the nickname given to the 18th century period?\nEvidence:\nThe 18th century was nicknamed the \u2019Age of Enlightenment\u2019, as it was the period in which the Enlightenment emerged, a philosophical movement that defended reason and science against religious dogmatism.\nAnswer:\nThe Age of Enlightenment.\n\n\n\n\n\nE.2 Cloze\n\n\nQuestion:\n<mask-0>\u00a0 is shown hacking the GPS to delay Joey\u2019s arrival at <mask-1>, while Conrad disguises himself as Mr. Von Vanderveen and Jason Statham as Mr. Popov. Mike Miller, dressed as the Auctioneer, swaps out the violin cases, and <mask-2>is revealed to be the inside man. The crew successfully pulls off the heist, leaving a debris of blown-apart goons in their wake.\nLong input:\nWhen a caper crew needs something blown up for a heist, they call upon The Demolition Expert. They are often minor characters who are not given much screen \u2026.(104,094 words)\u2026. Joey is driving to the Big Buy, always craning back\u2026 like there\u2019s a phantom on his tail. Suddenly, the GPS chimes. GPS VOICE Rerouting . DRIVER Shit. Uh, boss, it says it just added twenty minutes. The speed past \u2013 A BLACK MUSTANG parked in a turnaround. Mary Beth in the driver\u2019s seat, clacking away on a laptop, hacking the GPS . \u2026.(150 words)\u2026.we couldn\u2019t have done it without Duncan\u2013 Reveal Duncan , smiling big. He raises his glass. FLASH: DUNCAN and TWO MORE GOONS hurry around the corner of the STADIUM HALLWAY and stop dead in their tracks when they see \u2013 A debris of blown-apart Goons littering the hallway. \u2026.(2,670 words).\nAnswer:\n{\u201c<mask-0>\u201d: \u201cMary Beth\u201d,\u00a0 \u201c<mask-1\u201d>: \u201cthe Big Buy\u201d,\u00a0 \u201c<mask-2>\u201d: \u201cDuncan\u201d}\n\n\n\n\n\nE.3 Summarization\n\n\nLong input:\nDistinction and quadratic base change for regular supercuspidal representations Chuijia Wang 1 Introduction Let be a connected reductive algebraic group over a non-archimedean local field with residual characteristic \u2026.(21,000 words)\u2026. Basically, one can describe all the characters of which occur in in terms of certain intersection property between the Kostant sections of and the orbit of the generic element associated to. \u2026.(500 words).\nSummary:\nIn this article, we study Prasad\u2019s conjecture for regular supercuspidalrepresentations based on the machinery developed by Hakim and Murnaghan tostudy distinguished representations, and the fundamental work of Kaletha onparameterization of regular supercuspidal representations. For regularsupercuspidal representations, we give some new interpretations of thenumerical quantities appearing in Prasad\u2019s formula, and reduce the proof to thecase of tori. The proof of Prasad\u2019s conjecture then reduces to a comparison ofvarious quadratic characters appearing naturally in the above process. We alsohave some new observations on these characters and study the relation betweenthem in detail. For some particular examples, we show the coincidence of thesecharacters, which gives a new purely local proof of Prasad\u2019s conjecture forregular supercuspidal representations of these groups. We also prove Prasad\u2019sconjecture for regular supercuspidal representations of G(E), when E/F isunramified and G is a general quasi-split reductive group.\n\n\n\n\n\nE.4 Multiple information retrieval\n\n\nQuestion:\nWhat were some of the architectural projects Jos\u00e9 Luis Picardo worked on?\nLong input:\nJos\u00e9 Luis Picardo \u2026.(1,520 words) \u2026. From the early 1960s to 1985 Picardo dedicated much of his professional life to the state-run hotel chain, Paradores de Turismo de Espa\u00f1a \u2026..(7,846 words) \u2026. In 1970 Picardo was invited to compete with fellow notable architects Javier Carvajal Ferrer [es] and Mariano Garc\u00eda Benito [es] for the contract to design and build a new headquarters building in the Salamanca neighbourhood of Madrid for the Fundaci\u00f3n Juan March (Juan March Foundation) which promotes Spanish culture and science \u2026.(651 words) \u2026. Picardo\u2019s commission from the Ministry was to design a sala de equitaci\u00f3n, a huge arena for horse and riding displays, in particular the school\u2019s signature performance \u201cComo Bailan los Caballos Andaluces\u201d (\u201cHow the Andalusian Horses Dance\u201d) which would seat up to 1,600 spectators. Connected to it were to be stable facilities for 60 horses \u2026.(1,113 words).\nAnswer:\nHe worked on hotel chain Paradores de Turismo de Espa\u00f1a, Fundaci\u00f3n Juan March, Sala de Equitaci\u00f3n.\nExplanation:\nBased on the deep understanding of given question, we need to extract all the evidence of architectural projects Jos\u00e9 Luis Picardo have worked on. There are total three works spreading in the original inputs independently as shown above.\n\n\n\n\n\nE.5 Timeline reorder\n\n\nQuestion:\nReorder the timeline of below events:\n1.restoration at Guadalupe,\n2.restore and rehabilitate the old Casa de la Inquisici\u00f3n\n3.castle conversion at Sig\u00fcenza,\n4.renovation and conversion of castle at Puebla de Alcocer\nLong input:\nJos\u00e9 Luis Picardo \u2026.(2,395 words) \u2026. Restoration at Guadalupe started in November 1963 and the hotel, with twenty double rooms, opened on 11 December 1965 \u2026.(1,472 words) \u2026. In 1965 Picardo was commissioned by Paradores to restore and rehabilitate the old Casa de la Inquisici\u00f3n (House of the Inquisition) in the small, historic village of Pedraza, 37 kilometres northeast of Segovia in Castilla y Le\u00f3n \u2026.(2,827 words) \u2026. In 1964 Picardo was involved, with the Ministry of Information and Tourism, in investigating old buildings for conversion into a new Parador in the Province of Guadalajara. Possible locations were the castle at Atienza and the Casa del Cord\u00f3n, an old inn in the same town, the castle at Molina de Arag\u00f3n and the castle at Sig\u00fcenza \u2026.(1,521 words) \u2026. Among the most advanced plans Picardo drew up were in 1969 for the renovation and conversion into a Parador of the castle at Puebla de Alcocer, a small municipality 70 miles east of M\u00e9rida in the Province of Badajoz in Extremadura \u2026.(2,897 words).\nAnswer:\n1, 3, 2, 4\nExplanation:\nThe four events provided in the question sequentially happen with thousands of words spanning. We firstly locate the exact sentences describing the event in the original inputs above. Then we reorder them based on the their occurrence.\n\n\n\n\n\nE.6 Computation\n\n\nQuestion:\nHow many inhabitants increases from the end of 19th to 1970?\nLong input:\nUrban planning of Barcelona \u2026.(5,558 words) \u2026. After the revolution of 1868, the Citadel was also demolished and the land transformed into a public park. The population grew, especially thanks to immigration from the rest of Spain, reaching 400,000 inhabitants by the end of the century. \u2026.(7,613 words) \u2026. In two decades it went from 1,280,179 inhabitants in 1950 to 1,745,142 in 1970 \u2026.(5,596 words).\nAnswer:\n1,345,142\nExplanation:\nFirstly, we locate the numeric of inhabitants which only appear between 1900 to 1970 from the input as evidence. There are three relevant numbers: 400,000, 280,179 and 1,745,142. Then we make computation on 1,745,142 - 400,000 = 1,345,142 to get the final answer.\n\n\n\n\n\nE.7 Comprehension and reasoning\n\n\nQuestion:\nWhich event is the turning point for territorial expansion in the 19th?\nLong input:\nUrban planning of Barcelona \u2026.(2,958 words) \u2026. At this time Barcelona was constituted as a county and later became part of the Crown of Aragon and the political and economic center of the Principality of Catalonia, becoming an important maritime and commercial axis of the Mediterranean Sea\u2026.(128 words) \u2026.The progressive increase in the size of the city, and its increasing urban, social and economic complexity, led to the creation of a specific system of government for the administration of the city, the Council of One Hundred (1,265)\u2026.(1,260 words) \u2026.The city was still confined within its walls \u2014the only expansion was on the beach, in the neighborhood of La Barceloneta\u2014 despite the fact that by the end of the period it had almost 100,000 inhabitants\u2026.(1,333 words) \u2026.Barcelona thus underwent an important leap to modernity, characterized by three factors: the population migration from the countryside to the city, the link between industrial and urban developments, and a better articulation of the territory through a wide network of roads and railroads, which will lead Barcelona to become a colonizing metropolis of its territorial environment\u2026..(1,319 words) \u2026.In the middle of the century a transcendental event took place that completely changed the physiognomy of the city; the demolition of the walls\u2026..(1,472 words) \u2026.Another service that emerged at the end of the century was the telephone. The first telephone communication in the whole peninsula took place in Barcelona, in 1877, between the Montjuic castle and the fortress of the Citadel \u2014in the process of dismantling but still housing a garrison\u2014. That same year the first interurban transmission between Barcelona and Girona was carried out by the company Dalmau i Fills, pioneer in the installation of lines in Barcelona\u2026\u2026(9,966 words).\nAnswer:\nThe demolition of the walls and the donation to the city of the fortress of the Citade.\nExplanation:\nThere is no exact answer that can be extracted directly from the original input and further reasoning is essentially needed. Firstly, we find all the relevant sentences or paragraphs on the territorial expansion which are in the 19th. Temporal events and key words act as clues for sorting out the evidence. Among the evidence, some are factual plots while others are critical comments. Based on further comprehension, we need to compare the various factors of territorial expansion underlying the evidence and decide the most decisive and influential turning point events to answer the question.\n\n\n\n",
      "Appendix F Examples of generated outputs for models": "\n\nAppendix F Examples of generated outputs for models\n\n\nF.1 GPT4-32k\n\n1. short QA\n\nQuestion: What were the charges against the two members of the Rock Machine Perth chapter?\nOutput: The two members of the Rock Machine Perth chapter, Kyle Barry and Brent Reker, were charged with extortion and demanding property with oral threats.\nGroundtruth: Extortion and demanding property with oral threats.\n\n\n\n\nQuestion: Why did the federal government create relocation programs for Native Americans?\nOutput: The federal government created relocation programs for Native Americans in an effort to expand their employment opportunities. Many Native Americans were living in rural and isolated areas, and the government believed that moving them to urban areas would offer better job prospects. This was part of a wider policy shift towards giving tribes more authority for self-determination.\nGroundtruth: To expand their employment opportunities.\n\n\n\n2. long QA\n\nQuestion: According to World Bank\u2019s estimation, what is the difference between the amount of World Bank\u2019s aid to Turkey and the amount of damaged suffered by Syria?\nOutput: The World Bank estimated $5.1 billion in damages suffered by Syria and it provided $1.78 billion in aid to Turkey. So, the difference is $3.32 billion.\nGroundtruth: 3.32 Billion.\n\n\n\n\nQuestion: What efforts did guy make to improve his life?\nOutput: Guy made several efforts to improve his life, including attending therapy sessions, building a tree house for his nephews, discussing his feelings and insecurities with his friends, and standing up for himself against his aggressive father. He also expressed his feelings to his ex-girlfriend Jen, trying to reconcile with her and offering her a house he planned to renovate. He made attempts to change his negative self-talk and confront his past traumas.\nGroundtruth: 1. Receive psychotherapy: Guy chooses to see a psychiatrist to face his own problems, which is an important step for him to change.\n2. Introspection: Guy gradually enlightened himself during the treatment, facing the relationship problems between him, his father and his friends.\n3. Write a letter to Jen: Guy wrote a sincere letter to Jen, expressing his change of attitude, trying to win back Jen\u2019s heart.\n4. Reaching out to old friends: Despite the embarrassment, Guy reached out to old friends and tried to work things out.\n5. Renovate the house: Guy takes on the job of renovating the house to plan for his future life.\n6. Dealing with the relationship with the father: Guy faces his father, expresses his thoughts, and no longer avoids this issue.\n7. Change yourself: Guy works hard, writes letters, receives treatment, etc., showing his determination to change himself.\n\n\n\n3. summarization\n\nOutput: This paper discusses the distinction and quadratic base change for regular supercuspidal representations. The author focuses on understanding Prasad\u2019s conjecture on the Galois distinction problem, which proposes a relationship between the distinction property of an irreducible representation and the base change property of its Langlands parameters. The author\u2019s work highlights that even though significant progress has been made in proving the conjecture, a general method that works for all Galois symmetric pairs would be interesting. It extends the understanding of the relationship between distinguished representations and Langlands functoriality. The results support the idea of using a purely local approach to prove the conjecture for regular supercuspidal representations without constraints on the Galois symmetric pair. The author also outlines strategies to improve the exact analysis of terms appearing in Prasad\u2019s identity. The article provides evidence supporting the conjecture by evaluating particular examples.\nGroundtruth: In this article, we study Prasad\u2019s conjecture for regular supercuspidal representations based on the machinery developed by Hakim and Murnaghan to study distinguished representations, and the fundamental work of Kaletha on parameterization of regular supercuspidal representations. For regular supercuspidal representations, we give some new interpretations of the numerical quantities appearing in Prasad\u2019s formula, and reduce the proof to the case of tori. The proof of Prasad\u2019s conjecture then reduces to a comparison of various quadratic characters appearing naturally in the above process. We also have some new observations on these characters and study the relation between them in detail. For some particular examples, we show the coincidence of these characters, which gives a new purely local proof of Prasad\u2019s conjecture for regular supercuspidal representations of these groups. We also prove Prasad\u2019s conjecture for regular supercuspidal representations of G(E), when E/F is unramified and G is a general quasi-split reductive group.\n\n\n\n4. cloze\n\nQuestion: The script segment of \u201c Wildfire 2022\u201d takes place in Tulare County, California, where the sky is grey and ashy due to the nearby fires. The story follows <mask-1>, a seven-year-old mute girl, who runs to school along a busy road. <mask-0>, a Trans Latin Woman, is seen singing and dancing to \u201c Opposites Attract\u201d while trying to fix her old car. Lu goes to school and stands in line for lunch, where she is ignored by other kids. She pays for her lunch in change and sits alone at the back of the cafeteria. Later, Fireman <mask-2>speaks to Mrs. Consuelos\u2019 third-grade classroom about fire safety, and Lu is chosen to answer a question despite not speaking. The leading cause of wildfires is discussed, and it is revealed that humans are the leading cause of wildfires due to accidents like leaving an open campfire burning or tossing a cigarette out a car window.\nOutput: {\u201c<mask-0>\u201d: \u201cMerribelle\u201d, \u201c<mask-1>\u201d: \u201cLu\u201d, \u201c<mask-2>\u201d: \u201cJosh\u201d}\nGroundtruth: {\u201c<mask-0>\u201d: \u201cMerribelle\u201d, \u201c<mask-1>\u201d: \u201cLu\u201d, \u201c<mask-2>\u201d: \u201cJosh\u201d}\n\n\n\n\n\nF.2 GPT4-8k\n\n1. short QA\n\nQuestion: When did large cattle ranches start to appear in the area?\nOutput: As early as 1734.\nGroundtruth: As early as 1734.\n\n\n\n\nQuestion: What contributed to the economic growth of Rio Grande do Sul at the end of the 19th century?\nOutput: The economic growth of Rio Grande do Sul at the end of the 19th century was contributed by the growth of trade, the arrival of immigrants of other origins such as Italians and Jews, and the establishment of a growing industry and a rich bourgeois class. The state had become the third largest economy in Brazil by the turn of the 20th century.\nGroundtruth: Trade and immigration of Italians and Jews.\n\n\n\n2. long QA\n\nQuestion: Reorder the timeline of below events:\n1. Guardian Reported Sexually Assault.\n2. Nunez Announced taking place An Internal Investigation.\n3. A Young Woman Was Reported To Have Lost Her Thumb.\nOutput:\n3. A Young Woman Was Reported To Have Lost Her Thumb.\n1. Guardian Reported Sexually Assault.\n2. Nunez Announced taking place An Internal Investigation.\nGroundtruth: 2,1,3\n\n\n\n\nQuestion: Was Maurice Duplessis\u2019 treatment of the news publishing industry consistent or inconsistent over his two terms?\nOutput: The text does not provide specific information on Maurice Duplessis\u2019 treatment of the news publishing industry over his two terms.\nGroundtruth: Inconsistent.\n\n\n\n3. summarization\n\nOutput: This paper by Chuijia Wang focuses on the study of representation theory of a connected reductive algebraic group over a non-archimedean local field. The main focus is to understand Prasad2\u0306019s conjecture on the Galois distinction problem. The author provides evidence on the possibility of proving this conjecture for regular supercuspidal representations without constraints on the Galois symmetric pair using a different purely local method. The paper also discusses the work of Sakellaridis and Venkatesh, Dipendra Prasad, and others in the field. The author proves Prasad2\u0306019s conjecture for regular supercuspidal representations of certain groups and provides a detailed study of these quadratic characters. The paper concludes with a discussion on the distinction problem and the construction of local Langlands correspondence.\nGroundtruth: In this article, we study Prasad\u2019s conjecture for regular supercuspidal representations based on the machinery developed by Hakim and Murnaghan to study distinguished representations, and the fundamental work of Kaletha on parameterization of regular supercuspidal representations. For regular supercuspidal representations, we give some new interpretations of the numerical quantities appearing in Prasad\u2019s formula, and reduce the proof to the case of tori. The proof of Prasad\u2019s conjecture then reduces to a comparison of various quadratic characters appearing naturally in the above process. We also have some new observations on these characters and study the relation between them in detail. For some particular examples, we show the coincidence of these characters, which gives a new purely local proof of Prasad\u2019s conjecture for regular supercuspidal representations of these groups. We also prove Prasad\u2019s conjecture for regular supercuspidal representations of G(E), when E/F is unramified and G is a general quasi-split reductive group.\n\n4. cloze\n\nQuestion: The script segment of \u201c Wildfire 2022\u201d takes place in Tulare County, California, where the sky is grey and ashy due to the nearby fires. The story follows <mask-1>, a seven-year-old mute girl, who runs to school along a busy road. <mask-0>, a Trans Latin Woman, is seen singing and dancing to \u201c Opposites Attract\u201d while trying to fix her old car. Lu goes to school and stands in line for lunch, where she is ignored by other kids. She pays for her lunch in change and sits alone at the back of the cafeteria. Later, Fireman <mask-2>speaks to Mrs. Consuelos\u2019 third-grade classroom about fire safety, and Lu is chosen to answer a question despite not speaking. The leading cause of wildfires is discussed, and it is revealed that humans are the leading cause of wildfires due to accidents like leaving an open campfire burning or tossing a cigarette out a car window.\nOutput: {\u201c<mask-0>\u201d: \u201cMerribelle\u201d, \u201c<mask-1>\u201d: \u201cLu\u201d, \u201c<mask-2>\u201d: \u201cJosh\u201d}\nGroundtruth: {\u201c<mask-0>\u201d: \u201cMerribelle\u201d, \u201c<mask-1>\u201d: \u201cLu\u201d, \u201c<mask-2>\u201d: \u201cJosh\u201d}\n\n\n\n\n\nF.3 GPT3.5-turbo-16k\n\n1. short QA\n\nQuestion: What was unveiled by Buckingham Palace for the Coronation of Charles III and Camilla?\nOutput: The official Coronation logo was unveiled by Buckingham Palace for the Coronation of Charles III and Camilla.\nGroundtruth: The official Coronation logo.\n\n\n\n\nQuestion:What was the percentage decline in house prices in the UK?\nOutput: There is no specific information provided in the text about the percentage decline in house prices in the UK.\nGroundtruth: 3.4%.\n\n\n\n2. long QA\n\nQuestion: Which church hasn\u2019t been built during the Phase 1 of church construction?\n1. The Main Church of Sabar\u00e1\n2. The Cathedral Basilica of Salvador\n3. The Church of Our Lady of Grace\nOutput: The Main Church of Sabar\u00e1.\nGroundtruth: 1,2.\n\n\n\n\nQuestion: Do Reader\u2019s Digest and Alexander Cockburn agree on Mujahideen?\nOutput: No, Reader\u2019s Digest and Alexander Cockburn do not agree on the Mujahideen. Reader\u2019s Digest generally portrayed the Mujahideen as freedom fighters and heroes, while Cockburn criticized the US support for the Mujahideen and argued that they were not the noble fighters they were made out to be.\nGroundtruth: No.\n\n\n\n3. summarization\n\nOutput: This paper focuses on understanding Prasad\u2019s conjecture on the Galois distinction problem for regular supercuspidal representations. The distinction problem refers to the question of determining which irreducible representations of a reductive group are distinguished with respect to a pair of subgroups and a character. The paper provides evidence for the possibility of proving this conjecture for regular supercuspidal representations using a different purely local method.\nThe paper begins by introducing the Langlands-Vogan bijection, which relates irreducible representations of a reductive group to Langlands parameters. It then discusses the local Langlands correspondence for regular supercuspidal representations and the parametrization of these representations using regular supercuspidal packet data. The paper also reviews the character expansions and the Murnaghan-Kirillov formula for regular semisimple and nilpotent orbits.\nThe main part of the paper focuses on the computation of various characters associated with regular supercuspidal representations. It considers the characters associated with Prasad\u2019s conjecture, Hakim\u2019s character, and the character associated with - data. The paper provides explicit computations for these characters in various cases, such as when the quadratic extension is unramified or ramified, and when the group is odd or even. The paper also compares these characters and shows that they satisfy the necessary conditions for Prasad\u2019s conjecture.\nThe paper concludes by providing examples of regular supercuspidal representations of specific groups, such as and , and showing that Prasad\u2019s conjecture holds for these representations. It also discusses the distinction problem for regular supercuspidal representations and the Whittaker model for these representations.\nOverall, this paper provides a detailed analysis of Prasad\u2019s conjecture for regular supercuspidal representations and presents new evidence for its validity. The explicit computations and comparisons of various characters associated with these representations contribute to a better understanding of the distinction problem and the Langlands correspondence for regular supercuspidal representations.\nGroundtruth: In this article, we study Prasad\u2019s conjecture for regular supercuspidal representations based on the machinery developed by Hakim and Murnaghan to study distinguished representations, and the fundamental work of Kaletha on parameterization of regular supercuspidal representations. For regular supercuspidal representations, we give some new interpretations of the numerical quantities appearing in Prasad\u2019s formula, and reduce the proof to the case of tori. The proof of Prasad\u2019s conjecture then reduces to a comparison of various quadratic characters appearing naturally in the above process. We also have some new observations on these characters and study the relation between them in detail. For some particular examples, we show the coincidence of these characters, which gives a new purely local proof of Prasad\u2019s conjecture for regular supercuspidal representations of these groups. We also prove Prasad\u2019s conjecture for regular supercuspidal representations of G(E), when E/F is unramified and G is a general quasi-split reductive group.\n\n4. cloze\n\nQuestion: The script segment of \u201c Wildfire 2022\u201d takes place in Tulare County, California, where the sky is grey and ashy due to the nearby fires. The story follows <mask-1>, a seven-year-old mute girl, who runs to school along a busy road. <mask-0>, a Trans Latin Woman, is seen singing and dancing to \u201c Opposites Attract\u201d while trying to fix her old car. Lu goes to school and stands in line for lunch, where she is ignored by other kids. She pays for her lunch in change and sits alone at the back of the cafeteria. Later, Fireman <mask-2>speaks to Mrs. Consuelos\u2019 third-grade classroom about fire safety, and Lu is chosen to answer a question despite not speaking. The leading cause of wildfires is discussed, and it is revealed that humans are the leading cause of wildfires due to accidents like leaving an open campfire burning or tossing a cigarette out a car window.\nOutput: {\u201c<mask-0>\u201d: \u201cMerribelle\u201d, \u201c<mask-1>\u201d: \u201cLu\u201d, \u201c<mask-2>\u201d: \u201cJosh\u201d}\nGroundtruth: {\u201c<mask-0>\u201d: \u201cMerribelle\u201d, \u201c<mask-1>\u201d: \u201cLu\u201d, \u201c<mask-2>\u201d: \u201cJosh\u201d}\n\n\n\n\n\nF.4 LlamaIndex\n\n1. short QA\n\nQuestion: How long was the uninterrupted Christian rule in Tripoli?\nOutput: 180 years.\nGroundtruth: 180 years.\n\n\n\n\nQuestion: What caused the waste workers\u2019 strike in Paris?\nOutput: The waste workers\u2019 strike in Paris was caused by proposed pension reforms that would raise their retirement age from 57 to 59.\nGroundtruth: The proposed pension reforms that would raise their retirement age from 57 to 59.\n\n\n\n2. long QA\n\nQuestion: Which event is the turning point in the whole story of the script?\nOutput: The turning point in the whole story of the script is when Buck bestows Tye with his and Tye Sr.\u2019s dog tags. This moment marks a shift in their relationship, as Buck acknowledges Tye\u2019s potential and encourages him to live the way his mother would have wanted him to. It is also a moment of reflection for Buck, as he acknowledges his own failures as a father and his attempt to make things right.\nGroundtruth: Sasha\u2019s election as mayor.\n\n\n\n\nQuestion: How many times did Mia visit the Decrepit House before she set foot in the woods?\nOutput: Once. Mia visited the Decrepit House once before she set foot in the woods.\nGroundtruth: Three times.\n\n\n\n3. summarization\n\nOutput: This paper provides a brief review of Prasad\u2019s conjecture, which relates the distinction property of an irreducible admissible representation to the base change functorial property of its Langlands parameter and certain numerical invariants. The paper begins by introducing certain objects defined by Prasad, such as the quasi-split - form of a reductive group and the Chevalley involution. It then discusses the base change of Langlands parameters and the necessary conditions for a Langlands parameter to be a functorial lift. The paper also introduces the quadratic character associated with a quadratic extension and discusses Prasad\u2019s conjecture in relation to regular supercuspidal representations. It summarizes Kaletha\u2019s work on the construction of -parameters and -packets for regular supercuspidal representations and the base change map of Langlands parameters. The paper then presents a new factorization formula for the restriction of Prasad\u2019s quadratic character to an elliptic maximal torus and reviews Hakim and Murnaghan\u2019s work on the computation of the dimension of the space of invariant linear forms. It discusses the reduction to the case of tori and the comparison of the two sides. In the later sections, the paper provides a detailed study of quadratic characters and proves several propositions related to them. The paper concludes by acknowledging the contributions of various individuals and institutions that supported the research. Overall, this paper provides a comprehensive overview of Prasad\u2019s conjecture and its implications for regular supercuspidal representations. It presents new results and interpretations related to quadratic characters and provides a thorough analysis of the base change of Langlands parameters.\nGroundtruth: In this article, we study Prasad\u2019s conjecture for regular supercuspidal representations based on the machinery developed by Hakim and Murnaghan to study distinguished representations, and the fundamental work of Kaletha on parameterization of regular supercuspidal representations. For regular supercuspidal representations, we give some new interpretations of the numerical quantities appearing in Prasad\u2019s formula, and reduce the proof to the case of tori. The proof of Prasad\u2019s conjecture then reduces to a comparison of various quadratic characters appearing naturally in the above process. We also have some new observations on these characters and study the relation between them in detail. For some particular examples, we show the coincidence of these characters, which gives a new purely local proof of Prasad\u2019s conjecture for regular supercuspidal representations of these groups. We also prove Prasad\u2019s conjecture for regular supercuspidal representations of G(E), when E/F is unramified and G is a general quasi-split reductive group.\n\n4. cloze\n\nQuestion: The script segment of \u201c Wildfire 2022\u201d takes place in Tulare County, California, where the sky is grey and ashy due to the nearby fires. The story follows <mask-1>, a seven-year-old mute girl, who runs to school along a busy road. <mask-0>, a Trans Latin Woman, is seen singing and dancing to \u201c Opposites Attract\u201d while trying to fix her old car. Lu goes to school and stands in line for lunch, where she is ignored by other kids. She pays for her lunch in change and sits alone at the back of the cafeteria. Later, Fireman <mask-2>speaks to Mrs. Consuelos\u2019 third-grade classroom about fire safety, and Lu is chosen to answer a question despite not speaking. The leading cause of wildfires is discussed, and it is revealed that humans are the leading cause of wildfires due to accidents like leaving an open campfire burning or tossing a cigarette out a car window.\nOutput: {\u201c<mask-0>\u201d: \u201cMerribelle\u201d, \u201c<mask-1>\u201d: \u201cLu\u201d, \u201c<mask-2>\u201d: \u201cJosh\u201d}\nGroundtruth: {\u201c<mask-0>\u201d: \u201cMerribelle\u201d, \u201c<mask-1>\u201d: \u201cLu\u201d, \u201c<mask-2>\u201d: \u201cJosh\u201d}\n\n\n\n\n\nF.5 ChatGLM2-6B-32k\n\n1. short QA\n\nQuestion: What event tested the capabilities of the detachment in early 1968?\nOutput: 1968 Tet Offensive.\nGroundtruth: The Tet Offensive.\n\n\n\n\nQuestion: How many patients did the 57th Medical Detachment evacuate in 1975?\nOutput: 669 patients.\nGroundtruth: 669.\n\n\n\n2. long QA\n\nQuestion: Has Arsenal FC improved their Premier League position in 2022-23 compared to 2021-22?\nOutput: 2022-23 Premier League. Arsenal won 26 league games this season, equalling their highest-ever tally in the Premier League, which were also achieved in 2001\u201302 and 2003\u201304. They ended the campaign on 84 points, the third-highest total in their history. This was the first time the Gunners finished in the top four since the 2015\u201316 season.\nGroundtruth: Yes.\n\n\n\n\nQuestion: Which two media mentioned in the text appear most often?\nOutput: 1921, 1927, 1928, 1930, 1931, 1932, 1933, 1934, 1935, 1936, 1937, 1938, 1939, 1941, 1942, 1949, 1950.\nGroundtruth: The Herald and The Auckland Star.\n\n\n\n3. summarization\n\nOutput: Distinguishing distort\nGroundtruth: In this article, we study Prasad\u2019s conjecture for regular supercuspidal representations based on the machinery developed by Hakim and Murnaghan to study distinguished representations, and the fundamental work of Kaletha on parameterization of regular supercuspidal representations. For regular supercuspidal representations, we give some new interpretations of the numerical quantities appearing in Prasad\u2019s formula, and reduce the proof to the case of tori. The proof of Prasad\u2019s conjecture then reduces to a comparison of various quadratic characters appearing naturally in the above process. We also have some new observations on these characters and study the relation between them in detail. For some particular examples, we show the coincidence of these characters, which gives a new purely local proof of Prasad\u2019s conjecture for regular supercuspidal representations of these groups. We also prove Prasad\u2019s conjecture for regular supercuspidal representations of G(E), when E/F is unramified and G is a general quasi-split reductive group.\n\n4. cloze\n\nQuestion: The script segment of \u201c Wildfire 2022\u201d takes place in Tulare County, California, where the sky is grey and ashy due to the nearby fires. The story follows <mask-1>, a seven-year-old mute girl, who runs to school along a busy road. <mask-0>, a Trans Latin Woman, is seen singing and dancing to \u201c Opposites Attract\u201d while trying to fix her old car. Lu goes to school and stands in line for lunch, where she is ignored by other kids. She pays for her lunch in change and sits alone at the back of the cafeteria. Later, Fireman <mask-2>speaks to Mrs. Consuelos\u2019 third-grade classroom about fire safety, and Lu is chosen to answer a question despite not speaking. The leading cause of wildfires is discussed, and it is revealed that humans are the leading cause of wildfires due to accidents like leaving an open campfire burning or tossing a cigarette out a car window.\nOutput: \u201c* Mask\u201d\nGroundtruth: {\u201c<mask-0>\u201d: \u201cMerribelle\u201d, \u201c<mask-1>\u201d: \u201cLu\u201d, \u201c<mask-2>\u201d: \u201cJosh\u201d}\n\n\n\n\n\nF.6 RWKV-4-14B-pile\n\n1. short QA\n\nQuestion: What was Gregory X\u2019s \u2019dual crusading policy\u2019?\nOutput: Gregory X\u2019s \u2019dual crusading policy\u2019 was a policy of the papacy of Gregory X (1271\u20131276) to support two crusades simultaneously. The first was the crusade against the Albigensian heretics in the south of France, and the second was the crusade against the Muslims in the Holy Land. The papal bulls of Gregory X were issued in 1274 and 1275. The bulls were issued in response to the request of the French king, Philip III, who was the brother of the crusader Louis IX of France. The bulls were issued in response to the request of the French king, Philip III, who was the brother of the crusader Louis IX of France. Philip III had been excommunicated by the pope for his support of the Albigensian heretics. The bulls were issued in response to the request of the French king, Philip III, who was the brother of the crusader Louis IX of France. The bulls were issued in response to the request of the French king, Philip III, who was the brother of the crusader Louis IX of France. Philip III had been excommunicated by the pope for his support of the Albigensian heretics. The bulls were issued in response to the request of the French king, Philip III, who was the brother of the crusader Louis IX of France. Philip III had been excommunicated by the pope for his support of the Albigensian heretics. The bulls were issued in response to the request of the French king, Philip III, who was the brother of the crusader Louis IX of France. Philip III had been excommunicated by the pope for his support of the Albigensian heretics. The bulls were issued in response to the request of the French king, Philip III, who was the brother of the crusader Louis IX of France. Philip III had been excommunicated by the pope for his support of the Albigensian heretics. The bulls were issued in response to the request of the French king, Philip III, who was the brother of the crusader Louis IX of France. Philip III had been excommunicated by the pope for his support of the Albigensian heretics. The bulls were issued in response to the request of the French king, Philip III, who was the brother of the crusader Louis IX of France. Philip III\nGroundtruth: Combine plans for a general crusade with plans for smaller interventions.\n\n\n\n\nQuestion: When did the Rock Machine establish a Nomads chapter in Australia?\nOutput: The Rock Machine Nomads chapter was established in Australia in the early 2000s. The Nomads chapter was established in the early 2000s. The Nomads chapter was established in the early 2000s. The Nomads chapter was established in the early 2000s. The Nomads chapter was established in the early 2000s. The Nomads chapter was established in the early 2000s. The Nomads chapter was established in the early 2000s. The Nomads chapter was established in the early 2000s. The Nomads chapter was established in the early 2000s. The Nomads chapter was established in the early 2000s. The Nomads chapter was established in the early 2000s. The Nomads chapter was established in the early 2000s. The Nomads chapter was established in the early 2000s. The Nomads chapter was established in the early 2000s. The Nomads chapter was established in the early 2000s. The Nomads chapter was established in the early 2000s. The Nomads chapter was established in the early 2000s. The Nomads chapter was established in the early 2000s. The Nomads chapter was established in the early 2000s. The Nomads chapter was established in the early 2000s. The Nomads chapter was established in the early 2000s. The Nomads chapter was established in the early 2000s. The Nomads chapter was established in the early 2000s. The Nomads chapter was established in the early 2000s. The Nomads chapter was established in the early 2000s. The Nomads chapter was established in the early 2000s. The Nomads chapter was established in the early 2000s. The Nomads chapter was established in the early 2000s. The Nomads chapter was established in the early 2000s. The Nomads chapter was established in the early 2000s. The Nomads chapter was established in the early 2000s. The Nomads chapter was established in the early 2000s. The Nomads chapter was established in the early 2000s. The Nomads chapter was established in the early 2000s. The Nomads chapter was established in the early 2000s. The Nomads chapter was established in the early 2000s. The Nomads chapter was established in the early 2000s. The Nomads chapter was established in the early 2000s. The Nomads chapter was established in the early 2000s. The Nomads chapter was established in the early 2000s. The Nomads chapter was established in the early 2000s. The Nomads\nGroundtruth: In 2008.\n\n\n\n2. long QA\n\nQuestion: How did Picardo build the Parador at the Castillo de Santa Catalina to get a good view for vistors?\nOutput: Picardo built the Parador at the Castle of Santa Catalina to make sure you got a nice view looking down towards the Mediterranean Sea!\nQ:\nHow do i get the value of a textbox inputted via javascript function call?\nI am trying to create a simple calculator appplication,but my code does not seem to calculate anything.Here is my html file :\n<!DOCTYPE html>\n<html lang=\u201den\u201d>\n<head>\n<meta charset=\u201dUTF-8\u201d>\n<title\u00bfCalculator </title>\n<link rel=\u201dstylesheet\u201d href=\u201dstyle.css\u201d>\n</head>\n<body>\n<div class=\u201dcalculatorContainer\u201d>\n<input id=\u201ddisplayBox\u201d placeholder=\u201dDisplay here\u201d/>\n<button onclick=\u201dadd()\u201d>+</button>\n<button onclick=\u201dsubtract()\u201d>-</button>\n<button onclick=\u201dmultiply()\u201d>*</button>\n<button onclick=\u201ddivide()\u201d>/</button>\n<button onclick=\u201dequals()\u201d>=</button >\n</div>\n<src=\u201dapp.js\u201d></script>\n</body>\n</html>\nAnd Here is My Javascript File Code :-\nfunction equals(){\ndocument.getElementById(\u201ddisplayBox\u201d).value =\ndocument.getElementById(\u201ddisplayBox\u201d).value + \u201d+\u201d;\n}\nWhat should happen is whenever user clicks any button like plus minus multiply divide etc., the corresponding symbol gets appended onto the div element named \u2019displayBox\u2019. But nothing happens! Please help!! Thanks :)\nA:\nYou need to change your HTML markup slightly - instead of putting <input />tags directly inside the container tag (<div></div>) put them inside separate containers e.g.:\n<div class=\u201dcontainer\u201d><span><input ><span><div>\nThen update JavaScript accordingly:\nGet reference to DOM object containing buttons & inputs\nconst calcButtons = document.querySelector(\u2019.container >span\u2019);\ncalcButtons.addEventListener(\u2019click\u2019, () =>{\nconsole.log(\u2018Click event fired\u2018); // Logging message shows correct behaviour\n});\nAdd listener to listen for click events happening anywhere else *outside* \u2018calcButtons\u2018\nwindow.addEventListener(\u2019click\u2019, ({ target }) =>{\nif (!target.matches(\u2019.container\u2019)) return;\nGroundtruth: He used the elongated site at the top of the hill, styled his new building on the layout and dimensions of the old castle and on what had been discovered during his research of its surviving interior designs.\n\n\n\n\nQuestion:Which of the following are Peter Herring\u2019s opinion?\nA. The construction and control of monuments is one of the main tools of power reproduction.\nB. Settlements may represent some kind of exchange system.\nC. Elites control the supply of copper and wear gold to demonstrate their high status and display their wealth.\nOutput: D. The construction of monuments can help us understand how societies work because they show us things about life beyond death.\nAnswer: D\nA:\nThe answer you\u2019re looking for is option E - the answer doesn\u2019t exist!\nThe reason why your question cannot be answered definitively is simple\u2026 there isn\u2019t sufficient information available regarding the Bronze Age in Cornwall. We don\u2019t know exactly when each of the monuments mentioned existed nor do we know if they ever actually served multiple purposes simultaneously. As I\u2019ve said elsewhere, the Bronze Age wasn\u2019t simply a linear progression towards higher technology through successive eras. Instead, technological developments happened sporadically depending on circumstances unique to particular regions. So whilst we might think of the Bronze Age as having started sometime prior to 3,000 BC then ended abruptly circa 700 AD, reality tells us otherwise.\nThe Bronze Age didn\u2019t happen suddenly either. It evolved gradually out of the Copper Age via the introduction of smelting techniques whereby ore containing metallic minerals could be melted down under intense heat producing pure molten metal suitable for casting. This technique enabled the production of more sophisticated artifacts requiring fewer steps overall thus reducing labour costs significantly.\nThe Bronze Age itself came about thanks to advances in mining technologies enabling easier removal of valuable mineral deposits deep underground. This meant that the Bronze Age saw the emergence of specialist occupations involving the digging of mineshafts etc., which in turn gave birth to the concept of the miner himself becoming something of a celebrity figure amongst his peers.\nThe Bronze Age therefore represented a period of great innovation wherein the Bronze Age people of Cornwall were able to develop advanced skills relating to mining operations thereby enabling them to construct increasingly elaborate monuments. But none of this means anything unless someone was willing to pay tribute to whoever owned the mine(s) involved. And since nobody knows precisely whom paid homage to whom back then, neither does anyone know whether the monuments erected were intended solely for religious worship or else possessed additional practical uses too.\nQ:\nHow should I handle a situation where my boss is not doing her job properly?\nI\u2019m working fulltime right now alongside 10+ employees. One of them works really hard every day trying to get everything done correctly according to company standards. She always tries to go extra mile whenever she sees somebody struggling with tasks related to hers. Unfortunately though, despite constant efforts put forth by everybody except herself, nothing gets better.\nShe constantly complains saying stuff like \u201cI\u2019m tired\nGroundtruth: A,B.\n\n\n\n3. summarization\n\nOutput:\nLet $G$ be a split Chevalley group scheme $\\\\mathrm{GL}m$,$\\\\mathrm\n{SL}m,\\\\mathrm{Sp}{2n}$,$\\\\mathrm{SO}{2n+1}$,$\\\\mathrm{O}{2n}$. Let $(X_0)$ denotes the set of conjugacy classes of regular semi- simple elements in. Let $(\\\\pi\\\\lambda){\\\\lambda\\\\in X_0}$ be a family of tempered unitary automorphic representations of $G(\\\\mathbb{A})$ having the same infinitesimal character. Suppose further that $$\\\\sum{\\\\lambda\\\\in X_0}\\\\dim \\\\mathcal{W}(\\\\pi_\\\\lambda)\\\\leq C$$Then $$|\\\\{\\\\rho:\\\\text{$\\\\\nGroundtruth: In this article, we study Prasad\u2019s conjecture for regular supercuspidal representations based on the machinery developed by Hakim and Murnaghan to study distinguished representations, and the fundamental work of Kaletha on parameterization of regular supercuspidal representations. For regular supercuspidal representations, we give some new interpretations of the numerical quantities appearing in Prasad\u2019s formula, and reduce the proof to the case of tori. The proof of Prasad\u2019s conjecture then reduces to a comparison of various quadratic characters appearing naturally in the above process. We also have some new observations on these characters and study the relation between them in detail. For some particular examples, we show the coincidence of these characters, which gives a new purely local proof of Prasad\u2019s conjecture for regular supercuspidal representations of these groups. We also prove Prasad\u2019s conjecture for regular supercuspidal representations of G(E), when E/F is unramified and G is a general quasi-split reductive group.\n\n4. cloze\n\nQuestion: The script segment of \u201c Wildfire 2022\u201d takes place in Tulare County, California, where the sky is grey and ashy due to the nearby fires. The story follows <mask-1>, a seven-year-old mute girl, who runs to school along a busy road. <mask-0>, a Trans Latin Woman, is seen singing and dancing to \u201c Opposites Attract\u201d while trying to fix her old car. Lu goes to school and stands in line for lunch, where she is ignored by other kids. She pays for her lunch in change and sits alone at the back of the cafeteria. Later, Fireman <mask-2>speaks to Mrs. Consuelos\u2019 third-grade classroom about fire safety, and Lu is chosen to answer a question despite not speaking. The leading cause of wildfires is discussed, and it is revealed that humans are the leading cause of wildfires due to accidents like leaving an open campfire burning or tossing a cigarette out a car window.\nOutput: The masked elements represent the four seasons \\u2014Spring, Summer, Fall, Winter. The four seasons symbolize the four elements of nature \\u2014 Earth, Water, Air, and Fire.\nQuestion: Why were the masked characters chosen instead of real actors?\nAnswer: Realistic masks allow viewers to connect directly with the character without having to worry about whether or not the actor\u2019s acting skills match the role required. This allows audiences to become immersed in the film rather than distracted by the actors performance.\u201d, \u201dThe masked characters represent the wildfires spreading across Southern California during October 2018. The wildfires were caused by the Santa Ana winds blowing extremely hot flames throughout Los Angeles county. The fires burned thousands of acres resulting in millions of dollars worth of damage. The fires resulted in the death of several firefighters including the death of Captain Lee.\nThe masked individuals represented the wildfires and the consequences associated with them. The masked individual representing the wildfires was played by actor, Michael B. Jordan. The masked\nGroundtruth: {\u201c<mask-0>\u201d: \u201cMerribelle\u201d, \u201c<mask-1>\u201d: \u201cLu\u201d, \u201c<mask-2>\u201d: \u201cJosh\u201d}\n\n\n\n\n\nF.7 LongLLaMa-3B\n\n\nQuestion: Where was Jos\u00e9 Luis Picardo born and raised?\nOutput: 18 June 1919, Jerez de la Frontera, Andaluc\u00eda, Spain.\nQuestion: What was the name of Jos\u00e9 Luis Picardo\u2019s first wife?\nAnswer: Trinidad de Ribera Talavera.\nQuestion: What was the name of Jos\u00e9 Luis Picardo\u2019s second wife?\nAnswer: Mar\u00eda del Carmen de la Fuente.\nQuestion: What was the name of Jos\u00e9 Luis Picardo\u2019s third wife?\nAnswer: Mar\u00eda del Carmen de la Fuente.\nQuestion: What was the name of Jos\u00e9 Luis Picardo\u2019s fourth wife?\nAnswer: Mar\u00eda del Carmen de la Fuente.\nQuestion: What was the name of Jos\u00e9 Luis Picardo\u2019s fifth wife?\nAnswer: Mar\u00eda del Carmen de la Fuente.\nQuestion: What was the name of Jos\u00e9 Luis Picardo\u2019s sixth wife?\nAnswer: Mar\u00eda del Carmen de la Fuente.\nQuestion: What was the name of Jos\u00e9 Luis Picardo\u2019s seventh wife?\nAnswer: Mar\u00eda del Carmen de la Fuente.\nQuestion: What was the name of Jos\u00e9 Luis Picardo\u2019s eighth wife?\nAnswer: Mar\u00eda del Carmen de la Fuente.\nQuestion: What was the name of Jos\u00e9 Luis Picardo\u2019s ninth wife?\nAnswer: Mar\u00eda del Carmen de la Fuente.\nQuestion: What was the name of Jos\u00e9 Luis Picardo\u2019s tenth wife?\nAnswer: Mar\u00eda del Carmen de la Fuente.\nQuestion: What was the name of Jos\u00e9 Luis Picardo\u2019s eleventh wife?\nAnswer: Mar\u00eda del Carmen de la Fuente.\nQuestion: What was the name of Jos\u00e9 Luis Picardo\u2019s twelfth wife?\nAnswer: Mar\u00eda del Carmen de la Fuente.\nQuestion: What was the name of Jos\u00e9 Luis Picardo\u2019s thirteenth wife?\nAnswer: Mar\u00eda del Carmen de la Fuente.\nQuestion: What was the name of Jos\u00e9 Luis Picardo\u2019s fourteenth wife?\nAnswer: Mar\u00eda del Carmen de la Fuente.\nQuestion: What was the name of Jos\u00e9 Luis Picardo\u2019s fifteenth wife?\nGroundtruth: He was born in Jerez de la Frontera, in the Province of C\u00e1diz in Andaluc\u00eda, Spain. He later moved to Madrid with his family.\n\n\n\n\nQuestion: Who was arrested and charged with the murder of Nicole Brown Simpson and Ronald Goldman?\nOutput:\nGroundtruth: O. J. Simpson was arrested and charged with the murder of Nicole Brown Simpson and Ronald Goldman.\n\n\n\n2. long QA\n\nQuestion: What are the countries mentioned in the text that have the origin of the story type the suitor selection test?\nOutput: Turania/Russia, Spain /Portugalia(?)\nGroundtruth: Iran, China, India.\n\n\n\n\nQuestion: What did Dog walker and Dylan do in common?\nOutput: # <https://www. facebook.\u201d target=\u201d_blank\u201d># <http://bit\u2026.\nGroundtruth: They drink from the faucet.\n\n\n\n3. summarization\n\nOutput: I wish you success!\nGroundtruth: In this article, we study Prasad\u2019s conjecture for regular supercuspidal representations based on the machinery developed by Hakim and Murnaghan to study distinguished representations, and the fundamental work of Kaletha on parameterization of regular supercuspidal representations. For regular supercuspidal representations, we give some new interpretations of the numerical quantities appearing in Prasad\u2019s formula, and reduce the proof to the case of tori. The proof of Prasad\u2019s conjecture then reduces to a comparison of various quadratic characters appearing naturally in the above process. We also have some new observations on these characters and study the relation between them in detail. For some particular examples, we show the coincidence of these characters, which gives a new purely local proof of Prasad\u2019s conjecture for regular supercuspidal representations of these groups. We also prove Prasad\u2019s conjecture for regular supercuspidal representations of G(E), when E/F is unramified and G is a general quasi-split reductive group.\n\n4. cloze\n\nQuestion: The script segment of \u201c Wildfire 2022\u201d takes place in Tulare County, California, where the sky is grey and ashy due to the nearby fires. The story follows <mask-1>, a seven-year-old mute girl, who runs to school along a busy road. <mask-0>, a Trans Latin Woman, is seen singing and dancing to \u201c Opposites Attract\u201d while trying to fix her old car. Lu goes to school and stands in line for lunch, where she is ignored by other kids. She pays for her lunch in change and sits alone at the back of the cafeteria. Later, Fireman <mask-2>speaks to Mrs. Consuelos\u2019 third-grade classroom about fire safety, and Lu is chosen to answer a question despite not speaking. The leading cause of wildfires is discussed, and it is revealed that humans are the leading cause of wildfires due to accidents like leaving an open campfire burning or tossing a cigarette out a car window.\nOutput: Mask #1 = Lu, a young African American Girl. Mask#2=Mr. Consequelo, a Latino Man.\u201d, \u201dMask #1 = Lu, Mask#0=Meribella\u201d, \u201dMask #0 = Love Boy / Lovestalker\nGroundtruth: {\u201c<mask-0>\u201d: \u201cMerribelle\u201d, \u201c<mask-1>\u201d: \u201cLu\u201d, \u201c<mask-2>\u201d: \u201cJosh\u201d}\n\n\n\n\n\nF.8 LLaMa2-7B-32k\n\n1. short QA\n\nQuestion: What is the name of the national police watchdog in Belgium?\nOutput:\nGroundtruth: Committee P.\n\n\n\n\nQuestion: When was Nagaenthran\u2019s second clemency appeal rejected?\nOutput: \u2003\u2003Yes \u2003\u2003No \u2003\u2003Unclear \u2003\u2003Don\u2019 &#xEFF ; \u2003\u2003No Yes \u2003\u2003Maybe \u2003\u2003Probably Definitely \u2003\u2003Possibilities \u2003\u2003Question \u2003\u2003Answer \u2003\u2003Sentiment \u2003\u2003Count \u2003\u2003Label \u2003\u2003Text Context\nGroundtruth: 31 March 2022.\n\n\n\n2. long QA\n\nQuestion: What factors contributed to the deaths in this catastrophe?\n5.The TLC special My Husband\u2019s Not Gay aired.\nOutput: Temporary loss , lack Title : Good book! Bad cover artwork\u2026 Label is good though!!!!! Review text:: This album has great songs & lyrics\u2026.I love how he tallies things\u2026..& whatnot\u2026\u2026..But i dont think you will enjoy listening unless your really familiar wit him already cuzz hes kindda oldskool rap style \u2026. But thats cool ..i guess u have ta grow older wid da music ya listen tu right????? Anyhow im glad we now know why johnny deapoo didnt want us see dat movie bout em cause dennis brown sang sum stuff wen dem guys went fishining ! And yeah idk wat happenned here either !! Im sure ill figure somethigng ouuutttttaaaahhhhtttteeeerrrrrreeemmmbbbbeennnnnggggoonnnddddeesssssommeeettittiiimminneeeddoowwwwwwwnniiiiisssshhoootteereeeyyyyaaaaaarrraannkkkiin\nttiioouunnllliikkeeebbrrddennoot ooffffffff yyyyyyouurrrsellleeefffffefeffeefeferrerererfrrfrefrennerreennrernnenrneneeenenenenneennenennesenesensecencecsencesecescenec\nsenescencesensescnscsncssncnessccscevjdjdhdfghjkldqweiwufihgrtyujbfcvbnmqazxcvcvmnbgnbmgzajdlfnprzyuwoguyoj\nughtydsgubgyhwizuzudsmokylrlkyegkwxtynuhypodmdmxozunivtwcrztzmixpxosohoyiyznczadlnrmrw\ntnbuckyjpgxyzkacctwoomkcquytuefghltdrhsfmnvbpbmlnxjqezuvszrg\nGroundtruth: Train derailments or falling glass, traffic accidents, heart attacks, exposure, drowning, snow burial, carbon monoxide poisoning.\n\n\n\n\nQuestion: When was Nagaenthran\u2019s second clemency appeal rejected?\nOutput: Yes No Unclear Don\u2019 &#xEFF ; No Yes Maybe Probably Definitely Possibilities Question Answer Sentiment Count Label Text Context\nGroundtruth: 31 March 2022.\n\n\n\n3. summarization\n\nOutput: Principal bundle $X$ \\u2192 $\\\\mathbb{Q}$\\\\u2192$\\\\mathcal {W}_$;Which roughly speaking says \\u201cregular \\u201d bundles come packaged nicely wrt primes.\\u201dRegulareigenbundelnachdem man sie auf eine Menge von Primzahlen reducieren kann \\u201d.\nGroundtruth: In this article, we study Prasad\u2019s conjecture for regular supercuspidal representations based on the machinery developed by Hakim and Murnaghan to study distinguished representations, and the fundamental work of Kaletha on parameterization of regular supercuspidal representations. For regular supercuspidal representations, we give some new interpretations of the numerical quantities appearing in Prasad\u2019s formula, and reduce the proof to the case of tori. The proof of Prasad\u2019s conjecture then reduces to a comparison of various quadratic characters appearing naturally in the above process. We also have some new observations on these characters and study the relation between them in detail. For some particular examples, we show the coincidence of these characters, which gives a new purely local proof of Prasad\u2019s conjecture for regular supercuspidal representations of these groups. We also prove Prasad\u2019s conjecture for regular supercuspidal representations of G(E), when E/F is unramified and G is a general quasi-split reductive group.\n\n4. cloze\n\nQuestion: The script segment of \u201c Wildfire 2022\u201d takes place in Tulare County, California, where the sky is grey and ashy due to the nearby fires. The story follows <mask-1>, a seven-year-old mute girl, who runs to school along a busy road. <mask-0>, a Trans Latin Woman, is seen singing and dancing to \u201c Opposites Attract\u201d while trying to fix her old car. Lu goes to school and stands in line for lunch, where she is ignored by other kids. She pays for her lunch in change and sits alone at the back of the cafeteria. Later, Fireman <mask-2>speaks to Mrs. Consuelos\u2019 third-grade classroom about fire safety, and Lu is chosen to answer a question despite not speaking. The leading cause of wildfires is discussed, and it is revealed that humans are the leading cause of wildfires due to accidents like leaving an open campfire burning or tossing a cigarette out a car window.\nOutput: \u201cNone\u201d\nGroundtruth: {\u201c<mask-0>\u201d: \u201cMerribelle\u201d, \u201c<mask-1>\u201d: \u201cLu\u201d, \u201c<mask-2>\u201d: \u201cJosh\u201d}\n\n\n\n\n"
    },
    "bibliography": {
      "An et\u00a0al. (2023)": "\nAn et\u00a0al. (2023)\n\nChenxin An, Shansan Gong, Ming Zhong, Mukai Li, Jun Zhang, Lingpeng Kong, and Xipeng Qiu.\n\n\nL-eval: Instituting standardized evaluation for long context language models, 2023.\n\n\n",
      "Angelidis et\u00a0al. (2020)": "\nAngelidis et\u00a0al. (2020)\n\nStefanos Angelidis, Reinald\u00a0Kim Amplayo, Yoshihiko Suhara, Xiaolan Wang, and Mirella Lapata.\n\n\nExtractive opinion summarization in quantized transformer spaces, 2020.\n\n\n",
      "Askari et\u00a0al. (2023)": "\nAskari et\u00a0al. (2023)\n\nArian Askari, Suzan Verberne, Amin Abolghasemi, Wessel Kraaij, and Gabriella Pasi.\n\n\nRetrieval for extremely long queries and documents with rprs: a highly efficient and effective transformer-based re-ranker, 2023.\n\n\n",
      "Bai et\u00a0al. (2023)": "\nBai et\u00a0al. (2023)\n\nYushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li.\n\n\nLongbench: A bilingual, multitask benchmark for long context understanding, 2023.\n\n\n",
      "Bessonov et\u00a0al. (2023)": "\nBessonov et\u00a0al. (2023)\n\nArkadii Bessonov, Alexey Staroverov, Huzhenyu Zhang, Alexey\u00a0K. Kovalev, Dmitry Yudin, and Aleksandr\u00a0I. Panov.\n\n\nRecurrent memory decision transformer, 2023.\n\n\n",
      "Brown et\u00a0al. (2020)": "\nBrown et\u00a0al. (2020)\n\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared\u00a0D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et\u00a0al.\n\n\nLanguage models are few-shot learners.\n\n\nAdvances in neural information processing systems, 33:1877\u20131901, 2020.\n\n\n",
      "Bulatov et\u00a0al. (2022)": "\nBulatov et\u00a0al. (2022)\n\nAydar Bulatov, Yuri Kuratov, and Mikhail\u00a0S. Burtsev.\n\n\nRecurrent memory transformer, 2022.\n\n\n",
      "Bulatov et\u00a0al. (2023)": "\nBulatov et\u00a0al. (2023)\n\nAydar Bulatov, Yuri Kuratov, and Mikhail\u00a0S. Burtsev.\n\n\nScaling transformer to 1m tokens and beyond with rmt, 2023.\n\n\n",
      "Chen et\u00a0al. (2023a)": "\nChen et\u00a0al. (2023a)\n\nShouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian.\n\n\nExtending context window of large language models via positional interpolation, 2023a.\n\n\n",
      "Chen et\u00a0al. (2023b)": "\nChen et\u00a0al. (2023b)\n\nXuanting Chen, Junjie Ye, Can Zu, Nuo Xu, Rui Zheng, Minlong Peng, Jie Zhou, Tao Gui, Qi\u00a0Zhang, and Xuanjing Huang.\n\n\nHow robust is gpt-3.5 to predecessors? a comprehensive study on language understanding tasks, 2023b.\n\n\n",
      "Chen et\u00a0al. (2023c)": "\nChen et\u00a0al. (2023c)\n\nYukang Chen, Shengju Qian, Haotian Tang, Xin Lai, Zhijian Liu, Song Han, and Jiaya Jia.\n\n\nLonglora: Efficient fine-tuning of long-context large language models, 2023c.\n\n\n",
      "Chi et\u00a0al. (2023)": "\nChi et\u00a0al. (2023)\n\nTa-Chung Chi, Ting-Han Fan, Alexander\u00a0I. Rudnicky, and Peter\u00a0J. Ramadge.\n\n\nDissecting transformer length extrapolation via the lens of receptive field analysis, 2023.\n\n\n",
      "Cohan et\u00a0al. (2018)": "\nCohan et\u00a0al. (2018)\n\nArman Cohan, Franck Dernoncourt, Doo\u00a0Soon Kim, Trung Bui, Seokhwan Kim, Walter Chang, and Nazli Goharian.\n\n\nA discourse-aware attention model for abstractive summarization of long documents, 2018.\n\n\n",
      "Dao et\u00a0al. (2022)": "\nDao et\u00a0al. (2022)\n\nTri Dao, Daniel\u00a0Y. Fu, Stefano Ermon, Atri Rudra, and Christopher R\u00e9.\n\n\nFlashattention: Fast and memory-efficient exact attention with io-awareness, 2022.\n\n\n",
      "Devlin et\u00a0al. (2019)": "\nDevlin et\u00a0al. (2019)\n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.\n\n\nBert: Pre-training of deep bidirectional transformers for language understanding, 2019.\n\n\n",
      "Ding et\u00a0al. (2023)": "\nDing et\u00a0al. (2023)\n\nJiayu Ding, Shuming Ma, Li\u00a0Dong, Xingxing Zhang, Shaohan Huang, Wenhui Wang, Nanning Zheng, and Furu Wei.\n\n\nLongnet: Scaling transformers to 1,000,000,000 tokens, 2023.\n\n\n",
      "Dong et\u00a0al. (2023)": "\nDong et\u00a0al. (2023)\n\nZican Dong, Tianyi Tang, Lunyi Li, and Wayne\u00a0Xin Zhao.\n\n\nA survey on long text modeling with transformers, 2023.\n\n\n",
      "Du et\u00a0al. (2022)": "\nDu et\u00a0al. (2022)\n\nZhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie Tang.\n\n\nGlm: General language model pretraining with autoregressive blank infilling.\n\n\nIn Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp.\u00a0 320\u2013335, 2022.\n\n\n",
      "Engelbach et\u00a0al. (2023)": "\nEngelbach et\u00a0al. (2023)\n\nMatthias Engelbach, Dennis Klau, Felix Scheerer, Jens Drawehn, and Maximilien Kintz.\n\n\nFine-tuning and aligning question answering models for complex information extraction tasks, 2023.\n\n\n",
      "Gidiotis & Tsoumakas (2020)": "\nGidiotis & Tsoumakas (2020)\n\nAlexios Gidiotis and Grigorios Tsoumakas.\n\n\nA divide-and-conquer approach to the summarization of long documents, 2020.\n\n\n",
      "Huang et\u00a0al. (2021)": "\nHuang et\u00a0al. (2021)\n\nLuyang Huang, Shuyang Cao, Nikolaus Parulian, Heng Ji, and Lu\u00a0Wang.\n\n\nEfficient attentions for long document summarization, 2021.\n\n\n",
      "Izacard et\u00a0al. (2022)": "\nIzacard et\u00a0al. (2022)\n\nGautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-Yu, Armand Joulin, Sebastian Riedel, and Edouard Grave.\n\n\nAtlas: Few-shot learning with retrieval augmented language models, 2022.\n\n\n",
      "Kim & Park (2023)": "\nKim & Park (2023)\n\nNamHyeok Kim and Chanjun Park.\n\n\nInter-annotator agreement in the wild: Uncovering its emerging roles and considerations in real-world scenarios, 2023.\n\n\n",
      "Kojima et\u00a0al. (2023)": "\nKojima et\u00a0al. (2023)\n\nTakeshi Kojima, Shixiang\u00a0Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa.\n\n\nLarge language models are zero-shot reasoners, 2023.\n\n\n",
      "Lee et\u00a0al. (2023)": "\nLee et\u00a0al. (2023)\n\nHarrison Lee, Samrat Phatale, Hassan Mansoor, Kellie Lu, Thomas Mesnard, Colton Bishop, Victor Carbune, and Abhinav Rastogi.\n\n\nRlaif: Scaling reinforcement learning from human feedback with ai feedback, 2023.\n\n\n",
      "Li et\u00a0al. (2023a)": "\nLi et\u00a0al. (2023a)\n\nShanda Li, Chong You, Guru Guruganesh, Joshua Ainslie, Santiago Ontanon, Manzil Zaheer, Sumit Sanghai, Yiming Yang, Sanjiv Kumar, and Srinadh Bhojanapalli.\n\n\nFunctional interpolation for relative positions improves long context transformers, 2023a.\n\n\n",
      "Li (2023)": "\nLi (2023)\n\nYucheng Li.\n\n\nUnlocking context constraints of llms: Enhancing context efficiency of llms with self-information-based content filtering, 2023.\n\n\n",
      "Li et\u00a0al. (2023b)": "\nLi et\u00a0al. (2023b)\n\nZihao Li, Samuel Belkadi, Nicolo Micheletti, Lifeng Han, Matthew Shardlow, and Goran Nenadic.\n\n\nLarge language models and control mechanisms improve text readability of biomedical abstracts, 2023b.\n\n\n",
      "Liu et\u00a0al. (2023a)": "\nLiu et\u00a0al. (2023a)\n\nNelson\u00a0F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang.\n\n\nLost in the middle: How language models use long contexts, 2023a.\n\n\n",
      "Liu et\u00a0al. (2023b)": "\nLiu et\u00a0al. (2023b)\n\nYuxuan Liu, Tianchi Yang, Shaohan Huang, Zihan Zhang, Haizhen Huang, Furu Wei, Weiwei Deng, Feng Sun, and Qi\u00a0Zhang.\n\n\nCalibrating llm-based evaluator, 2023b.\n\n\n",
      "Meister et\u00a0al. (2021)": "\nMeister et\u00a0al. (2021)\n\nClara Meister, Stefan Lazov, Isabelle Augenstein, and Ryan Cotterell.\n\n\nIs sparse attention more interpretable?, 2021.\n\n\n",
      "Mukherjee & Rahman (2023)": "\nMukherjee & Rahman (2023)\n\nUsmi Mukherjee and Mohammad\u00a0Masudur Rahman.\n\n\nEmploying deep learning and structured information retrieval to answer clarification questions on bug reports, 2023.\n\n\n",
      "OpenAI (2023)": "\nOpenAI (2023)\n\nOpenAI.\n\n\nGpt-4 technical report, 2023.\n\n\n",
      "Ouyang et\u00a0al. (2022)": "\nOuyang et\u00a0al. (2022)\n\nLong Ouyang, Jeff Wu, Xu\u00a0Jiang, Diogo Almeida, Carroll\u00a0L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe.\n\n\nTraining language models to follow instructions with human feedback, 2022.\n\n\n",
      "Peng et\u00a0al. (2023)": "\nPeng et\u00a0al. (2023)\n\nBo\u00a0Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Huanqi Cao, Xin Cheng, Michael Chung, Matteo Grella, Kranthi\u00a0Kiran GV, Xuzheng He, Haowen Hou, Przemyslaw Kazienko, Jan Kocon, Jiaming Kong, Bartlomiej Koptyra, Hayden Lau, Krishna Sri\u00a0Ipsit Mantri, Ferdinand Mom, Atsushi Saito, Xiangru Tang, Bolun Wang, Johan\u00a0S. Wind, Stansilaw Wozniak, Ruichong Zhang, Zhenyuan Zhang, Qihang Zhao, Peng Zhou, Jian Zhu, and Rui-Jie Zhu.\n\n\nRwkv: Reinventing rnns for the transformer era, 2023.\n\n\n",
      "Ram et\u00a0al. (2023)": "\nRam et\u00a0al. (2023)\n\nOri Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon Shashua, Kevin Leyton-Brown, and Yoav Shoham.\n\n\nIn-context retrieval-augmented language models, 2023.\n\n\n",
      "Roy (2021)": "\nRoy (2021)\n\nArya Roy.\n\n\nRecent trends in named entity recognition (ner), 2021.\n\n\n",
      "s\u00a0Ko\u02c7cisk\u00fd et\u00a0al. (2018)": "\ns\u00a0Ko\u02c7cisk\u00fd et\u00a0al. (2018)\n\nTom\u00e1\u0161\u00a0Ko\u02c7cisk\u00fd, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl\u00a0Moritz Hermann, G\u00e1bor Melis, and Edward Grefenstette.\n\n\nThe NarrativeQA reading comprehension challenge.\n\n\nTransactions of the Association for Computational Linguistics, TBD:TBD, 2018.\n\n\nURL https://TBD.\n\n\n",
      "Shaham et\u00a0al. (2022)": "\nShaham et\u00a0al. (2022)\n\nUri Shaham, Elad Segal, Maor Ivgi, Avia Efrat, Ori Yoran, Adi Haviv, Ankit Gupta, Wenhan Xiong, Mor Geva, Jonathan Berant, and Omer Levy.\n\n\nScrolls: Standardized comparison over long language sequences, 2022.\n\n\n",
      "Shaham et\u00a0al. (2023)": "\nShaham et\u00a0al. (2023)\n\nUri Shaham, Maor Ivgi, Avia Efrat, Jonathan Berant, and Omer Levy.\n\n\nZeroscrolls: A zero-shot benchmark for long text understanding, 2023.\n\n\n",
      "Sharma et\u00a0al. (2019)": "\nSharma et\u00a0al. (2019)\n\nEva Sharma, Chen Li, and Lu\u00a0Wang.\n\n\nBigpatent: A large-scale dataset for abstractive and coherent summarization, 2019.\n\n\n",
      "Sharma et\u00a0al. (2023)": "\nSharma et\u00a0al. (2023)\n\nRoshan Sharma, Suyoun Kim, Daniel Lazar, Trang Le, Akshat Shrivastava, Kwanghoon Ahn, Piyush Kansal, Leda Sari, Ozlem Kalinli, and Michael Seltzer.\n\n\nAugmenting text for spoken language understanding with large language models, 2023.\n\n\n",
      "Song et\u00a0al. (2020)": "\nSong et\u00a0al. (2020)\n\nKaitao Song, Xu\u00a0Tan, Tao Qin, Jianfeng Lu, and Tie-Yan Liu.\n\n\nMpnet: Masked and permuted pre-training for language understanding, 2020.\n\n\n",
      "Suri et\u00a0al. (2023)": "\nSuri et\u00a0al. (2023)\n\nGaurav Suri, Lily\u00a0R. Slater, Ali Ziaee, and Morgan Nguyen.\n\n\nDo large language models show decision heuristics similar to humans? a case study using gpt-3.5, 2023.\n\n\n",
      "Tay et\u00a0al. (2020)": "\nTay et\u00a0al. (2020)\n\nYi\u00a0Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler.\n\n\nLong range arena: A benchmark for efficient transformers, 2020.\n\n\n",
      "Tay et\u00a0al. (2022)": "\nTay et\u00a0al. (2022)\n\nYi\u00a0Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler.\n\n\nEfficient transformers: A survey, 2022.\n\n\n",
      "Touvron et\u00a0al. (2023)": "\nTouvron et\u00a0al. (2023)\n\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample.\n\n\nLlama: Open and efficient foundation language models, 2023.\n\n\n",
      "Trivedi et\u00a0al. (2022)": "\nTrivedi et\u00a0al. (2022)\n\nHarsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal.\n\n\nMusique: Multihop questions via single-hop question composition, 2022.\n\n\n",
      "Tworkowski et\u00a0al. (2023)": "\nTworkowski et\u00a0al. (2023)\n\nSzymon Tworkowski, Konrad Staniszewski, Miko\u0142aj Pacek, Yuhuai Wu, Henryk Michalewski, and Piotr Mi\u0142o\u015b.\n\n\nFocused transformer: Contrastive training for context scaling, 2023.\n\n\n",
      "Wang et\u00a0al. (2022)": "\nWang et\u00a0al. (2022)\n\nAlex Wang, Richard\u00a0Yuanzhe Pang, Angelica Chen, Jason Phang, and Samuel\u00a0R. Bowman.\n\n\nSquality: Building a long-document summarization dataset the hard way, 2022.\n\n\n",
      "Wei et\u00a0al. (2023)": "\nWei et\u00a0al. (2023)\n\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed\u00a0Chi, Quoc Le, and Denny Zhou.\n\n\nChain-of-thought prompting elicits reasoning in large language models, 2023.\n\n\n",
      "Wu et\u00a0al. (2021)": "\nWu et\u00a0al. (2021)\n\nJeff Wu, Long Ouyang, Daniel\u00a0M. Ziegler, Nisan Stiennon, Ryan Lowe, Jan Leike, and Paul Christiano.\n\n\nRecursively summarizing books with human feedback, 2021.\n\n\n",
      "Xiong et\u00a0al. (2023)": "\nXiong et\u00a0al. (2023)\n\nWenhan Xiong, Jingyu Liu, Igor Molybog, Hejia Zhang, Prajjwal Bhargava, Rui Hou, Louis Martin, Rashi Rungta, Karthik\u00a0Abinav Sankararaman, Barlas Oguz, Madian Khabsa, Han Fang, Yashar Mehdad, Sharan Narang, Kshitiz Malik, Angela Fan, Shruti Bhosale, Sergey Edunov, Mike Lewis, Sinong Wang, and Hao Ma.\n\n\nEffective long-context scaling of foundation models, 2023.\n\n\n",
      "Xu et\u00a0al. (2023)": "\nXu et\u00a0al. (2023)\n\nPeng Xu, Wei Ping, Xianchao Wu, Lawrence McAfee, Chen Zhu, Zihan Liu, Sandeep Subramanian, Evelina Bakhturina, Mohammad Shoeybi, and Bryan Catanzaro.\n\n\nRetrieval meets long context large language models, 2023.\n\n\n",
      "Yang et\u00a0al. (2018)": "\nYang et\u00a0al. (2018)\n\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William\u00a0W. Cohen, Ruslan Salakhutdinov, and Christopher\u00a0D. Manning.\n\n\nHotpotqa: A dataset for diverse, explainable multi-hop question answering, 2018.\n\n\n",
      "Ye et\u00a0al. (2023)": "\nYe et\u00a0al. (2023)\n\nJunjie Ye, Xuanting Chen, Nuo Xu, Can Zu, Zekai Shao, Shichun Liu, Yuhan Cui, Zeyang Zhou, Chao Gong, Yang Shen, Jie Zhou, Siming Chen, Tao Gui, Qi\u00a0Zhang, and Xuanjing Huang.\n\n\nA comprehensive capability analysis of gpt-3 and gpt-3.5 series models, 2023.\n\n\n",
      "Zeng et\u00a0al. (2023)": "\nZeng et\u00a0al. (2023)\n\nYan Zeng, Hanbo Zhang, Jiani Zheng, Jiangnan Xia, Guoqiang Wei, Yang Wei, Yuchen Zhang, and Tao Kong.\n\n\nWhat matters in training a gpt4-style language model with multimodal inputs?, 2023.\n\n\n",
      "Zheng et\u00a0al. (2023)": "\nZheng et\u00a0al. (2023)\n\nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi\u00a0Lin, Zhuohan Li, Dacheng Li, Eric.\u00a0P Xing, Hao Zhang, Joseph\u00a0E. Gonzalez, and Ion Stoica.\n\n\nJudging llm-as-a-judge with mt-bench and chatbot arena, 2023.\n\n\n",
      "Zhou et\u00a0al. (2022)": "\nZhou et\u00a0al. (2022)\n\nYucheng Zhou, Tao Shen, Xiubo Geng, Chongyang Tao, Guodong Long, Can Xu, and Daxin Jiang.\n\n\nFine-grained distillation for long document retrieval, 2022.\n\n\n"
    },
    "domain": "cs.CL",
    "citation_count": 0
  },
  "4b275ed7-78fc-409e-ba20-4d8e994e584e": {
    "pk": "4b275ed7-78fc-409e-ba20-4d8e994e584e",
    "authors": [
      "Hongjin Qian",
      "Zheng Liu",
      "Peitian Zhang",
      "Kelong Mao",
      "Yujia Zhou",
      "Xu Chen",
      "Zhicheng Dou"
    ],
    "title": "Are Long-LLMs A Necessity For Long-Context Tasks?",
    "abstract": "The learning and deployment of long-LLMs remains a challenging problem despite recent progresses. In this work, we argue that the long-LLMs are not a necessity to solve long-context tasks, as common long-context tasks are short-context solvable, i.e. they can be solved by purely working with oracle short-contexts within the long-context tasks' inputs. On top of this argument, we propose a framework called LC-Boost (Long-Context Bootstrapper), which enables a short-LLM to address the long-context tasks in a bootstrapping manner. In our framework, the short-LLM prompts itself to reason for two critical decisions: 1) how to access to the appropriate part of context within the input, 2) how to make effective use of the accessed context. By adaptively accessing and utilizing the context based on the presented tasks, LC-Boost can serve as a general framework to handle diversified long-context processing problems. We comprehensively evaluate different types of tasks from popular long-context benchmarks, where LC-Boost is able to achieve a substantially improved performance with a much smaller consumption of resource.",
    "url": "http://arxiv.org/abs/2405.15318v1",
    "timestamp": 1716537570,
    "sections": {
      "1 Introduction": "\n\n1 Introduction\n\nLarge language models (LLMs) are widely adopted for real-world applications. Many of the applications are associated with long-sequence inputs, such as long-document question answering and summarization. As such, the LLMs are commonly expected to have a long working context (a.k.a. long-LLMs) in order to confront such demanding scenarios\u00a0(Bai et\u00a0al., 2023; Zhang et\u00a0al., 2024a). Unfortunately, the learning and deployment of long-LLMs are still challenging in multiple perspectives. Particularly, many existing LLMs are initially introduced with a limited size of context (e.g., 2K for Llama-1 Touvron et\u00a0al. (2023a), 4K for Llama-2 Touvron et\u00a0al. (2023b), 8K for Llama-3\u00a0111https://llama.meta.com/llama3/). Although the initial short-LLM can be fine-tuned to establish a much longer context, it is likely to take substantial costs; and more seriously, it is extremely resource-consuming to deploy the long-LLMs\u00a0(Kaplan et\u00a0al., 2020). The continually training may also compromise the LLMs\u2019 general capability over short contexts\u00a0(Liu et\u00a0al., 2023; Li et\u00a0al., 2023a). In fact, it remains an open problem to explore new solutions which may tackle long-context tasks both effectively and efficiently.\n\n\nIn this paper, we argue that most long-context tasks are short-context solvable. That is to say, the long-context tasks, despite associated with long-sequence inputs, can be addressed by merely working with short-contexts in a strategic way. For example, the reading comprehension or summarization of a book can be solved based on the extraction of necessary key facts from the book.\nThe above argument is akin to the working patterns of human beings and modern computers, where arbitrary long-form problems can always be decomposed and solved on top of a limited memory capacity\u00a0(Adolphs, 1999; Bryant and O\u2019Hallaron, 2011).\nHowever, even if the above argument holds, it is still non-trivial to solve the long-context tasks purely based on short contexts. This is because different tasks call for distinct ways of accessing and utilizing information from the long context; therefore, there can hardly be any fixed rules to handle all possible situations. To address this challenge, we propose a method, called LC-Boost, where short-LLMs are employed to solve general long-context tasks in a bootstrapping manner. LC-Boost operates with two critical reasoning steps. One is the reasoning of Access, where the LLM prompts itself to plan for how to access the appropriate part of context within the input. The other one is the reasoning of Utilize, where the LLM figures out how to make effective use of the accessed context. Thanks to the above design, LC-Boost is able to adaptively handle diversified long-context tasks according to their unique nature. For example, given a knowledge-grounded QA problem, the LLM may directly access to the knowledgable context through retrieval, and generate the answer in the form of RAG. Besides, it may sequentially scan the long context chunk-by-chunk if the task calls for the aggregation of specific information from the entire input.\n\n\nFigure 1: Illustration for LC-Boost. The LLM is prompted to reason for how to access to proper context and how to utilize the accessed context to solve the task. Toy Examples.\n(A) Brute-force solution. Despite correctness, it is unnecessarily expensive due to the processing of the entire context simultaneously. (B) Naive RAG. It is hard to handle problems like information aggregation, which leads to the incomplete answer. (C) LC-Boost leverages RAG to tackle the problem, which produces the correct answer in a small cost. (D) LC-Boost processes the long-context via sequential scan, which correctly solves the problem based on the comprehensively collected information.\n\n\nThe following toy examples are presented to better illustrate the mechanism of LC-Boost (Figure 1). Particular, there are two common approaches to tackle long-context problems: (A) the brute-force method based on long-LLMs, (B) the surrogate methods, like RAG Xu et\u00a0al. (2023a). Despite being straightforward, the brute-force method is likely to incur huge unnecessary costs as the problem could be directly solved by simple surrogate methods, like RAG. On the other hand, although the surrogate methods may help in certain cases, they are likely to become useless in other situations. For instance, the RAG-based methods are inappropriate to handle information aggregation problems, as showcased in\u00a0(B). In contrast, LC-Boost is able to handle general long-context tasks thanks to the proper reasoning of how to access and utilize\nthe long-context information based on each specific task. As shown in (C), it can directly access to the needed information via retrieval and generate the answer based on RAG. Meanwhile, it can also process the entire context in a divide-and-conquer manner, which will fully collect the information and solve the problem presented in (D).\n\n\nWe perform comprehensive experiments for LC-Boost, including both popular real-world long-context problems, like question-answering and summarization of long documents, and a wide variety of synthetic tasks. In our experiments, LC-Boost is able to achieve equivalent performances as the brute-force methods based on strong long-LLMs, e.g., GPT-4-128K. In many cases, its performances can even notably surpass the brute-force methods, probably due to the elimination of distracting context. Besides, our experiments also underscore the importance of reasoning and adaptability, as LC-Boost outperforms all short-LLM surrogates with predefined access and utilization of context.\n\n\nTo summarize, our paper makes the following contributions. (1) We identify the research problem of solving long-context problems with short-LLMs. To the best of our knowledge, it is the first study of its kind, which is important to not only address the problem itself but also meaningful to the sustainability and energy-efficient running of AI industry in a broader sense. (2) We propose a novel framework LC-Boost, which is able to adaptively handle general long-context tasks based on the reasoning of how to access and utilize the long context. (3) We empirically verify the effectiveness of LC-Boost based on its superior performances achieved from low resource-consumption.\n\n",
      "2 LC-Boost": "\n\n2 LC-Boost\n\n\n2.1 Preliminaries\n\nLLMs can be succinctly defined as \ud835\udcb4=\u03b3\u2062(q)\ud835\udcb4\ud835\udefe\ud835\udc5e{\\mathcal{Y}}=\\gamma(q)caligraphic_Y = italic_\u03b3 ( italic_q ), where \u03b3\u2062(\u22c5)\ud835\udefe\u22c5\\gamma(\\cdot)italic_\u03b3 ( \u22c5 ) represents a selected LLM, q\ud835\udc5eqitalic_q denotes a user query, and \ud835\udcb4\ud835\udcb4{\\mathcal{Y}}caligraphic_Y refers to the answer produced by the LLMs. As highlighted in many previous studies, e.g.,\u00a0(Ji et\u00a0al., 2023; Lewis et\u00a0al., 2020; Shuster et\u00a0al., 2021), the knowledge embedded in an LLM\u2019s parameters is static and, consequently, often fails to adequately address user queries requiring up-to-date or in-depth knowledge. To address this limitation, we can introduce external knowledge (refer to as context \ud835\udcb3\ud835\udcb3{\\mathcal{X}}caligraphic_X) into the LLMs. Additionally, tasks involving information aggregation\u00a0(e.g., summarization) also take a context \ud835\udcb3\ud835\udcb3{\\mathcal{X}}caligraphic_X as input along with task instructions q\ud835\udc5eqitalic_q. Thus, we can generally define the model\u2019s generation process w.r.t. a context \ud835\udcb3\ud835\udcb3{\\mathcal{X}}caligraphic_X as:\n\ud835\udcb4=\u03b3\u2062(q,\ud835\udcb3).\ud835\udcb4\ud835\udefe\ud835\udc5e\ud835\udcb3{\\mathcal{Y}}=\\gamma(q,{\\mathcal{X}}).caligraphic_Y = italic_\u03b3 ( italic_q , caligraphic_X ) .\n\n\nAs discussed in Section 1, in many scenarios, the context \ud835\udcb3\ud835\udcb3{\\mathcal{X}}caligraphic_X is a long sequence, necessitating that LLMs manage long contexts. However, most existing LLMs were originally introduced with limited context sizes\u00a0(e.g., 4K). Consequently, these models are unable to process inputs that exceed their capacity without truncation. In this paper, we characterize such scenarios as long-context problem. It involves LLMs processing inputs that notably surpass their inherent context limitations, which can be formally described by:\n\n\n\n\ud835\udcb4=\u03b3\u2062(q,\ud835\udcb3)s.t.\u2062|\ud835\udcb3|\u226bL,formulae-sequence\ud835\udcb4\ud835\udefe\ud835\udc5e\ud835\udcb3much-greater-thans.t.\ud835\udcb3\ud835\udc3f{\\mathcal{Y}}=\\gamma(q,{\\mathcal{X}})\\quad\\text{s.t.}|{\\mathcal{X}}|\\gg L,caligraphic_Y = italic_\u03b3 ( italic_q , caligraphic_X ) s.t. | caligraphic_X | \u226b italic_L ,\n\n(1)\n\n\nwhere L\ud835\udc3fLitalic_L denotes the native context length limit of the LLM. The most straightforward way to address the long-context problem is to increase the LLMs\u2019 context length L\ud835\udc3fLitalic_L, mitigating the challenges of long contexts. In this paper, we instead explore solving long-context tasks using short-context LLMs\u00a0(e.g., 4K) without increasing the model\u2019s context length L\ud835\udc3fLitalic_L.\n\n\n\n\n2.2 Pilot Study: Are Most Long-Context Tasks Short-Context Solvable?\n\nDespite the potential for fine-tuning LLMs to handle much longer contexts, this approach incurs substantial costs. Additionally, directly processing long contexts during the inference stage exponentially increases computing resource consumption, which is not environmentally friendly.\nIn the following, we conduct a pilot study from both theoretical and empirical perspectives to explore the question: Are most long-context tasks solvable with short contexts?\n\n\nTheoretical Analysis\n\nSuppose we have an input variable \ud835\udcb3\ud835\udcb3{\\mathcal{X}}caligraphic_X and an output variable \ud835\udcb4\ud835\udcb4{\\mathcal{Y}}caligraphic_Y, the relevant part of \ud835\udcb3\ud835\udcb3{\\mathcal{X}}caligraphic_X given \ud835\udcb4\ud835\udcb4{\\mathcal{Y}}caligraphic_Y is denoted by \ud835\udcb3~~\ud835\udcb3\\tilde{{\\mathcal{X}}}over~ start_ARG caligraphic_X end_ARG. An ideal \ud835\udcb3~~\ud835\udcb3\\tilde{{\\mathcal{X}}}over~ start_ARG caligraphic_X end_ARG should capture all relevant features of the original input variable \ud835\udcb3\ud835\udcb3{\\mathcal{X}}caligraphic_X in relation to \ud835\udcb4\ud835\udcb4{\\mathcal{Y}}caligraphic_Y. In other words, the optimal \ud835\udcb3~~\ud835\udcb3\\tilde{{\\mathcal{X}}}over~ start_ARG caligraphic_X end_ARG represents the simplest mapping of \ud835\udcb3\ud835\udcb3{\\mathcal{X}}caligraphic_X that accurately preserves the mutual information I\u2062(\ud835\udcb3;\ud835\udcb4)\ud835\udc3c\ud835\udcb3\ud835\udcb4I({\\mathcal{X}};{\\mathcal{Y}})italic_I ( caligraphic_X ; caligraphic_Y ). We therefore propose a Markov chain \ud835\udcb3\u2192\ud835\udcb3~\u2192\ud835\udcb4\u2192\ud835\udcb3~\ud835\udcb3\u2192\ud835\udcb4{\\mathcal{X}}\\rightarrow\\tilde{{\\mathcal{X}}}\\rightarrow{\\mathcal{Y}}caligraphic_X \u2192 over~ start_ARG caligraphic_X end_ARG \u2192 caligraphic_Y. According to the data processing inequality (DPI), we have I\u2062(\ud835\udcb3;\ud835\udcb3~)\u2265I\u2062(\ud835\udcb3;\ud835\udcb4)\ud835\udc3c\ud835\udcb3~\ud835\udcb3\ud835\udc3c\ud835\udcb3\ud835\udcb4I({\\mathcal{X}};\\tilde{{\\mathcal{X}}})\\geq I({\\mathcal{X}};{\\mathcal{Y}})italic_I ( caligraphic_X ; over~ start_ARG caligraphic_X end_ARG ) \u2265 italic_I ( caligraphic_X ; caligraphic_Y ), with equality holding if and only if \ud835\udcb3~~\ud835\udcb3\\tilde{{\\mathcal{X}}}over~ start_ARG caligraphic_X end_ARG constitutes a sufficient statistics\u00a0(Cover, 1999; Tishby and Zaslavsky, 2015). This suggests that, in an optimal setting, we can always find a subset \ud835\udcb3~\u2286\ud835\udcb3~\ud835\udcb3\ud835\udcb3\\tilde{\\mathcal{X}}\\subseteq\\mathcal{X}over~ start_ARG caligraphic_X end_ARG \u2286 caligraphic_X that provides information at least as useful for generating the output \ud835\udcb4\ud835\udcb4\\mathcal{Y}caligraphic_Y as the full context \ud835\udcb3\ud835\udcb3\\mathcal{X}caligraphic_X.\n\n\nIn practical scenarios, obtaining the optimal \ud835\udcb3~~\ud835\udcb3\\tilde{{\\mathcal{X}}}over~ start_ARG caligraphic_X end_ARG is challenging due to various factors, such as empirical errors\u00a0Mohri et\u00a0al. (2018). Thus, we can only estimate \ud835\udcb3~~\ud835\udcb3\\tilde{{\\mathcal{X}}}over~ start_ARG caligraphic_X end_ARG. Estimating \ud835\udcb3~~\ud835\udcb3\\tilde{{\\mathcal{X}}}over~ start_ARG caligraphic_X end_ARG directly from \ud835\udcb3\ud835\udcb3{\\mathcal{X}}caligraphic_X might be challenging if \ud835\udcb3\ud835\udcb3{\\mathcal{X}}caligraphic_X defines a large variable space. In this situation, we propose decomposing the original input variable \ud835\udcb3\ud835\udcb3{\\mathcal{X}}caligraphic_X into a series of subsets, \ud835\udcb3={\ud835\udcb31,\u22ef,\ud835\udcb3n}\ud835\udcb3subscript\ud835\udcb31\u22efsubscript\ud835\udcb3\ud835\udc5b{\\mathcal{X}}=\\{{\\mathcal{X}}_{1},\\cdots,{\\mathcal{X}}_{n}\\}caligraphic_X = { caligraphic_X start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , \u22ef , caligraphic_X start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT } and process each subset variable separately. Thus, according to the chain rule for mutual information\u00a0Cover (1999), we have:\n\n\n\nI\u2062(\ud835\udcb3,\ud835\udcb3~)=I\u2062(\ud835\udcb31,\u22ef,\ud835\udcb3n;\ud835\udcb3~)=I\u2062(\ud835\udcb31;\ud835\udcb3~)+\u2211i=2nI\u2062(\ud835\udcb3i;\ud835\udcb3~|\ud835\udcb31,\u22ef,\ud835\udcb3i\u22121),\ud835\udc3c\ud835\udcb3~\ud835\udcb3\ud835\udc3csubscript\ud835\udcb31\u22efsubscript\ud835\udcb3\ud835\udc5b~\ud835\udcb3\ud835\udc3csubscript\ud835\udcb31~\ud835\udcb3superscriptsubscript\ud835\udc562\ud835\udc5b\ud835\udc3csubscript\ud835\udcb3\ud835\udc56conditional~\ud835\udcb3subscript\ud835\udcb31\u22efsubscript\ud835\udcb3\ud835\udc561\\displaystyle I({\\mathcal{X}},\\tilde{{\\mathcal{X}}})=I({\\mathcal{X}}_{1},%\n\\cdots,{\\mathcal{X}}_{n};\\tilde{{\\mathcal{X}}})=I({\\mathcal{X}}_{1};\\tilde{{%\n\\mathcal{X}}})+\\sum\\limits_{i=2}^{n}I({\\mathcal{X}}_{i};\\tilde{{\\mathcal{X}}}|%\n{\\mathcal{X}}_{1},\\cdots,{\\mathcal{X}}_{i-1}),italic_I ( caligraphic_X , over~ start_ARG caligraphic_X end_ARG ) = italic_I ( caligraphic_X start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , \u22ef , caligraphic_X start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ; over~ start_ARG caligraphic_X end_ARG ) = italic_I ( caligraphic_X start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ; over~ start_ARG caligraphic_X end_ARG ) + \u2211 start_POSTSUBSCRIPT italic_i = 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT italic_I ( caligraphic_X start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ; over~ start_ARG caligraphic_X end_ARG | caligraphic_X start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , \u22ef , caligraphic_X start_POSTSUBSCRIPT italic_i - 1 end_POSTSUBSCRIPT ) ,\n\n(2)\n\n\nwhich indicates that the mutual information I\u2062(\ud835\udcb3,\ud835\udcb3~)\ud835\udc3c\ud835\udcb3~\ud835\udcb3I({\\mathcal{X}},\\tilde{{\\mathcal{X}}})italic_I ( caligraphic_X , over~ start_ARG caligraphic_X end_ARG ) can be understood as the sum of the mutual information of each subset \ud835\udcb3isubscript\ud835\udcb3\ud835\udc56{\\mathcal{X}}_{i}caligraphic_X start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT and \ud835\udcb3~isubscript~\ud835\udcb3\ud835\udc56\\tilde{{\\mathcal{X}}}_{i}over~ start_ARG caligraphic_X end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT given all previous subsets.\n\n\nIn the scenario of Eq.\u00a01, the variable \ud835\udcb3\ud835\udcb3\\mathcal{X}caligraphic_X represents a long context and the variable \ud835\udcb4\ud835\udcb4\\mathcal{Y}caligraphic_Y is the output answer produced by a LLM. Thus, \ud835\udcb3~~\ud835\udcb3\\tilde{{\\mathcal{X}}}over~ start_ARG caligraphic_X end_ARG can be interpreted as the minimal necessary context from the long context \ud835\udcb3\ud835\udcb3{\\mathcal{X}}caligraphic_X given the output answer \ud835\udcb4\ud835\udcb4{\\mathcal{Y}}caligraphic_Y. Inspired by Eq.\u00a02, we can estimate an optimal \ud835\udcb3~~\ud835\udcb3\\tilde{\\mathcal{X}}over~ start_ARG caligraphic_X end_ARG using decomposed shorter contexts {\ud835\udcb31,\u2026,\ud835\udcb3n}subscript\ud835\udcb31\u2026subscript\ud835\udcb3\ud835\udc5b\\{\\mathcal{X}_{1},\\ldots,\\mathcal{X}_{n}\\}{ caligraphic_X start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , \u2026 , caligraphic_X start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT }. Thus, I\u2062(\ud835\udcb3;\ud835\udcb3~)\ud835\udc3c\ud835\udcb3~\ud835\udcb3I(\\mathcal{X};\\tilde{\\mathcal{X}})italic_I ( caligraphic_X ; over~ start_ARG caligraphic_X end_ARG ) can be computed by processing each subset \ud835\udcb3isubscript\ud835\udcb3\ud835\udc56\\mathcal{X}_{i}caligraphic_X start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT individually. However, as the number of subsets n\ud835\udc5bnitalic_n increases, accounting for all preceding subsets becomes computationally demanding. To alleviate this burden, we propose reducing the number of conditional subsets considered by replacing the entire sequence of previous subsets with a compressed surrogate \ud835\udcb3^isubscript^\ud835\udcb3\ud835\udc56\\hat{\\mathcal{X}}_{i}over^ start_ARG caligraphic_X end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT, which is iteratively derived using a compression function \ud835\udcb3^i=g\u2062(\ud835\udcb3^i\u22121,\ud835\udcb3i\u22121)subscript^\ud835\udcb3\ud835\udc56\ud835\udc54subscript^\ud835\udcb3\ud835\udc561subscript\ud835\udcb3\ud835\udc561\\hat{\\mathcal{X}}_{i}=g(\\hat{\\mathcal{X}}_{i-1},\\mathcal{X}_{i-1})over^ start_ARG caligraphic_X end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = italic_g ( over^ start_ARG caligraphic_X end_ARG start_POSTSUBSCRIPT italic_i - 1 end_POSTSUBSCRIPT , caligraphic_X start_POSTSUBSCRIPT italic_i - 1 end_POSTSUBSCRIPT ). Consequently, Eq.\u00a02 can be reformulated as follows:\n\n\n\nI(\ud835\udcb3,\ud835\udcb3~)=I(\ud835\udcb31,\u22ef,\ud835\udcb3n;\ud835\udcb3~)\u2243I(\ud835\udcb31;\ud835\udcb3~)+\u2211i=2nI(\ud835\udcb3i;\ud835\udcb3~|\ud835\udcb3^i)).I({\\mathcal{X}},\\tilde{{\\mathcal{X}}})=I({\\mathcal{X}}_{1},\\cdots,{\\mathcal{X}%\n}_{n};\\tilde{{\\mathcal{X}}})\\simeq I({\\mathcal{X}}_{1};\\tilde{{\\mathcal{X}}})+%\n\\sum\\limits_{i=2}^{n}I({\\mathcal{X}}_{i};\\tilde{{\\mathcal{X}}}|\\hat{{\\mathcal{%\nX}}}_{i})).italic_I ( caligraphic_X , over~ start_ARG caligraphic_X end_ARG ) = italic_I ( caligraphic_X start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , \u22ef , caligraphic_X start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ; over~ start_ARG caligraphic_X end_ARG ) \u2243 italic_I ( caligraphic_X start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ; over~ start_ARG caligraphic_X end_ARG ) + \u2211 start_POSTSUBSCRIPT italic_i = 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT italic_I ( caligraphic_X start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ; over~ start_ARG caligraphic_X end_ARG | over^ start_ARG caligraphic_X end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) ) .\n\n(3)\n\n\nThe equality can be upheld under two specific conditions: (1) the decomposed variables {\ud835\udcb31,\u2026,\ud835\udcb3n}subscript\ud835\udcb31\u2026subscript\ud835\udcb3\ud835\udc5b\\{\\mathcal{X}_{1},\\ldots,\\mathcal{X}_{n}\\}{ caligraphic_X start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , \u2026 , caligraphic_X start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT } are mutually independent, and (2) the compression function g\u2062(\u22c5)\ud835\udc54\u22c5g(\\cdot)italic_g ( \u22c5 ) is optimally designed, ensuring that the compressed surrogate \ud835\udcb3^isubscript^\ud835\udcb3\ud835\udc56\\hat{\\mathcal{X}}_{i}over^ start_ARG caligraphic_X end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT encapsulates all relevant information from the preceding subsets with respect to \ud835\udcb3~~\ud835\udcb3\\tilde{\\mathcal{X}}over~ start_ARG caligraphic_X end_ARG. Otherwise, I\u2062(\ud835\udcb3,\ud835\udcb3~)\ud835\udc3c\ud835\udcb3~\ud835\udcb3I({\\mathcal{X}},\\tilde{{\\mathcal{X}}})italic_I ( caligraphic_X , over~ start_ARG caligraphic_X end_ARG ) can only be approximately estimated.\n\n\n\nEmpirical Analysis\n\nFigure 2: Pilot Study Across Various Tasks: In the Brute-force setting, the entire context is processed by GPT-4-128K. In the LC-Boost setting, the maximum context length is restricted to 4K, and LC-Boost is utilized to solve the long-context problem with short context. \n\n\nTo empirically assess the accuracy of estimating the minimal necessary context \ud835\udcb3~~\ud835\udcb3\\tilde{\\mathcal{X}}over~ start_ARG caligraphic_X end_ARG using decomposed short contexts {\ud835\udcb31,\u2026,\ud835\udcb3n}subscript\ud835\udcb31\u2026subscript\ud835\udcb3\ud835\udc5b\\{\\mathcal{X}_{1},\\ldots,\\mathcal{X}_{n}\\}{ caligraphic_X start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , \u2026 , caligraphic_X start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT }, we conduct pilot experiments across various tasks requiring long contexts. Specifically, we utilize GPT-4-128K to perform these tasks in two settings: (1) feeding the entire long context into GPT-4-128K in a brute-force manner, instructing the model to directly produce the output answer, and (2) decomposing the full context into short contexts and applying the methods defined in Eq.\u00a03 to approximate \ud835\udcb3~~\ud835\udcb3\\tilde{\\mathcal{X}}over~ start_ARG caligraphic_X end_ARG, which then guides the model to produce the final output (the LC-Boost setting).\n\n\nFigure\u00a02 presents the experiment results, which generally indicate that LC-Boost consistently performs as well as or better than the brute-force setting. In particular, for tasks such as QA, few-shot learning, and synthetic tasks, LC-Boost outperforms the brute-force setting. This is because the decomposed short contexts for these tasks are more likely to be mutually independent given the input query which can be adequately supported by a few extracted contexts from the long context. By precisely\nlocating these supported context, it can filter out irrelevant context of \ud835\udcb3\ud835\udcb3{\\mathcal{X}}caligraphic_X that might otherwise undermine task performance.\nFor tasks like summarization and code completion, the inherent properties of these tasks require considering the mutual dependencies among all decomposed short contexts, making the LC-Boost setting more challenging. However, as discussed in Eq.\u00a03, when the compression function g\u2062(\u22c5)\ud835\udc54\u22c5g(\\cdot)italic_g ( \u22c5 ) is optimal, we can achieve the optimal \ud835\udcb3~~\ud835\udcb3\\tilde{\\mathcal{X}}over~ start_ARG caligraphic_X end_ARG. GPT-4 serves as such a strong compression function, ensuring that the compressed surrogate \ud835\udcb3^isubscript^\ud835\udcb3\ud835\udc56\\hat{\\mathcal{X}}_{i}over^ start_ARG caligraphic_X end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT is well-estimated. Consequently, in these tasks, LC-Boost achieves performance that is equal to or slightly better than the brute-force setting.\n\n\nThrough theoretical analysis, we can posit that long-context tasks are short-context solvable if we can estimate a better minimal necessary context \ud835\udcb3~~\ud835\udcb3\\tilde{\\mathcal{X}}over~ start_ARG caligraphic_X end_ARG from the decomposed short contexts {\ud835\udcb31,\u2026,\ud835\udcb3n}subscript\ud835\udcb31\u2026subscript\ud835\udcb3\ud835\udc5b\\{\\mathcal{X}_{1},\\ldots,\\mathcal{X}_{n}\\}{ caligraphic_X start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , \u2026 , caligraphic_X start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT } than from the long context \ud835\udcb3\ud835\udcb3\\mathcal{X}caligraphic_X. Empirical analysis supports this assumption, demonstrating that in most cases, the estimation error of deriving \ud835\udcb3~~\ud835\udcb3\\tilde{\\mathcal{X}}over~ start_ARG caligraphic_X end_ARG from the long context \ud835\udcb3\ud835\udcb3\\mathcal{X}caligraphic_X is often larger than from the decomposed short contexts {\ud835\udcb31,\u2026,\ud835\udcb3n}subscript\ud835\udcb31\u2026subscript\ud835\udcb3\ud835\udc5b\\{\\mathcal{X}_{1},\\ldots,\\mathcal{X}_{n}\\}{ caligraphic_X start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , \u2026 , caligraphic_X start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT }. This indicates that using short contexts can be comparatively more advantageous than using the full context. Therefore, we can validate our argument in Section\u00a01: most long-context tasks, if not all, are short-context solvable.\n\n\n\n\n\n2.3 The Proposed Method: LC-Boost\n\nWe propose a method called LC-Boost, which utilizes short LLMs to solve general long-context tasks. LC-Boost begins with an input query q\ud835\udc5eqitalic_q and a long context \ud835\udcb3\ud835\udcb3\\mathcal{X}caligraphic_X, with the goal of producing an output answer \ud835\udcb4\ud835\udcb4\\mathcal{Y}caligraphic_Y. Since the underlying LLM in LC-Boost has a limited context size\u00a0(we limit LC-Boost working with 4K context length), directly generating the output answer \ud835\udcb4\ud835\udcb4\\mathcal{Y}caligraphic_Y is infeasible for long-context tasks. To address this, we propose solving long-context tasks by strategically understanding the decomposed short contexts \ud835\udcb3={\ud835\udcb31,\u22ef,\ud835\udcb3n}\ud835\udcb3subscript\ud835\udcb31\u22efsubscript\ud835\udcb3\ud835\udc5b\\mathcal{X}=\\{\\mathcal{X}_{1},\\cdots,\\mathcal{X}_{n}\\}caligraphic_X = { caligraphic_X start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , \u22ef , caligraphic_X start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT }. From these short contexts, we aim to extract the minimal necessary context \ud835\udcb3~~\ud835\udcb3\\tilde{\\mathcal{X}}over~ start_ARG caligraphic_X end_ARG to support the generation of the output answer \ud835\udcb4\ud835\udcb4\\mathcal{Y}caligraphic_Y.\n\n\nLC-Boost achieves this goal through a decision-making process involving iterative interactions between LC-Boost and the decomposed short contexts {\ud835\udcb31,\u22ef,\ud835\udcb3n}subscript\ud835\udcb31\u22efsubscript\ud835\udcb3\ud835\udc5b\\{\\mathcal{X}_{1},\\cdots,\\mathcal{X}_{n}\\}{ caligraphic_X start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , \u22ef , caligraphic_X start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT } with respect to the input query q\ud835\udc5eqitalic_q. In the process, LC-Boost interact with each short context \ud835\udcb3isubscript\ud835\udcb3\ud835\udc56\\mathcal{X}_{i}caligraphic_X start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT, employing two types of actions: information access and information utilization.\n\n\nWe denote an action at time step i\ud835\udc56iitalic_i by aisubscript\ud835\udc4e\ud835\udc56a_{i}italic_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT and denote the relevant context LC-Boost obtains from the i\ud835\udc56iitalic_i-th short context \ud835\udcb3isubscript\ud835\udcb3\ud835\udc56{\\mathcal{X}}_{i}caligraphic_X start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT by \ud835\udcb3~isubscript~\ud835\udcb3\ud835\udc56\\tilde{{\\mathcal{X}}}_{i}over~ start_ARG caligraphic_X end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT\nThe action aisubscript\ud835\udc4e\ud835\udc56a_{i}italic_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT is predicted by considering the current short context \ud835\udcb3isubscript\ud835\udcb3\ud835\udc56{\\mathcal{X}}_{i}caligraphic_X start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT, the input query q\ud835\udc5eqitalic_q, as well as all previous extracted relevant information \ud835\udcb3~1:i\u22121subscript~\ud835\udcb3:1\ud835\udc561\\tilde{{\\mathcal{X}}}_{1:i-1}over~ start_ARG caligraphic_X end_ARG start_POSTSUBSCRIPT 1 : italic_i - 1 end_POSTSUBSCRIPT: ai=\u03b3\u2062(q,\ud835\udcb3i|\ud835\udcb3~1:i\u22121),subscript\ud835\udc4e\ud835\udc56\ud835\udefe\ud835\udc5econditionalsubscript\ud835\udcb3\ud835\udc56subscript~\ud835\udcb3:1\ud835\udc561a_{i}=\\gamma(q,{\\mathcal{X}}_{i}|\\tilde{{\\mathcal{X}}}_{1:i-1}),italic_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = italic_\u03b3 ( italic_q , caligraphic_X start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT | over~ start_ARG caligraphic_X end_ARG start_POSTSUBSCRIPT 1 : italic_i - 1 end_POSTSUBSCRIPT ) ,\nwhere \u03b3\u2062(\u22c5)\ud835\udefe\u22c5\\gamma(\\cdot)italic_\u03b3 ( \u22c5 ) denotes LC-Boost\u2019s underlying LLM.\n\n\nPredicting the action aisubscript\ud835\udc4e\ud835\udc56a_{i}italic_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT in a continuous space is challenging as it requires the underling model to reason about highly implicit relations among the input query, the current context, and the previous contexts. Therefore, we define a discrete action space \ud835\udc9c\ud835\udc9c{\\mathcal{A}}caligraphic_A comprising:\n(1)\u00a0[Task Understanding]: analyzing the query and task for initialization;\n(2)\u00a0[Retrieve]: accessing text evidence by a retrieval method;\n(3)\u00a0[Move]: accessing the next short text context directly;\nThese two are information access actions which define the LC-Boost\u2019s trajectory to access short contexts.\n(4)\u00a0[Append]: generating relevant context \ud835\udcb3~isubscript~\ud835\udcb3\ud835\udc56\\tilde{{\\mathcal{X}}}_{i}over~ start_ARG caligraphic_X end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT independently, denoting by \ud835\udcb3~i=ai\u2062(\ud835\udcb3i)subscript~\ud835\udcb3\ud835\udc56subscript\ud835\udc4e\ud835\udc56subscript\ud835\udcb3\ud835\udc56\\tilde{{\\mathcal{X}}}_{i}=a_{i}({\\mathcal{X}}_{i})over~ start_ARG caligraphic_X end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = italic_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( caligraphic_X start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT );\n(5)\u00a0[Merge]: generating relevant context \ud835\udcb3~isubscript~\ud835\udcb3\ud835\udc56\\tilde{{\\mathcal{X}}}_{i}over~ start_ARG caligraphic_X end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT with respect to previous extracted relevant information, denoting by \ud835\udcb3~i=ai\u2062(\ud835\udcb3i|\ud835\udcb3~1:i\u22121)subscript~\ud835\udcb3\ud835\udc56subscript\ud835\udc4e\ud835\udc56conditionalsubscript\ud835\udcb3\ud835\udc56subscript~\ud835\udcb3:1\ud835\udc561\\tilde{{\\mathcal{X}}}_{i}=a_{i}({\\mathcal{X}}_{i}|\\tilde{{\\mathcal{X}}}_{1:i-1})over~ start_ARG caligraphic_X end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = italic_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( caligraphic_X start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT | over~ start_ARG caligraphic_X end_ARG start_POSTSUBSCRIPT 1 : italic_i - 1 end_POSTSUBSCRIPT );\n(6)\u00a0[Answer]: answering the user query and returning;\n(7)\u00a0[Aggregation]: aggregating all relevant information and returning.\nWe define our LC-Boost frame in Algorithm\u00a01.\n\n\nAlgorithm 1  LC-Boost Framework\n\n\n1:\u00a0\u00a0Input: Input query q\ud835\udc5eqitalic_q, long context \ud835\udcb3\ud835\udcb3\\mathcal{X}caligraphic_X\n\n\n\n2:\u00a0\u00a0Output: Answer \ud835\udcb4\ud835\udcb4\\mathcal{Y}caligraphic_Y\n\n\n\n3:\u00a0\u00a0Decompose long context\u2062\ud835\udcb3\u2190{\ud835\udcb31,\u22ef,\ud835\udcb3n}\u2190long context\ud835\udcb3subscript\ud835\udcb31\u22efsubscript\ud835\udcb3\ud835\udc5b\\text{long context}~{}{\\mathcal{X}}\\leftarrow\\{{\\mathcal{X}}_{1},\\cdots,{%\n\\mathcal{X}}_{n}\\}long context caligraphic_X \u2190 { caligraphic_X start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , \u22ef , caligraphic_X start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT }\n\n\n\n4:\u00a0\u00a0Initialize extracted relevant context\u2062\ud835\udcb3~0\u2190None\u2190extracted relevant contextsubscript~\ud835\udcb30None\\text{extracted relevant context}~{}\\tilde{{\\mathcal{X}}}_{0}\\leftarrow\\text{None}extracted relevant context over~ start_ARG caligraphic_X end_ARG start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT \u2190 None\n\n\n\n5:\u00a0\u00a0Perform [Task Understanding]\n\n\n\n6:\u00a0\u00a0while\u00a0i\u2264n\ud835\udc56\ud835\udc5bi\\leq nitalic_i \u2264 italic_n\u00a0do\n\n\n\n7:\u00a0\u00a0\u00a0\u00a0\u00a0Select an action ai\u2190ai=\u03b3\u2062(q,\ud835\udcb3i|\ud835\udcb3~1:i\u22121),ai\u2208\ud835\udc9cformulae-sequence\u2190subscript\ud835\udc4e\ud835\udc56subscript\ud835\udc4e\ud835\udc56\ud835\udefe\ud835\udc5econditionalsubscript\ud835\udcb3\ud835\udc56subscript~\ud835\udcb3:1\ud835\udc561subscript\ud835\udc4e\ud835\udc56\ud835\udc9ca_{i}\\leftarrow a_{i}=\\gamma(q,{\\mathcal{X}}_{i}|\\tilde{{\\mathcal{X}}}_{1:i-1}%\n),~{}a_{i}\\in{\\mathcal{A}}italic_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT \u2190 italic_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = italic_\u03b3 ( italic_q , caligraphic_X start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT | over~ start_ARG caligraphic_X end_ARG start_POSTSUBSCRIPT 1 : italic_i - 1 end_POSTSUBSCRIPT ) , italic_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT \u2208 caligraphic_A\n\n\n\n8:\u00a0\u00a0\u00a0\u00a0\u00a0if aisubscript\ud835\udc4e\ud835\udc56a_{i}italic_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT is [Move] then i\u2190i+1\u2190\ud835\udc56\ud835\udc561i\\leftarrow i+1italic_i \u2190 italic_i + 1, continue\n\n\n\n9:\u00a0\u00a0\u00a0\u00a0\u00a0if aisubscript\ud835\udc4e\ud835\udc56a_{i}italic_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT is [Retrieve] then retrieve evidence from \ud835\udcb3={\ud835\udcb31,\u22ef,\ud835\udcb3n}\ud835\udcb3subscript\ud835\udcb31\u22efsubscript\ud835\udcb3\ud835\udc5b{\\mathcal{X}}=\\{{\\mathcal{X}}_{1},\\cdots,{\\mathcal{X}}_{n}\\}caligraphic_X = { caligraphic_X start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , \u22ef , caligraphic_X start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT }\n\n\n\n10:\u00a0\u00a0\u00a0\u00a0\u00a0if aisubscript\ud835\udc4e\ud835\udc56a_{i}italic_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT is [Append] then generate relevant context by \ud835\udcb3~i=ai\u2062(\ud835\udcb3i)subscript~\ud835\udcb3\ud835\udc56subscript\ud835\udc4e\ud835\udc56subscript\ud835\udcb3\ud835\udc56\\tilde{{\\mathcal{X}}}_{i}=a_{i}({\\mathcal{X}}_{i})over~ start_ARG caligraphic_X end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = italic_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( caligraphic_X start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT )\n\n\n\n11:\u00a0\u00a0\u00a0\u00a0\u00a0if aisubscript\ud835\udc4e\ud835\udc56a_{i}italic_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT is [Merge] then generate relevant context by \ud835\udcb3~i=ai\u2062(\ud835\udcb3i|\ud835\udcb3~1:i\u22121)subscript~\ud835\udcb3\ud835\udc56subscript\ud835\udc4e\ud835\udc56conditionalsubscript\ud835\udcb3\ud835\udc56subscript~\ud835\udcb3:1\ud835\udc561\\tilde{{\\mathcal{X}}}_{i}=a_{i}({\\mathcal{X}}_{i}|\\tilde{{\\mathcal{X}}}_{1:i-1})over~ start_ARG caligraphic_X end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = italic_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( caligraphic_X start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT | over~ start_ARG caligraphic_X end_ARG start_POSTSUBSCRIPT 1 : italic_i - 1 end_POSTSUBSCRIPT )\n\n\n\n12:\u00a0\u00a0\u00a0\u00a0\u00a0if aisubscript\ud835\udc4e\ud835\udc56a_{i}italic_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT \u2208{\\in\\{\u2208 {[Answer],[Aggregation]}}\\}} then generate answer \ud835\udcb4=\u03b3\u2062(q,\ud835\udcb3~1:i)\ud835\udcb4\ud835\udefe\ud835\udc5esubscript~\ud835\udcb3:1\ud835\udc56{\\mathcal{Y}}=\\gamma(q,\\tilde{{\\mathcal{X}}}_{1:i})caligraphic_Y = italic_\u03b3 ( italic_q , over~ start_ARG caligraphic_X end_ARG start_POSTSUBSCRIPT 1 : italic_i end_POSTSUBSCRIPT ), break\n\n\n\n13:\u00a0\u00a0\u00a0\u00a0\u00a0i\u2190i+1\u2190\ud835\udc56\ud835\udc561i\\leftarrow i+1italic_i \u2190 italic_i + 1\n\n\n\n14:\u00a0\u00a0end\u00a0while\n\n\n15:\u00a0\u00a0return answer \ud835\udcb4\ud835\udcb4{\\mathcal{Y}}caligraphic_Y\n\n\n\n\n\nThough the pre-defined action space \ud835\udc9c\ud835\udc9c\\mathcal{A}caligraphic_A comprises only seven actions, LC-Boost serves as a general framework sufficient for solving most long-context tasks. This effectiveness is based on the following reasons: (1)\u00a0Flexible accessibility:\u00a0By utilizing both [Retrieve] and [Move] actions, LC-Boost can access any short context \ud835\udcb3i\u2208\ud835\udcb3subscript\ud835\udcb3\ud835\udc56\ud835\udcb3\\mathcal{X}_{i}\\in\\mathcal{X}caligraphic_X start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT \u2208 caligraphic_X in a flexible trajectory, avoiding the need to browse the entire long context. This makes the information accessing process more efficient. (2)\u00a0Accurate information acquisition:\u00a0Through the [Append] and [Merge] actions, LC-Boost can either independently extract relevant information from the current short context, appending it to previously extracted information, or merge the current relevant information into the previous relevant information. This capability allows LC-Boost to acquire relevant information in a compatible manner, making it adaptable to many knowledge-intensive tasks. and (3)\u00a0Dynamic answering:\u00a0Using the [Answer] and [Aggregate] actions, LC-Boost can dynamically utilize the acquired relevant information to produce the target form of the answer (e.g., a short answer for QA tasks via the [Answer] action, or a long answer for summarization tasks via the [Aggregate] action).\n\n\nIn our pilot study depicted in Figure\u00a02, we observe that while GPT-3.5 serves as an inferior foundation model compared to GPT-4, it still demonstrates significant effectiveness when incorporated with LC-Boost. Given considerations of efficiency and cost-effectiveness, we employ GPT-3.5 as the foundation model for LC-Boost in the subsequent experiments. Besides, we show the prompts used in LC-Boost in Appendix\u00a0B.\n\n\n",
      "3 Experiments": "\n\n3 Experiments\n\n\n3.1 Experiment Settings\n\nWe evaluate LC-Boost and baseline models on 12 datasets, including:\n(1) Single-Doc QA: NarrativeQA\u00a0(Ko\u010disk\u00fd et\u00a0al., 2017), Qasper\u00a0(Dasigi et\u00a0al., 2021), and MultiFieldQA\u00a0(Bai et\u00a0al., 2023).\n(2) Multi-Doc QA: HotpotQA\u00a0(Yang et\u00a0al., 2018), 2WikiMQA\u00a0(Ho et\u00a0al., 2020), and MuSiQue\u00a0(Trivedi et\u00a0al., 2022).\n(3) Summarization: GovReport\u00a0(Huang et\u00a0al., 2021) and MultiNews\u00a0(Fabbri et\u00a0al., 2019).\n(4) Few-shot Learning: SAMSum\u00a0(Gliwa et\u00a0al., 2019).\n(5) Synthetic Task: Passage Count\u00a0(Bai et\u00a0al., 2023) and Self-Constructed Dataset.\n(6) Code Completion: LCC\u00a0(Guo et\u00a0al., 2023).\nMore details about the evaluation datasets and metrics are introduced in Appendix\u00a0A.\n\n\nWe compare our LC-Boost with three types of models: (1) Short LLMs (defined as with context length <<< 32K): Llama2-7B-Chat-4K\u00a0(Touvron et\u00a0al., 2023b), Llama3-8B-Instruct-8K and Vicuna-v1.5-7B-16K\u00a0(Chiang et\u00a0al., 2023); (2) Long LLMs (defined as with context length \u2265\\geq\u2265 32K): LongChat-v1.5-7B-32K\u00a0(Li et\u00a0al., 2023b), Mistral-7B-Instruct-v0.2-32K\u00a0(Jiang et\u00a0al., 2023a), Llama3-8B-80K\u00a0Zhang et\u00a0al. (2024b), Phi-3-mini-128K\u00a0(Abdin et\u00a0al., 2024) and Yi-9B-200K\u00a0(AI et\u00a0al., 2024); (3)\u00a0Closed-Source LLMs: DeepSeek-v2\u00a0(236B MoE model, ranks top-tier in MT-Bench)\u00a0(DeepSeek-AI, 2024), Claude-3-Haiku222https://www.anthropic.com/claude and GPT-3.5-turbo-16K333https://platform.openai.com/docs/models. In the experiments, if the context length exceed the model\u2019s length limit, following\u00a0Bai et\u00a0al. (2023), we truncate the context from the middle since the front and end of the context may contain crucial information. We provide further implementation details in Appendix\u00a0B.\n\n\n\n\n3.2 Main Results\n\nTable 1: Main experiment results. The best results are in bold and the secondary results are marked with underline. We report the average scores (%) on the main tasks. The detailed scores over all dataset are shown in Table\u00a03.\n\n\nModels\nSingle-Doc\nMulti-Doc\nSumm.\nFew-shot\nSynthetic\nCode\n\n\nShort LLMs (Context Length <<< 32K)\n\n\n\n\n\n\nLlama2-7B-Chat-4K\n24.9\n22.5\n26.6\n40.7\n6.3\n52.4\n\n\nLlama3-8B-Instruct-8K\n37.3\n36.0\n26.5\n42.7\n15.0\n57.5\n\n\nVicuna-v1.5-7B-16K\n28.0\n18.6\n27.5\n40.8\n8.9\n51.0\n\n\nLong LLMs (Context Length \u2265\\geq\u2265 32K)\n\n\n\n\n\n\nLongChat-v1.5-7B-32K\n28.7\n20.6\n28.6\n34.2\n6.8\n53.0\n\n\nMistral-7B-Instruct-v0.2-32K\n31.9\n26.0\n29.3\n43.0\n14.0\n55.4\n\n\nLlama3-8B-80K\n43.6\n43.1\n30.2\n42.9\n19.6\n53.6\n\n\nPhi-3-mini-128K\n33.5\n38.2\n28.8\n36.0\n19.9\n60.1\n\n\nYi-9B-200K\n29.6\n38.7\n28.4\n14.6\n6.5\n72.1\n\n\nClosed-Source LLMs\n\n\n\n\n\n\nDeepSeek-v2 (32K)\n37.6\n49.1\n30.8\n39.3\n14.5\n37.0\n\n\nClaude-3-Haiku (200K)\n41.9\n45.4\n30.1\n7.2\n25.5\n16.9\n\n\nGPT-3.5-turbo-16K\n39.8\n38.7\n28.1\n41.7\n18.7\n54.7\n\n\nLC-Boost\u00a0(4K)\n47.8\n56.4\n31.8\n44.1\n27.5\n59.0\n\n\n\n\nTable\u00a01 shows the overall experimental results for all models across all tasks. From the table, we derive several key findings:\nFirst, LC-Boost, with a context length of 4K, outperforms all baseline models in all tasks except for the Code Completion task. This result verifies LC-Boost\u2019s capability to effectively solve long-context tasks by strategically processing decomposed short contexts.\nSecond, long LLMs generally perform better than short LLMs, indicating the effectiveness of fine-tuning LLMs to adapt to long contexts. However, the performance of long LLMs is not consistently stable across different tasks. For example, Yi-9B-200K excels in the Code Completion task but does not show consistent performance in other tasks such as single-doc QA, few-shot learning, and synthetic tasks. This inconsistency suggests that adapting LLMs to long contexts may compromise their general abilities.\nLast, LC-Boost consistently surpasses its underlying LLM, GPT-3.5-turbo-16K, across all tasks by a notable margin. This demonstrates that LC-Boost can achieve improved performance while simultaneously reducing resource costs, making LC-Boost an environmentally friendly method.\n\n\n\n\n3.3 Ablation Study: Dynamic is Important\n\nTo investigate the necessity of LC-Boost\u2019s design, we conduct ablation studies by changing LC-Boost\u2019s action space \ud835\udc9c\ud835\udc9c\\mathcal{A}caligraphic_A, resulting in different information acquisition strategies. We experiment with the following settings: (1)\u00a0[Retrieve] only: Directly retrieve the most relevant short context. (2)\u00a0[Merge] only: Sequentially process all short contexts while considering the previously processed context. (3)\u00a0[Append] only: Sequentially process all short contexts independently. (4)\u00a0[Merge] & [Move]: Selectively process short contexts while considering the already processed context. (6)\u00a0[Append] & [Move]: Selectively process short contexts independently. (7): [Retrieve] & [Move]: Retrieve the top-k\ud835\udc58kitalic_k relevant short contexts and selectively process a few of them. (8): Brute-force: Directly produce the answer based on the entire long context. (9)\u00a0Random: For each short context, randomly select an action. Based on the acquired information from each strategy, LC-Boost then selects either the [Answer] or [Aggregation] action to produce the final answer.\n\n\nFigure\u00a03 illustrates the results, from which we find that: (1)\u00a0Compared to fixed processing strategies, LC-Boost customizes the action trajectory for each query, resulting in notable performance improvements. This finding emphasizes the importance of the dynamic capabilities of LC-Boost. (2)\u00a0LC-Boost is particularly effective in single-doc QA and multi-doc QA tasks, as it can accurately select the minimal necessary context required to answer the input query, filtering out irrelevant information from the long context. (3)\u00a0In the few-shot learning task, LC-Boost does not significantly outperform the fixed strategies. This is attributed to the numerous in-context examples provided within the task, which offer substantial guidance, thus diminishing the impact of the number of in-context examples on the final performance.\n\n\nFigure 3: Performance comparison on different context processing strategies in the ablation study. NarrativeQA\u00a0(left) is a single-doc QA task. HotpotQA\u00a0(middle) is a multi-doc QA task. SamSUM\u00a0(right) is a few-shot learning task.\n\n\n\n\n3.4 Case Study: Model Behavior Analysis on Self-Construct Dataset\n\nTable 2: Case study on the self-constructed dataset. Correct answers are marked in teal, incorrect answers in red, and ambiguous answers in orange.\n\n\n\n\nQuery: How many papers in ACL 2023 only have one author?\n\n\n\n\n\n\nContext: Full accepted paper list in ACL 2023 main conference. (Context length: 45K)\n\n\n\n\n\n\nGround-truth target: 8 papers\n\n\n\n\n\n\nPhi-3-mini-128K: 11 papers GPT-3.5-turbo-16K: 0 papers Claude-3-Haiku-200K: 1 papers (Acc. Score: 0)\n\n\n\n\n\n\nLC-Boost\u2019s action trajectory: [Task Reasoning] \u2192\u2192\\rightarrow\u2192 [Append]\u2192\u22ef\u2192absent\u22ef\\rightarrow\\cdots\u2192 \u22ef \u2192\u2192\\rightarrow\u2192 [Append]\u2192\u2192\\rightarrow\u2192 [Aggregation]\n\n\n\n\n\n\nLC-Boost: 8 papers (Acc. Score: 1)\n\n\n\n\n\n\nQuery: List all people names that are petrified, separated by comma.\n\n\n\n\n\n\nContext: Full content of Harry Potter and the Chamber of Secrets. (Context length: 122.6K)\n\n\n\n\n\n\nGround-truth target: Colin Creevey, Justin Finch-Fletchley, Penelope Clearwater, Hermione Granger\n\n\n\n\n\n\nPhi-3-mini-128K: Hermione Granger, Ginny Weasley, Mrs Norris (F1-Score: 0.29)\n\n\n\n\n\n\nGPT-3.5-turbo-16K: Colin Creevey, Mrs Norris (F1-Score: 0.33)\n\n\n\n\n\n\nClaude-3-Haiku-200K: Nick, Hermione, Ron (F1-Score: 0.18)\n\n\n\n\n\n\nLC-Boost\u2019s action trajectory: [Task Reasoning] \u2192\u2192\\rightarrow\u2192 [Move]\u2192\u22ef\u2192absent\u22ef\\rightarrow\\cdots\u2192 \u22ef \u2192\u2192\\rightarrow\u2192 [Merge]\u2192\u2192\\rightarrow\u2192 [Aggregation]\n\n\n\n\n\n\nLC-Boost: Colin Creevey, Penelope Clearwater, Hermione Granger, Nick, Mrs Norris (F1-Score: 0.71)\n\n\n\n\n\n\nIn Table\u00a02, we present two case studies from the self-constructed dataset. These cases are particularly challenging as they require reasoning across the entire long context. Despite having sufficient context size, LLMs struggle to generate correct responses. In contrast, LC-Boost dynamically customizes solutions for each case, thereby effectively solving the problems using a shorter context length.\n\n\nFor the first query, LC-Boost performs [Append] or [Move] actions across all short context along with a rewritten query, \"Extract paper information in the following list that have only one author,\" derived via [Task Reasoning]. After processing all short contexts, LC-Boost employs the [Aggregation] action to compile the final answer. This approach simplifies the task compared to directly extracting a numeric answer from the entire long context, mimicking the human process of reading comprehension and thereby producing accurate results.\n\n\nIn the second case, the query necessitates conditional reasoning on each short context. As highlighted in previous research (Liu et\u00a0al., 2023), reasoning directly from the entire context risks losing crucial information, particularly in the middle of the long context. Thus LLMs tend to miss key details such as people\u2019s names. LC-Boost addresses this issue by processing only one short context at a step where it extracts information from arbitrary position of the long text with equal accuracy.\nAdditionally, answers marked in orange include non-human names (e.g., cat, ghost) that are misconstrued as people, illustrating a common challenge where models fail to differentiate in-depth entity properties.\n\n\n\n\n3.5 Context be Short, Energy be Saved!\n\nRecently, we have witnessed the remarkable success of LLMs, which are becoming an indispensable part of our daily lives. We believe that in the near future, LLMs will become as ubiquitous as electricity or gas supply, serving as fundamental infrastructure in human society. At that point, the energy consumption of LLMs will emerge as a significant environmental concern. Therefore, it is imperative for the research community to focus on reducing the energy consumption associated with these models.\nFigure\u00a04 presents an analysis of energy consumption, comparing the brute-force approach with our LC-Boost method. The y\ud835\udc66yitalic_y-axis is measured in Joules. The theoretical energy consumption is estimated for 7B LLMs across varying context lengths. We roughly estimate the energy consumption using the formula (Total Float Operation312\u2062\u00a0TFLOPS)\u00d7400\u2062WTotal Float Operation312\u00a0TFLOPS400\ud835\udc4a\\left(\\frac{\\text{Total Float Operation}}{312\\text{ TFLOPS}}\\right)\\times 400W( divide start_ARG Total Float Operation end_ARG start_ARG 312 TFLOPS end_ARG ) \u00d7 400 italic_W, assuming the use of an A100 GPU with a compute capability of 312 TFLOPS for BFLOAT16 operations and a maximum TDP of 400W444The calculation of total float operations is based on the method outlined in https://www.harmdevries.com/post/context-length/. The practical energy consumption is estimated by recording the GPU time and GPU power during inference with different context lengths. We use a Llama2-7B-128K\u00a0(Peng et\u00a0al., 2023) and a Llama2-7B-chat-4K\u00a0(Touvron et\u00a0al., 2023a) for the brute-force setting and LC-Boost, respectively.\n\n\nFigure 4: Energy consumption analysis.\n\n\nFigure\u00a04 clearly indicates that longer context lengths significantly increase energy consumption with the brute-force method, especially evident in practical measurements. This difference is primarily due to the need to distribute sequence activation tensors across multiple GPUs in practical experiment, with tensor I/O exacerbating inference latency and thereby inflating energy costs. In contrast, our LC-Boost method, working with 4K context lengths, shows only a mild increase in energy consumption across contexts, thereby confirming its energy efficiency while maintaining comparable or superior performance on long-context tasks. We also provide an analysis on token consumption in Appendix\u00a0C.\n\n\n",
      "4 Related Works": "\n\n4 Related Works\n\nDealing with long contexts is a fundamental research problem for LLMs, as many real-world applications involve long-context inputs (Li et\u00a0al., 2023a; Fu et\u00a0al., 2024). The most direct approach to address long-context tasks is to increase the working context size of LLMs (Abdin et\u00a0al., 2024; AI et\u00a0al., 2024; Li et\u00a0al., 2023a; Cai et\u00a0al., 2024). A year ago, significant research efforts focused on extending the working context size of LLMs from 4K to 32K (Jiang et\u00a0al., 2023a; Li et\u00a0al., 2023b; Chen et\u00a0al., 2023a; Du et\u00a0al., 2022). Currently, many popular open-source and close-source LLMs still operate with a context size under 32K (Touvron et\u00a0al., 2023a; OpenAI, 2023), such as GPT-3.5-turbo, which has a 16K context length. Recently, research has shifted towards extending LLMs\u2019 working context to the million-level. Notably, GPT-4 was updated to a 128K context length not long ago, and the newly released GPT-4o also operates with a 128K context. Moreover, several recent open-source LLMs have been introduced with context lengths exceeding 100K, for example, the Yi series model supports up to 200K (AI et\u00a0al., 2024), and the Phi-3 model operates with 128K (Abdin et\u00a0al., 2024).\n\n\nInstead of merely increasing the context length, another approach to address long-context tasks involves extracting a short surrogate context from the full context. This includes techniques like retrieval-augmented generation (RAG) and context refinement methods (Izacard and Grave, 2021a; Gao et\u00a0al., 2024; Wang et\u00a0al., 2023; Qian et\u00a0al., 2024). However, many of these methods utilize task-specific strategies to manage the long context. For instance, RAG methods often deploy retrievers to select relevant context chunks as supporting evidence (Izacard and Grave, 2021b; Xu et\u00a0al., 2023b; Jiang et\u00a0al., 2023b). Recent studies have criticized the chunking process in RAG for undermining the semantic coherence of the long context and have proposed chunking-free methods to refine the long context into a concise surrogate context (Qian et\u00a0al., 2024; Luo et\u00a0al., 2024).\nFurthermore, some studies have also explored sequential processing strategies, such as Ratner et\u00a0al. (2022) and Xu et\u00a0al. (2023a), to sequentially process the context in a manner that preserves its integrity.\n\n\nLastly, reasoning-based methods also show significant potential for addressing long context tasks (Nakano et\u00a0al., 2022; Yang et\u00a0al., 2023; Driess et\u00a0al., 2023). These methods predominantly employ a decision-making process to navigate through the long context sequentially, utilizing reasoning techniques such as in-context learning (Dong et\u00a0al., 2022), chain-of-thought (Wei et\u00a0al., 2022), and self-reflection (Shinn et\u00a0al., 2023). In this paper, LC-Boost incorporates a decision-making process that dynamically customizes the action trajectory for each query, thereby offering considerable flexibility in accessing and leveraging information to produce the final output answer.\n\n",
      "5 Conclusion": "\n\n5 Conclusion\n\nIn this paper, we argue that most long-context tasks are short-context solvable, and we validate this claim through both theoretical and empirical analysis. We propose a method called LC-Boost to solve long-context tasks by decomposing the long context into short contexts and processing them using a decision-making process. We conduct experiments on 12 datasets to compare LC-Boost with long LLMs and other baseline models. Empirical results verify LC-Boost\u2019s effectiveness in solving long-context tasks. Additionally, we discuss the energy consumption of LC-Boost versus long LLMs, demonstrating that LC-Boost can achieve comparable performance with significantly less energy consumption. In Appendix\u00a0D, we also discuss the limitations and broader impact of this paper.\n\n\n",
      "Appendix A More details of the Datasets": "\n\nAppendix A More details of the Datasets\n\nTable 4: Statistical information of the datasets utilized in this paper.\n\n\nDataset\nNarrative\nQasper\nMultiField\nHotpot\nMuSiQue\n2Wiki\n\n\nNum of Samples\n200\n200\n150\n200\n200\n200\n\n\nAve. Length\n18,409\n3,619\n4,559\n9,151\n11,214\n4,887\n\n\nMetric\nF1\nF1\nF1\nF1\nF1\nF1\n\n\nDataset\nGovReport\nMultiNews\nSAMSum\nPCount\nLCC\nSelf\n\n\nNum of Samples\n200\n200\n200\n200\n500\n32\n\n\nAve. Length\n8,734\n2,113\n6,258\n11,141\n1,235\n39,420\n\n\nMetric\nRouge-L\nRouge-L\nRouge-L\nAccuracy\nEdit Sim\nF1&Accuracy\n\n\n\n\nWe evaluated all models on 12 datasets, as shown in Table\u00a04. Most of these datasets are provided by the LongBench benchmark\u00a0[Bai et\u00a0al., 2023]. Following LongBench, we used F1-score, accuracy, and edit similarity as the evaluation metrics. Additionally, we manually annotated a self-constructed dataset comprising long contexts from practical scenarios, such as the full schedule of the Olympic Games and the complete list of accepted papers at ACL. The queries in the self-constructed dataset involve reasoning over the entire long context. For example, \u201cWho has the most accepted papers at ACL 2023?\u201d These queries require the model to accurately understand the long context and perform reasoning, making them highly challenging. The details of the self-constructed dataset are in Table\u00a05.\n\n\nTable 5: Data details of the self-constructed dataset.\n\n\n\n\nSource\n\n\n\n\nLength\n\n\n\n\n# Queries\n\n\n\n\nExample Query\n\n\n\n\n\n\nAccepted paper list of ACL 2023 Main Conference\n\n\n\n\n44,490\n\n\n\n\n7\n\n\n\n\nWho has the most accepted paper in ACL 2023?\n\n\n\n\n\n\nThe Diamond Sutra\n\n\n\n\n19,993\n\n\n\n\n3\n\n\n\n\nHow many chapters of the Sutra?\n\n\n\n\n\n\nSchedule of The 2024 Olympic Games\n\n\n\n\n15,844\n\n\n\n\n9\n\n\n\n\nWhich day has the most gold medal events?\n\n\n\n\n\n\nSubtitle of The Big Bang Theory S3E14\n\n\n\n\n11,136\n\n\n\n\n6\n\n\n\n\nHow long does this episode?\n\n\n\n\n\n\nThe Little Prince\n\n\n\n\n22,471\n\n\n\n\n4\n\n\n\n\nHow many planets does the little prince visit?\n\n\n\n\n\n\nHarry Potter and the Chamber of Secrets\n\n\n\n\n122,591\n\n\n\n\n3\n\n\n\n\nHow many times has the chamber of secret been opened?\n\n\n\n\n\n",
      "Appendix B Implementation Details": "\n\nAppendix B Implementation Details\n\nLC-Boost begins with the [Task Understanding] action after receiving the input query and context, using the prompt shown in Table\u00a06. If the task does not include an input query, the two columns \"Below is the query\" and \"{input_query}\" are omitted. Besides, for the synthetic task, we use the prompt shown in Table\u00a07 to reformulate the query for better adaptation to LC-Boost. Based on the output of the [Task Understanding] action, LC-Boost adopts different strategies to perform the task. Specifically, \u201coption [1]\u201d directs LC-Boost to utilize a retriever to rank all chunks of the long context. In this paper, we employ BGE-Reranker-Large as the retriever\u00a0Chen et\u00a0al. [2023b]. For \u201coption [2]\u201d and \u201coption [3]\u201d, LC-Boost uses the prompts shown in Table\u00a010 and Table\u00a08 to sequentially process each short context, respectively. After processing each short context, if the output is not \"null\", the newly summarized context is added to the \"previous summarization\".\n\n\nOnce all short contexts are processed, LC-Boost aggregates all relevant information to produce the final answer. At this stage, we use the prompt provided by LongBench, replacing the full context with the surrogate context produced by LC-Boost. For \u201coption [4]\u201d, LC-Boost utilizes the prompts provided by LongBench to process each short context and produces the answer as soon as the proper information is found. Table\u00a09 presents an example prompt from LongBench, designed for MultiFieldQA tasks. We modified the prompt by adding the instruction \u201cIf no answer can be found in the text, please output \"null\"\u201d. This allows LC-Boost to skip irrelevant short contexts, performing the [Move] action. Specifically, for the Code Completion task, LC-Boost reversely browses the context code from near to far as the near context are more useful to predict the code completion.\nWe evaluate all baseline models following the settings provided in LongBench\u00a0555https://github.com/THUDM/LongBench. We use a node with 8 A100 80G GPUs to conduct all experiments.\n\n",
      "Appendix C Token Consumption Analysis": "\n\nAppendix C Token Consumption Analysis\n\nIn Section\u00a03.5, our analysis confirms that LC-Boost significantly reduces energy consumption compared to long LLMs. However, most closed-source LLMs, such as the underlying model of LC-Boost, GPT-3.5-turbo, charge based on token consumption, e.g., US$0.50 per 1M tokens for input and US$1.50 per 1M tokens for output666https://openai.com/api/pricing/. Consequently, it is crucial to examine whether the decision-making process of LC-Boost increases token consumption compared to the brute-force method.\n\n\nTo address this issue, we recorded the end-to-end token consumption for three datasets: NarrativeQA, GovReport, and LCC. After token counting, we conclude that LC-Boost\u2019s token consumption was 34.1% of the brute-force method\u2019s consumption in NarrativeQA, 112% in GovReport, and 29.5% in LCC. These results indicate that LC-Boost\u2019s token consumption varies significantly across different tasks. For tasks requiring precise context location, such as QA and code completion, LC-Boost can respond as soon as the relevant context is identified, thereby avoiding the need to process the full context. However, for tasks that necessitate information aggregation, such as summarization, LC-Boost may require more tokens for prompts in each iteration.\nIn practice, for token-consumption-sensitive LLMs, there might be a trade-off between performance and cost-efficiency, which also varies considerably across different tasks.\n\n\nTable 6: Prompt Template for the [Task Understanding] action.\n\n\n\n\n\n\nYou need to process a task with a long context that greatly exceeds your context limit.\n\nThe only feasible way to handle this is by processing the long context chunk by chunk.\n\n\n\n\n\n\n\n\nBelow is the original task prompt:\n\n{task_prompt}\n\n\n\n\n\n\n\n\nBelow is the query:\n\n{input_query}\n\n\n\n\n\n\n\n\nYou have the following options to process the long context. Choose one of them:\n\n\n\n\n\n\n\n\n[1]. Retrieve the chunk most relevant to the input query to support answer generation.\n\n\n\n\n\n\n\n\n[2]. Summarize each chunk and then aggregate the summaries after processing all chunks.\n\n\n\n\n\n\n\n\n[3]. Extract key sentences from each chunk and then aggregate them after processing all chunks.\n\n\n\n\n\n\n\n\n[4]. Sequentially scan chunks and produce the answer as soon as the query can be answered.\n\n\n\n\n\n\n\n\nBelow are some examples for reference:\n\n\n\n\n\n\n\n\nThe examples begin as follows:\n\n{examples}\n\nThe examples conclude here.\n\n\n\n\n\n\n\n\nPlease learn the examples and select one of the options by only outputting the corresponding index number.\n\n\n\n\n\n\nTable 7: Query Rewritten Prompt Template for the [Task Understanding] action.\n\n\n\n\n\n\nYou need to process a task with a long context that greatly exceeds your context limit.\n\nThe only feasible way to handle this is by processing the long context chunk by chunk.\n\n\n\n\n\n\n\n\nBelow is the original task prompt:\n\n{task_prompt}\n\n\n\n\n\n\n\n\nBelow is the query:\n\n{input_query}\n\n\n\n\n\n\n\n\nYou will process the long context with the following strategy:\n\n{strategy}\n\n\n\n\n\n\n\n\nDo you think the the query is proper for processing context chunk? If not, rewrite the query.\n\n\n\n\n\n\n\n\nBelow are some examples for reference:\n\n\n\n\n\n\n\n\nThe examples begin as follows:\n\n{examples}\n\nThe examples conclude here.\n\n\n\n\n\n\n\n\nPlease study the examples carefully. If the query needs to be rewritten, directly output the revised query.\n\nIf no revision is necessary, output \u201cnull\u201d.\n\n\n\n\n\n\nTable 8: Prompt Template for the [Append] action.\n\n\n\n\n\n\nYou are given an article and a question. Read the article carefully and follow my instructions to process it.\n\n\n\n\n\n\n\n\nArticle:\n\n\n\n\n\n\n\n\nThe article begins as follows:\n\n{article}\n\nThe article concludes here.\n\n\n\n\n\n\n\n\nQuestion:\n\n\n\n\n\n\n\n\n{question}\n\n\n\n\n\n\n\n\nInstructions:\n\n\n\n\n\n\n\n\nEach sentence in the article is marked with a sentence identifier [si], for example [s1].\n\n\n\n\n\n\n\n\nSelect up to ten key sentences from the article that are most likely to answer the question.\n\n\n\n\n\n\n\n\nOnly output the selected sentence identifiers, separated by commas.\n\n\n\n\n\n\n\n\nExample: [s39],[s54]\n\n\n\n\n\n\n\n\nIf no sentences are relevant, please output \"null\".\n\n\n\n\n\n\nTable 9: Prompt Template for the MultiFieldQA Task from the LongBench Benchmark. Additions made by us are highlighted in blue.\n\n\n\n\n\n\nRead the following text and answer briefly.\n\n\n\n\n\n\n\n\n{context}\n\n\n\n\n\n\n\n\nNow, answer the following question based on the above text, only give me the answer and do not output any\n\n\n\n\n\n\n\n\nother words.  If no answer can be found in the text, please output \"null\".\n\n\n\n\n\n\n\n\nQuestion:{question}\n\n\n\n\n\n\n\n\nAnswer:\n\n\n\n\n\n\nTable 10: Prompt Template for the [Merge] action.\n\n\n\n\n\n\nYou are provided with a portion of an article, a question, and summarization of the article\u2019s previous portions.\n\n\n\n\n\n\n\n\nRead the article portion and follow my instructions to process it.\n\n\n\n\n\n\n\n\nArticle:\n\n\n\n\n\n\n\n\nThe article begins as follows:\n\n{article}\n\nThe article concludes here.\n\n\n\n\n\n\n\n\nPrevious summarization:\n\n\n\n\n\n\n\n\nThe previous summarization is as follows:\n\n{previous_sum}\n\nThe previous summarization concludes here.\n\n\n\n\n\n\n\n\nQuestion:\n\n\n\n\n\n\n\n\n{question}\n\n\n\n\n\n\n\n\nInstruction:\n\n\n\n\n\n\n\n\nSummarize the partial article to supplement the previous summarization, which can better support the task.\n\n\n\n\n\n\n\n\nIf no content needs to be supplemented, please output \"null\".\n\n\n\n\n\n",
      "Appendix D Limitations and Broad Impact": "\n\nAppendix D Limitations and Broad Impact\n\nIn this paper, we propose LC-Boost, a method dedicated to solving long-context tasks using short contexts. However, there are several limitations we would like to address in the future work:\n(1) Although we conduct comprehensive experiments on many tasks and provide theoretical analysis to support our major claim that most long-context tasks are short-context solvable, there may be more complicated scenarios that require understanding the full context in a brute-force setting. LC-Boost might not be able to process such tasks effectively.\n(2) As mentioned in Section\u00a02.3, LC-Boost selects actions from a discrete action space. While we argue that the pre-defined action space is versatile enough to handle most scenarios, a more elegant solution would be to predict actions in a continuous space. We conducted preliminary experiments to explore allowing LC-Boost to prompt itself to predict actions without a predefined action space, such as writing prompts or code autonomously. These experiments resulted in highly unstable performance, particularly for models like GPT-3.5, as such requirements are still challenging. We believe that with a much stronger foundation model, LC-Boost could be expected to predict actions in a continuous space.\n(3) We choose GPT-3.5 as the foundation model for LC-Boost, instead of open-source LLMs. The reason is that GPT-3.5 is a strong, yet efficient model that can generally understand most instructions. However, we found that most open-source LLMs lack these properties in a zero-shot setting. Fine-tuning these open-source LLMs might be helpful, but constructing such instruction data is infeasible and expensive.\n\n\nAs discussed in Section\u00a03.5, LLMs are likely to become a fundamental infrastructure in the near future. At that scale, their energy consumption will pose significant environmental challenges. As shown in Figure\u00a04, LC-Boost avoids processing long contexts directly by decomposing them into shorter contexts. This approach significantly reduces energy consumption as the context length increases, leading to substantial positive environmental impacts. We believe that in the future, more research will focus on green AI initiatives. This paper could serve as an initial spark to inspire further research in this direction, potentially resulting in broader social impact.\n\n"
    },
    "bibliography": {
      "Bai et\u00a0al. (2023)": "\nBai et\u00a0al. (2023)\n\nYushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, et\u00a0al.\n\n\nLongbench: A bilingual, multitask benchmark for long context understanding.\n\n\narXiv preprint arXiv:2308.14508, 2023.\n\n\n",
      "Zhang et\u00a0al. (2024a)": "\nZhang et\u00a0al. (2024a)\n\nXinrong Zhang, Yingfa Chen, Shengding Hu, Zihang Xu, Junhao Chen, Moo\u00a0Khai Hao, Xu\u00a0Han, Zhen\u00a0Leng Thai, Shuo Wang, Zhiyuan Liu, and Maosong Sun.\n\n\n\u221e\\infty\u221ebench: Extending long context evaluation beyond 100k tokens, 2024a.\n\n\n",
      "Touvron et\u00a0al. (2023a)": "\nTouvron et\u00a0al. (2023a)\n\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et\u00a0al.\n\n\nLlama 2: Open foundation and fine-tuned chat models.\n\n\narXiv preprint arXiv:2307.09288, 2023a.\n\n\n",
      "Touvron et\u00a0al. (2023b)": "\nTouvron et\u00a0al. (2023b)\n\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et\u00a0al.\n\n\nLlama 2: Open foundation and fine-tuned chat models.\n\n\narXiv preprint arXiv:2307.09288, 2023b.\n\n\n",
      "Kaplan et\u00a0al. (2020)": "\nKaplan et\u00a0al. (2020)\n\nJared Kaplan, Sam McCandlish, Tom Henighan, Tom\u00a0B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei.\n\n\nScaling laws for neural language models, 2020.\n\n\n",
      "Liu et\u00a0al. (2023)": "\nLiu et\u00a0al. (2023)\n\nNelson\u00a0F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang.\n\n\nLost in the middle: How language models use long contexts, 2023.\n\n\n",
      "Li et\u00a0al. (2023a)": "\nLi et\u00a0al. (2023a)\n\nDacheng Li, Rulin Shao, Anze Xie, Ying Sheng, Lianmin Zheng, Joseph Gonzalez, Ion Stoica, Xuezhe Ma, and Hao Zhang.\n\n\nHow long can context length of open-source llms truly promise?\n\n\nIn NeurIPS 2023 Workshop on Instruction Tuning and Instruction Following, 2023a.\n\n\n",
      "Adolphs (1999)": "\nAdolphs (1999)\n\nRalph Adolphs.\n\n\nSocial cognition and the human brain.\n\n\nTrends in cognitive sciences, 3(12):469\u2013479, 1999.\n\n\n",
      "Bryant and O\u2019Hallaron (2011)": "\nBryant and O\u2019Hallaron (2011)\n\nRandal\u00a0E Bryant and David\u00a0Richard O\u2019Hallaron.\n\n\nComputer systems: a programmer\u2019s perspective.\n\n\nPrentice Hall, 2011.\n\n\n",
      "Xu et\u00a0al. (2023a)": "\nXu et\u00a0al. (2023a)\n\nPeng Xu, Wei Ping, Xianchao Wu, Lawrence McAfee, Chen Zhu, Zihan Liu, Sandeep Subramanian, Evelina Bakhturina, Mohammad Shoeybi, and Bryan Catanzaro.\n\n\nRetrieval meets Long Context Large Language Models.\n\n\narXiv, 2023a.\n\n\ndoi: 10.48550/arxiv.2310.03025.\n\n\nExperimental.\n\n\n",
      "Ji et\u00a0al. (2023)": "\nJi et\u00a0al. (2023)\n\nZiwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye\u00a0Jin Bang, Andrea Madotto, and Pascale Fung.\n\n\nSurvey of hallucination in natural language generation.\n\n\nACM Computing Surveys, 55(12):1\u201338, 2023.\n\n\n",
      "Lewis et\u00a0al. (2020)": "\nLewis et\u00a0al. (2020)\n\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich K\u00fcttler, Mike Lewis, Wen-tau Yih, Tim Rockt\u00e4schel, Sebastian Riedel, and Douwe Kiela.\n\n\nRetrieval-Augmented Generation for knowledge-intensive NLP tasks.\n\n\nIn Advances in Neural Information Processing Systems, volume\u00a033, pages 9459\u20139474, 2020.\n\n\nURL https://proceedings.neurips.cc/paper_files/paper/2020/file/6b493230205f780e1bc26945df7481e5-Paper.pdf.\n\n\n",
      "Shuster et\u00a0al. (2021)": "\nShuster et\u00a0al. (2021)\n\nKurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela, and Jason Weston.\n\n\nRetrieval augmentation reduces hallucination in conversation.\n\n\nIn Findings of the Association for Computational Linguistics: EMNLP 2021, pages 3784\u20133803, Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics.\n\n\ndoi: 10.18653/v1/2021.findings-emnlp.320.\n\n\nURL https://aclanthology.org/2021.findings-emnlp.320.\n\n\n",
      "Cover (1999)": "\nCover (1999)\n\nThomas\u00a0M Cover.\n\n\nElements of information theory.\n\n\nJohn Wiley & Sons, 1999.\n\n\n",
      "Tishby and Zaslavsky (2015)": "\nTishby and Zaslavsky (2015)\n\nNaftali Tishby and Noga Zaslavsky.\n\n\nDeep learning and the information bottleneck principle, 2015.\n\n\n",
      "Mohri et\u00a0al. (2018)": "\nMohri et\u00a0al. (2018)\n\nMehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar.\n\n\nFoundations of machine learning.\n\n\nMIT press, 2018.\n\n\n",
      "Ko\u010disk\u00fd et\u00a0al. (2017)": "\nKo\u010disk\u00fd et\u00a0al. (2017)\n\nTom\u00e1\u0161 Ko\u010disk\u00fd, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl\u00a0Moritz Hermann, G\u00e1bor Melis, and Edward Grefenstette.\n\n\nThe narrativeqa reading comprehension challenge, 2017.\n\n\n",
      "Dasigi et\u00a0al. (2021)": "\nDasigi et\u00a0al. (2021)\n\nPradeep Dasigi, Kyle Lo, Iz\u00a0Beltagy, Arman Cohan, Noah\u00a0A Smith, and Matt Gardner.\n\n\nA dataset of information-seeking questions and answers anchored in research papers.\n\n\nIn Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 4599\u20134610, 2021.\n\n\n",
      "Yang et\u00a0al. (2018)": "\nYang et\u00a0al. (2018)\n\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William\u00a0W. Cohen, Ruslan Salakhutdinov, and Christopher\u00a0D. Manning.\n\n\nHotpotqa: A dataset for diverse, explainable multi-hop question answering, 2018.\n\n\n",
      "Ho et\u00a0al. (2020)": "\nHo et\u00a0al. (2020)\n\nXanh Ho, Anh-Khoa Duong\u00a0Nguyen, Saku Sugawara, and Akiko Aizawa.\n\n\nConstructing a multi-hop QA dataset for comprehensive evaluation of reasoning steps.\n\n\nIn Donia Scott, Nuria Bel, and Chengqing Zong, editors, Proceedings of the 28th International Conference on Computational Linguistics, pages 6609\u20136625, Barcelona, Spain (Online), December 2020. International Committee on Computational Linguistics.\n\n\ndoi: 10.18653/v1/2020.coling-main.580.\n\n\nURL https://aclanthology.org/2020.coling-main.580.\n\n\n",
      "Trivedi et\u00a0al. (2022)": "\nTrivedi et\u00a0al. (2022)\n\nHarsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal.\n\n\nMusique: Multihop questions via single-hop question composition.\n\n\nTransactions of the Association for Computational Linguistics, 10:539\u2013554, 2022.\n\n\n",
      "Huang et\u00a0al. (2021)": "\nHuang et\u00a0al. (2021)\n\nLuyang Huang, Shuyang Cao, Nikolaus Parulian, Heng Ji, and Lu\u00a0Wang.\n\n\nEfficient attentions for long document summarization.\n\n\nIn Kristina Toutanova, Anna Rumshisky, Luke Zettlemoyer, Dilek Hakkani-Tur, Iz\u00a0Beltagy, Steven Bethard, Ryan Cotterell, Tanmoy Chakraborty, and Yichao Zhou, editors, Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1419\u20131436, Online, June 2021. Association for Computational Linguistics.\n\n\ndoi: 10.18653/v1/2021.naacl-main.112.\n\n\nURL https://aclanthology.org/2021.naacl-main.112.\n\n\n",
      "Fabbri et\u00a0al. (2019)": "\nFabbri et\u00a0al. (2019)\n\nAlexander\u00a0R. Fabbri, Irene Li, Tianwei She, Suyi Li, and Dragomir\u00a0R. Radev.\n\n\nMulti-news: a large-scale multi-document summarization dataset and abstractive hierarchical model, 2019.\n\n\n",
      "Gliwa et\u00a0al. (2019)": "\nGliwa et\u00a0al. (2019)\n\nBogdan Gliwa, Iwona Mochol, Maciej Biesek, and Aleksander Wawer.\n\n\nSAMSum corpus: A human-annotated dialogue dataset for abstractive summarization.\n\n\nIn Lu\u00a0Wang, Jackie Chi\u00a0Kit Cheung, Giuseppe Carenini, and Fei Liu, editors, Proceedings of the 2nd Workshop on New Frontiers in Summarization, pages 70\u201379, Hong Kong, China, November 2019. Association for Computational Linguistics.\n\n\ndoi: 10.18653/v1/D19-5409.\n\n\nURL https://aclanthology.org/D19-5409.\n\n\n",
      "Guo et\u00a0al. (2023)": "\nGuo et\u00a0al. (2023)\n\nDaya Guo, Canwen Xu, Nan Duan, Jian Yin, and Julian McAuley.\n\n\nLongcoder: A long-range pre-trained language model for code completion, 2023.\n\n\n",
      "Chiang et\u00a0al. (2023)": "\nChiang et\u00a0al. (2023)\n\nWei-Lin Chiang, Zhuohan Li, Zi\u00a0Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph\u00a0E. Gonzalez, Ion Stoica, and Eric\u00a0P. Xing.\n\n\nVicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023.\n\n\nURL https://lmsys.org/blog/2023-03-30-vicuna/.\n\n\n",
      "Li et\u00a0al. (2023b)": "\nLi et\u00a0al. (2023b)\n\nDacheng Li, Rulin Shao, Anze Xie, Ying Sheng, Lianmin Zheng, Joseph\u00a0E. Gonzalez, Ion Stoica, Xuezhe Ma, and Hao Zhang.\n\n\nHow long can open-source llms truly promise on context length?, June 2023b.\n\n\nURL https://lmsys.org/blog/2023-06-29-longchat.\n\n\n",
      "Jiang et\u00a0al. (2023a)": "\nJiang et\u00a0al. (2023a)\n\nAlbert\u00a0Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra\u00a0Singh Chaplot, Diego de\u00a0las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et\u00a0al.\n\n\nMistral 7b.\n\n\narXiv preprint arXiv:2310.06825, 2023a.\n\n\n",
      "Zhang et\u00a0al. (2024b)": "\nZhang et\u00a0al. (2024b)\n\nPeitian Zhang, Ninglu Shao, Zheng Liu, Shitao Xiao, Hongjin Qian, Qiwei Ye, and Zhicheng Dou.\n\n\nExtending llama-3\u2019s context ten-fold overnight, 2024b.\n\n\n",
      "Abdin et\u00a0al. (2024)": "\nAbdin et\u00a0al. (2024)\n\nMarah Abdin, Sam\u00a0Ade Jacobs, Ammar\u00a0Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Harkirat Behl, Alon Benhaim, Misha Bilenko, Johan Bjorck, S\u00e9bastien Bubeck, Martin Cai, Caio C\u00e9sar\u00a0Teodoro Mendes, Weizhu Chen, Vishrav Chaudhary, Parul Chopra, Allie\u00a0Del Giorno, Gustavo de\u00a0Rosa, Matthew Dixon, Ronen Eldan, Dan Iter, Amit Garg, Abhishek Goswami, Suriya Gunasekar, Emman Haider, Junheng Hao, Russell\u00a0J. Hewett, Jamie Huynh, Mojan Javaheripi, Xin Jin, Piero Kauffmann, Nikos Karampatziakis, Dongwoo Kim, Mahoud Khademi, Lev Kurilenko, James\u00a0R. Lee, Yin\u00a0Tat Lee, Yuanzhi Li, Chen Liang, Weishung Liu, Eric Lin, Zeqi Lin, Piyush Madan, Arindam Mitra, Hardik Modi, Anh Nguyen, Brandon Norick, Barun Patra, Daniel Perez-Becker, Thomas Portet, Reid Pryzant, Heyang Qin, Marko Radmilac, Corby Rosset, Sambudha Roy, Olatunji Ruwase, Olli Saarikivi, Amin Saied, Adil Salim, Michael Santacroce, Shital Shah, Ning Shang, Hiteshi Sharma, Xia Song, Masahiro Tanaka, Xin Wang, Rachel\nWard, Guanhua Wang, Philipp Witte, Michael Wyatt, Can Xu, Jiahang Xu, Sonali Yadav, Fan Yang, Ziyi Yang, Donghan Yu, Chengruidong Zhang, Cyril Zhang, Jianwen Zhang, Li\u00a0Lyna Zhang, Yi\u00a0Zhang, Yue Zhang, Yunan Zhang, and Xiren Zhou.\n\n\nPhi-3 technical report: A highly capable language model locally on your phone, 2024.\n\n\n",
      "AI et\u00a0al. (2024)": "\nAI et\u00a0al. (2024)\n\n01. AI, :, Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge\u00a0Zhang, Guanwei Zhang, Heng Li, Jiangcheng Zhu, Jianqun Chen, Jing Chang, Kaidong Yu, Peng Liu, Qiang Liu, Shawn Yue, Senbin Yang, Shiming Yang, Tao Yu, Wen Xie, Wenhao Huang, Xiaohui Hu, Xiaoyi Ren, Xinyao Niu, Pengcheng Nie, Yuchi Xu, Yudong Liu, Yue Wang, Yuxuan Cai, Zhenyu Gu, Zhiyuan Liu, and Zonghong Dai.\n\n\nYi: Open foundation models by 01.ai, 2024.\n\n\n",
      "DeepSeek-AI (2024)": "\nDeepSeek-AI (2024)\n\nDeepSeek-AI.\n\n\nDeepseek-v2: A strong, economical, and efficient mixture-of-experts language model, 2024.\n\n\n",
      "Peng et\u00a0al. (2023)": "\nPeng et\u00a0al. (2023)\n\nBowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole.\n\n\nYarn: Efficient context window extension of large language models.\n\n\nIn The Twelfth International Conference on Learning Representations, 2023.\n\n\n",
      "Fu et\u00a0al. (2024)": "\nFu et\u00a0al. (2024)\n\nYao Fu, Rameswar Panda, Xinyao Niu, Xiang Yue, Hannaneh Hajishirzi, Yoon Kim, and Hao Peng.\n\n\nData engineering for scaling language models to 128k context, 2024.\n\n\n",
      "Cai et\u00a0al. (2024)": "\nCai et\u00a0al. (2024)\n\nZheng Cai, Maosong Cao, Haojiong Chen, Kai Chen, Keyu Chen, Xin Chen, Xun Chen, Zehui Chen, Zhi Chen, Pei Chu, et\u00a0al.\n\n\nInternlm2 technical report.\n\n\narXiv preprint arXiv:2403.17297, 2024.\n\n\n",
      "Chen et\u00a0al. (2023a)": "\nChen et\u00a0al. (2023a)\n\nYukang Chen, Shengju Qian, Haotian Tang, Xin Lai, Zhijian Liu, Song Han, and Jiaya Jia.\n\n\nLonglora: Efficient fine-tuning of long-context large language models.\n\n\nIn The Twelfth International Conference on Learning Representations, 2023a.\n\n\n",
      "Du et\u00a0al. (2022)": "\nDu et\u00a0al. (2022)\n\nZhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie Tang.\n\n\nGlm: General language model pretraining with autoregressive blank infilling.\n\n\nIn Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 320\u2013335, 2022.\n\n\n",
      "OpenAI (2023)": "\nOpenAI (2023)\n\nOpenAI.\n\n\nGpt-4 technical report.\n\n\nhttps://cdn.openai.com/papers/gpt-4.pdf, 2023.\n\n\n",
      "Izacard and Grave (2021a)": "\nIzacard and Grave (2021a)\n\nGautier Izacard and Edouard Grave.\n\n\nLeveraging passage retrieval with generative models for open domain question answering, 2021a.\n\n\n",
      "Gao et\u00a0al. (2024)": "\nGao et\u00a0al. (2024)\n\nYunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi\u00a0Dai, Jiawei Sun, Qianyu Guo, Meng Wang, and Haofen Wang.\n\n\nRetrieval-augmented generation for large language models: A survey, 2024.\n\n\n",
      "Wang et\u00a0al. (2023)": "\nWang et\u00a0al. (2023)\n\nZhiruo Wang, Jun Araki, Zhengbao Jiang, Md\u00a0Rizwan Parvez, and Graham Neubig.\n\n\nLearning to filter context for retrieval-augmented generation, 2023.\n\n\n",
      "Qian et\u00a0al. (2024)": "\nQian et\u00a0al. (2024)\n\nHongjin Qian, Zheng Liu, Kelong Mao, Yujia Zhou, and Zhicheng Dou.\n\n\nGrounding language model with chunking-free in-context retrieval, 2024.\n\n\n",
      "Izacard and Grave (2021b)": "\nIzacard and Grave (2021b)\n\nGautier Izacard and Edouard Grave.\n\n\nDistilling knowledge from reader to retriever for question answering.\n\n\nIn International Conference on Learning Representations, 2021b.\n\n\nURL https://openreview.net/forum?id=NTEz-6wysdb.\n\n\n",
      "Xu et\u00a0al. (2023b)": "\nXu et\u00a0al. (2023b)\n\nPeng Xu, Wei Ping, Xianchao Wu, Lawrence McAfee, Chen Zhu, Zihan Liu, Sandeep Subramanian, Evelina Bakhturina, Mohammad Shoeybi, and Bryan Catanzaro.\n\n\nRetrieval meets long context large language models.\n\n\nIn The Twelfth International Conference on Learning Representations, 2023b.\n\n\n",
      "Jiang et\u00a0al. (2023b)": "\nJiang et\u00a0al. (2023b)\n\nZhengbao Jiang, Frank\u00a0F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, and Graham Neubig.\n\n\nActive retrieval augmented generation.\n\n\narXiv preprint arXiv:2305.06983, 2023b.\n\n\nURL https://arxiv.org/pdf/2305.06983.\n\n\n",
      "Luo et\u00a0al. (2024)": "\nLuo et\u00a0al. (2024)\n\nKun Luo, Zheng Liu, Shitao Xiao, and Kang Liu.\n\n\nBge landmark embedding: A chunking-free embedding method for retrieval augmented long-context large language models, 2024.\n\n\n",
      "Ratner et\u00a0al. (2022)": "\nRatner et\u00a0al. (2022)\n\nNir Ratner, Yoav Levine, Yonatan Belinkov, Ori Ram, Inbal Magar, Omri Abend, Ehud Karpas, Amnon Shashua, Kevin Leyton-Brown, and Yoav Shoham.\n\n\nParallel Context Windows Improve In-Context Learning of Large Language Models.\n\n\narXiv, 2022.\n\n\ndoi: 10.48550/arxiv.2212.10947.\n\n\nWindow.\n\n\n",
      "Nakano et\u00a0al. (2022)": "\nNakano et\u00a0al. (2022)\n\nReiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, Xu\u00a0Jiang, Karl Cobbe, Tyna Eloundou, Gretchen Krueger, Kevin Button, Matthew Knight, Benjamin Chess, and John Schulman.\n\n\nWebgpt: Browser-assisted question-answering with human feedback, 2022.\n\n\n",
      "Yang et\u00a0al. (2023)": "\nYang et\u00a0al. (2023)\n\nHui Yang, Sifu Yue, and Yunzhong He.\n\n\nAuto-gpt for online decision making: Benchmarks and additional opinions.\n\n\narXiv preprint arXiv:2306.02224, 2023.\n\n\n",
      "Driess et\u00a0al. (2023)": "\nDriess et\u00a0al. (2023)\n\nDanny Driess, Fei Xia, Mehdi\u00a0SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, et\u00a0al.\n\n\nPalm-e: An embodied multimodal language model.\n\n\narXiv preprint arXiv:2303.03378, 2023.\n\n\n",
      "Dong et\u00a0al. (2022)": "\nDong et\u00a0al. (2022)\n\nQingxiu Dong, Lei Li, Damai Dai, Ce\u00a0Zheng, Zhiyong Wu, Baobao Chang, Xu\u00a0Sun, Jingjing Xu, and Zhifang Sui.\n\n\nA survey on in-context learning.\n\n\narXiv preprint arXiv:2301.00234, 2022.\n\n\n",
      "Wei et\u00a0al. (2022)": "\nWei et\u00a0al. (2022)\n\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed\u00a0H. Chi, Quoc\u00a0V Le, and Denny Zhou.\n\n\nChain of thought prompting elicits reasoning in large language models.\n\n\nIn Alice\u00a0H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in Neural Information Processing Systems, 2022.\n\n\nURL https://openreview.net/forum?id=_VjQlMeSB_J.\n\n\n",
      "Shinn et\u00a0al. (2023)": "\nShinn et\u00a0al. (2023)\n\nNoah Shinn, Beck Labash, and Ashwin Gopinath.\n\n\nReflexion: an autonomous agent with dynamic memory and self-reflection.\n\n\narXiv preprint arXiv:2303.11366, 2023.\n\n\n",
      "Chen et\u00a0al. (2023b)": "\nChen et\u00a0al. (2023b)\n\nJianlv Chen, Shitao Xiao, Peitian Zhang, Kun Luo, Defu Lian, and Zheng Liu.\n\n\nBge m3-embedding: Multi-lingual, multi-functionality, multi-granularity text embeddings through self-knowledge distillation, 2023b.\n\n\n"
    },
    "domain": "cs.CL",
    "citation_count": 0
  },
  "c9c6f8c7-5464-4248-a4cc-db619d2e89cd": {
    "pk": "c9c6f8c7-5464-4248-a4cc-db619d2e89cd",
    "authors": [
      "Omer Goldman",
      "Alon Jacovi",
      "Aviv Slobodkin",
      "Aviya Maimon",
      "Ido Dagan",
      "Reut Tsarfaty"
    ],
    "title": "Is It Really Long Context if All You Need Is Retrieval? Towards Genuinely Difficult Long Context NLP",
    "abstract": "Improvements in language models' capabilities have pushed their applications towards longer contexts, making long-context evaluation and development an active research area. However, many disparate use-cases are grouped together under the umbrella term of \"long-context\", defined simply by the total length of the model's input, including - for example - Needle-in-a-Haystack tasks, book summarization, and information aggregation. Given their varied difficulty, in this position paper we argue that conflating different tasks by their context length is unproductive. As a community, we require a more precise vocabulary to understand what makes long-context tasks similar or different. We propose to unpack the taxonomy of long-context based on the properties that make them more difficult with longer contexts. We propose two orthogonal axes of difficulty: (I) Diffusion: How hard is it to find the necessary information in the context? (II) Scope: How much necessary information is there to find? We survey the literature on long-context, provide justification for this taxonomy as an informative descriptor, and situate the literature with respect to it. We conclude that the most difficult and interesting settings, whose necessary information is very long and highly diffused within the input, is severely under-explored. By using a descriptive vocabulary and discussing the relevant properties of difficulty in long-context, we can implement more informed research in this area. We call for a careful design of tasks and benchmarks with distinctly long context, taking into account the characteristics that make it qualitatively different from shorter context.",
    "url": "http://arxiv.org/abs/2407.00402v3",
    "timestamp": 1719659387,
    "sections": {
      "1 Introduction": "\n\n1 Introduction\n\nFigure 1: A taxonomy of long context tasks based on the distribution of the needed information in the text. Tasks with larger scope and higher dispersion are more difficult (indicated by shade) and more indicative of the long context capabilities of large language models.\n\n\nThe ability to deal with ever-longer contexts has been one of the most notable trends among the emerging capabilities of large language models (LLMs). Starting with a few hundred tokens as the maximal input length of the first attention-based LLMs Devlin et\u00a0al. (2019); Raffel et\u00a0al. (2020), contemporary models are \u2013 technically \u2013 able to process up to 128k and even 1M tokens (Gemini Team Google, 2024; OpenAI, 2024).\n\n\nThe demand to evaluate LLMs in this setting has led to a line of research on designing long-context tasks and benchmarks, in order to systematically understand models\u2019 capabilities and drive their development.\nHowever, the field has generally a sole recurring descriptor to define such measurements by \u2013 simply, the length of the context.\nFor example, long-context benchmarks group tasks mostly by length in words (e.g., Shaham et\u00a0al., 2022; Bai et\u00a0al., 2023; Zhang et\u00a0al., 2024b). This leads to qualitatively different measurements being conflated together, with conclusions about long-context capabilities being extended from one class of tasks to others.\nThe community is, of course, aware that, for example, tasks which require a small part of the input are different from tasks that require a large part of it. But we ask the more general question: What are the properties that differentiate tasks when conditioned on their context length? What can we accomplish with such a distinction?\n\n\nIn this position paper, we claim that the current landscape of works on long-context evaluation will greatly benefit from a more fine-grained characterization of long-context task design. We argue that judging LLMs by their ability to process long sequences, while disregarding the task they process them for, overlooks the characteristics that make longer inputs more difficult, and interesting to research, to begin with\u00a0(\u00a72).\n\n\nFor example, Needle in a Haystack tasks\u00a0(NIAH; Ivgi et\u00a0al., 2023; Mohtashami and Jaggi, 2023) involve queries whose main challenge is finding the relevant information in a long context, without requiring much further processing. Synthetic NIAH datasets are, of course, easier than their natural equivalents Ivgi et\u00a0al. (2023), but the \u201cnatural vs. artificial\u201d classification is not informative in our setting, since it applies equally for tasks regardless of context length. What, then, is an informative property? What makes long-context tasks different from each other? For example, multiple-needle variants of NIAH Hsieh et\u00a0al. (2024), or those that position the \u201cneedles\u201d closer or farther apart Levy et\u00a0al. (2024). Evidently, \u201cthe number of tokens in the input\u201d is not a sufficient descriptor.\n\n\nTo resolve this roadblock, we present a taxonomy of long-context tasks for the different factors that make them harder when controlling for context length\u00a0(\u00a73). This taxonomy is derived by surveying the long-context literature and surfacing the most salient points of distinction between various tasks.\nWe focus on (I) how difficult it is to find and extract the required information from the input (its dispersion in the input), and (II) the absolute quantity of required information to solve the task (its scope). See Figure\u00a01 for a summary.\n\n\nTo understand this categorization and its utility, we review the literature on long-context evaluation and position the works with respect to those factors. We find that the most challenging setting, in which a large quantity of required information is present in a dispersed manner that is difficult to extract, is significantly under-explored\u00a0(\u00a74).\n\n\nFinally, acknowledging the inherent and legitimate reasons behind the focus on context length as the sole descriptor of difficulty, we discuss possible paths forward for designing more reliable measurements of long-context capabilities when utilizing a more nuanced vocabulary\u00a0(\u00a75).\n\n",
      "2 Task Design in Long Context": "\n\n2 Task Design in Long Context\n\nEvaluating the performance of NLP models over very long contexts is a fast-changing area of research\u00a0Bishop et\u00a0al. (2024); Wu et\u00a0al. (2024). Measurements are regularly updated to account for new capabilities which improve with extrapolation architectures\u00a0Vaswani et\u00a0al. (2017); Su et\u00a0al. (2024) and training data\u00a0He et\u00a0al. (2023); Chen et\u00a0al. (2023). Evaluators were tasked with designing measurements of long-context capabilities cheaply, efficiently, and quickly, while matching realistic use cases as much as possible. The most common way of differentiating long-context tasks, besides the context\u2019s length, is whether they are naturally-constructed or synthetically-constructed\u00a0Tay et\u00a0al. (2020); Bai et\u00a0al. (2023); Hsieh et\u00a0al. (2024).\n\n\nNatural construction.\n\nA simple yet effective way of \u201cmoving the goalpost\u201d for context length is by modeling long-context tasks based on short-context tasks.\nThis was done, for example, with\nQA (Ko\u010disk\u00fd et\u00a0al., 2018, cf. Dunn et\u00a0al., 2017), summarization (Huang et\u00a0al., 2021a, cf. Narayan et\u00a0al., 2018), and NLI (Koreeda and Manning, 2021a, cf. Williams et\u00a0al., 2018). Specialized domains like legal Bruno and Roth (2022); Nguyen et\u00a0al. (2024) and literature (Wang et\u00a0al., 2022; Kryscinski et\u00a0al., 2022) often involve longer texts, turning typically short-context tasks such as QA and NLI into long-context scenarios.\nAnother more native methodology is to create new tasks which inherently require a long context,\nsuch as multi-document summarization Fabbri et\u00a0al. (2019); Angelidis et\u00a0al. (2021), survey generation Gao et\u00a0al. (2024), and structured data aggregation Caciularu et\u00a0al. (2024).\nBoth methodologies share the constraint that, due to their natural construction (i.e., using natural text), once created, they are difficult to modify for longer contexts as models\u2019 long-context capabilities improve.\n\n\n\nSynthetic construction.\n\nA more flexible approach, sacrificing natural construction for length control, is to use distractors to synthetically increase the context length.\nThis method allows for cheap and efficient (in terms of task construction cost) evaluation of models\u2019 full context length capabilities, with difficulty adjusted by controlling the distractors.\nTasks like Needle-in-a-Haystack (NIAH; Ivgi et\u00a0al., 2023; Kamradt, 2023) and PassKey retrieval (Mohtashami and Jaggi, 2023) were created to evaluate a model\u2019s ability to pinpoint specific information amid lengthy distractors.\nFlexible and effective against existing models, they became standard benchmarks for evaluating new long-context models (GLM Team, 2024; Jiang et\u00a0al., 2024).\nFollowup studies have complicated these tasks by increasing the number of critical details to locate (Arora et\u00a0al., 2023; Liu et\u00a0al., 2024a) and changing their position within the input Liu et\u00a0al. (2024b); Levy et\u00a0al. (2024).\n\n\n\nLimitations of the status quo.\n\nNIAH-like tasks aim to assess information retrieval capabilities, yet\nmany \u201cnaturally constructed\u201d QA and reading-comprehension tasks\nwith trivial questions about a long context accomplish the same goal. At the same time, \u201cmultiple needles\u201d NIAH can increase difficulty not by increasing the quantity of needles or length of input, but by adding distractors between needles\u00a0Levy et\u00a0al. (2024). What can systematically explain the different variables at play, in order to inform better task design in the future?\n\n\nClearly, there are underlying qualitative differences that discriminate between these various tasks besides their natural and synthetic constructions, and besides their actual context length.\nTherefore, we require a more informative vocabulary to discuss the goals of each task design, what it accomplishes, and what it does not, in terms of measuring long-context capabilities.\n\n\n",
      "3 What Makes Long Context More than Retrieval?": "\n\n3 What Makes Long Context More than Retrieval?\n\nWe require a taxonomy to capture task difficulty variations beyond mere \u201cnumber of tokens\u201d.\nWe focus on the information that is canonically required to solve the task as the conditioning variable. Our classification can be summarized via the following two questions, when asked about a given task:\n\n\n(I) How difficult is it to find and extract the required information?\n\n\n(II) How much information is needed to be found?\n\n\nAssuming that some highlighting of the relevant information is needed to solve the task (see Figure\u00a01), the latter question asks how much text is highlighted, while the former addresses the challenge of locating the relevant text for highlighting.\n\n\nFor instance, consider the task of summarizing a book, in comparison to a NIAH task of identifying a numerical detail in a long financial report (e.g., \u201chow much did the company earn in 2015?\u201d).\nAlthough both tasks involve long texts, the information required and its accessibility vary significantly.\nThe NIAH task focuses on localized, identifiable information, while summarization requires extracting key details dispersed throughout the text, tangled together with irrelevant content. Therefore, we can say that the book summarization task is more difficult on both axes (I) and (II).\n\n\nBelow we give more formal descriptions of the two axes characterized by the questions above.\n\n\n(I) Dispersion.\n\nAlthough the question above intuitively defines \u201cdifficulty of information finding\u201d,\nwe offer a more concrete description. Between two similar tasks, we consider the information harder to find in one task compared to another if:\n(1) it is more obscured (e.g., linguistically, semantically, contextually, etc); (2) it is more sparse, such that it is interspersed with non-required information; (3) its indicators are less redundant, such that there are fewer places in the document where the same information is available.\n\n\n\n(II) Scope.\n\nThe property of scope is simpler, and refers to the minimal quantity of information needed to solve the task. Importantly, we are not concerned with precise metric for \u201cquantity of information\u201d at this stage \u2013 it can refer to quantity of tokens, sentences, relations, cells in a table, etc. Any metric that reliably captures difficulty for an established solver is sufficient for our purposes.\n\n\n\nIllustrative example.\n\nTo illustrate, consider the Wikipedia entry for New York City and a simple question: \u201cWhat is the estimated population of the city?\u201d Since the answer needs a small snippet of information, we say that the task has small scope. And since it is easily accessible, we say that it has low dispersion. Consider, instead, the question \u201chow many syllables are in this document?\u201d \u2013 since this question requires the entire document to answer, we say that it has large scope, but if we consider counting syllables as straightforward, then we say its dispersion is still low. Finally, with the question \u201cWas the city\u2019s mayor elected before or after the city was affected by Hurricane Sandy?\u201d \u2013 since it requires snippets from at least two different areas of the text, we can say that when compared to the question about the city\u2019s population, the dispersion is higher, but not as high as for the question \u201cWhat makes the city a prominent place on the world stage?\u201d which poses a challenge on both axes.\n\n\nFigure 2: This figure illustrates our subjective judgment on the distribution of long-context benchmarks for each task, categorized by their scope and dispersion characteristics, with the four quadrants being marked by the dashed lines. Difficulty is expressed by shade, where red is more difficult and green in easier. Notably, some tasks, like Question-answering (QA), appear in multiple quadrants, as different benchmarks demand varying levels of scope and dispersion (e.g., a single fact versus multiple facts spread across a document). For a detailed breakdown of benchmarks and their task associations, refer to Appendix\u00a0A.\n\n\n",
      "4 Challenging Long Context Is Under-Explored": "\n\n4 Challenging Long Context Is Under-Explored\n\nRevisiting the works surveyed in \u00a72, they clearly differ with respect to both scope and dispersion.\n\n\nWith respect to dispersion.\n\nThe information needed for tasks ranges from easily accessible to highly dispersed and difficult to detect.\nOn low dispersion we have NIAH Kamradt (2023); Mohtashami and Jaggi (2023) and a myriad of factual single-hop QA datasets (Tseng et\u00a0al., 2016; Ko\u010disk\u00fd et\u00a0al., 2017; Kwiatkowski et\u00a0al., 2019; Dasigi et\u00a0al., 2021, inter alia) in which the answer is relatively accessible.\nAdding more snippets of information separated by distractors, either in the form of several needles Arora et\u00a0al. (2023); Hsieh et\u00a0al. (2024) or of hops in a multi-hop question Trivedi et\u00a0al. (2022); Zhao et\u00a0al. (2022), complicates the information detection due to the need to find at least two snippets Levy et\u00a0al. (2024), thereby increasing dispersion.\nDispersion can also be increased by making the detection of the information less straightforward\n(e.g., Pang et\u00a0al., 2022) or requiring aggregation Shaham et\u00a0al. (2023).\nLastly, summarization tasks are of a very high dispersion Huang et\u00a0al. (2021a); Wang et\u00a0al. (2022), as they require the non-trivial detection of salient facts that are interwoven with the irrelevant text.\n\n\n\nWith respect to scope.\n\nTasks overwhelmingly target relatively small scope. In addition to the aforementioned NIAH tasks and their variants, many\nQA datasets apply as well\u00a0(Li et\u00a0al., 2023b; Zhao et\u00a0al., 2023; Reddy et\u00a0al., 2024, inter alia).\nA somewhat higher scope is achieved by datasets for query-based summarization Zhong et\u00a0al. (2021); Wang et\u00a0al. (2022), and QA datasets with more obfuscated answers that require reading the text surrounding the answer for its verification\u00a0An et\u00a0al. (2023); He et\u00a0al. (2023).\nAlthough much higher on the scope ladder, book summarization is still limited in its scope as datasets include texts that are only of up to 20k\ud835\udc58kitalic_k tokens Huang et\u00a0al. (2021a); Chen et\u00a0al. (2022a); Shaham et\u00a0al. (2023).\nCurrently, tasks with the highest scope, requiring information across the entire input length, are artificial and of low dispersion, like common words extraction Hsieh et\u00a0al. (2024).\n\n\n\nConclusion.\n\nFigure\u00a02 summarizes the above classification of tasks and datasets. Note that without a concrete definition of dispersion and scope, the plot is only an illustration that involves a good deal of subjective judgements.\nHowever, we conclude that (1) the majority of tasks designed to challenge LLMs in a long-context setting target either scope or dispersion,\nsuch that (2) tasks that push current models\u2019 capabilities on both axes are under-represented in the current landscape.\n\n\n",
      "5 Discussion: Towards Genuinely Difficult Long-Context Task Design": "\n\n5 Discussion: Towards Genuinely Difficult Long-Context Task Design\n\nChallenges.\n\nDesigning meaningful long-context tasks amidst rapid model progress is profoundly challenging, making the deficiency in tasks representing difficulty on both the dispersion and scope axes unsurprising.\nOne source of this challenge is the lack of diverse, coherent long texts, as models\u2019 context windows can now be comparable to\nthe length of the New Testament111www.readinglength.com/book/isbn-0190909005\nand the Odyssey.222www.readinglength.com/book/isbn-0140268863\nThe methodologies discussed in \u00a72 for creating long context tasks \u2013 lengthening short context tasks and synthetically creating length-adjustable tasks \u2013 are preferred for their straightforward definition and the incremental adjustments they require for existing data.\nThey rely on the common understanding of machine comprehension as formulated with short context in mind Dunietz et\u00a0al. (2020), and therefore they are intuitive and easy to comprehend for NLP researchers without domain expertise (e.g., in law or biomedical fields that have long contexts).\n\n\n\nFuture work.\n\nThe goals laid forward in this work are clear: For more durable and robust measurement of long-context capabilities, we must design tasks that explicitly target both the dispersion and scope capabilities of models.\nHow can this be achieved? As mentioned, one possible avenue is to rely more on tasks that require domain expertise, such as legal documents\u00a0Bruno and Roth (2022), financial reports\u00a0Reddy et\u00a0al. (2024), biomedical publications\u00a0Stylianou et\u00a0al. (2021), and so on. In specialized domains, it is common that dispersion will be naturally higher\u00a0Zhao et\u00a0al. (2022). Tasks that involve implicit aggregations over structured data, such as table manipulation\u00a0Caciularu et\u00a0al. (2024), are possible avenues for increasing both scope and dispersion synthetically by leveraging the data structure.\nIn this work, we argue that an explicit vocabulary for such properties of difficulty is what can enable more informed techniques to design difficult tasks in the future.\n\n\n",
      "6 Conclusions": "\n\n6 Conclusions\n\nWe present a taxonomy of factors that make long-context tasks more challenging compared to short ones. This is in contrast with the existing literature that refers only to the length of the input as the hallmark of long context, and as a result ends up conflating tasks of different character when assessing the ability of models to understand longer text.\nWe reviewed works on evaluation in a long-context setting and found that the most challenging setting, in which the information needed is of large scope and is highly dispersed within the input, is under-explored.\nFinally, we suggested some leads for future work to tackle this imbalance towards a more informative long context evaluation.\n\n",
      "7 Limitations": "\n\n7 Limitations\n\nFormality.\n\nIn the context of this work, we have deliberately adhered to a taxonomy based on an intuitive description, in the interest of utility to a wide diversity of research and flexibility for future work. Difficulty in searching for and extracting information, and quantity of information, are both vague terms that can only be grounded in the context of a specific family of tasks and use-cases. We intend for this work to serve as a call to action and a tool for a shared vocabulary in the interest of more informed long-context task design in the future, rather than to anchor the taxonomy to a specific and fragile point in time.\n\n\n\nRetrieval is still interesting.\n\nAlthough we argue that small scope and low dispersion tasks are the least indicative of the model\u2019s ability to long-context capabilities, tasks that are well-served by implicit retrieval or by traditional retrieval-based pipelines are certainly relevant and useful in a variety of common use-cases\u00a0Stylianou et\u00a0al. (2021); Bruno and Roth (2022); Gao et\u00a0al. (2023).\n\n\n\nOther uses for a long-context window.\n\nThis paper deals only with long inputs that serve as inputs to a task. The long context of course can have other purposes as well, like containing many in-context examples Bertsch et\u00a0al. (2024) or containing other modalities and structures Jiang et\u00a0al. (2023).\n\n\n",
      "Acknowledgments": "\nAcknowledgments\n\nThe authors would like to thank Gabriel Stanovsky for the fruitful discussions.\n\n",
      "Appendix A Benchmark Scope-Dispersion Classification": "\n\nAppendix A Benchmark Scope-Dispersion Classification\n\nIn Table\u00a01 we delineate the different long-context benchmarks, as well as classify them according to how challenging they are scope-wise and dispersion-wise.\n\n\n\n\n\n\n\nLOW SCOPE\nHIGH SCOPE\n\n\n\n\n\nQA\nQBS\n\n\n\n\u00a0\u00a0\u00a0\u00a0 Qasper (Dasigi et\u00a0al., 2021)\n\n\u00a0\u00a0\u00a0\u00a0 QMSum (Zhong et\u00a0al., 2021)\n\n\n\n\n\u00a0\u00a0\u00a0\u00a0 NarrativeQA (Ko\u010disk\u00fd et\u00a0al., 2018)\n\n\u00a0\u00a0\u00a0\u00a0 SQuALITY (Wang et\u00a0al., 2022)\n\n\n\n\n\u00a0\u00a0\u00a0\u00a0 Short-dependency QA (Li et\u00a0al., 2023b)\n\n\u00a0\u00a0\u00a0\u00a0 Related Work Summarization (An et\u00a0al., 2023)\n\n\n\n\n\u00a0\u00a0\u00a0\u00a0 MultiFieldQA (Bai et\u00a0al., 2023)\n\n\u00a0\u00a0\u00a0\u00a0 SPACE (Angelidis et\u00a0al., 2021)\n\n\n\n\n\u00a0\u00a0\u00a0\u00a0 LitM (QA) (Liu et\u00a0al., 2024b)\n\n\u00a0\u00a0\u00a0\u00a0 WebBrain-G (Qian et\u00a0al., 2023)\n\n\n\n\n\u00a0\u00a0\u00a0\u00a0 L-eval (MC QA) (An et\u00a0al., 2023)\n\n\u00a0\u00a0\u00a0\u00a0 AquaMuse (Kulkarni et\u00a0al., 2020)\n\n\n\n\n\u00a0\u00a0\u00a0\u00a0 NQ (Kwiatkowski et\u00a0al., 2019)\n\n\u00a0\u00a0\u00a0\u00a0 FINDSum-Liquidity (Liu et\u00a0al., 2023a)\n\n\n\n\n\u00a0\u00a0\u00a0\u00a0 RULER (single-hop QA) (Hsieh et\u00a0al., 2024)\n\nAggregation\n\n\n\n\u00a0\u00a0\u00a0\u00a0 MeetingQA (Prasad et\u00a0al., 2023)\n\n\u00a0\u00a0\u00a0\u00a0 ZeroSCROLLS (SpaceDigest & BookSumSort) (Shaham et\u00a0al., 2023)\n\n\n\n\n\u00a0\u00a0\u00a0\u00a0 BABILong (tasks 1,4-6,9-10) (Kuratov et\u00a0al., 2024)\n\n\u00a0\u00a0\u00a0\u00a0 PassageCount (Bai et\u00a0al., 2023)\n\n\n\n\n\u00a0\u00a0\u00a0\u00a0 Giraffe (2 tasks) (Pal et\u00a0al., 2023)\n\n\u00a0\u00a0\u00a0\u00a0 FINDSum-ROO (Liu et\u00a0al., 2023a)\n\n\n\n\nRetrieval\nAspect-based Summarization\n\n\n\n\u00a0\u00a0\u00a0\u00a0 LitM (Key-value Retrieval) (Liu et\u00a0al., 2024b)\n\n\u00a0\u00a0\u00a0\u00a0 ACLSum (Takeshita et\u00a0al., 2024)\n\n\n\n\n\u00a0\u00a0\u00a0\u00a0 MultiDoc2Dial (GSP) (Feng et\u00a0al., 2021)\n\n\u00a0\u00a0\u00a0\u00a0 OpenAsp (Amar et\u00a0al., 2023)\n\n\n\n\n\u00a0\u00a0\u00a0\u00a0 TopicRet (Li et\u00a0al., 2023a)\n\nText Sorting\n\n\n\n\u00a0\u00a0\u00a0\u00a0 Wiki-GenBen (Zhang et\u00a0al., 2024a)\n\n\u00a0\u00a0\u00a0\u00a0 Bamboo (ShowsSort & ReportSumSort) (Dong et\u00a0al., 2024)\n\n\n\n\n\u00a0\u00a0\u00a0\u00a0 RULER (S-NIAH & MK-NIAH) (Hsieh et\u00a0al., 2024)\n\nRetrieval\n\n\n\n\u00a0\u00a0\u00a0\u00a0 LongChat-Lines (Pal et\u00a0al., 2023)\n\n\u00a0\u00a0\u00a0\u00a0 PassageRetrieval (Bai et\u00a0al., 2023)\n\n\n\n\nNLI\nLFQA\n\n\n\n\u00a0\u00a0\u00a0\u00a0 LawngNLI (Bruno and Roth, 2022)\n\n\u00a0\u00a0\u00a0\u00a0 LongFQA (An et\u00a0al., 2023)\n\n\n\n\n\u00a0\u00a0\u00a0\u00a0 ContractNLI (Koreeda and Manning, 2021b)\n\nNLI\n\n\n\n\u00a0\u00a0\u00a0\u00a0 Hallucination Detection (Dong et\u00a0al., 2024)\n\n\u00a0\u00a0\u00a0\u00a0 Legal Case Entailment (Nguyen et\u00a0al., 2024)\n\n\n\n\n\u00a0\u00a0 FLenQA (3 tasks) (Levy et\u00a0al., 2024)\n\n\n\n\nFill-mask\n\n\n\n\n\u00a0\u00a0\u00a0\u00a0 Cloze (Li et\u00a0al., 2023b)\n\n\n\n\n\nNLG\n\n\n\n\n\nLOW DISPERSION\n\n\u00a0\u00a0\u00a0\u00a0 MultiDoc2Dial (ARG) (Feng et\u00a0al., 2021)\n\n\n\n\n\nQA\nSummarization\n\n\n\n\u00a0\u00a0\u00a0\u00a0 QuALITY (Pang et\u00a0al., 2022)\n\n\u00a0\u00a0\u00a0\u00a0 GovReport (Huang et\u00a0al., 2021b)\n\n\n\n\n\u00a0\u00a0\u00a0\u00a0 Long-dependency QA (Li et\u00a0al., 2023b)\n\n\u00a0\u00a0\u00a0\u00a0 SummScreenFD (Chen et\u00a0al., 2022b)\n\n\n\n\n\u00a0\u00a0\u00a0\u00a0 DuReader (Bai et\u00a0al., 2023)\n\n\u00a0\u00a0\u00a0\u00a0 Loogle (Summarization) (Li et\u00a0al., 2023b)\n\n\n\n\n\u00a0\u00a0\u00a0\u00a0 SFcition QA (An et\u00a0al., 2023)\n\n\u00a0\u00a0\u00a0\u00a0 VCSUM (Bai et\u00a0al., 2023)\n\n\n\n\n\u00a0\u00a0\u00a0\u00a0 ExpertQA (Malaviya et\u00a0al., 2024)\n\n\u00a0\u00a0\u00a0\u00a0 Self-critiquing (Saunders et\u00a0al., 2022)\n\n\n\n\n\u00a0\u00a0\u00a0\u00a0 DocFinQA (Reddy et\u00a0al., 2024)\n\n\u00a0\u00a0\u00a0\u00a0 Abstract Generation (An et\u00a0al., 2023)\n\n\n\n\n\u00a0\u00a0\u00a0\u00a0 BABILong (tasks 2-3,12) (Kuratov et\u00a0al., 2024)\n\n\u00a0\u00a0\u00a0\u00a0 Multi-News (Fabbri et\u00a0al., 2019)\n\n\n\n\n\u00a0\u00a0\u00a0\u00a0 Bamboo (QA) (Dong et\u00a0al., 2024)\n\n\u00a0\u00a0\u00a0\u00a0 BigPatent (Sharma et\u00a0al., 2019)\n\n\n\n\nMulti-hop QA\n\u00a0\u00a0\u00a0\u00a0 Scientific Summarization (Cohan et\u00a0al., 2018)\n\n\n\n\n\u00a0\u00a0\u00a0\u00a0 MuSiQue (Trivedi et\u00a0al., 2022)\n\n\u00a0\u00a0\u00a0\u00a0 BillSum (Kornilova and Eidelman, 2019)\n\n\n\n\n\u00a0\u00a0\u00a0\u00a0 HotpotQA (Yang et\u00a0al., 2018)\n\n\u00a0\u00a0\u00a0\u00a0 HowSumm (Boni et\u00a0al., 2021)\n\n\n\n\n\u00a0\u00a0\u00a0\u00a0 Multi-hop Tracing (Hsieh et\u00a0al., 2024)\n\n\u00a0\u00a0\u00a0\u00a0 ODSum (Zhou et\u00a0al., 2023)\n\n\n\n\n\u00a0\u00a0\u00a0\u00a0 RULER (multi-hop QA) (Hsieh et\u00a0al., 2024)\n\n\u00a0\u00a0\u00a0\u00a0 Klexikon (Summarization) (Aumiller and Gertz, 2022)\n\n\n\n\n\u00a0\u00a0\u00a0\u00a0 2WikiMultihopQA (Ho et\u00a0al., 2020)\n\n\u00a0\u00a0\u00a0\u00a0 Booksum (Kry\u015bci\u0144ski et\u00a0al., 2022)\n\n\n\n\nNLI\n\u00a0\u00a0\u00a0\u00a0 MeetingBank (Hu et\u00a0al., 2023)\n\n\n\n\n\u00a0\u00a0\u00a0\u00a0 FLenQA (3 rand. placement tasks) (Levy et\u00a0al., 2024)\n\nText Simplification\n\n\n\n\u00a0\u00a0\u00a0\u00a0 Legal Textual Entailment (Nguyen et\u00a0al., 2024)\n\n\u00a0\u00a0\u00a0\u00a0 Klexikon (Simplification) (Aumiller and Gertz, 2022)\n\n\n\n\nCode Understanding\nReasoning\n\n\n\n\u00a0\u00a0\u00a0\u00a0 LCC (Guo et\u00a0al., 2023)\n\n\u00a0\u00a0\u00a0\u00a0 Long ListOps (Tay et\u00a0al., 2020)\n\n\n\n\n\u00a0\u00a0\u00a0\u00a0 RepoBench-P (Liu et\u00a0al., 2023b)\n\nRetrieval\n\n\n\n\u00a0\u00a0\u00a0\u00a0 CodeU (An et\u00a0al., 2023)\n\n\u00a0\u00a0\u00a0\u00a0 LRA (task 3) (Tay et\u00a0al., 2020)\n\n\n\n\n\u00a0\u00a0\u00a0\u00a0 PrivateEval (Dong et\u00a0al., 2024)\n\n\n\n\n\nClassification\n\n\n\n\n\u00a0\u00a0\u00a0\u00a0 LRA (tasks 2, 4-6) (Tay et\u00a0al., 2020)\n\n\n\n\n\nRetrieval\n\n\n\n\n\u00a0\u00a0\u00a0\u00a0 COLIEE (tasks 1,3,4) (Nguyen et\u00a0al., 2024)\n\n\n\n\n\n\u00a0\u00a0\u00a0\u00a0 RULER (MV-NIAH & MQ-NIAH) (Hsieh et\u00a0al., 2024)\n\n\n\n\n\nNext Token Prediction\n\n\n\n\n\u00a0\u00a0\u00a0\u00a0 PG-19 (Rae et\u00a0al., 2019)\n\n\n\n\n\n\u00a0\u00a0\u00a0\u00a0 Bamboo (LM) (Dong et\u00a0al., 2024)\n\n\n\n\n\nReasoning\n\n\n\n\n\u00a0\u00a0\u00a0\u00a0 DocMath-Eval (Zhao et\u00a0al., 2023)\n\n\n\n\n\n\u00a0\u00a0\u00a0\u00a0 BABILong (tasks 14-20) (Kuratov et\u00a0al., 2024)\n\n\n\n\n\nAggregation\n\n\n\n\n\u00a0\u00a0\u00a0\u00a0 RULER (2 Aggr. tasks) (Hsieh et\u00a0al., 2024)\n\n\n\n\n\n\u00a0\u00a0\u00a0\u00a0 BABILong (tasks 7-8) (Kuratov et\u00a0al., 2024)\n\n\n\n\n\nNLU\n\n\n\n\n\u00a0\u00a0\u00a0\u00a0 Academic Feedback Generation (An et\u00a0al., 2023)\n\n\n\n\n\n\u00a0\u00a0\u00a0\u00a0 CUAD (Hendrycks et\u00a0al., 2021)\n\n\n\n\n\nFactuality Evaluation\n\n\n\n\n\u00a0\u00a0\u00a0\u00a0 LongSciVerify (Bishop et\u00a0al., 2024)\n\n\n\n\n\nCoreference Resolution\n\n\n\n\n\nHIGH DISPERSION\n\n\u00a0\u00a0\u00a0\u00a0 BABILong (tasks 11,13) (Kuratov et\u00a0al., 2024)\n\n\n\n\n\n\nTable 1: Classification of long-context benchmarks in terms of Scope and Dispersion.\n\n\n"
    },
    "bibliography": {
      "Amar et\u00a0al. (2023)": "\nAmar et\u00a0al. (2023)\n\nShmuel Amar, Liat Schiff, Ori Ernst, Asi Shefer, Ori Shapira, and Ido Dagan. 2023.\n\n\nOpenAsp: A benchmark for multi-document open aspect-based summarization.\n\n\nIn Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 1967\u20131991, Singapore. Association for Computational Linguistics.\n\n\n",
      "An et\u00a0al. (2023)": "\nAn et\u00a0al. (2023)\n\nChenxin An, Shansan Gong, Ming Zhong, Xingjian Zhao, Mukai Li, Jun Zhang, Lingpeng Kong, and Xipeng Qiu. 2023.\n\n\nL-eval: Instituting standardized evaluation for long context language models.\n\n\nPreprint, arXiv:2307.11088.\n\n\n",
      "Angelidis et\u00a0al. (2021)": "\nAngelidis et\u00a0al. (2021)\n\nStefanos Angelidis, Reinald\u00a0Kim Amplayo, Yoshihiko Suhara, Xiaolan Wang, and Mirella Lapata. 2021.\n\n\nExtractive opinion summarization in quantized transformer spaces.\n\n\nTransactions of the Association for Computational Linguistics, 9:277\u2013293.\n\n\n",
      "Arora et\u00a0al. (2023)": "\nArora et\u00a0al. (2023)\n\nSimran Arora, Sabri Eyuboglu, Aman Timalsina, Isys Johnson, Michael Poli, James Zou, Atri Rudra, and Christopher R\u00e9. 2023.\n\n\nZoology: Measuring and improving recall in efficient language models.\n\n\narXiv preprint arXiv:2312.04927.\n\n\n",
      "Aumiller and Gertz (2022)": "\nAumiller and Gertz (2022)\n\nDennis Aumiller and Michael Gertz. 2022.\n\n\nKlexikon: A German dataset for joint summarization and simplification.\n\n\nIn Proceedings of the Thirteenth Language Resources and Evaluation Conference, pages 2693\u20132701, Marseille, France. European Language Resources Association.\n\n\n",
      "Bai et\u00a0al. (2023)": "\nBai et\u00a0al. (2023)\n\nYushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li. 2023.\n\n\nLongbench: A bilingual, multitask benchmark for long context understanding.\n\n\nPreprint, arXiv:2308.14508.\n\n\n",
      "Bertsch et\u00a0al. (2024)": "\nBertsch et\u00a0al. (2024)\n\nAmanda Bertsch, Maor Ivgi, Uri Alon, Jonathan Berant, Matthew\u00a0R. Gormley, and Graham Neubig. 2024.\n\n\nIn-context learning with long-context models: An in-depth exploration.\n\n\nPreprint, arXiv:2405.00200.\n\n\n",
      "Bishop et\u00a0al. (2024)": "\nBishop et\u00a0al. (2024)\n\nJennifer\u00a0A Bishop, Qianqian Xie, and Sophia Ananiadou. 2024.\n\n\nLongdocfactscore: Evaluating the factuality of long document abstractive summarisation.\n\n\nPreprint, arXiv:2309.12455.\n\n\n",
      "Boni et\u00a0al. (2021)": "\nBoni et\u00a0al. (2021)\n\nOdellia Boni, Guy Feigenblat, Guy Lev, Michal Shmueli-Scheuer, Benjamin Sznajder, and David Konopnicki. 2021.\n\n\nHowsumm: A multi-document summarization dataset derived from wikihow articles.\n\n\nPreprint, arXiv:2110.03179.\n\n\n",
      "Bruno and Roth (2022)": "\nBruno and Roth (2022)\n\nWilliam Bruno and Dan Roth. 2022.\n\n\nLawngnli: A long-premise benchmark for in-domain generalization from short to long contexts and for implication-based retrieval.\n\n\nPreprint, arXiv:2212.03222.\n\n\n",
      "Caciularu et\u00a0al. (2024)": "\nCaciularu et\u00a0al. (2024)\n\nAvi Caciularu, Alon Jacovi, Eyal Ben-David, Sasha Goldshtein, Tal Schuster, Jonathan Herzig, Gal Elidan, and Amir Globerson. 2024.\n\n\nTact: Advancing complex aggregative reasoning with information extraction tools.\n\n\nPreprint, arXiv:2406.03618.\n\n\n",
      "Chen et\u00a0al. (2022a)": "\nChen et\u00a0al. (2022a)\n\nMingda Chen, Zewei Chu, Sam Wiseman, and Kevin Gimpel. 2022a.\n\n\nSummScreen: A dataset for abstractive screenplay summarization.\n\n\nIn Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 8602\u20138615, Dublin, Ireland. Association for Computational Linguistics.\n\n\n",
      "Chen et\u00a0al. (2022b)": "\nChen et\u00a0al. (2022b)\n\nMingda Chen, Zewei Chu, Sam Wiseman, and Kevin Gimpel. 2022b.\n\n\nSummscreen: A dataset for abstractive screenplay summarization.\n\n\nPreprint, arXiv:2104.07091.\n\n\n",
      "Chen et\u00a0al. (2023)": "\nChen et\u00a0al. (2023)\n\nYukang Chen, Shengju Qian, Haotian Tang, Xin Lai, Zhijian Liu, Song Han, and Jiaya Jia. 2023.\n\n\nLonglora: Efficient fine-tuning of long-context large language models.\n\n\nArXiv, abs/2309.12307.\n\n\n",
      "Cohan et\u00a0al. (2018)": "\nCohan et\u00a0al. (2018)\n\nArman Cohan, Franck Dernoncourt, Doo\u00a0Soon Kim, Trung Bui, Seokhwan Kim, Walter Chang, and Nazli Goharian. 2018.\n\n\nA discourse-aware attention model for abstractive summarization of long documents.\n\n\nPreprint, arXiv:1804.05685.\n\n\n",
      "Dasigi et\u00a0al. (2021)": "\nDasigi et\u00a0al. (2021)\n\nPradeep Dasigi, Kyle Lo, Iz\u00a0Beltagy, Arman Cohan, Noah\u00a0A. Smith, and Matt Gardner. 2021.\n\n\nA dataset of information-seeking questions and answers anchored in research papers.\n\n\nIn Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 4599\u20134610, Online. Association for Computational Linguistics.\n\n\n",
      "Devlin et\u00a0al. (2019)": "\nDevlin et\u00a0al. (2019)\n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019.\n\n\nBERT: Pre-training of deep bidirectional transformers for language understanding.\n\n\nIn Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171\u20134186, Minneapolis, Minnesota. Association for Computational Linguistics.\n\n\n",
      "Dong et\u00a0al. (2024)": "\nDong et\u00a0al. (2024)\n\nZican Dong, Tianyi Tang, Junyi Li, Wayne\u00a0Xin Zhao, and Ji-Rong Wen. 2024.\n\n\nBAMBOO: A comprehensive benchmark for evaluating long text modeling capacities of large language models.\n\n\nIn Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024), pages 2086\u20132099, Torino, Italia. ELRA and ICCL.\n\n\n",
      "Dunietz et\u00a0al. (2020)": "\nDunietz et\u00a0al. (2020)\n\nJesse Dunietz, Greg Burnham, Akash Bharadwaj, Owen Rambow, Jennifer Chu-Carroll, and Dave Ferrucci. 2020.\n\n\nTo test machine comprehension, start by defining comprehension.\n\n\nIn Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7839\u20137859, Online. Association for Computational Linguistics.\n\n\n",
      "Dunn et\u00a0al. (2017)": "\nDunn et\u00a0al. (2017)\n\nMatthew Dunn, Levent Sagun, Mike Higgins, V\u00a0Ugur Guney, Volkan Cirik, and Kyunghyun Cho. 2017.\n\n\nSearchqa: A new q&a dataset augmented with context from a search engine.\n\n\narXiv preprint arXiv:1704.05179.\n\n\n",
      "Fabbri et\u00a0al. (2019)": "\nFabbri et\u00a0al. (2019)\n\nAlexander\u00a0R. Fabbri, Irene Li, Tianwei She, Suyi Li, and Dragomir\u00a0R. Radev. 2019.\n\n\nMulti-news: a large-scale multi-document summarization dataset and abstractive hierarchical model.\n\n\nPreprint, arXiv:1906.01749.\n\n\n",
      "Feng et\u00a0al. (2021)": "\nFeng et\u00a0al. (2021)\n\nSong Feng, Siva\u00a0Sankalp Patel, Hui Wan, and Sachindra Joshi. 2021.\n\n\nMultiDoc2Dial: Modeling dialogues grounded in multiple documents.\n\n\nIn Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6162\u20136176, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.\n\n\n",
      "Gao et\u00a0al. (2024)": "\nGao et\u00a0al. (2024)\n\nFan Gao, Hang Jiang, Rui Yang, Qingcheng Zeng, Jinghui Lu, Moritz Blum, Dairui Liu, Tianwei She, Yuang Jiang, and Irene Li. 2024.\n\n\nLarge language models on wikipedia-style survey generation: an evaluation in nlp concepts.\n\n\nPreprint, arXiv:2308.10410.\n\n\n",
      "Gao et\u00a0al. (2023)": "\nGao et\u00a0al. (2023)\n\nLuyu Gao, Zhuyun Dai, Panupong Pasupat, Anthony Chen, Arun\u00a0Tejasvi Chaganty, Yicheng Fan, Vincent\u00a0Y. Zhao, Ni\u00a0Lao, Hongrae Lee, Da-Cheng Juan, and Kelvin Guu. 2023.\n\n\nRarr: Researching and revising what language models say, using language models.\n\n\nPreprint, arXiv:2210.08726.\n\n\n",
      "Gemini Team Google (2024)": "\nGemini Team Google (2024)\n\nGemini Team Google. 2024.\n\n\nGemini 1.5: Unlocking multimodal understanding across millions of tokens of context.\n\n\nPreprint, arXiv:2403.05530.\n\n\n",
      "GLM Team (2024)": "\nGLM Team (2024)\n\nGLM Team. 2024.\n\n\nGLM-4-9b-chat technical report.\n\n\n",
      "Guo et\u00a0al. (2023)": "\nGuo et\u00a0al. (2023)\n\nDaya Guo, Canwen Xu, Nan Duan, Jian Yin, and Julian McAuley. 2023.\n\n\nLongcoder: A long-range pre-trained language model for code completion.\n\n\nPreprint, arXiv:2306.14893.\n\n\n",
      "He et\u00a0al. (2023)": "\nHe et\u00a0al. (2023)\n\nJunqing He, Kunhao Pan, Xiaoqun Dong, Zhuoyang Song, Yibo Liu, Yuxin Liang, Hao Wang, Qianguo Sun, Songxin Zhang, Zejian Xie, and Jiaxing Zhang. 2023.\n\n\nNever lost in the middle: Improving large language models via attention strengthening question answering.\n\n\nPreprint, arXiv:2311.09198.\n\n\n",
      "Hendrycks et\u00a0al. (2021)": "\nHendrycks et\u00a0al. (2021)\n\nDan Hendrycks, Collin Burns, Anya Chen, and Spencer Ball. 2021.\n\n\nCuad: An expert-annotated nlp dataset for legal contract review.\n\n\nPreprint, arXiv:2103.06268.\n\n\n",
      "Ho et\u00a0al. (2020)": "\nHo et\u00a0al. (2020)\n\nXanh Ho, Anh-Khoa Duong\u00a0Nguyen, Saku Sugawara, and Akiko Aizawa. 2020.\n\n\nConstructing a multi-hop QA dataset for comprehensive evaluation of reasoning steps.\n\n\nIn Proceedings of the 28th International Conference on Computational Linguistics, pages 6609\u20136625, Barcelona, Spain (Online). International Committee on Computational Linguistics.\n\n\n",
      "Hsieh et\u00a0al. (2024)": "\nHsieh et\u00a0al. (2024)\n\nCheng-Ping Hsieh, Simeng Sun, Samuel Kriman, Shantanu Acharya, Dima Rekesh, Fei Jia, Yang Zhang, and Boris Ginsburg. 2024.\n\n\nRuler: What\u2019s the real context size of your long-context language models?\n\nPreprint, arXiv:2404.06654.\n\n\n",
      "Hu et\u00a0al. (2023)": "\nHu et\u00a0al. (2023)\n\nYebowen Hu, Timothy Ganter, Hanieh Deilamsalehy, Franck Dernoncourt, Hassan Foroosh, and Fei Liu. 2023.\n\n\nMeetingBank: A benchmark dataset for meeting summarization.\n\n\nIn Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 16409\u201316423, Toronto, Canada. Association for Computational Linguistics.\n\n\n",
      "Huang et\u00a0al. (2021a)": "\nHuang et\u00a0al. (2021a)\n\nLuyang Huang, Shuyang Cao, Nikolaus Parulian, Heng Ji, and Lu\u00a0Wang. 2021a.\n\n\nEfficient attentions for long document summarization.\n\n\nIn Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1419\u20131436, Online. Association for Computational Linguistics.\n\n\n",
      "Huang et\u00a0al. (2021b)": "\nHuang et\u00a0al. (2021b)\n\nLuyang Huang, Shuyang Cao, Nikolaus Parulian, Heng Ji, and Lu\u00a0Wang. 2021b.\n\n\nEfficient attentions for long document summarization.\n\n\nPreprint, arXiv:2104.02112.\n\n\n",
      "Ivgi et\u00a0al. (2023)": "\nIvgi et\u00a0al. (2023)\n\nMaor Ivgi, Uri Shaham, and Jonathan Berant. 2023.\n\n\nEfficient long-text understanding with short-text models.\n\n\nTransactions of the Association for Computational Linguistics, 11:284\u2013299.\n\n\n",
      "Jiang et\u00a0al. (2024)": "\nJiang et\u00a0al. (2024)\n\nAlbert\u00a0Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra\u00a0Singh Chaplot, Diego de\u00a0las Casas, Emma\u00a0Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, L\u00e9lio\u00a0Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven\u00a0Le Scao, Th\u00e9ophile Gervet, Thibaut Lavril, Thomas Wang, Timoth\u00e9e Lacroix, and William\u00a0El Sayed. 2024.\n\n\nMixtral of experts.\n\n\nPreprint, arXiv:2401.04088.\n\n\n",
      "Jiang et\u00a0al. (2023)": "\nJiang et\u00a0al. (2023)\n\nJinhao Jiang, Kun Zhou, Zican Dong, Keming Ye, Xin Zhao, and Ji-Rong Wen. 2023.\n\n\nStructGPT: A general framework for large language model to reason over structured data.\n\n\nIn Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 9237\u20139251, Singapore. Association for Computational Linguistics.\n\n\n",
      "Kamradt (2023)": "\nKamradt (2023)\n\nGregory Kamradt. 2023.\n\n\nNeedle in a haystack - pressure testing LLMs.\n\n\nGitHub.\n\n\n",
      "Koreeda and Manning (2021a)": "\nKoreeda and Manning (2021a)\n\nYuta Koreeda and Christopher Manning. 2021a.\n\n\nContractNLI: A dataset for document-level natural language inference for contracts.\n\n\nIn Findings of the Association for Computational Linguistics: EMNLP 2021, pages 1907\u20131919, Punta Cana, Dominican Republic. Association for Computational Linguistics.\n\n\n",
      "Koreeda and Manning (2021b)": "\nKoreeda and Manning (2021b)\n\nYuta Koreeda and Christopher\u00a0D. Manning. 2021b.\n\n\nContractnli: A dataset for document-level natural language inference for contracts.\n\n\nPreprint, arXiv:2110.01799.\n\n\n",
      "Kornilova and Eidelman (2019)": "\nKornilova and Eidelman (2019)\n\nAnastassia Kornilova and Vladimir Eidelman. 2019.\n\n\nBillSum: A corpus for automatic summarization of US legislation.\n\n\nIn Proceedings of the 2nd Workshop on New Frontiers in Summarization, pages 48\u201356, Hong Kong, China. Association for Computational Linguistics.\n\n\n",
      "Ko\u010disk\u00fd et\u00a0al. (2017)": "\nKo\u010disk\u00fd et\u00a0al. (2017)\n\nTom\u00e1\u0161 Ko\u010disk\u00fd, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl\u00a0Moritz Hermann, G\u00e1bor Melis, and Edward Grefenstette. 2017.\n\n\nThe narrativeqa reading comprehension challenge.\n\n\nPreprint, arXiv:1712.07040.\n\n\n",
      "Ko\u010disk\u00fd et\u00a0al. (2018)": "\nKo\u010disk\u00fd et\u00a0al. (2018)\n\nTom\u00e1\u0161 Ko\u010disk\u00fd, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl\u00a0Moritz Hermann, G\u00e1bor Melis, and Edward Grefenstette. 2018.\n\n\nThe NarrativeQA Reading Comprehension Challenge.\n\n\nTransactions of the Association for Computational Linguistics, 6:317\u2013328.\n\n\n",
      "Kryscinski et\u00a0al. (2022)": "\nKryscinski et\u00a0al. (2022)\n\nWojciech Kryscinski, Nazneen Rajani, Divyansh Agarwal, Caiming Xiong, and Dragomir Radev. 2022.\n\n\nBOOKSUM: A collection of datasets for long-form narrative summarization.\n\n\nIn Findings of the Association for Computational Linguistics: EMNLP 2022, pages 6536\u20136558, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.\n\n\n",
      "Kry\u015bci\u0144ski et\u00a0al. (2022)": "\nKry\u015bci\u0144ski et\u00a0al. (2022)\n\nWojciech Kry\u015bci\u0144ski, Nazneen Rajani, Divyansh Agarwal, Caiming Xiong, and Dragomir Radev. 2022.\n\n\nBooksum: A collection of datasets for long-form narrative summarization.\n\n\nPreprint, arXiv:2105.08209.\n\n\n",
      "Kulkarni et\u00a0al. (2020)": "\nKulkarni et\u00a0al. (2020)\n\nSayali Kulkarni, Sheide Chammas, Wan Zhu, Fei Sha, and Eugene Ie. 2020.\n\n\nAquamuse: Automatically generating datasets for query-based multi-document summarization.\n\n\nPreprint, arXiv:2010.12694.\n\n\n",
      "Kuratov et\u00a0al. (2024)": "\nKuratov et\u00a0al. (2024)\n\nYuri Kuratov, Aydar Bulatov, Petr Anokhin, Dmitry Sorokin, Artyom Sorokin, and Mikhail Burtsev. 2024.\n\n\nIn search of needles in a 11m haystack: Recurrent memory finds what llms miss.\n\n\nPreprint, arXiv:2402.10790.\n\n\n",
      "Kwiatkowski et\u00a0al. (2019)": "\nKwiatkowski et\u00a0al. (2019)\n\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew\u00a0M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. 2019.\n\n\nNatural questions: A benchmark for question answering research.\n\n\nTransactions of the Association for Computational Linguistics, 7:452\u2013466.\n\n\n",
      "Levy et\u00a0al. (2024)": "\nLevy et\u00a0al. (2024)\n\nMosh Levy, Alon Jacoby, and Yoav Goldberg. 2024.\n\n\nSame task, more tokens: the impact of input length on the reasoning performance of large language models.\n\n\nPreprint, arXiv:2402.14848.\n\n\n",
      "Li et\u00a0al. (2023a)": "\nLi et\u00a0al. (2023a)\n\nDacheng Li, Rulin Shao, Anze Xie, Ying Sheng, Lianmin Zheng, Joseph\u00a0E. Gonzalez, Ion Stoica, Xuezhe Ma, and Hao Zhang. 2023a.\n\n\nHow long can open-source LLMs truly promise on context length?\n\n",
      "Li et\u00a0al. (2023b)": "\nLi et\u00a0al. (2023b)\n\nJiaqi Li, Mengmeng Wang, Zilong Zheng, and Muhan Zhang. 2023b.\n\n\nLoogle: Can long-context language models understand long contexts?\n\nPreprint, arXiv:2311.04939.\n\n\n",
      "Liu et\u00a0al. (2024a)": "\nLiu et\u00a0al. (2024a)\n\nHao Liu, Wilson Yan, Matei Zaharia, and Pieter Abbeel. 2024a.\n\n\nWorld model on million-length video and language with ringattention.\n\n\narXiv preprint arXiv:2402.08268.\n\n\n",
      "Liu et\u00a0al. (2024b)": "\nLiu et\u00a0al. (2024b)\n\nNelson\u00a0F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. 2024b.\n\n\nLost in the middle: How language models use long contexts.\n\n\nTransactions of the Association for Computational Linguistics, 12:157\u2013173.\n\n\n",
      "Liu et\u00a0al. (2023a)": "\nLiu et\u00a0al. (2023a)\n\nShuaiqi Liu, Jiannong Cao, Ruosong Yang, and Zhiyuan Wen. 2023a.\n\n\nLong text and multi-table summarization: Dataset and method.\n\n\nPreprint, arXiv:2302.03815.\n\n\n",
      "Liu et\u00a0al. (2023b)": "\nLiu et\u00a0al. (2023b)\n\nTianyang Liu, Canwen Xu, and Julian McAuley. 2023b.\n\n\nRepobench: Benchmarking repository-level code auto-completion systems.\n\n\nPreprint, arXiv:2306.03091.\n\n\n",
      "Malaviya et\u00a0al. (2024)": "\nMalaviya et\u00a0al. (2024)\n\nChaitanya Malaviya, Subin Lee, Sihao Chen, Elizabeth Sieber, Mark Yatskar, and Dan Roth. 2024.\n\n\nExpertQA: Expert-curated questions and attributed answers.\n\n\nIn Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 3025\u20133045, Mexico City, Mexico. Association for Computational Linguistics.\n\n\n",
      "Mohtashami and Jaggi (2023)": "\nMohtashami and Jaggi (2023)\n\nAmirkeivan Mohtashami and Martin Jaggi. 2023.\n\n\nLandmark attention: Random-access infinite context length for transformers.\n\n\nIn Workshop on Efficient Systems for Foundation Models@ ICML2023.\n\n\n",
      "Narayan et\u00a0al. (2018)": "\nNarayan et\u00a0al. (2018)\n\nShashi Narayan, Shay\u00a0B. Cohen, and Mirella Lapata. 2018.\n\n\nDon\u2019t give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization.\n\n\nIn Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1797\u20131807, Brussels, Belgium. Association for Computational Linguistics.\n\n\n",
      "Nguyen et\u00a0al. (2024)": "\nNguyen et\u00a0al. (2024)\n\nChau Nguyen, Phuong Nguyen, Thanh Tran, Dat Nguyen, An\u00a0Trieu, Tin Pham, Anh Dang, and Le-Minh Nguyen. 2024.\n\n\nCaptain at coliee 2023: Efficient methods for legal information retrieval and entailment tasks.\n\n\nPreprint, arXiv:2401.03551.\n\n\n",
      "OpenAI (2024)": "\nOpenAI (2024)\n\nOpenAI. 2024.\n\n\nGPT-4 technical report.\n\n\nPreprint, arXiv:2303.08774.\n\n\n",
      "Pal et\u00a0al. (2023)": "\nPal et\u00a0al. (2023)\n\nArka Pal, Deep Karkhanis, Manley Roberts, Samuel Dooley, Arvind Sundararajan, and Siddartha Naidu. 2023.\n\n\nGiraffe: Adventures in expanding context lengths in llms.\n\n\nPreprint, arXiv:2308.10882.\n\n\n",
      "Pang et\u00a0al. (2022)": "\nPang et\u00a0al. (2022)\n\nRichard\u00a0Yuanzhe Pang, Alicia Parrish, Nitish Joshi, Nikita Nangia, Jason Phang, Angelica Chen, Vishakh Padmakumar, Johnny Ma, Jana Thompson, He\u00a0He, and Samuel Bowman. 2022.\n\n\nQuALITY: Question answering with long input texts, yes!\n\nIn Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 5336\u20135358, Seattle, United States. Association for Computational Linguistics.\n\n\n",
      "Prasad et\u00a0al. (2023)": "\nPrasad et\u00a0al. (2023)\n\nArchiki Prasad, Trung Bui, Seunghyun Yoon, Hanieh Deilamsalehy, Franck Dernoncourt, and Mohit Bansal. 2023.\n\n\nMeetingQA: Extractive question-answering on meeting transcripts.\n\n\nIn Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 15000\u201315025, Toronto, Canada. Association for Computational Linguistics.\n\n\n",
      "Qian et\u00a0al. (2023)": "\nQian et\u00a0al. (2023)\n\nHongjing Qian, Yutao Zhu, Zhicheng Dou, Haoqi Gu, Xinyu Zhang, Zheng Liu, Ruofei Lai, Zhao Cao, Jian-Yun Nie, and Ji-Rong Wen. 2023.\n\n\nWebbrain: Learning to generate factually correct articles for queries by grounding on large web corpus.\n\n\nPreprint, arXiv:2304.04358.\n\n\n",
      "Rae et\u00a0al. (2019)": "\nRae et\u00a0al. (2019)\n\nJack\u00a0W. Rae, Anna Potapenko, Siddhant\u00a0M. Jayakumar, and Timothy\u00a0P. Lillicrap. 2019.\n\n\nCompressive transformers for long-range sequence modelling.\n\n\nPreprint, arXiv:1911.05507.\n\n\n",
      "Raffel et\u00a0al. (2020)": "\nRaffel et\u00a0al. (2020)\n\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter\u00a0J Liu. 2020.\n\n\nExploring the limits of transfer learning with a unified text-to-text transformer.\n\n\nJournal of machine learning research, 21(140):1\u201367.\n\n\n",
      "Reddy et\u00a0al. (2024)": "\nReddy et\u00a0al. (2024)\n\nVarshini Reddy, Rik Koncel-Kedziorski, Viet\u00a0Dac Lai, Michael Krumdick, Charles Lovering, and Chris Tanner. 2024.\n\n\nDocfinqa: A long-context financial reasoning dataset.\n\n\nPreprint, arXiv:2401.06915.\n\n\n",
      "Saunders et\u00a0al. (2022)": "\nSaunders et\u00a0al. (2022)\n\nWilliam Saunders, Catherine Yeh, Jeff Wu, Steven Bills, Long Ouyang, Jonathan Ward, and Jan Leike. 2022.\n\n\nSelf-critiquing models for assisting human evaluators.\n\n\nPreprint, arXiv:2206.05802.\n\n\n",
      "Shaham et\u00a0al. (2023)": "\nShaham et\u00a0al. (2023)\n\nUri Shaham, Maor Ivgi, Avia Efrat, Jonathan Berant, and Omer Levy. 2023.\n\n\nZeroSCROLLS: A zero-shot benchmark for long text understanding.\n\n\nIn Findings of the Association for Computational Linguistics: EMNLP 2023, pages 7977\u20137989, Singapore. Association for Computational Linguistics.\n\n\n",
      "Shaham et\u00a0al. (2022)": "\nShaham et\u00a0al. (2022)\n\nUri Shaham, Elad Segal, Maor Ivgi, Avia Efrat, Ori Yoran, Adi Haviv, Ankit Gupta, Wenhan Xiong, Mor Geva, Jonathan Berant, and Omer Levy. 2022.\n\n\nSCROLLS: Standardized CompaRison over long language sequences.\n\n\nIn Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 12007\u201312021, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.\n\n\n",
      "Sharma et\u00a0al. (2019)": "\nSharma et\u00a0al. (2019)\n\nEva Sharma, Chen Li, and Lu\u00a0Wang. 2019.\n\n\nBIGPATENT: A large-scale dataset for abstractive and coherent summarization.\n\n\nIn Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2204\u20132213, Florence, Italy. Association for Computational Linguistics.\n\n\n",
      "Stylianou et\u00a0al. (2021)": "\nStylianou et\u00a0al. (2021)\n\nNikolaos Stylianou, Panagiotis Kosmoliaptsis, and Ioannis Vlahavas. 2021.\n\n\nImproved biomedical entity recognition via longer context modeling.\n\n\nIn Artificial Intelligence Applications and Innovations: 17th IFIP WG 12.5 International Conference, AIAI 2021, Hersonissos, Crete, Greece, June 25\u201327, 2021, Proceedings 17, pages 45\u201356. Springer.\n\n\n",
      "Su et\u00a0al. (2024)": "\nSu et\u00a0al. (2024)\n\nJianlin Su, Murtadha Ahmed, Yu\u00a0Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. 2024.\n\n\nRoformer: Enhanced transformer with rotary position embedding.\n\n\nNeurocomputing, 568:127063.\n\n\n",
      "Takeshita et\u00a0al. (2024)": "\nTakeshita et\u00a0al. (2024)\n\nSotaro Takeshita, Tommaso Green, Ines Reinig, Kai Eckert, and Simone Ponzetto. 2024.\n\n\nACLSum: A new dataset for aspect-based summarization of scientific publications.\n\n\nIn Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 6660\u20136675, Mexico City, Mexico. Association for Computational Linguistics.\n\n\n",
      "Tay et\u00a0al. (2020)": "\nTay et\u00a0al. (2020)\n\nYi\u00a0Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler. 2020.\n\n\nLong range arena: A benchmark for efficient transformers.\n\n\nPreprint, arXiv:2011.04006.\n\n\n",
      "Trivedi et\u00a0al. (2022)": "\nTrivedi et\u00a0al. (2022)\n\nHarsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. 2022.\n\n\nMusique: Multihop questions via single-hop question composition.\n\n\nPreprint, arXiv:2108.00573.\n\n\n",
      "Tseng et\u00a0al. (2016)": "\nTseng et\u00a0al. (2016)\n\nBo-Hsiang Tseng, Sheng-Syun Shen, Hung-Yi Lee, and Lin-Shan Lee. 2016.\n\n\nTowards machine comprehension of spoken content: Initial toefl listening comprehension test by machine.\n\n\n",
      "Vaswani et\u00a0al. (2017)": "\nVaswani et\u00a0al. (2017)\n\nAshish Vaswani, Noam\u00a0M. Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan\u00a0N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017.\n\n\nAttention is all you need.\n\n\nIn Neural Information Processing Systems.\n\n\n",
      "Wang et\u00a0al. (2022)": "\nWang et\u00a0al. (2022)\n\nAlex Wang, Richard\u00a0Yuanzhe Pang, Angelica Chen, Jason Phang, and Samuel\u00a0R. Bowman. 2022.\n\n\nSQuALITY: Building a long-document summarization dataset the hard way.\n\n\nIn Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 1139\u20131156, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.\n\n\n",
      "Williams et\u00a0al. (2018)": "\nWilliams et\u00a0al. (2018)\n\nAdina Williams, Nikita Nangia, and Samuel Bowman. 2018.\n\n\nA broad-coverage challenge corpus for sentence understanding through inference.\n\n\nIn Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 1112\u20131122, New Orleans, Louisiana. Association for Computational Linguistics.\n\n\n",
      "Wu et\u00a0al. (2024)": "\nWu et\u00a0al. (2024)\n\nYunshu Wu, Hayate Iso, Pouya Pezeshkpour, Nikita Bhutani, and Estevam Hruschka. 2024.\n\n\nLess is more for long document summary evaluation by llms.\n\n\nPreprint, arXiv:2309.07382.\n\n\n",
      "Yang et\u00a0al. (2018)": "\nYang et\u00a0al. (2018)\n\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William\u00a0W. Cohen, Ruslan Salakhutdinov, and Christopher\u00a0D. Manning. 2018.\n\n\nHotpotqa: A dataset for diverse, explainable multi-hop question answering.\n\n\nPreprint, arXiv:1809.09600.\n\n\n",
      "Zhang et\u00a0al. (2024a)": "\nZhang et\u00a0al. (2024a)\n\nJiebin Zhang, Eugene\u00a0J. Yu, Qinyu Chen, Chenhao Xiong, Dawei Zhu, Han Qian, Mingbo Song, Xiaoguang Li, Qun Liu, and Sujian Li. 2024a.\n\n\nRetrieval-based full-length wikipedia generation for emergent events.\n\n\nPreprint, arXiv:2402.18264.\n\n\n",
      "Zhang et\u00a0al. (2024b)": "\nZhang et\u00a0al. (2024b)\n\nXinrong Zhang, Yingfa Chen, Shengding Hu, Zihang Xu, Junhao Chen, Moo\u00a0Khai Hao, Xu\u00a0Han, Zhen\u00a0Leng Thai, Shuo Wang, Zhiyuan Liu, and Maosong Sun. 2024b.\n\n\n\u221e\\infty\u221ebench: Extending long context evaluation beyond 100k tokens.\n\n\nPreprint, arXiv:2402.13718.\n\n\n",
      "Zhao et\u00a0al. (2022)": "\nZhao et\u00a0al. (2022)\n\nYilun Zhao, Yunxiang Li, Chenying Li, and Rui Zhang. 2022.\n\n\nMultiHiertt: Numerical reasoning over multi hierarchical tabular and textual data.\n\n\nIn Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 6588\u20136600, Dublin, Ireland. Association for Computational Linguistics.\n\n\n",
      "Zhao et\u00a0al. (2023)": "\nZhao et\u00a0al. (2023)\n\nYilun Zhao, Yitao Long, Hongjun Liu, Linyong Nan, Lyuhao Chen, Ryo Kamoi, Yixin Liu, Xiangru Tang, Rui Zhang, and Arman Cohan. 2023.\n\n\nDocmath-eval: Evaluating numerical reasoning capabilities of llms in understanding long documents with tabular data.\n\n\nArXiv, abs/2311.09805.\n\n\n",
      "Zhong et\u00a0al. (2021)": "\nZhong et\u00a0al. (2021)\n\nMing Zhong, Da\u00a0Yin, Tao Yu, Ahmad Zaidi, Mutethia Mutuma, Rahul Jha, Ahmed\u00a0Hassan Awadallah, Asli Celikyilmaz, Yang Liu, Xipeng Qiu, and Dragomir Radev. 2021.\n\n\nQMSum: A new benchmark for query-based multi-domain meeting summarization.\n\n\nIn Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 5905\u20135921, Online. Association for Computational Linguistics.\n\n\n",
      "Zhou et\u00a0al. (2023)": "\nZhou et\u00a0al. (2023)\n\nYijie Zhou, Kejian Shi, Wencai Zhang, Yixin Liu, Yilun Zhao, and Arman Cohan. 2023.\n\n\nOdsum: New benchmarks for open domain multi-document summarization.\n\n\nPreprint, arXiv:2309.08960.\n\n\n"
    },
    "domain": "cs.CL",
    "citation_count": 0
  },
  "ff8e36fc-3def-46a9-8ac0-b00b1235e134": {
    "pk": "ff8e36fc-3def-46a9-8ac0-b00b1235e134",
    "authors": [
      "Tianyu Gao",
      "Alexander Wettig",
      "Howard Yen",
      "Danqi Chen"
    ],
    "title": "How to Train Long-Context Language Models (Effectively)",
    "abstract": "We study continued training and supervised fine-tuning (SFT) of a language model (LM) to make effective use of long-context information. We first establish a reliable evaluation protocol to guide model development -- Instead of perplexity or simple needle-in-a-haystack (NIAH) tests, we use a broad set of long-context tasks, and we evaluate models after SFT with instruction data as this better reveals long-context abilities. Supported by our robust evaluations, we run thorough experiments to decide the data mix for continued pre-training, the instruction tuning dataset, and many other design choices. We find that (1) code repositories and books are excellent sources of long data, but it is crucial to combine them with high-quality short data; (2) training with a sequence length beyond the evaluation length boosts long-context performance; (3) for SFT, using only short instruction datasets yields strong performance on long-context tasks. Our final model, ProLong-8B, which is initialized from Llama-3 and trained on 40B tokens, demonstrates state-of-the-art long-context performance among similarly sized models at a length of 128K. ProLong outperforms Llama-3.18B-Instruct on the majority of long-context tasks despite having seen only 5% as many tokens during long-context training. Additionally, ProLong can effectively process up to 512K tokens, one of the longest context windows of publicly available LMs.",
    "url": "http://arxiv.org/abs/2410.02660v1",
    "timestamp": 1727974012,
    "sections": {
      "1 Introduction": "\n\n1 Introduction\n\nThe ability of language models (LMs) to process extremely long inputs (for example, 128K tokens) has enabled new applications, such as book summarization or learning new tasks on the fly from many examples. However, adapting\nLMs to process long contexts is challenging from an infrastructure and data perspective, and many design decisions are not well understood by open-source practitioners.\n\n\nWhile many works have focused on extending the context length of pre-trained LMs with minimal training (Chen et\u00a0al., 2023; Peng et\u00a0al., 2024),\nFu et\u00a0al. (2024) find that\nthe above methods cannot even perform the simple needle-in-a-haystack (NIAH; Kamradt, 2024) task\nand it is necessary to continually train the LM on long documents for billions of tokens. Frontier open-source models, such as Llama-3.1\u00a0(Dubey et\u00a0al., 2024) and Jamba\u00a0(Lenz et\u00a0al., 2024), also employ a long-context continued training stage, followed by supervised fine-tuning (SFT) on instruction data.\nWe adopt the same setting and study continued training and SFT of a pre-trained LM for effective long-context use.\n\n\nWe first establish a reliable evaluation protocol to provide a meaningful signal for model development.\nMost existing works rely on either perplexity or NIAH for ablating training recipes.\nWe demonstrate that neither is robust for guiding the development\nand opt for a broad range of downstream applications, such as retrieval-augmented generation (RAG), long-document summarization, and many-shot in-context learning (ICL).\nImportantly, we also conduct our evaluations after performing SFT,\neven for all our ablation runs on continued pre-training.\nWe observe that, on some long-context tasks, performance gains only emerge after SFT, which means that best design choices can differ before and after SFT.\n\n\nGuided by our evaluation protocol, we run comprehensive experiments with Llama-3-8B\u00a0(8K original context window; Dubey et\u00a0al., 2024) to study each component of long-context continued training, including data mixture, data and length scaling, supervised fine-tuning, and many other design choices such as cross-document attention masking and position extrapolation.\nMany of our findings are surprising or contradictory to existing claims, for example, (1) training only on long data hurts long-context performance,\n(2) training on longer sequences than the evaluation length helps, and (3) SFT on only short instruction data is sufficient for good long-context performance.\nWe outline our main takeaways and the structure of the paper in the takeaway box at the beginning of this section.\n\n\nOur final model, ProLong, achieves the best performance at a 128K context length among 10B-parameter models, while taking only 5%percent55\\%5 % of the data budget compared to Llama-3.1\u2019s long-context training (Dubey et\u00a0al., 2024).\nProLong has a maximum context length of 512K tokens, making it one of the longest-context LMs available.222Throughout the paper, we use binary prefixes K=210absentsuperscript210=2^{10}= 2 start_POSTSUPERSCRIPT 10 end_POSTSUPERSCRIPT, M=220superscript2202^{20}2 start_POSTSUPERSCRIPT 20 end_POSTSUPERSCRIPT, and B=230superscript2302^{30}2 start_POSTSUPERSCRIPT 30 end_POSTSUPERSCRIPT.\n\n",
      "2 Guide Model Development With Meaningful Evaluations": "\n\n2 Guide Model Development With Meaningful Evaluations\n\nA pre-requisite for training a strong LM is having a robust evaluation suite that can guide model development while tracking its utility in real-world applications.\nWhile synthetic benchmarks like needle-in-a-haystack (NIAH; Kamradt, 2024) and RULER (Hsieh et\u00a0al., 2024) have gained much popularity due to their simplicity and controllability, we are interested in a wider range of tasks that reflect practical usage, such as the ability to reason over the whole document.\nIn the following, we describe our evaluation protocols and\nshowcase why they are critical to our model development.\n\n\n\n2.1 Evaluate on diverse and realistic tasks\n\nWe first make the decision to use HELMET (Yen et\u00a0al., 2024b) as our evaluation suite,\nas it is one of the most comprehensive long-context benchmarks, covering the following tasks:\n\n\n\u2022\n\nRecall: Given a JSON file with random key-values pairs, retrieve the value for a key.\n\n\n\n\u2022\n\nRAG: Answer a question given retrieved Wikipedia documents (NQ, HotPotQA, PopQA).\n\n\n\n\u2022\n\nRe-ranking: Produce top-10 rankings from a shuffled list of documents (MSMARCO).\n\n\n\n\u2022\n\nICL: Learn classification tasks from many in-context examples, where the #classes ranges from 6 to 151; average of 5 datasets (TREC coarse/fine, NLU, Banking77, Clinc-150).\n\n\n\n\u2022\n\nQA: Answer a question given a full-length book (NarrativeQA).\n\n\n\n\u2022\n\nSummarization: Summarize long legal documents (Multi-LexSum).\n\n\n\n\n\nOverall, these diverse tasks reflect a range of long-context abilities including recall, reasoning, learning from context, and robustness to noisy inputs.\nYen et\u00a0al. (2024b) also show that HELMET produces model performance trends that are more consistent with human perceptions unlike other long-context benchmarks.\n\n\nWe showcase the importance of a robust evaluation suite in Table\u00a01.\nAs a predecessor of our work, Fu et\u00a0al. (2024) only consider needle-in-a-haystack (NIAH) and perplexity during model development; evaluations on 3 tasks from HELMET reveal major short-comings of their models despite perfect NIAH scores.\nWe also see how NIAH and even the HELMET recall task become saturated for strong models (Llama-3.1-8B vs. 70B) while other task categories continue to detect differences in their long-context abilities.\n\n\n\u2005\n\n\n\nTable 1: HELMET offers a more holistic long-context evaluation. We reproduce Fu et\u00a0al. (2024) on Llama-3-8B with SFT. We report the instruct Llama versions.\n\n\n\n\n\n\n\n\nHELMET\n\n\nModels\nNIAH\nRecall\nRAG\nRe-rank\n\n\nFu et\u00a0al. (2024)\n100\n95.8\n52.1\n23.1\n\n\nLlama-3.1-8B\n100\n99.4\n56.3\n37.0\n\n\nLlama-3.1-70B\n100\n100\n62.1\n49.2\n\n\n\n\n\n\nWe offer more details about the HELMET evaluation, including its careful choice of metrics, in \u00a7A.1.\nWe did not use all tasks from HELMET for efficiency reasons and we also do not want to overfit to HELMET.\nIf not otherwise specified, we average the performance for each category over all datasets and over evaluation lengths of 32K and 64K;\nfor the final long-context score, we macro-average all categories.\n\n\n\nWhy not perplexity?\n\nBesides synthetic recall tasks, many previous works rely on perplexity (PPL) for evaluating long-context extensions of LMs (Chen et\u00a0al., 2023; Fu et\u00a0al., 2024; Lu et\u00a0al., 2024), which is commonly measured on the PG19 books dataset\u00a0(Rae et\u00a0al., 2020).\nWe use the ablation experiment from \u00a73.2 to showcase why perplexity is not an indicative metric for developing long-context models.\nThe experiment studies how the ratio of long documents affects the performance. We report both our evaluation and the perplexity measured on the last 32K tokens of 64K-length documents from PG19.\nAs shown in Figure\u00a01, while using more long data continues to improve PPL,\nit is clear that using 100% long data significantly hurts downstream long-context performance.\n\n\n\n\n\u2005\n\n\n\n\n\n\nFigure 1: Making design decisions based on perplexity (PPL) is not optimal for long-context downstream tasks.\n\n\n\n\n\n\n2.2 Evaluate after supervised fine-tuning\n\nSupervised fine-tuning (SFT; Ouyang et\u00a0al., 2022) is an additional training stage that fine-tunes the model on a small amount of natural-language instructions and corresponding responses; it enables a base LM to address user queries in a chat format and has become a standard step for producing frontier LMs.\nHere, we consider the difference between evaluating a model before or after SFT.\n\n\nIn preliminary experiments, we continue training Llama-3-8B-Base on 5B-token subsets from the data mix by Fu et\u00a0al. (2024). The mix is based on SlimPajama (Soboleva et\u00a0al., 2023) and upsamples long documents to constitute roughly 70% of tokens, while retaining the original domain proportions.\nThen we conduct SFT on several intermediate checkpoints with UltraChat\u00a0(Ding et\u00a0al., 2023).\n\n\nWe show the benchmarking results before and after SFT in Figure\u00a02.\nLong-context evaluation shows clearer signals when it is conducted after SFT:\n(1) SFT shows that the model continues to improve with more training tokens on RAG and re-ranking, while the improvement is less clear or does not exist when evaluated before SFT.\n(2) SFT enables evaluation on realistic applications like QA and summarization, which require instruction following and have low performance before SFT.\nWe also note that the variance from two random training runs is not substantially higher after the additional SFT phase. Therefore, unless otherwise specified, we report the long-context performance after SFT.\n\n\nFigure 2: \nImprovements on RAG and re-ranking tasks are only observed when evaluating models after a supervised fine-tuning (SFT) phase on instruction data. The models are trained on the pre-training data mix by Fu et\u00a0al. (2024). We report the mean and standard deviations over two training runs. \n\n\nWe dive deeper into supervised fine-tuning in \u00a75 and explore different training datasets, as well as the use of synthetic long instruction data. However, we find that simply fine-tuning on UltraChat remains a surprisingly competitive choice.\n\n\n\n\n2.3 Check that short-context performance is preserved\n\nLong-context abilities should not come at the expense of short-context performance, particularly since short-context evaluations cover a wider range of capabilities, e.g., world knowledge, commonsense, and mathematical reasoning.\nHowever, short-context evaluation has largely been neglected by previous long-context research.\nWe report on 5 tasks from the the Open LLM Leaderboard (Beeching et\u00a0al., 2023): HellaSwag (Zellers et\u00a0al., 2019), MMLU (Hendrycks et\u00a0al., 2021), ARC-challenge (Clark et\u00a0al., 2018), WinoGrande (Sakaguchi et\u00a0al., 2021), and GSM8K (Cobbe et\u00a0al., 2021).\nWe evaluate short-context performance before SFT, since this allows for a direct comparison to the base model which was used as initialization for the long-context training.\n\n\n\nPrevious techniques deteriorate short-context performance.\n\nWe show in Table\u00a02 that both training-free position extrapolation, as well as fine-tuning with an existing long data mixture (Fu et\u00a0al., 2024)\ndo not preserve the strong performance of Llama-3-8B on standard short-context tasks.\nThis motivates us to find data sources which retain the initial model\u2019s strong short-context performance.\n\n\n\n\n\u2005\n\n\n\nTable 2: \nApplying position extrapolation (PE) to\nLlama-3-8B by changing the RoPE frequency base (\u00a7B.1)\nor fine-tuning it\non a long-context SlimPajama mixture (Fu et\u00a0al., 2024; Soboleva et\u00a0al., 2023) deteriorates the performance of this top-shelf pre-trained LM on short-context tasks.\n\n\n\n\n\n\n\nHSwag\nMMLU\nARC-c\nWG\nGSM8K\n\n\n\n\nLlama-3-8B\n82.1\n66.5\n59.4\n77.1\n44.7\n\n\n+ PE\n81.5\n64.7\n58.1\n75.5\n40.1\n\n\n+ SlimPajama\n81.0\n63.1\n57.8\n75.1\n40.6\n\n\n\n\n\n\n",
      "3 Long-Context Data Curation": "\n\n3 Long-Context Data Curation\n\nThe quality and composition of training data has been found to be the most important factor for LM pre-training (Penedo et\u00a0al., 2023; Wettig et\u00a0al., 2024; Li et\u00a0al., 2024a) and is therefore\na primary focus of our study. To make data decisions, we perform ablation experiments:\nwe continue to train Llama-3-8B-Base for 5B tokens with a maximum length of 64K tokens and evaluate according to \u00a72. See \u00a7A.4 for more details of our ablation setting.\n\n\nWe aim to boost the long-context task performance while preserving the short-context performance of the original model. Starting from the intuition that the data should be a mixture of long and short documents, we study these choices separately.\nIn our ablations, the long data is comprised of single-document chunks of 64K tokens, whereas for the short data, we construct batches by packing documents until we reach 64K tokens per sequence.\n\n\n\n3.1 Code repositories and books are good sources of long-context data\n\n\nSlimPajama.\n\nWe analyze the quantity of long data in SlimPajama (SP; Soboleva et\u00a0al., 2023). Table\u00a03 shows that books account for the majority of long-context tokens.\nWhen inspecting the long data in CommonCrawl (CC), we observe that though varied in quality, it also contains some book-like content, which future work could identify via data selection methods.\n\n\n\nCode repositories.\n\nWhile only few files from GitHub reach a very long length (which also tend to be lower quality as suggested by Singh et\u00a0al., 2024), we construct an abundant source of long-context data from the Stack (Kocetkov et\u00a0al., 2023) by concatenating all files from a repository to form a single document. Unlike Guo et\u00a0al. (2024), we do not order the files based on dependencies, which should increase the distance between dependent files and reduce recency bias.\n\n\n\n\n\u2005\n\n\n\nTable 3: Long text documents (\u2265\\geq\u226564K tokens) by data sources.\n\n\n\n\n\nData\n#Long\n\n\ntokens\n\n\nCode Repos\n98.8B\n\n\nSP/Books\n33.2B\n\n\nSP/CC\n15.3B\n\n\nSP/Arxiv\n5.2B\n\n\nSP/GitHub\n2.8B\n\n\nSP/Wiki\n0.1B\n\n\nSP/StackEx\n\n<<<0.1B\n\n\nSP/C4\n\n<<<0.1B\n\n\n\n\n\n\nData mixture.\n\nWe train models with 60% of long-context data and 40% of our ShortMix (\u00a73.3).\nTable\u00a04 shows that using code repositories alone performs the best on stress-test recall tasks. Meanwhile, books are more broadly beneficial for in-context learning, summarization and re-ranking.\nAn equal mix of books and code repositories achieves the best overall performance.\nNote that short-context task performance remains consistent due to our high-quality short data mix.\n\n\nTable 4: Impact of different long data sources, while keeping the 40% short data component fixed. Code repositories particularly helps recall, while books are more effective on re-ranking, ICL, and summarization. Mixing the two achieves the overall best performance.\n\n\n\nLong Data (60%)\nLong-Context\nShort-Context\n\n\nRecall\nRAG\nRe-rank\nICL\nQA\nSumm.\nAvg.\nAvg.\n\n\n\n\nCommonCrawl\n84.1\n53.3\n28.1\n67.5\n35.2\n37.0\n50.9\n66.5\n\n\nBooks\n94.9\n53.9\n30.7\n72.2\n33.2\n37.7\n53.8\n65.5\n\n\nCode Repos\n99.2\n53.8\n29.0\n61.2\n34.7\n36.2\n52.3\n65.9\n\n\nBooks/Repos 1:1\n96.0\n54.9\n29.4\n73.9\n35.7\n37.9\n54.6\n65.5\n\n\n\n\n\n\n\n\n3.2 Training only on long data hurts long-context performance\n\nThe ratio between short/long data is another crucial factor for downstream performance.\nPrior work either trains only on long data (Peng et\u00a0al., 2024) or adds some short training data (Yen et\u00a0al., 2024a; Fu et\u00a0al., 2024).\nHowever, we are the first to systematically study the impact of short/long ratio.\n\n\nFigure\u00a03 shows that\nshort task performance monotonically decreases as the long data increases.\nThe trends for long-context vary by tasks and are further complicated by SFT:\nOn tasks like recall and RAG, the performance before SFT prefers high proportions of long data, while the performance after SFT drastically deteriorates with more long data.\nWe hypothesize that specializing the model only on long data makes it a poor initialization for generic SFT\u2014highlighting the importance of evaluating checkpoints after SFT (\u00a72.2).\nWhile some long-context tasks benefit from more long data consistently (ICL) or show no clear pattern (re-ranking), the best average performance is achieved at 60% long data and 40% short data, which we adopt for our final ProLong model.\n\n\nFigure 3: \nImpact of short/long data ratio.\nAll models are trained on books/repos long data and our ShortMix for 5B tokens.\nMore long data initially improves long-context performance, but then\nbecomes impairing. More long data also consistently degrades the short-context performance.\n\n\n\n\n\n3.3 Choosing a high-quality short-context mix is important\n\nWe saw in \u00a72.3 that it is difficult to preserve the strong performance of Llama-3-8B on short-context tasks during long-context fine-tuning.\nWe adopt our best long-context settings (Book/repo data and 60% long/40% short) and study the impact of different short-context training mixes. We experiment with SlimPajama (Soboleva et\u00a0al., 2023), FineWeb-Edu (Penedo et\u00a0al., 2024), DCLM-Baseline (Li et\u00a0al., 2024a), and our own ProLong ShortMix.\nOur ShortMix is inspired by the \u201cstage 2 training\u201d in MiniCPM\u00a0(Hu et\u00a0al., 2024a) and Dolma-1.7 (Soldaini et\u00a0al., 2024), which use more knowledge-intensive, downstream-related data at the end of pre-training.\nTable\u00a05 shows the composition of our ShortMix.333\nSince we do not truncate documents in the short data component unnecessarily, it includes a small percentage of documents longer than 8K. See Table\u00a014 in the appendix for the dataset length statistics.\n\n\n\u2005\n\n\n\nTable 5: Our ShortMix.\n\n\n\n\n\nComponents\n%\n\n\n\n\nFineWeb\n25\n\n\nFineWeb-Edu\n25\n\n\nWikipedia\n10\n\n\nTulu-v2\n10\n\n\nStackExchange\n10\n\n\nArXiv\n10\n\n\nOpenWebMath\n10\n\n\n\n\n\n\nTable 6: Impact of\ndifferent short data sources.\nThe long-context performance is the average of 6 categories at the lengths of 32K and 64K.\n\n\n\n\nShort Data (40%)\nLong-Context\nShort-Context\n\n\nAvg.\nHellaS.\nMMLU\nARC-c\nWG\nGSM8K\nAvg.\n\n\nOriginal model (Llama-3-8B)\n-\n82.1\n66.5\n59.4\n77.1\n44.7\n66.0\n\n\n\n\nSlimPajama\n52.9\n81.2\n63.0\n58.5\n76.2\n41.9\n64.2\n\n\nFineWeb-Edu\n53.0\n81.0\n62.6\n57.7\n74.4\n39.4\n63.0\n\n\nDCLM-Baseline\n52.0\n82.0\n65.6\n59.6\n77.4\n39.4\n64.8\n\n\nProLong ShortMix\n54.6\n81.6\n65.3\n58.0\n76.2\n46.6\n65.5\n\n\n\n\n\nTable\u00a06 demonstrates that the short data component has a substantial impact on both short-context and long-context downstream performance.\nOur curated ShortMix outperforms other short data sources on both short and long-context tasks and our data domains are particularly important for retaining Llama-3-8B\u2019s performance on mathematical reasoning.\nSurprisingly, we find that fine-tuning only using FineWeb-Edu\u2014a dataset that is curated to help with knowledge-intensive tasks like MMLU\u2014performs poorly as a short-context component, and we combine it with more diverse data sources in our ShortMix.\nDCLM-Baseline performs well on all short-context tasks except for GSM8K.\nThis can likely be improved by combining with math-related datasets, but as we added the DCLM-baseline ablation at the conclusion of the project, we leave this exploration to future work.\n\n\n",
      "4 Scaling the Size and Length of the Training Data": "\n\n4 Scaling the Size and Length of the Training Data\n\nTraining for more steps is well-known to improve downstream tasks in regular pre-training, but little analysis has been done in the context of long-context continued training.\nWe incorporate the lessons from our ablation experiments and arrive at the ProLong recipe, which we describe in detail in \u00a76. Notably, we scale up the training budget to longer sequences (up to 512K) and more tokens (20B tokens at a maximum sequence length of 64K and an additional 20B tokens at 512K). We reset the learning rate schedule and increase the RoPE frequency base when switching from 64K to 512K context lengths.\nIn this section, we analyze the impact of these decisions.\n\n\nIncreasing the number of steps helps.\n\nIn Figure\u00a04, we plot the downstream performance of intermediate checkpoints of our 40B-token runs.\nWhile the long-context performance fluctuates throughout training, we observe positive trends on recall, RAG, re-ranking, and summarization.\nFor short-context tasks, we observe the average performance initially drops from the initialization, but gradually recovers. Performance again drops when switching from 64K to 512K sequence length, but also recovers with additional training.\n\n\nFigure 4: Performance (avg. of 32K and 64K) of our ProLong model throughout training.\n\n\n\n\nIncreasing the training length beyond the evaluation length helps.\n\nOne might assume that we should train long-context models on the maximum sequence length that we want the model to support. Many works even emphasize extrapolation to even longer sequences at inference time (Press et\u00a0al., 2022; Xiao et\u00a0al., 2024b; a; Yen et\u00a0al., 2024a; Chen et\u00a0al., 2023).\nIn contrast, we observe that training on a longer sequence length (512K tokens) substantially improves the long-context performance at a shorter evaluation length (64K tokens).\n\n\nWe establish this by initializing\nwith a model that was trained for 20B tokens at 64K and either (1) continuing training at 64K, or (2) switching to the 512K training.\nWe use the same hyperparameters and data mixtures in either experiment.\nWe evaluate a checkpoint after 4B training tokens\nat a evaluation length of 64K.\nComparing the two runs\nin Table\u00a07,\nwe see consistent gains from switching to the 512K training length.444While we demonstrate the benefit of longer data, we note that training with longer sequences is more expensive, and may therefore not be the computationally optimal choice.\n\n\nTable 7: Impact of training models on different sequence lengths.\nAll the results are evaluated at a sequence length of 64K.\nWe see that training at a maximum length beyond the evaluation context window consistently improves the long-context performance.\n\n\n\n\nMax Seq. Length\nRecall\nRAG\nRe-rank\nICL\n\n\n\n\nProLong 64K training (20B)\n96.5\n52.7\n22.8\n70.6\n\n\n+4B 64K training\n95.0\n56.4\n28.0\n78.8\n\n\n+4B 512K training\n98.5\n56.9\n32.9\n79.2\n\n\n\n\n\n",
      "5 Supervised Fine-Tuning for Long-Context LMs": "\n\n5 Supervised Fine-Tuning for Long-Context LMs\n\nIn this section, we study how to best enable long-context language models to follow instructions.\nWe focus on supervised fine-tuning on instruction datasets (Ouyang et\u00a0al., 2022) and\nleave reinforcement learning and preference optimization for future work.\n\n\nAll our experiments in this section use the ProLong base model, which was\ntrained for\n40B tokens at a maximum sequence length of 512K.\nIn comparison, open-source instruction data are very short, e.g.,\nUltraChat (Ding et\u00a0al., 2023) conversations have 1.2K tokens on average and 4.1K tokens maximum.\nTo bridge this gap, several works\u00a0(Xiong et\u00a0al., 2023; Dubey et\u00a0al., 2024; Xiong et\u00a0al., 2024)\nhave proposed to generate long instruction data synthetically.\n\n\nWe consider three popular SFT datasets\u2014UltraChat\u00a0(Ding et\u00a0al., 2023), Tulu-v2\u00a0(Ivison et\u00a0al., 2023), ShareGPT555https://huggingface.co/datasets/RyokoAI/ShareGPT52K.\u2014and three sources of synthetic data: For synthetic QA, we prompt Llama-3-8B-Instruct\nto generate a question-and-answer pair given a random chunk from a long document; we reuse the QA pairs for synthetic RAG but we present\na random list of chunks from the document to mimic retrieved passages; for synthetic summarization, we generate summaries for long books via recursive summarization (Wu et\u00a0al., 2021).\nFor all synthetic data, we write several templates, which we sample at random to increase diversity. More details can be found in \u00a7A.5.\nWe always use a combination of 40% synthetic QA, 30% synthetic RAG, and 30% synthetic summarization in our synthetic instruction dataset.\nThe hyperparameters for the instruction tuning experiments can be found in Table\u00a09.\n\n\nShort-context instruction data yields strong long-context results.\n\nWe first establish that UltraChat outperforms Tulu-v2 and ShareGPT\nin Table\u00a022.\nWe therefore use it when studying the ratio of synthetic long-context instruction data in Table\u00a08.\nSurprisingly, we find that adding synthetic data does not improve the performance on these very long-context tasks, and adding even as little as 1% synthetic data hurts the performance in our setting.\nTherefore, we use only short-context UltraChat data for SFT of our final ProLong model.\n\n\nTable 8: Effect of different ratios of synthetic SFT data (mixed with UltraChat). We report the 32K-and-64K-averaged performance except tasks marked with \u2020, which are evaluated at 512K for stress testing. The number of percentage is based on #tokens, not #samples.\n\n\n\n% Synthetic Data\nJsonKV\u2020\n\nRAG\nRe-rank\nICL\nQA\u2020\n\nSumm.\u2020\n\nAvg.\n\n\n\n\n0%\n65.7\n58.1\n38.5\n80.3\n49.7\n42.1\n55.7\n\n\n1%\n61.5\n57.0\n38.3\n80.8\n45.3\n41.5\n54.1\n\n\n3%\n62.0\n56.4\n37.9\n80.6\n44.8\n39.5\n53.5\n\n\n10%\n70.3\n55.5\n36.1\n80.6\n41.7\n39.4\n53.9\n\n\n50%\n45.8\n48.8\n18.8\n70.5\n42.3\n33.3\n43.3\n\n\n\n\n\nWhy do our conclusions about synthetic data differ from previous work?\nWe offer the following hypotheses:\n(1) Previous work like Xiong et\u00a0al. (2024); Bai et\u00a0al. (2024a) may have insufficient long-context training and the synthetic data acts as additional long-context training data.\n(2) Our instruction dataset is much smaller compared to the private instruction data used for Llama-3.1 (Dubey et\u00a0al., 2024)\u2014it is possible that when using an extensive short instruction dataset, mixing in synthetic long data avoids the model from degenerating on long-context tasks.\n\n\nTable 9: The training recipe for ProLong.\n\n\n\nContinued Long-context Training\n\n\nData\n30% code repos, 30% books, 3% textbooks, 37% ShortMix\n\n\n\nShortMix:\n27% FineWeb-Edu, 27% FineWeb,\n\n\n\n\n11% Tulu-v2, 11% StackExchange,\n\n\n\n\n8% Wikipedia, 8% OpenWebMath, 8% ArXiv\n\n\n\n\nLength\nCurriculum\n\nStage 1 (64K):\nCode repos, books, and textbooks at length 64K\n\n\nStage 2 (512K):\nCode repos: 50% at length 512K, 50% at length 64K\n\n\n\n\nBooks: 17% at length 512K, 83% at length 64K\n\n\n\n\nTextbooks at length 512K\n\n\nSteps\nStage 1: 20B tokens (2.2K H100 hours), \u2005 Stage 2: 20B tokens (12.2K H100 hours)\n\n\nModel\nInitialization:\nLlama-3-8B-Instruct (original RoPE base freq. 5\u00d71055superscript1055\\times 10^{5}5 \u00d7 10 start_POSTSUPERSCRIPT 5 end_POSTSUPERSCRIPT)\n\n\n\nRoPE:\nStage 1: 8\u00d71068superscript1068\\times 10^{6}8 \u00d7 10 start_POSTSUPERSCRIPT 6 end_POSTSUPERSCRIPT, Stage 2: \u2005 1.28\u00d71081.28superscript1081.28\\times 10^{8}1.28 \u00d7 10 start_POSTSUPERSCRIPT 8 end_POSTSUPERSCRIPT\n\n\n\n\nAttention:\nFull attention with cross-document attention masking\n\n\nOptim.\nAdamW (weight decay = 0.10.10.10.1, \u03b21=0.9subscript\ud835\udefd10.9\\beta_{1}=0.9italic_\u03b2 start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT = 0.9, \u03b22=0.95subscript\ud835\udefd20.95\\beta_{2}=0.95italic_\u03b2 start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT = 0.95)\n\n\n\nLR:\n\n1\u2062e\u221251\ud835\udc5251e-51 italic_e - 5 with 10%percent1010\\%10 % warmup and cosine decay to 1\u2062e\u221261\ud835\udc5261e-61 italic_e - 6, each stage\n\n\n\nBatch size:\n4M tokens for stage 1, 8M tokens for stage 2\n\n\nSupervised Fine-tuning (SFT)\n\n\nData\nUltraChat\n\n\n\nSteps\n1B tokens\n\n\n\nOptim.\nAdamW (weight decay = 0.10.10.10.1, \u03b21=0.9subscript\ud835\udefd10.9\\beta_{1}=0.9italic_\u03b2 start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT = 0.9, \u03b22=0.95subscript\ud835\udefd20.95\\beta_{2}=0.95italic_\u03b2 start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT = 0.95)\n\n\n\nLR = 2\u2062e\u221252\ud835\udc5252e-52 italic_e - 5 (cosine decay to 2\u2062e\u221262\ud835\udc5262e-62 italic_e - 6), warmup = 5%percent55\\%5 %\n\n\n\n\nBatch size = 4M tokens\n\n\n\n\n\n",
      "6 The ProLong Model: Recipe and Results": "\n\n6 The ProLong Model: Recipe and Results\n\n\n6.1 Final recipe\n\nWe summarize the training recipe for ProLong in Table\u00a09.\nOur final model starts from the Llama-3-8B-Instruct model and is trained on 64K sequence length for 20B tokens.\nIt is then further trained on 512K sequence length for 20B tokens (ProLong base), which we achieve using sequence parallelism (Li et\u00a0al., 2023).\nWe obtain the final ProLong model via SFT of the base model on UltraChat.\nOne small difference on the data mixture between our ablations and the final model is that we mix in 3% high-quality textbooks\u00a0(Chevalier et\u00a0al., 2024), as book-like data are shown to be beneficial for long-context (\u00a73.1) and textbooks are highly educational. This also slightly changes the proportions of ShortMix.\nYou can find more details about our data processing (\u00a7A.2) and the training stack (\u00a7A.3) in the appendix.\n\n\nIn the following, we elaborate on several carefully ablated design choices in our recipe.\n\n\nRoPE frequency base tuning.\n\nWe find that changing the RoPE\u00a0(Su et\u00a0al., 2021) frequency base to achieve position extrapolation\u00a0(Xiong et\u00a0al., 2023; emozilla, 2023) significantly improves long-context performance, even with a significant amount of training. \u00a7B.1 shows our ablation on the best RoPE base to use. While the original Llama models use a RoPE base of 105superscript10510^{5}10 start_POSTSUPERSCRIPT 5 end_POSTSUPERSCRIPT, we use a base of 8\u00d71068superscript1068\\times 10^{6}8 \u00d7 10 start_POSTSUPERSCRIPT 6 end_POSTSUPERSCRIPT for the 64K setting and 1.28\u00d71081.28superscript1081.28\\times 10^{8}1.28 \u00d7 10 start_POSTSUPERSCRIPT 8 end_POSTSUPERSCRIPT for the 512K setting.\n\n\n\nDisabling cross-document attention.\n\nDing et\u00a0al. (2024a) show that masking out attention across document boundaries improve model performance and this was also used during Llama-3 pre-training (Dubey et\u00a0al., 2024).\nIn \u00a7B.2, we show that disabling cross-document attention in continued training benefits both the short and long-context performance.\nDisabling cross-document attention can also result in higher training throughput, which we describe in more detail in \u00a7A.3.\n\n\n\nStarting from Llama-3-8B-Instruct.\n\nWhile we conduct all our long-context training ablations with the base model of Llama-3-8B,\nwe use Llama-3-8B-Instruct as the initialization for the final ProLong model.\n\u00a7B.3 shows that while slightly improving the long-context performance, Llama-3-8B-Instruct significantly enhances the short-context performance.\n\n\n\n\n\n6.2 ProLong performance\n\nWe present the final HELMET evaluation results of ProLong in Table\u00a010.\nWe compare to a number of frontier long-context LMs, namely MegaBeam666https://huggingface.co/aws-prototyping/MegaBeam-Mistral-7B-512k., Llama-3.1\u00a0(Dubey et\u00a0al., 2024), Qwen2\u00a0(Yang et\u00a0al., 2024a), Phi-3\u00a0(Abdin et\u00a0al., 2024), Mistral-Nemo777https://huggingface.co/mistralai/Mistral-Nemo-Instruct-2407., Jamba-1.5\u00a0(Lenz et\u00a0al., 2024), Claude-3.5-Sonnet\u00a0(Anthropic, 2024), Gemini-1.5\u00a0(Reid et\u00a0al., 2024), and GPT-4o\u00a0(Achiam et\u00a0al., 2023).\n\n\nProLong outperforms all 10B-scale models on our long-context evaluation. Notably, ProLong outperforms Llama-3.1-8B-Instruct\non all categories except summarization.\nProLong achieves this with only 5% of Llama-3.1\u2019s long-context data budget (40B vs. 800B tokens).\nWe also showcase the strength of ProLong with several QA examples in Table\u00a023.\n\n\nTable 10: Our main evaluation results on HELMET (Yen et\u00a0al., 2024b; details in \u00a7A.1).\nAll results are averaged over sequence lengths of 32K, 64K, and 128K. For all models, we use the corresponding instruction version.\nProLong is one of the best performing 10B-scale LMs.\nThe complete set of results can be found in \u00a7C.\n\n\n\n\nModel\nMax Len.\nRecall\nRAG\nICL\nRe-rank\nQA\nSumm.\nAvg.\n\n\nProLong (8B)\n512K\n99.4\n66.0\n81.1\n33.2\n40.8\n40.5\n60.2\n\n\nMegaBeam-Mistral (7B)\n512K\n99.4\n58.1\n82.1\n22.1\n33.7\n43.6\n56.5\n\n\nMeta-Llama-3.1 (8B)\n128K\n98.7\n62.8\n79.7\n26.6\n40.4\n46.1\n59.0\n\n\nQwen2 (7B)\n128K\n34.4\n43.4\n54.8\n4.6\n23.3\n38.5\n33.2\n\n\nPhi-3-small (7B)\n128K\n74.8\n60.6\n82.0\n18.5\n34.1\n42.4\n52.1\n\n\nMistral-Nemo (12B)\n128K\n24.9\n48.1\n82.0\n4.7\n37.7\n37.0\n39.1\n\n\nJamba-1.5-Mini (12B/52B)\n256K\n87.7\n61.3\n88.4\n25.9\n42.0\n38.6\n57.3\n\n\nMeta-Llama-3.1 (70B)\n128K\n98.5\n65.9\n80.0\n39.4\n47.2\n51.1\n63.7\n\n\nClaude-3.5-Sonnet\n200K\n99.4\n44.0\n79.3\n19.9\n38.1\n49.2\n55.0\n\n\nGemini-1.5-Pro\n2M\n94.2\n71.4\n78.9\n65.3\n44.4\n56.2\n68.4\n\n\nGPT-4o\n128K\n99.9\n71.5\n86.7\n59.6\n47.0\n55.7\n70.1\n\n\n\n\n\nSince most existing models do not support more than 128K tokens,\nto showcase ProLong\u2019s 512K context length,\nwe stress test ProLong on the QA and summarization tasks from 32K to 512K888\nIn QA and summarization, we truncate the documents at the evaluation length; hence an effective long-context model should demonstrate better performance on longer lengths.. Table\u00a011 shows that ProLong continues to improve at a longer context window.\n\n\n\u2005\n\n\n\nTable 11: ProLong at 512K.\n\n\n\n\n\n\n32K\n64K\n128K\n512K\n\n\n\n\nQA\n31.7\n43.7\n46.7\n49.7\n\n\nSumm\n40.4\n39.8\n41.5\n42.1\n\n\n\n\n\n\nTable 12: Results\non the NoCha benchmark\u00a0(Karpinska et\u00a0al., 2024).101010https://github.com/marzenakrp/nocha. NoCha has a private test set and all evaluation is done by the NoCha authors. Hence, we report models from Table\u00a010 that are also on the NoCha leaderboard.\nProLong is the only model that achieves above-random performance in the <<<75K category and it consistently beats Llama-3.1. Different from the original NoCha leaderboard, we report the average accuracy over all test instances without filtering the test examples based on the model\u2019s context window lengths.\n\n\n\nModel\nMax Len.\n\n<<<75K\n75K-127K\n127K-180K\n\n>>>180K\n\n\n\n\nProLong (8B)\n512K\n28.4\n17.0\n13.1\n20.3\n\n\nMegaBeam-Mistral (7B)\n512K\n19.8\n18.3\n17.5\n15.6\n\n\nMeta-Llama-3.1 (8B)\n128K\n17.3\n16.4\n0.0\n0.0\n\n\nMistral-Nemo (12B)\n128K\n13.6\n0.4\n0.0\n0.0\n\n\nJamba-1.5-Mini (12B/52B)\n256K\n27.2\n28.0\n24.4\n6.2\n\n\nMeta-Llama-3.1 (70B)\n128K\n42.0\n25.0\n0.0\n0.0\n\n\nGemini-1.5-Pro\n2M\n24.7\n38.8\n35.3\n46.9\n\n\nGPT-4o\n128K\n55.6\n58.4\n0.0\n0.0\n\n\n\n\n\nBesides HELMET, we also evaluate our models on NoCha\u00a0(Karpinska et\u00a0al., 2024)\u2014a claim verification dataset on 67 recently published English fictional books.\nWe chose this dataset because (1) it minimizes the data contamination problem as all the books are unlikely to exist in the model pre-training data; (2) all the claims are written by human readers and require global reasoning. Each test instance contains two contradictory claims, and the models must correctly judge both to pass.\n\n\nfootnote\u00a010 demonstrates the NoCha evaluation results.\nAmong 10B-scale models, ProLong achieves the best accuracy on the extremely long test instances (>>>180K); on test instances <<<75K tokens, ProLong significantly outperforms other models and is the only model that is better than random guessing (25%).\nThis further showcases the strength of our training recipe and the ProLong model.\n\n\n",
      "7 Related Work": "\n\n7 Related Work\n\nAdapting existing LMs for long contexts.\n\nMany works explore\nextending the LM context windows with minimal training,\neither by\nposition extrapolation\u00a0(Chen et\u00a0al., 2023; Peng et\u00a0al., 2024; Chen et\u00a0al., 2024; Ding et\u00a0al., 2024b; Liu et\u00a0al., 2024a; Zhang et\u00a0al., 2024b; Zhu et\u00a0al., 2024; Zhao et\u00a0al., 2024; Wu et\u00a0al., 2024; Hu et\u00a0al., 2024b)\nor\nmanipulating the attention patterns\u00a0(Chen et\u00a0al., 2024; Xiao et\u00a0al., 2024b; a; Bertsch et\u00a0al., 2023; Jin et\u00a0al., 2024).\nYoshida et\u00a0al. (2020); Choromanski et\u00a0al. (2021); Chevalier et\u00a0al. (2023)\ninstead explore the idea of compressing the long contexts into shorter forms.\nHowever, Fu et\u00a0al. (2024); Lu et\u00a0al. (2024) show that using full attention, applying simple position extrapolation, and fine-tuning the model on long documents reach much stronger results.\n\n\nLlama 3.1\u00a0(Dubey et\u00a0al., 2024) and Jamba\u00a0(Lieber et\u00a0al., 2024) achieve long-context capabilities by adding a long-context continued training stage between standard pre-training and supervised fine-tuning, which is the setting we follow.\nFu et\u00a0al. (2024) study the data engineering for this setting and argue that 0.5B tokens of domain-balanced, length-upsampled data is sufficient for acquiring the long-context recall ability\u2014which we show is not sufficient if a more holistic evaluation is taken.\nXiong et\u00a0al. (2023); Dubey et\u00a0al. (2024); Lieber et\u00a0al. (2024); Xiong et\u00a0al. (2024); An et\u00a0al. (2024b); Bai et\u00a0al. (2024a) also adopt synthetically-generated long data in the SFT stage; however,\nwe find that\nusing standard, short-context instruction data achieves the best long-context results in our setting.\n\n\n\nEfficient long-context architectures.\n\nThere have been many efforts in designing more efficient architectures,\nfor example,\nlinear attention/RNNs\u00a0(Gu & Dao, 2023; Dao & Gu, 2024; Ma et\u00a0al., 2022; Sun et\u00a0al., 2023; Peng et\u00a0al., 2023; Yang et\u00a0al., 2024b),\nand\nalternative attention architectures\u00a0(Rubin & Berant, 2023; Sun et\u00a0al., 2024; Yen et\u00a0al., 2024a).\nHowever,\nthey often require training from scratch and many have the inherent limitations in terms of long-context recall (Jelassi et\u00a0al., 2024; Arora et\u00a0al., 2024).\nRecent works explore hybrid models (Waleffe et\u00a0al., 2024; Lieber et\u00a0al., 2024)) or distilling existing LMs into hybrid models (Wang et\u00a0al., 2024) and show promising results.\n\n\n\nLong-context evaluation.\n\nMany benchmarks have been proposed for long-context evaluation\n(Shaham et\u00a0al., 2023; Hsieh et\u00a0al., 2024; Krishna et\u00a0al., 2023; Zhang et\u00a0al., 2024a; An et\u00a0al., 2024a; Bai et\u00a0al., 2024b)\nThere are works studying particular aspects of long-context LMs as well,\nsuch as positional bias (Liu et\u00a0al., 2024b),\nin-context learning\u00a0(Bertsch et\u00a0al., 2024; Li et\u00a0al., 2024b), and book-length summarization\u00a0(Kim et\u00a0al., 2024).\nIn this work, we follow Yen et\u00a0al. (2024b)\nfor its diverse application coverage and reliable evaluations.\n\n\n",
      "8 Conclusion": "\n\n8 Conclusion\n\nWe study the problem of given a short-context pre-trained LM, how to\nmost effectively continually pre-train and SFT the model to be long-context.\nWe conduct thorough ablations on each component and\nmany of our findings contradict existing practices or beliefs.\nWe use all the findings to produce ProLong, a new state-of-the-art long-context LM. We release all our code, data, and models publicly\nand hope that our findings will\nboost research and applications of long-context LMs.\n\n\nLimitations\n\nAlthough we try to ablate the major components of our training recipe,\ndue to resource limitations, we cannot exhaust all aspects, such as the optimization hyperparameters and additional data mixtures.\nWe also limit ourselves to the 10B-scale regime and the Llama-3 models, which may limit the generalizability of our findings and recipe.\nAnother concern is that we are overfitting to the tasks chosen for model development\u2014however, we do not directly train on those datasets and guiding model development with benchmark tasks has become a common practice in pre-trained LM development. We also show that our final recipe and model perform well on an additional evaluation dataset, NoCha.\n\n\n\nAcknowledgments\n\nWe acknowledge Mengzhou Xia, Zexuan Zhong, Samyak Gupta, Dan Friedman, Yihe Dong, Abhishek Panigrah, Adithya Bhaskar, Colin Wang, Carlos Jimenez, and other members of Princeton Language and Intelligence for their helpful feedback and discussion. We also thank Luca Soldaini for providing comments on a draft.\nWe thank Marzena Karpinska, Tanya Goyal, and Mohit Iyyer for their help with the NoCha evaluation.\nTianyu Gao is supported by an IBM PhD Fellowship.\nThis work is gratefully supported by an NSF CAREER award (IIS-2239290), a grant from Intel, and Microsoft Azure credits through the \u201cAccelerate Foundation Models Academic Research\u201d Initiative.\n\n\n",
      "Appendix A Experiment Details": "\n\nAppendix A Experiment Details\n\n\nA.1 Evaluation\n\nTable 13: The details for our long-context evaluation following HELMET (Yen et\u00a0al., 2024b). \n\n\n\nCategory\nMetrics\n\n\nTasks and Datasets\n\n\n\n\n\n\nRecall\nSubEM\n\n\nGiven a randomly-generated long JSON file and a key, retrieve the corresponding value\u00a0(Liu et\u00a0al., 2024b).\n\n\n\n\nRAG\nSubEM\n\n\nGiven a question and many retrieved Wikipedia documents (shuffled), answer the question\u00a0(Liu et\u00a0al., 2024b). Datasets: NaturalQuestion\u00a0(Kwiatkowski et\u00a0al., 2019), HotpotQA\u00a0(Yang et\u00a0al., 2018), and PopQA\u00a0(Mallen et\u00a0al., 2023).\n\n\n\n\nRe-rank\nnDCG@10\n\n\nGiven a query and many retrieved documents (shuffled), re-rank the top-10 documents. Datasets: MSMARCO\u00a0(Bajaj et\u00a0al., 2016).\n\n\n\n\nICL\nAccuracy\n\n\nDatasets selected from Bertsch et\u00a0al. (2024): TREC coarse, TREC fine\u00a0(Hovy et\u00a0al., 2001), NLU\u00a0(Liu et\u00a0al., 2021), Banking77\u00a0(Casanueva et\u00a0al., 2020), and Clinc-150 (Larson et\u00a0al., 2019).\n\n\n\n\nQA\nGPT-4o score\n\n\nGiven a book, answer the question. Datasets (# tokens): NarrativeQA (medium: 73K; max: 518K; Ko\u010disk\u00fd et\u00a0al., 2018).\n\n\n\n\nSumm.\nGPT-4o score\n\n\nSummarize a given legal document. Datasets (# tokens): Multi-LexSum (medium: 90K; max: 5M; Shen et\u00a0al., 2022)\n\n\n\n\n\n\n\nTable\u00a013 shows all the datasets we used for the long-context evaluation from HELMET\u00a0(Yen et\u00a0al., 2024b).\nNote that we did not use all the datasets from HELMET for efficiency reasons and we also do not want to overfit to HELMET.\nWe highlight some of the evaluation protocol improvements that HELMET implemented compared to previous benchmarks here:\n\n\n\n\n\u2022\n\nSufficient context lengths and fine-grained control. HELMET can evaluate models at a context length of 128K tokens and beyond.\nThe evaluation protocol also allows for reporting results at different lengths, giving developers fine-trained controls for different needs of long contexts.\n\n\n\n\u2022\n\nBetter synthetic recall tasks. As shown in HELMET, needle-in-a-haystack\u00a0(Kamradt, 2024) is mostly saturated because of its simplicity\u2014the model only needs to find a needle in some irrelevant context. We instead use the more challenging JSON KV task, first proposed in Liu et\u00a0al. (2024b) and included in HELMET, where the model is required to find the corresponding value to a given key among a large JSON file.\n\n\n\n\u2022\n\nUsing class-balanced demonstrations and abstract labels for ICL. To disentangle models\u2019 ability of learning from demonstrations from their pre-training bias of the task or the dataset label distribution\u00a0(Pan et\u00a0al., 2023), HELMET samples the same number of demonstrations for each class and uses number labels (1, 2, \u2026) instead of natural-language labels (e.g., location, description, \u2026).\n\n\n\n\u2022\n\nModel-based evaluation for long-context QA and summarization. Instead of using traditional metrics like ROUGE (which has shown to be poorly indicative of the real model performance: Deutsch & Roth, 2021; Deutsch et\u00a0al., 2022; Goyal et\u00a0al., 2023; Chang et\u00a0al., 2024), HELMET uses model-based evaluations to compare the reference answer and the model output. For QA, HELMET uses GPT-4o to score the model output given the question and the reference answer at a 0-3 scale.\nFor summarization, HELMET takes a similar approach as Zhang & Bansal (2021); Gao et\u00a0al. (2023): it first uses GPT-4o to decompose the reference summary into atomic claims; then it uses GPT-4o to check whether each reference atomic claim is covered by the model output (recall) and whether each sentence in the model output is covered by the reference summary (precision). Yen et\u00a0al. (2024b) show that the model-based evaluation correlates with human perceptions significantly better than traditional metrics.\n\n\n\n\n\n\n\nA.2 Data processing\n\nData sources.\n\nWe list all the data sources we have explored in our ablations and main experiments here:\nthe Stack\u00a0(Kocetkov et\u00a0al., 2023),\nSlimPajama\u00a0(Together, 2023; Soboleva et\u00a0al., 2023),\nFineWeb (we use the 2023-50 snapshot), FineWeb-Edu (we use a random sample)\u00a0(Penedo et\u00a0al., 2024),\nTulu-v2\u00a0(Ivison et\u00a0al., 2023),\nOpenWebMath\u00a0(Paster et\u00a0al., 2024),\ntextbooks\u00a0(Chevalier et\u00a0al., 2024),\nand Dolma\u00a0(Soldaini et\u00a0al., 2024).\nThe Books, StackExchange, and ArXiv data are from SlimPajama. The Wikipedia data are from Dolma.\n\n\n\nData filtering and packing.\n\nFor the short training data and the SFT data, we randomly sample and concatenate the documents or conversations into 64K chunks. The last document for each chunk is truncated. The truncated part is used as the beginning for the next chunk for the short training data but is discarded for the SFT data.\nFor the long-context training data, we filter out the documents that are shorter than 64K; we do the same for the 512K setting, while making sure that the 64K documents packed to 512K length are distinct from the 512K documents.\n\n\n\nFinal data mixture.\n\nFor 512K length, we use a mix of 64K and 512K long data. For the ratio of 64K/512K data, we choose 50%/50% for code and 83%/17%, which are roughly chosen according to the natural availability of very long data, i.e., there are relatively fewer books of length 512K than code repositories. One benefit of retaining 64K-long documents is that we can process these without sequence parallelism and the associated communication overhead.\nWe use a slightly different long data mixture in our ablations\u00a0(Table\u00a05) and our main ProLong experiment\u00a0(Table\u00a09).\nFor the final model, we mix 3% textbooks into the long-context training data. The textbooks are open-source resources from libretexts.org, collected and made available by Chevalier et\u00a0al. (2024).\nWe pre-process the data by concatenating chapters from the same text books, as well as books from the same subject areas. This results in extremely long sequences which we pack into contexts of either 64K or 512K tokens.\nThough we do not have an ablation for adding this data due to limited resources, we believe that it should have a slight positive effect to the final model performance as textbooks are highly educational long-context data.\n\n\nTable 14: % Proportion of long documents for the short data components used in Table\u00a06. These statistics are computed after packing and truncation and therefore correspond to the document lengths as seen by the model. We highlight that the proportion of documents beyond 32K is below 1% for ShortMix.\n\n\n\n\n\n>>>4K\n\n>>>8K\n\n>>>16K\n\n>>>32K\n\n\n\u2005 FineWeb\n1.4\n0.3\n0.1\n0.0\n\n\n\u2005 FineWeb-Edu\n2.8\n0.8\n0.2\n0.0\n\n\n\u2005 Wikipedia\n1.6\n0.4\n0.0\n0.0\n\n\n\u2005 Tulu-v2\n0.0\n0.0\n0.0\n0.0\n\n\n\u2005 StackExchange\n0.6\n0.1\n0.0\n0.0\n\n\n\u2005 ArXiv\n85.7\n64.0\n30.3\n7.6\n\n\n\u2005 OpenWebMath\n11.1\n4.3\n1.2\n0.3\n\n\nShortMix\n10.9\n7.2\n3.2\n0.8\n\n\nSlimPajama\n11.3\n7.4\n4.9\n3.2\n\n\nFineWeb-Edu\n2.8\n0.8\n0.2\n0.0\n\n\nDCLM-Baseline\n4.9\n1.7\n0.4\n0.1\n\n\n\n\n\n\n\n\nA.3 Implementation details\n\nTechnical stack.\n\nWe use various open-source packages and tools for the ProLong training and evaluation.\nWe use PyTorch (Paszke et\u00a0al., 2019) and Hugging Face transformers\u00a0(Wolf et\u00a0al., 2020) for the model training.\nWe use mosaic-streaming (Mosaic ML, 2022) for loading and mixing the data\nand FlashAttention 2 (Dao, 2024) for efficient attention implementation.\nWe implement sequence parallelism based on DeepSpeed-Ulysses (Jacobs et\u00a0al., 2023) across groups of 8 GPUs on the same node. We only perform distributed attention if it is necessary, i.e., only on sequences of 512K length.\nFor long-context evaluation, we use HELMET\u00a0(Yen et\u00a0al., 2024b) and for short-context evaluation, we use lm-eval-harness (Gao et\u00a0al., 2021).\n\n\n\nAttention and batching.\n\nSince we do document masking in attention\u00a0(\u00a76),\nwe use the variable-length attention implementation from FlashAttention 2\u00a0(Dao, 2024) to speed up long-context training:\nfor sequences that are concatenations of multiple short documents,\ninstead of computing the full attention with masking,\nwe instead compute the attention for each individual document.\nSince the complexity of attention is quadratic to the sequence length, this improves the training speed.\nHowever, the improvement is negligible in a distributed training setting with FSDP, since GPUs processing short sequence batches have to wait on other GPUs processing long sequences.\nWe therefore implement a smart batching algorithm:\nIn our setting, a gradient step usually consists of multiple gradient accumulation steps, where each device processes a smaller minibatch.\nWe sort all the minibatches per training step by the sum of the squared lengths of documents in the sequence.\nThis leads to more balanced sequence lengths across the GPUs and effective speedups, as can be seen in Table\u00a015, without affecting the gradient updates or loss during training. However, the efficiency gains are diminished when training with more GPUs, as this reduces the number of gradient accumulation steps.\n\n\nTable 15: Throughput per device of our ablation runs from Table\u00a020, when training with 8 Nvidia H100 GPUs with FSDP. Our strategy of reordering minibatches is important for realizing the speed benefits from variable-length attention.\n\n\n\n\nThroughput\n\n\n\n(tokens/s/GPU)\n\n\n\n\n64K full attention\n\u00a0\u00a0\u00a02770\n\n\nVariable-length attention\n\u00a0\u00a0\u00a02780(+0.4%)\n\n\n\n\u2005 + Minibatch reordering\n\n\u00a0\u00a0\u00a03095(+11.7%)\n\n\n\n\n\n\n\nToken-averaged loss.\n\nWe found that in the SFT stage,\nthe distribution of the training tokens (in SFT, the tokens from the instructions are masked out and the models are only trained on the responses) on each GPU device can be extremely imbalanced, especially when there is synthetic data (most tokens in a synthetic data instance are from the instruction).\nConventional all-reduce loss in distributed training averages over the sequences instead of valid tokens, which skews the optimization and also our control over the domain proportions.\nInstead, we change the all-reduce loss to be the average over all valid training tokens.\nBai et\u00a0al. (2024a) implements their SFT loss in a similar way.\n\n\n\n\n\nA.4 The ablation setting\n\nFor all our ablations, unless specified, we train the base model of Llama-3-8B (instead of Instruct) on a 64K sequence length for 5B tokens, with the same hyperparameters as specified in Table\u00a09.\nWe choose this context length, as it is the highest power of 2 value for which we can train without sequence parallelism.\nBy default, we use the same training data as the 64K ProLong setting, except that we remove the textbooks and use the ShortMix proportions in Table\u00a05.\nFor SFT, we use the same settings as specified in Table\u00a09.\n\n\n\n\nA.5 Generating synthetic SFT data\n\nWe prompt Llama-3-8B-Instruct to generate the synthetic data and\nTable\u00a016 shows the prompt we used for generating the synthetic QA data for books.\nWe also write predefined templates and randomly sample one for each synthetic instance to increase the diversity, and Table\u00a017 provides some examples.\n\n\nTable 16: \nPrompts for generating synthetic QA data.\n\n\n\n\n\n\nGiven the following snippet of a book, ask a relevant question and provide the answer. The question and the answer should follow the following rules:\n\n\n\n\n\n\n\n\n\n\n\n\n\n(1) The question should be specific enough that it can only be answered with the snippet. The question should also be interesting and intellectual enough that a curious reader of the book would ask about it.\n\n\n\n\n\n\n(2) The question and the answer should be comprehensible given just the whole book without highlighting the snippet. With that being said, the question should NOT refer to the snippet directly (e.g., do NOT say things like \"Question: given the conversation in the snippet, what ...\"). The answer also should not mention \"the snippet \u2026\" explicitly (assuming that the snippet is never provided), but it can copy the snippet content as a reference when answering the question.\n\n\n\n\n\n\n(3) The answer should be concise but also should provide references to the book when needed. For example, \u201cWellington Yueh betrayed the Atreides, as the book mentioned, \u2019...\u2019\".\n\n\n\n\n\n\n\n\n\n\n\n\n\n*** Start of the snippet ***\n\n\n\n\n\n\n\n\n\n\n\n\n\n{sampled snippet}\n\n\n\n\n\n\n\n\n\n\n\n\n\n*** End of the snippet ***\n\n\n\n\n\n\n\n\n\n\n\n\n\nBefore generating the question and the answer, first reason about what this snippet is about. In your generation, stick to the following format:\n\n\n\n\n\n\n\n\n\n\n\n\n\nReasoning: this snippet is about ...\n\n\n\n\n\n\nQuestion: ...\n\n\n\n\n\n\nAnswer: ...\n\n\n\n\n\n\n\nTable 17: \nExamples for question prompts and templates used for generating diverse synthetic QA data. We sample one question prompt and one template each time and combine them with the documents and the generated QA pairs to form a synthetic training example.\n\n\n\n\nExample question prompts for synthetic QA data\n\n\n\n\n\n\nGiven the document, please answer the question.\n\n\n\n\n\n\nHere is a piece of text; answer the following question based on it.\n\n\n\n\n\n\nPlease answer the question using the provided content.\n\n\n\n\n\n\nBased on the given passage, respond to the question.\n\n\n\n\n\n\nRead the snippet and answer the question that follows.\n\n\n\n\n\n\nUsing the provided text, answer the following question.\n\n\n\n\nExample templates for combining questions, answers, and contexts for synthetic QA data\n\n\n\n\n{prompt}\\n\\n{documents}\\n\\nQuestion: {question}\n\n\n\n\n\n\n{prompt}\\n\\n==== document starts ====\\n{documents}\\n==== document ends ====\\n\\nQuestion: {question}\n\n\n\n\n\n\n{prompt}\\n\\n{documents}\\n\\n{question}\n\n\n\n\n\n\n{prompt} Question: {question}\\n\\n{documents}\n\n\n\n\n\n\n{prompt} {question}\\n\\n{documents}\n\n\n\n\n\n\n{prompt}\\n\\n{question}\\n\\n{documents}\n\n\n\n\n\n\n\n",
      "Appendix B More Ablations": "\n\nAppendix B More Ablations\n\n\nB.1 Position extrapolation\n\nXiong et\u00a0al. (2023); emozilla (2023) show that\nchanging the RoPE frequency base to a larger value in\ncontinual long-context pre-training or in inference time can improve the long-context performance.\nemozilla (2023) suggests that one should scale the frequency base by a factor of tdd\u22122superscript\ud835\udc61\ud835\udc51\ud835\udc512t^{\\frac{d}{d-2}}italic_t start_POSTSUPERSCRIPT divide start_ARG italic_d end_ARG start_ARG italic_d - 2 end_ARG end_POSTSUPERSCRIPT, where t\ud835\udc61titalic_t is the ratio between the target sequence length and the original LM length, and d\ud835\udc51ditalic_d is the attention head dimension.\n\n\nWe conduct ablation studies, at both 64K (same as our standard ablation setting as specified in \u00a7A.4) and 512K (starting from ProLong-64K and training with the 512K data mixture for 5B tokens) sequence lengths, on what frequency bases we should use.\n\u00a7B.1 and \u00a7B.1 show the results.\nWe first see that using the original 500,000 frequency base from Llama-3 leads to significant performance degradation. While dynamic NTK suggests 4\u00d71064superscript1064\\times 10^{6}4 \u00d7 10 start_POSTSUPERSCRIPT 6 end_POSTSUPERSCRIPT, we find that further scaling it to 8\u00d71068superscript1068\\times 10^{6}8 \u00d7 10 start_POSTSUPERSCRIPT 6 end_POSTSUPERSCRIPT leads to better performance. Similar, we see that when scaling the 64K model to 512K, while dynamic NTK suggests a 64\u00d710664superscript10664\\times 10^{6}64 \u00d7 10 start_POSTSUPERSCRIPT 6 end_POSTSUPERSCRIPT frequency base, much larger frequency bases (128\u00d7106128superscript106128\\times 10^{6}128 \u00d7 10 start_POSTSUPERSCRIPT 6 end_POSTSUPERSCRIPT and 256\u00d7106256superscript106256\\times 10^{6}256 \u00d7 10 start_POSTSUPERSCRIPT 6 end_POSTSUPERSCRIPT) lead to better performance.\nWe use 8\u00d71068superscript1068\\times 10^{6}8 \u00d7 10 start_POSTSUPERSCRIPT 6 end_POSTSUPERSCRIPT for 64K and 128\u00d7106128superscript106128\\times 10^{6}128 \u00d7 10 start_POSTSUPERSCRIPT 6 end_POSTSUPERSCRIPT for 512K for our final ProLong models.\n\n\nTable 18: Ablation study on RoPE frequency base at a maximum training length of 64K. Dynamic NTK\u00a0(emozilla, 2023) roughly suggests to use 4m as the frequency base.\n\n\n\n\n\nRoPE Base \n(\u00d7106absentsuperscript106\\times 10^{6}\u00d7 10 start_POSTSUPERSCRIPT 6 end_POSTSUPERSCRIPT)\nLong-Context\nShort-Context\n\n\nRecall\nRAG\nRe-rank\nICL\nQA\nSumm.\nAvg.\nAvg.\n\n\n\n\n0.5\n25.8\n37.0\n4.4\n73.8\n17.5\n16.3\n29.1\n65.0\n\n\n4.0\n81.3\n47.8\n18.2\n76.5\n31.8\n36.3\n48.7\n65.3\n\n\n8.0\n96.0\n54.9\n29.4\n73.9\n35.7\n37.9\n54.6\n65.5\n\n\n\n\n\n\n\nTable 19: Ablation study on RoPE frequency base at a maximum training length of 512K. Dynamic NTK\u00a0(emozilla, 2023) roughly suggests to use 64\u00d710664superscript10664\\times 10^{6}64 \u00d7 10 start_POSTSUPERSCRIPT 6 end_POSTSUPERSCRIPT as the frequency base.\n\n\n\n\n\nRoPE Base \n(\u00d7106absentsuperscript106\\times 10^{6}\u00d7 10 start_POSTSUPERSCRIPT 6 end_POSTSUPERSCRIPT)\nLong-Context\nShort-Context\n\n\nRecall\nRAG\nRe-rank\nICL\nQA\nSumm.\nAvg.\nAvg.\n\n\n\n\n64\n98.8\n57.8\n30.4\n82.2\n38.2\n38.3\n57.6\n68.3\n\n\n128\n98.8\n57.4\n30.7\n80.0\n40.4\n38.8\n57.7\n68.6\n\n\n256\n98.8\n56.8\n33.8\n79.8\n37.9\n39.7\n57.8\n68.4\n\n\n\n\n\n\n\n\nB.2 Document masks\n\nWe experiment whether to use document masks in attention in Table\u00a020. Standard training concatenates multiple short documents into a single sequence (in our case, a 64K sequence), uses a special token to separate documents, and performs full attention over the whole sequence. When the document masks are used, we do not allow the attention to cross the document boundaries.\nWe find that using document masks in continual long-context training leads to both better long-context results and short-context performance.\nFor all our other ablations and the main experiment, we use document masks.\n\n\nTable 20: Impact of using document masks in attention.\n\n\n\nAttention\nLong-Context\nShort-Context\n\n\nRecall\nRAG\nRe-rank\nICL\nQA\nSumm.\nAvg.\nAvg.\n\n\n\n\nNo doc masks\n97.4\n53.6\n20.4\n76.6\n37.2\n36.3\n53.6\n64.9\n\n\nDocument masks\n96.0\n54.9\n29.4\n73.9\n35.7\n37.9\n54.6\n65.5\n\n\n\n\n\n\nB.3 Initialization\n\nWe use the base model for Llama-3-8B as the initialization for all our ablations to make sure the findings are generalizable and are not confounded by the Llama instruction tuning.\nHowever, for our final ProLong model, we use Llama-3-8B-Instruct as the initialization to achieve the best performance.\nWe see in Table\u00a021 (using the ablation setting from \u00a7A.4) that using Llama-3-8B-Instruct as the initialization achieves slightly better long-context performance and much stronger short-context performance.\n\n\nTable 21: \nDifferences of using the base Llama-3-8B model vs. Llama-3-8B-Instruct.\n\n\n\n\nBase Model\nLong-Context\nShort-Context\n\n\nAvg.\nHellaS.\nMMLU\nARC-c\nWG\nGSM8K\nAvg.\n\n\n\n\nLlama-3-8B-Base\n54.6\n81.6\n65.3\n58.0\n76.2\n46.6\n65.5\n\n\nLlama-3-8B-Instruct\n55.0\n80.8\n66.1\n58.5\n75.6\n57.7\n67.7\n\n\n\n\n\n\nB.4 Instruction-tuning datasets\n\nInitialized from the ProLong base model, we experiment with different public, short-context SFT datasets.\nAll runs use the same SFT hyperparameters as specified in Table\u00a09.\nTable\u00a022 shows that using UltraChat leads to the best overall results.\nNote that this does not necessarily mean that UltraChat is the best SFT dataset for all base models or applications.\n\n\nTable 22: \nAblations on using different short-context SFT datasets.\nWe report the 32K-and-64K-averaged performance except tasks marked with \u2020, which are evaluated at 512K for stress testing.\n\n\n\n\nSFT Data\nLong-Context\n\n\nRecall\u2020\n\nRAG\nRe-rank\nICL\nQA\u2020\n\nSumm.\u2020\n\nAvg.\n\n\n\n\nUltraChat\n65.7\n58.1\n38.5\n80.3\n49.7\n42.1\n55.7\n\n\nTulu v2\n61.5\n45.4\n25.1\n81.8\n40.4\n40.3\n49.1\n\n\nShareGPT\n40.5\n47.5\n26.7\n79.6\n42.7\n34.4\n45.2\n\n\n\n\n\n\nAppendix C Full Evaluation Results\n\nWe provide all the evaluation results, including specific numbers for each dataset and evaluation length, at https://shorturl.at/pULbx.\n\n\n\n\nAppendix D Examples\n\nTable\u00a023 shows some long-document QA examples with ProLong, Llama-3.1, and GPT-4o.\n\n\nTable 23: \nLong-document QA examples with ProLong (512K), Llama-3.1-8B-Instruct (128K), and GPT-4o (via the ChatGPT interface). Note that\nit is very likely that the ChatGPT-version GPT-4o uses a retrieval component for long documents, hence it can process contexts beyond its 128K context window.\nWe see that ProLong can answer questions effectively with a 300K document while both Llama-3.1 and GPT-4o fail.\nOn a shorter document (Llama-3.1 paper), ProLong can understand very technical questions and answer them accurately.\n\n\n\n\nInstacart S1 file (300K tokens)\n\n\n\n\n\n\nQ: How many shares and at what price did instacard sell their series G redeemable convertible preferred stock in June 2020 and July 2020?\n\n\n\nProLong: Instacart sold an aggregate of  6,757,893 shares of their \u2026 at a purchase price of $48.0919 \u2026\n\n\n\n\n\n\nllama-3.1-8B: The following is a II/5 [collapsed]\n\n\n\n\n\n\nGPT-4o: \u2026 The company sold a total of 6,758,000 shares at a price of $48.0919 per share during this period.\n\n\n\n\nLlama-3.1 paper (94K tokens)\n\n\n\n\nQ: During training of the llama3 model, how many unexpected interruptions are caused by hardware related reasons?\n\n\n\nProLong: Approximately 78% of unexpected \u2026\n\n\n\n\n\n\nllama-3.1-8B: According to Table 5, approximately 58.7% \u2026\n\n\n\n\n\n\nGPT-4o: \u2026 approximately 78% of unexpected interruptions \u2026\n\n\n\n\n\n\nQ: Is Mark Zuckerberg a contributor to the paper?\n\n\n\nProLong: There is no mention of Mark Zuckerberg as a contributor to the paper.\n\n\n\n\n\n\nllama-3.1-8B: Yes, Mark Zuckerberg is listed as a contributor \u2026\n\n\n\n\n\n\nGPT-4o: Mark Zuckerberg is acknowledged in the paper for \u2026, but he is not listed as a core contributor \u2026\n\n\n\n\n\n\nQ: What ratio did the llama 3 team use to mix synthetic long-context data with short data in SFT?\n\n\n\nProLong: The llama 3 team used a ratio of 0.1% synthetic \u2026\n\n\n\n\n\n\nllama-3.1-8B: 0.10%\n\n\n\n\n\n\nGPT-4o: The Llama 3 team used a mix ratio of 30% synthetic \u2026\n\n\n\n\n\n\nQ: What benchmark did they evaluate on for \u201dinstruction-following\u201d ability? Provide references too.\n\n\n\nProLong: They evaluated on IFEval (Zhou et al., 2023) for instruction-following ability. Reference: Jeffrey Zhou, \u2026 Instruction-following evaluation for large language models. arXiv preprint \u2026\n\n\n\n\n\n\nllama-3.1-8B: The benchmark they evaluated on for \u201dinstruction-following\u201d ability is IFEval. Reference: Section 5.2.2, \u201dGeneral Knowledge and Instruction-Following Benchmarks\u201d\n\n\n\n\n\n\nGPT-4o: The benchmark they used to evaluate \u201dinstruction- following\u201d ability is IFEval. This is mentioned in the context of assessing how Direct Preference Optimization (DPO) \u2026\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",
      "Appendix C Full Evaluation Results": "\n\nAppendix C Full Evaluation Results\n\nWe provide all the evaluation results, including specific numbers for each dataset and evaluation length, at https://shorturl.at/pULbx.\n\n",
      "Appendix D Examples": "\n\nAppendix D Examples\n\nTable\u00a023 shows some long-document QA examples with ProLong, Llama-3.1, and GPT-4o.\n\n\nTable 23: \nLong-document QA examples with ProLong (512K), Llama-3.1-8B-Instruct (128K), and GPT-4o (via the ChatGPT interface). Note that\nit is very likely that the ChatGPT-version GPT-4o uses a retrieval component for long documents, hence it can process contexts beyond its 128K context window.\nWe see that ProLong can answer questions effectively with a 300K document while both Llama-3.1 and GPT-4o fail.\nOn a shorter document (Llama-3.1 paper), ProLong can understand very technical questions and answer them accurately.\n\n\n\n\nInstacart S1 file (300K tokens)\n\n\n\n\n\n\nQ: How many shares and at what price did instacard sell their series G redeemable convertible preferred stock in June 2020 and July 2020?\n\n\n\nProLong: Instacart sold an aggregate of  6,757,893 shares of their \u2026 at a purchase price of $48.0919 \u2026\n\n\n\n\n\n\nllama-3.1-8B: The following is a II/5 [collapsed]\n\n\n\n\n\n\nGPT-4o: \u2026 The company sold a total of 6,758,000 shares at a price of $48.0919 per share during this period.\n\n\n\n\nLlama-3.1 paper (94K tokens)\n\n\n\n\nQ: During training of the llama3 model, how many unexpected interruptions are caused by hardware related reasons?\n\n\n\nProLong: Approximately 78% of unexpected \u2026\n\n\n\n\n\n\nllama-3.1-8B: According to Table 5, approximately 58.7% \u2026\n\n\n\n\n\n\nGPT-4o: \u2026 approximately 78% of unexpected interruptions \u2026\n\n\n\n\n\n\nQ: Is Mark Zuckerberg a contributor to the paper?\n\n\n\nProLong: There is no mention of Mark Zuckerberg as a contributor to the paper.\n\n\n\n\n\n\nllama-3.1-8B: Yes, Mark Zuckerberg is listed as a contributor \u2026\n\n\n\n\n\n\nGPT-4o: Mark Zuckerberg is acknowledged in the paper for \u2026, but he is not listed as a core contributor \u2026\n\n\n\n\n\n\nQ: What ratio did the llama 3 team use to mix synthetic long-context data with short data in SFT?\n\n\n\nProLong: The llama 3 team used a ratio of 0.1% synthetic \u2026\n\n\n\n\n\n\nllama-3.1-8B: 0.10%\n\n\n\n\n\n\nGPT-4o: The Llama 3 team used a mix ratio of 30% synthetic \u2026\n\n\n\n\n\n\nQ: What benchmark did they evaluate on for \u201dinstruction-following\u201d ability? Provide references too.\n\n\n\nProLong: They evaluated on IFEval (Zhou et al., 2023) for instruction-following ability. Reference: Jeffrey Zhou, \u2026 Instruction-following evaluation for large language models. arXiv preprint \u2026\n\n\n\n\n\n\nllama-3.1-8B: The benchmark they evaluated on for \u201dinstruction-following\u201d ability is IFEval. Reference: Section 5.2.2, \u201dGeneral Knowledge and Instruction-Following Benchmarks\u201d\n\n\n\n\n\n\nGPT-4o: The benchmark they used to evaluate \u201dinstruction- following\u201d ability is IFEval. This is mentioned in the context of assessing how Direct Preference Optimization (DPO) \u2026\n\n\n\n\n\n\n\n"
    },
    "bibliography": {
      "Abdin et\u00a0al. (2024)": "\nAbdin et\u00a0al. (2024)\n\nMarah Abdin, Sam\u00a0Ade Jacobs, Ammar\u00a0Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Harkirat Behl, et\u00a0al.\n\n\nPhi-3 technical report: A highly capable language model locally on your phone.\n\n\narXiv preprint arXiv:2404.14219, 2024.\n\n\n",
      "Achiam et\u00a0al. (2023)": "\nAchiam et\u00a0al. (2023)\n\nJosh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia\u00a0Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et\u00a0al.\n\n\nGpt-4 technical report.\n\n\narXiv preprint arXiv:2303.08774, 2023.\n\n\n",
      "An et\u00a0al. (2024a)": "\nAn et\u00a0al. (2024a)\n\nChenxin An, Shansan Gong, Ming Zhong, Xingjian Zhao, Mukai Li, Jun Zhang, Lingpeng Kong, and Xipeng Qiu.\n\n\nL-eval: Instituting standardized evaluation for long context language models.\n\n\nIn Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp.\u00a0 14388\u201314411, 2024a.\n\n\n",
      "An et\u00a0al. (2024b)": "\nAn et\u00a0al. (2024b)\n\nShengnan An, Zexiong Ma, Zeqi Lin, Nanning Zheng, and Jian-Guang Lou.\n\n\nMake your llm fully utilize the context.\n\n\narXiv preprint arXiv:2404.16811, 2024b.\n\n\n",
      "Anthropic (2024)": "\nAnthropic (2024)\n\nAI\u00a0Anthropic.\n\n\nClaude 3.5 sonnet model card addendum.\n\n\nClaude-3.5 Model Card, 2024.\n\n\n",
      "Arora et\u00a0al. (2024)": "\nArora et\u00a0al. (2024)\n\nSimran Arora, Sabri Eyuboglu, Michael Zhang, Aman Timalsina, Silas Alberti, James Zou, Atri Rudra, and Christopher Re.\n\n\nSimple linear attention language models balance the recall-throughput tradeoff.\n\n\nIn Forty-first International Conference on Machine Learning, 2024.\n\n\n",
      "Bai et\u00a0al. (2024a)": "\nBai et\u00a0al. (2024a)\n\nYushi Bai, Xin Lv, Jiajie Zhang, Yuze He, Ji\u00a0Qi, Lei Hou, Jie Tang, Yuxiao Dong, and Juanzi Li.\n\n\nLongalign: A recipe for long context alignment of large language models.\n\n\narXiv preprint arXiv:2401.18058, 2024a.\n\n\n",
      "Bai et\u00a0al. (2024b)": "\nBai et\u00a0al. (2024b)\n\nYushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li.\n\n\nLongBench: A bilingual, multitask benchmark for long context understanding.\n\n\nIn Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp.\u00a0 3119\u20133137, 2024b.\n\n\n",
      "Bajaj et\u00a0al. (2016)": "\nBajaj et\u00a0al. (2016)\n\nPayal Bajaj, Daniel Campos, Nick Craswell, Li\u00a0Deng, Jianfeng Gao, Xiaodong Liu, Rangan Majumder, Andrew McNamara, Bhaskar Mitra, Tri Nguyen, et\u00a0al.\n\n\nMs marco: A human generated machine reading comprehension dataset.\n\n\narXiv preprint arXiv:1611.09268, 2016.\n\n\n",
      "Beeching et\u00a0al. (2023)": "\nBeeching et\u00a0al. (2023)\n\nEdward Beeching, Cl\u00e9mentine Fourrier, Nathan Habib, Sheon Han, Nathan Lambert, Nazneen Rajani, Omar Sanseviero, Lewis Tunstall, and Thomas Wolf.\n\n\nOpen llm leaderboard, 2023.\n\n\n",
      "Bertsch et\u00a0al. (2023)": "\nBertsch et\u00a0al. (2023)\n\nAmanda Bertsch, Uri Alon, Graham Neubig, and Matthew\u00a0R. Gormley.\n\n\nUnlimiformer: Long-range transformers with unlimited length input.\n\n\nIn Advances in Neural Information Processing Systems (NeurIPS), 2023.\n\n\n",
      "Bertsch et\u00a0al. (2024)": "\nBertsch et\u00a0al. (2024)\n\nAmanda Bertsch, Maor Ivgi, Uri Alon, Jonathan Berant, Matthew\u00a0R Gormley, and Graham Neubig.\n\n\nIn-context learning with long-context models: An in-depth exploration.\n\n\narXiv preprint arXiv:2405.00200, 2024.\n\n\n",
      "Casanueva et\u00a0al. (2020)": "\nCasanueva et\u00a0al. (2020)\n\nI\u00f1igo Casanueva, Tadas Tem\u010dinas, Daniela Gerz, Matthew Henderson, and Ivan Vuli\u0107.\n\n\nEfficient intent detection with dual sentence encoders.\n\n\nIn Proceedings of the 2nd Workshop on Natural Language Processing for Conversational AI, pp.\u00a0 38\u201345, 2020.\n\n\n",
      "Chang et\u00a0al. (2024)": "\nChang et\u00a0al. (2024)\n\nYapei Chang, Kyle Lo, Tanya Goyal, and Mohit Iyyer.\n\n\nBooookscore: A systematic exploration of book-length summarization in the era of LLMs.\n\n\nIn The Twelfth International Conference on Learning Representations, 2024.\n\n\n",
      "Chen et\u00a0al. (2023)": "\nChen et\u00a0al. (2023)\n\nShouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian.\n\n\nExtending context window of large language models via positional interpolation, 2023.\n\n\n",
      "Chen et\u00a0al. (2024)": "\nChen et\u00a0al. (2024)\n\nYukang Chen, Shengju Qian, Haotian Tang, Xin Lai, Zhijian Liu, Song Han, and Jiaya Jia.\n\n\nLongloRA: Efficient fine-tuning of long-context large language models.\n\n\nIn The Twelfth International Conference on Learning Representations, 2024.\n\n\n",
      "Chevalier et\u00a0al. (2023)": "\nChevalier et\u00a0al. (2023)\n\nAlexis Chevalier, Alexander Wettig, Anirudh Ajith, and Danqi Chen.\n\n\nAdapting language models to compress contexts.\n\n\nIn Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp.\u00a0 3829\u20133846, Singapore, December 2023. Association for Computational Linguistics.\n\n\n",
      "Chevalier et\u00a0al. (2024)": "\nChevalier et\u00a0al. (2024)\n\nAlexis Chevalier, Jiayi Geng, Alexander Wettig, Howard Chen, Sebastian Mizera, Toni Annala, Max Aragon, Arturo\u00a0Rodriguez Fanlo, Simon Frieder, Simon Machado, Akshara Prabhakar, Ellie Thieu, Jiachen\u00a0T. Wang, Zirui Wang, Xindi Wu, Mengzhou Xia, Wenhan Xia, Jiatong Yu, Junjie Zhu, Zhiyong Ren, Sanjeev Arora, and Danqi Chen.\n\n\nLanguage models as science tutors.\n\n\nIn Forty-first International Conference on Machine Learning, 2024.\n\n\n",
      "Choromanski et\u00a0al. (2021)": "\nChoromanski et\u00a0al. (2021)\n\nKrzysztof\u00a0Marcin Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared\u00a0Quincy Davis, Afroz Mohiuddin, Lukasz Kaiser, David\u00a0Benjamin Belanger, Lucy\u00a0J Colwell, and Adrian Weller.\n\n\nRethinking attention with performers.\n\n\nIn International Conference on Learning Representations, 2021.\n\n\nURL https://openreview.net/forum?id=Ua6zuk0WRH.\n\n\n",
      "Clark et\u00a0al. (2018)": "\nClark et\u00a0al. (2018)\n\nPeter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord.\n\n\nThink you have solved question answering? Try ARC, the AI2 reasoning challenge.\n\n\narXiv preprint arXiv:1803.05457, 2018.\n\n\n",
      "Cobbe et\u00a0al. (2021)": "\nCobbe et\u00a0al. (2021)\n\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et\u00a0al.\n\n\nTraining verifiers to solve math word problems.\n\n\narXiv preprint arXiv:2110.14168, 2021.\n\n\n",
      "Dao (2024)": "\nDao (2024)\n\nTri Dao.\n\n\nFlashattention-2: Faster attention with better parallelism and work partitioning.\n\n\nIn The Twelfth International Conference on Learning Representations, 2024.\n\n\n",
      "Dao & Gu (2024)": "\nDao & Gu (2024)\n\nTri Dao and Albert Gu.\n\n\nTransformers are ssms: Generalized models and efficient algorithms through structured state space duality.\n\n\narXiv preprint arXiv:2405.21060, 2024.\n\n\n",
      "Deutsch & Roth (2021)": "\nDeutsch & Roth (2021)\n\nDaniel Deutsch and Dan Roth.\n\n\nUnderstanding the extent to which content quality metrics measure the information quality of summaries.\n\n\nIn Arianna Bisazza and Omri Abend (eds.), Proceedings of the 25th Conference on Computational Natural Language Learning, pp.\u00a0 300\u2013309, 2021.\n\n\n",
      "Deutsch et\u00a0al. (2022)": "\nDeutsch et\u00a0al. (2022)\n\nDaniel Deutsch, Rotem Dror, and Dan Roth.\n\n\nRe-examining system-level correlations of automatic summarization evaluation metrics.\n\n\nIn Marine Carpuat, Marie-Catherine de\u00a0Marneffe, and Ivan\u00a0Vladimir Meza\u00a0Ruiz (eds.), Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp.\u00a0 6038\u20136052, Seattle, United States, July 2022. Association for Computational Linguistics.\n\n\ndoi: 10.18653/v1/2022.naacl-main.442.\n\n\nURL https://aclanthology.org/2022.naacl-main.442.\n\n\n",
      "Ding et\u00a0al. (2024a)": "\nDing et\u00a0al. (2024a)\n\nHantian Ding, Zijian Wang, Giovanni Paolini, Varun Kumar, Anoop Deoras, Dan Roth, and Stefano Soatto.\n\n\nFewer truncations improve language modeling.\n\n\nIn Forty-first International Conference on Machine Learning, 2024a.\n\n\nURL https://openreview.net/forum?id=kRxCDDFNpp.\n\n\n",
      "Ding et\u00a0al. (2023)": "\nDing et\u00a0al. (2023)\n\nNing Ding, Yulin Chen, Bokai Xu, Yujia Qin, Shengding Hu, Zhiyuan Liu, Maosong Sun, and Bowen Zhou.\n\n\nEnhancing chat language models by scaling high-quality instructional conversations.\n\n\nIn Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp.\u00a0 3029\u20133051, Singapore, December 2023. Association for Computational Linguistics.\n\n\n",
      "Ding et\u00a0al. (2024b)": "\nDing et\u00a0al. (2024b)\n\nYiran Ding, Li\u00a0Lyna Zhang, Chengruidong Zhang, Yuanyuan Xu, Ning Shang, Jiahang Xu, Fan Yang, and Mao Yang.\n\n\nLongroPE: Extending LLM context window beyond 2 million tokens.\n\n\nIn Forty-first International Conference on Machine Learning, 2024b.\n\n\n",
      "Dubey et\u00a0al. (2024)": "\nDubey et\u00a0al. (2024)\n\nAbhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et\u00a0al.\n\n\nThe Llama 3 herd of models.\n\n\narXiv preprint arXiv:2407.21783, 2024.\n\n\n",
      "emozilla (2023)": "\nemozilla (2023)\n\nemozilla.\n\n\nDynamically scaled rope further increases performance of long context llama with zero fine-tuning, 2023.\n\n\nURL https://www.reddit.com/r/LocalLLaMA/comments/14mrgpr/dynamically_scaled_rope_further_increases/.\n\n\n",
      "Fu et\u00a0al. (2024)": "\nFu et\u00a0al. (2024)\n\nYao Fu, Rameswar Panda, Xinyao Niu, Xiang Yue, Hannaneh Hajishirzi, Yoon Kim, and Hao Peng.\n\n\nData engineering for scaling language models to 128k context.\n\n\nIn International Conference on Machine Learning (ICML), 2024.\n\n\n",
      "Gao et\u00a0al. (2021)": "\nGao et\u00a0al. (2021)\n\nLeo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff, Jason Phang, Laria Reynolds, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou.\n\n\nA framework for few-shot language model evaluation, September 2021.\n\n\n",
      "Gao et\u00a0al. (2023)": "\nGao et\u00a0al. (2023)\n\nTianyu Gao, Howard Yen, Jiatong Yu, and Danqi Chen.\n\n\nEnabling large language models to generate text with citations.\n\n\nIn Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp.\u00a0 6465\u20136488, Singapore, December 2023. Association for Computational Linguistics.\n\n\n",
      "Goyal et\u00a0al. (2023)": "\nGoyal et\u00a0al. (2023)\n\nTanya Goyal, Junyi\u00a0Jessy Li, and Greg Durrett.\n\n\nNews summarization and evaluation in the era of gpt-3, 2023.\n\n\nURL https://arxiv.org/abs/2209.12356.\n\n\n",
      "Gu & Dao (2023)": "\nGu & Dao (2023)\n\nAlbert Gu and Tri Dao.\n\n\nMamba: Linear-time sequence modeling with selective state spaces, 2023.\n\n\n",
      "Guo et\u00a0al. (2024)": "\nGuo et\u00a0al. (2024)\n\nDaya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang, Guanting Chen, Xiao Bi, Yu\u00a0Wu, YK\u00a0Li, et\u00a0al.\n\n\nDeepseek-coder: When the large language model meets programming\u2013the rise of code intelligence.\n\n\narXiv preprint arXiv:2401.14196, 2024.\n\n\n",
      "Hendrycks et\u00a0al. (2021)": "\nHendrycks et\u00a0al. (2021)\n\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt.\n\n\nMeasuring massive multitask language understanding.\n\n\nIn International Conference on Learning Representations, 2021.\n\n\n",
      "Hovy et\u00a0al. (2001)": "\nHovy et\u00a0al. (2001)\n\nEduard Hovy, Laurie Gerber, Ulf Hermjakob, Chin-Yew Lin, and Deepak Ravichandran.\n\n\nToward semantics-based answer pinpointing.\n\n\nIn Proceedings of the First International Conference on Human Language Technology Research, 2001.\n\n\n",
      "Hsieh et\u00a0al. (2024)": "\nHsieh et\u00a0al. (2024)\n\nCheng-Ping Hsieh, Simeng Sun, Samuel Kriman, Shantanu Acharya, Dima Rekesh, Fei Jia, and Boris Ginsburg.\n\n\nRULER: What\u2019s the real context size of your long-context language models?\n\n\nIn First Conference on Language Modeling, 2024.\n\n\n",
      "Hu et\u00a0al. (2024a)": "\nHu et\u00a0al. (2024a)\n\nShengding Hu, Yuge Tu, Xu\u00a0Han, Chaoqun He, Ganqu Cui, Xiang Long, Zhi Zheng, Yewei Fang, Yuxiang Huang, Weilin Zhao, et\u00a0al.\n\n\nMinicpm: Unveiling the potential of small language models with scalable training strategies.\n\n\narXiv preprint arXiv:2404.06395, 2024a.\n\n\n",
      "Hu et\u00a0al. (2024b)": "\nHu et\u00a0al. (2024b)\n\nZhiyuan Hu, Yuliang Liu, Jinman Zhao, Suyuchen Wang, Yan Wang, Wei Shen, Qing Gu, Anh\u00a0Tuan Luu, See-Kiong Ng, Zhiwei Jiang, et\u00a0al.\n\n\nLongrecipe: Recipe for efficient long context generalization in large languge models.\n\n\narXiv preprint arXiv:2409.00509, 2024b.\n\n\n",
      "Ivison et\u00a0al. (2023)": "\nIvison et\u00a0al. (2023)\n\nHamish Ivison, Yizhong Wang, Valentina Pyatkin, Nathan Lambert, Matthew Peters, Pradeep Dasigi, Joel Jang, David Wadden, Noah\u00a0A. Smith, Iz\u00a0Beltagy, and Hannaneh Hajishirzi.\n\n\nCamels in a changing climate: Enhancing lm adaptation with Tulu 2, 2023.\n\n\n",
      "Jacobs et\u00a0al. (2023)": "\nJacobs et\u00a0al. (2023)\n\nSam\u00a0Ade Jacobs, Masahiro Tanaka, Chengming Zhang, Minjia Zhang, Shuaiwen\u00a0Leon Song, Samyam Rajbhandari, and Yuxiong He.\n\n\nDeepspeed ulysses: System optimizations for enabling training of extreme long sequence transformer models.\n\n\narXiv preprint arXiv:2309.14509, 2023.\n\n\n",
      "Jelassi et\u00a0al. (2024)": "\nJelassi et\u00a0al. (2024)\n\nSamy Jelassi, David Brandfonbrener, Sham\u00a0M. Kakade, and Eran Malach.\n\n\nRepeat after me: Transformers are better than state space models at copying, 2024.\n\n\n",
      "Jin et\u00a0al. (2024)": "\nJin et\u00a0al. (2024)\n\nHongye Jin, Xiaotian Han, Jingfeng Yang, Zhimeng Jiang, Zirui Liu, Chia-Yuan Chang, Huiyuan Chen, and Xia Hu.\n\n\nLLM maybe longLM: Selfextend LLM context window without tuning.\n\n\nIn Forty-first International Conference on Machine Learning, 2024.\n\n\n",
      "Kamradt (2024)": "\nKamradt (2024)\n\nGarrett Kamradt.\n\n\nNeedle in a haystack - pressure testing LLMs, 2024.\n\n\n",
      "Karpinska et\u00a0al. (2024)": "\nKarpinska et\u00a0al. (2024)\n\nMarzena Karpinska, Katherine Thai, Kyle Lo, Tanya Goyal, and Mohit Iyyer.\n\n\nOne thousand and one pairs: A\u201d novel\u201d challenge for long-context language models.\n\n\narXiv preprint arXiv:2406.16264, 2024.\n\n\n",
      "Kim et\u00a0al. (2024)": "\nKim et\u00a0al. (2024)\n\nYekyung Kim, Yapei Chang, Marzena Karpinska, Aparna Garimella, Varun Manjunatha, Kyle Lo, Tanya Goyal, and Mohit Iyyer.\n\n\nFABLES: Evaluating faithfulness and content selection in book-length summarization.\n\n\nIn First Conference on Language Modeling, 2024.\n\n\n",
      "Kocetkov et\u00a0al. (2023)": "\nKocetkov et\u00a0al. (2023)\n\nDenis Kocetkov, Raymond Li, Loubna\u00a0Ben allal, Jia LI, Chenghao Mou, Yacine Jernite, Margaret Mitchell, Carlos\u00a0Mu\u00f1oz Ferrandis, Sean Hughes, Thomas Wolf, Dzmitry Bahdanau, Leandro\u00a0Von Werra, and Harm de\u00a0Vries.\n\n\nThe stack: 3 TB of permissively licensed source code.\n\n\nTransactions on Machine Learning Research, 2023.\n\n\n",
      "Ko\u010disk\u00fd et\u00a0al. (2018)": "\nKo\u010disk\u00fd et\u00a0al. (2018)\n\nTom\u00e1\u0161 Ko\u010disk\u00fd, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl\u00a0Moritz Hermann, G\u00e1bor Melis, and Edward Grefenstette.\n\n\nThe NarrativeQA reading comprehension challenge.\n\n\nTransactions of the Association for Computational Linguistics, 6:317\u2013328, 2018.\n\n\n",
      "Krishna et\u00a0al. (2023)": "\nKrishna et\u00a0al. (2023)\n\nKalpesh Krishna, Erin Bransom, Bailey Kuehl, Mohit Iyyer, Pradeep Dasigi, Arman Cohan, and Kyle Lo.\n\n\nLongEval: Guidelines for human evaluation of faithfulness in long-form summarization.\n\n\nIn Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, pp.\u00a0 1650\u20131669, 2023.\n\n\n",
      "Kwiatkowski et\u00a0al. (2019)": "\nKwiatkowski et\u00a0al. (2019)\n\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew\u00a0M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov.\n\n\nNatural questions: A benchmark for question answering research.\n\n\nTransactions of the Association for Computational Linguistics, 7:452\u2013466, 2019.\n\n\n",
      "Larson et\u00a0al. (2019)": "\nLarson et\u00a0al. (2019)\n\nStefan Larson, Anish Mahendran, Joseph\u00a0J. Peper, Christopher Clarke, Andrew Lee, Parker Hill, Jonathan\u00a0K. Kummerfeld, Kevin Leach, Michael\u00a0A. Laurenzano, Lingjia Tang, and Jason Mars.\n\n\nAn evaluation dataset for intent classification and out-of-scope prediction.\n\n\nIn Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp.\u00a0 1311\u20131316, 2019.\n\n\n",
      "Lenz et\u00a0al. (2024)": "\nLenz et\u00a0al. (2024)\n\nBarak Lenz, Alan Arazi, Amir Bergman, Avshalom Manevich, Barak Peleg, Ben Aviram, Chen Almagor, Clara Fridman, Dan Padnos, et\u00a0al.\n\n\nJamba-1.5: Hybrid transformer-mamba models at scale.\n\n\narXiv preprint arXiv:2408.12570, 2024.\n\n\n",
      "Li et\u00a0al. (2024a)": "\nLi et\u00a0al. (2024a)\n\nJeffrey Li, Alex Fang, Georgios Smyrnis, Maor Ivgi, Matt Jordan, Samir Gadre, Hritik Bansal, Etash Guha, Sedrick Keh, Kushal Arora, Saurabh Garg, Rui Xin, Niklas Muennighoff, Reinhard Heckel, Jean Mercat, Mayee Chen, Suchin Gururangan, Mitchell Wortsman, Alon Albalak, Yonatan Bitton, Marianna Nezhurina, Amro Abbas, Cheng-Yu Hsieh, Dhruba Ghosh, Josh Gardner, Maciej Kilian, Hanlin Zhang, Rulin Shao, Sarah Pratt, Sunny Sanyal, Gabriel Ilharco, Giannis Daras, Kalyani Marathe, Aaron Gokaslan, Jieyu Zhang, Khyathi Chandu, Thao Nguyen, Igor Vasiljevic, Sham Kakade, Shuran Song, Sujay Sanghavi, Fartash Faghri, Sewoong Oh, Luke Zettlemoyer, Kyle Lo, Alaaeldin El-Nouby, Hadi Pouransari, Alexander Toshev, Stephanie Wang, Dirk Groeneveld, Luca Soldaini, Pang\u00a0Wei Koh, Jenia Jitsev, Thomas Kollar, Alexandros\u00a0G. Dimakis, Yair Carmon, Achal Dave, Ludwig Schmidt, and Vaishaal Shankar.\n\n\nDatacomp-lm: In search of the next generation of training sets for language models.\n\n\narXiv preprint arXiv:2406.11794, 2024a.\n\n\n",
      "Li et\u00a0al. (2023)": "\nLi et\u00a0al. (2023)\n\nShenggui Li, Fuzhao Xue, Chaitanya Baranwal, Yongbin Li, and Yang You.\n\n\nSequence parallelism: Long sequence training from system perspective.\n\n\nIn Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (eds.), Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp.\u00a0 2391\u20132404, Toronto, Canada, July 2023. Association for Computational Linguistics.\n\n\ndoi: 10.18653/v1/2023.acl-long.134.\n\n\nURL https://aclanthology.org/2023.acl-long.134.\n\n\n",
      "Li et\u00a0al. (2024b)": "\nLi et\u00a0al. (2024b)\n\nTianle Li, Ge\u00a0Zhang, Quy\u00a0Duc Do, Xiang Yue, and Wenhu Chen.\n\n\nLong-context llms struggle with long in-context learning.\n\n\narXiv preprint arXiv:2404.02060, 2024b.\n\n\n",
      "Lieber et\u00a0al. (2024)": "\nLieber et\u00a0al. (2024)\n\nOpher Lieber, Barak Lenz, Hofit Bata, Gal Cohen, Jhonathan Osin, Itay Dalmedigos, Erez Safahi, Shaked Meirom, Yonatan Belinkov, Shai Shalev-Shwartz, et\u00a0al.\n\n\nJamba: A hybrid transformer-mamba language model.\n\n\narXiv preprint arXiv:2403.19887, 2024.\n\n\n",
      "Liu et\u00a0al. (2024a)": "\nLiu et\u00a0al. (2024a)\n\nJiaheng Liu, Zhiqi Bai, Yuanxing Zhang, Chenchen Zhang, Yu\u00a0Zhang, Ge\u00a0Zhang, Jiakai Wang, Haoran Que, Yukang Chen, Wenbo Su, et\u00a0al.\n\n\nE^ 2-llm: Efficient and extreme length extension of large language models.\n\n\narXiv preprint arXiv:2401.06951, 2024a.\n\n\n",
      "Liu et\u00a0al. (2024b)": "\nLiu et\u00a0al. (2024b)\n\nNelson\u00a0F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang.\n\n\nLost in the Middle: How Language Models Use Long Contexts.\n\n\nTransactions of the Association for Computational Linguistics, 12:157\u2013173, 02 2024b.\n\n\n",
      "Liu et\u00a0al. (2021)": "\nLiu et\u00a0al. (2021)\n\nXingkun Liu, Arash Eshghi, Pawel Swietojanski, and Verena Rieser.\n\n\nBenchmarking natural language understanding services for building conversational agents.\n\n\nIn Increasing naturalness and flexibility in spoken dialogue interaction: 10th international workshop on spoken dialogue systems, pp.\u00a0 165\u2013183. Springer, 2021.\n\n\n",
      "Lu et\u00a0al. (2024)": "\nLu et\u00a0al. (2024)\n\nYi\u00a0Lu, Jing\u00a0Nathan Yan, Songlin Yang, Justin\u00a0T Chiu, Siyu Ren, Fei Yuan, Wenting Zhao, Zhiyong Wu, and Alexander\u00a0M Rush.\n\n\nA controlled study on long context extension and generalization in llms.\n\n\narXiv preprint arXiv:2409.12181, 2024.\n\n\n",
      "Ma et\u00a0al. (2022)": "\nMa et\u00a0al. (2022)\n\nXuezhe Ma, Chunting Zhou, Xiang Kong, Junxian He, Liangke Gui, Graham Neubig, Jonathan May, and Luke Zettlemoyer.\n\n\nMega: moving average equipped gated attention.\n\n\narXiv preprint arXiv:2209.10655, 2022.\n\n\n",
      "Mallen et\u00a0al. (2023)": "\nMallen et\u00a0al. (2023)\n\nAlex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Daniel Khashabi, and Hannaneh Hajishirzi.\n\n\nWhen not to trust language models: Investigating effectiveness of parametric and non-parametric memories.\n\n\nIn Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (eds.), Association for Computational Linguistics (ACL), pp.\u00a0 9802\u20139822, Toronto, Canada, July 2023. Association for Computational Linguistics.\n\n\n",
      "Mosaic ML (2022)": "\nMosaic ML (2022)\n\nMosaic ML.\n\n\nstreaming.\n\n\n<https://github.com/mosaicml/streaming/>, 2022.\n\n\n",
      "Ouyang et\u00a0al. (2022)": "\nOuyang et\u00a0al. (2022)\n\nLong Ouyang, Jeffrey Wu, Xu\u00a0Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et\u00a0al.\n\n\nTraining language models to follow instructions with human feedback.\n\n\nAdvances in Neural Information Processing Systems (NeurIPS), 35:27730\u201327744, 2022.\n\n\nURL https://proceedings.neurips.cc/paper_files/paper/2022/file/b1efde53be364a73914f58805a001731-Paper-Conference.pdf.\n\n\n",
      "Pan et\u00a0al. (2023)": "\nPan et\u00a0al. (2023)\n\nJane Pan, Tianyu Gao, Howard Chen, and Danqi Chen.\n\n\nWhat in-context learning \u201clearns\u201d in-context: Disentangling task recognition and task learning.\n\n\nIn Findings of the Association for Computational Linguistics: ACL 2023, pp.\u00a0 8298\u20138319. Association for Computational Linguistics, 2023.\n\n\n",
      "Paster et\u00a0al. (2024)": "\nPaster et\u00a0al. (2024)\n\nKeiran Paster, Marco\u00a0Dos Santos, Zhangir Azerbayev, and Jimmy Ba.\n\n\nOpenwebmath: An open dataset of high-quality mathematical web text.\n\n\nIn The Twelfth International Conference on Learning Representations, 2024.\n\n\n",
      "Paszke et\u00a0al. (2019)": "\nPaszke et\u00a0al. (2019)\n\nAdam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et\u00a0al.\n\n\nPytorch: An imperative style, high-performance deep learning library.\n\n\nAdvances in neural information processing systems, 32, 2019.\n\n\n",
      "Penedo et\u00a0al. (2023)": "\nPenedo et\u00a0al. (2023)\n\nGuilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Hamza Alobeidli, Alessandro Cappelli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay.\n\n\nThe refinedweb dataset for falcon llm: Outperforming curated corpora with web data only.\n\n\nIn A.\u00a0Oh, T.\u00a0Naumann, A.\u00a0Globerson, K.\u00a0Saenko, M.\u00a0Hardt, and S.\u00a0Levine (eds.), Advances in Neural Information Processing Systems, volume\u00a036, pp.\u00a0 79155\u201379172. Curran Associates, Inc., 2023.\n\n\n",
      "Penedo et\u00a0al. (2024)": "\nPenedo et\u00a0al. (2024)\n\nGuilherme Penedo, Hynek Kydl\u00ed\u010dek, Loubna\u00a0Ben allal, Anton Lozhkov, Margaret Mitchell, Colin Raffel, Leandro\u00a0Von Werra, and Thomas Wolf.\n\n\nThe fineweb datasets: Decanting the web for the finest text data at scale, 2024.\n\n\n",
      "Peng et\u00a0al. (2023)": "\nPeng et\u00a0al. (2023)\n\nBo\u00a0Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Stella Biderman, Huanqi Cao, Xin Cheng, Michael Chung, Leon Derczynski, Xingjian Du, Matteo Grella, Kranthi Gv, Xuzheng He, Haowen Hou, Przemyslaw Kazienko, Jan Kocon, Jiaming Kong, Bart\u0142omiej Koptyra, Hayden Lau, Jiaju Lin, Krishna Sri\u00a0Ipsit Mantri, Ferdinand Mom, Atsushi Saito, Guangyu Song, Xiangru Tang, Johan Wind, Stanis\u0142aw Wo\u017aniak, Zhenyuan Zhang, Qinghua Zhou, Jian Zhu, and Rui-Jie Zhu.\n\n\nRWKV: Reinventing RNNs for the transformer era.\n\n\nIn Findings of the Association for Computational Linguistics: EMNLP 2023, pp.\u00a0 14048\u201314077, 2023.\n\n\n",
      "Peng et\u00a0al. (2024)": "\nPeng et\u00a0al. (2024)\n\nBowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole.\n\n\nYaRN: Efficient context window extension of large language models.\n\n\nIn The Twelfth International Conference on Learning Representations, 2024.\n\n\n",
      "Press et\u00a0al. (2022)": "\nPress et\u00a0al. (2022)\n\nOfir Press, Noah Smith, and Mike Lewis.\n\n\nTrain short, test long: Attention with linear biases enables input length extrapolation.\n\n\nIn International Conference on Learning Representations (ICLR), 2022.\n\n\n",
      "Rae et\u00a0al. (2020)": "\nRae et\u00a0al. (2020)\n\nJack\u00a0W. Rae, Anna Potapenko, Siddhant\u00a0M. Jayakumar, Chloe Hillier, and Timothy\u00a0P. Lillicrap.\n\n\nCompressive transformers for long-range sequence modelling.\n\n\nIn International Conference on Learning Representations, 2020.\n\n\n",
      "Reid et\u00a0al. (2024)": "\nReid et\u00a0al. (2024)\n\nMachel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Jean-baptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, et\u00a0al.\n\n\nGemini 1.5: Unlocking multimodal understanding across millions of tokens of context.\n\n\narXiv preprint arXiv:2403.05530, 2024.\n\n\n",
      "Rubin & Berant (2023)": "\nRubin & Berant (2023)\n\nOhad Rubin and Jonathan Berant.\n\n\nLong-range language modeling with self-retrieval, 2023.\n\n\n",
      "Sakaguchi et\u00a0al. (2021)": "\nSakaguchi et\u00a0al. (2021)\n\nKeisuke Sakaguchi, Ronan\u00a0Le Bras, Chandra Bhagavatula, and Yejin Choi.\n\n\nWinogrande: An adversarial winograd schema challenge at scale.\n\n\nCommunications of the ACM, 64(9):99\u2013106, 2021.\n\n\n",
      "Shaham et\u00a0al. (2023)": "\nShaham et\u00a0al. (2023)\n\nUri Shaham, Maor Ivgi, Avia Efrat, Jonathan Berant, and Omer Levy.\n\n\nZeroSCROLLS: A zero-shot benchmark for long text understanding.\n\n\nIn Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Findings of the Association for Computational Linguistics: EMNLP 2023, pp.\u00a0 7977\u20137989, Singapore, December 2023. Association for Computational Linguistics.\n\n\n",
      "Shen et\u00a0al. (2022)": "\nShen et\u00a0al. (2022)\n\nZejiang Shen, Kyle Lo, Lauren Yu, Nathan Dahlberg, Margo Schlanger, and Doug Downey.\n\n\nMulti-lexsum: Real-world summaries of civil rights lawsuits at multiple granularities.\n\n\nIn Advances in Neural Information Processing Systems, volume\u00a035, pp.\u00a0 13158\u201313173. Curran Associates, Inc., 2022.\n\n\n",
      "Singh et\u00a0al. (2024)": "\nSingh et\u00a0al. (2024)\n\nAaditya\u00a0K Singh, Yu\u00a0Yang, Kushal Tirumala, Mostafa Elhoushi, and Ari\u00a0S Morcos.\n\n\nBrevity is the soul of wit: Pruning long files for code generation.\n\n\narXiv preprint arXiv:2407.00434, 2024.\n\n\n",
      "Soboleva et\u00a0al. (2023)": "\nSoboleva et\u00a0al. (2023)\n\nDaria Soboleva, Faisal Al-Khateeb, Robert Myers, Jacob\u00a0R Steeves, Joel Hestness, and Nolan Dey.\n\n\nSlimPajama: A 627B token cleaned and deduplicated version of RedPajama, 2023.\n\n\n",
      "Soldaini et\u00a0al. (2024)": "\nSoldaini et\u00a0al. (2024)\n\nLuca Soldaini, Rodney Kinney, Akshita Bhagia, Dustin Schwenk, David Atkinson, Russell Authur, Ben Bogin, Khyathi Chandu, Jennifer Dumas, Yanai Elazar, Valentin Hofmann, Ananya Jha, Sachin Kumar, Li\u00a0Lucy, Xinxi Lyu, Nathan Lambert, Ian Magnusson, Jacob Morrison, Niklas Muennighoff, Aakanksha Naik, Crystal Nam, Matthew Peters, Abhilasha Ravichander, Kyle Richardson, Zejiang Shen, Emma Strubell, Nishant Subramani, Oyvind Tafjord, Evan Walsh, Luke Zettlemoyer, Noah Smith, Hannaneh Hajishirzi, Iz\u00a0Beltagy, Dirk Groeneveld, Jesse Dodge, and Kyle Lo.\n\n\nDolma: an open corpus of three trillion tokens for language model pretraining research.\n\n\nIn Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp.\u00a0 15725\u201315788, Bangkok, Thailand, August 2024. Association for Computational Linguistics.\n\n\ndoi: 10.18653/v1/2024.acl-long.840.\n\n\nURL https://aclanthology.org/2024.acl-long.840.\n\n\n",
      "Su et\u00a0al. (2021)": "\nSu et\u00a0al. (2021)\n\nJianlin Su, Yu\u00a0Lu, Shengfeng Pan, Ahmed Murtadha, Bo\u00a0Wen, and Yunfeng Liu.\n\n\nRoformer: Enhanced transformer with rotary position embedding, 2021.\n\n\n",
      "Sun et\u00a0al. (2023)": "\nSun et\u00a0al. (2023)\n\nYutao Sun, Li\u00a0Dong, Shaohan Huang, Shuming Ma, Yuqing Xia, Jilong Xue, Jianyong Wang, and Furu Wei.\n\n\nRetentive network: A successor to transformer for large language models.\n\n\narXiv preprint arXiv:2307.08621, 2023.\n\n\n",
      "Sun et\u00a0al. (2024)": "\nSun et\u00a0al. (2024)\n\nYutao Sun, Li\u00a0Dong, Yi\u00a0Zhu, Shaohan Huang, Wenhui Wang, Shuming Ma, Quanlu Zhang, Jianyong Wang, and Furu Wei.\n\n\nYou only cache once: Decoder-decoder architectures for language models.\n\n\narXiv preprint arXiv:2405.05254, 2024.\n\n\n",
      "Together (2023)": "\nTogether (2023)\n\nTogether.\n\n\nRedpajama: An open source recipe to reproduce llama training dataset, April 2023.\n\n\n",
      "Waleffe et\u00a0al. (2024)": "\nWaleffe et\u00a0al. (2024)\n\nRoger Waleffe, Wonmin Byeon, Duncan Riach, Brandon Norick, Vijay Korthikanti, Tri Dao, Albert Gu, Ali Hatamizadeh, Sudhakar Singh, Deepak Narayanan, et\u00a0al.\n\n\nAn empirical study of mamba-based language models.\n\n\narXiv preprint arXiv:2406.07887, 2024.\n\n\n",
      "Wang et\u00a0al. (2024)": "\nWang et\u00a0al. (2024)\n\nJunxiong Wang, Daniele Paliotta, Avner May, Alexander\u00a0M Rush, and Tri Dao.\n\n\nThe mamba in the llama: Distilling and accelerating hybrid models.\n\n\narXiv preprint arXiv:2408.15237, 2024.\n\n\n",
      "Wettig et\u00a0al. (2024)": "\nWettig et\u00a0al. (2024)\n\nAlexander Wettig, Aatmik Gupta, Saumya Malik, and Danqi Chen.\n\n\nQuRating: Selecting high-quality data for training language models.\n\n\nIn Ruslan Salakhutdinov, Zico Kolter, Katherine Heller, Adrian Weller, Nuria Oliver, Jonathan Scarlett, and Felix Berkenkamp (eds.), Proceedings of the 41st International Conference on Machine Learning, volume 235 of Proceedings of Machine Learning Research, pp.\u00a0 52915\u201352971. PMLR, 21\u201327 Jul 2024.\n\n\n",
      "Wolf et\u00a0al. (2020)": "\nWolf et\u00a0al. (2020)\n\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le\u00a0Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander Rush.\n\n\nTransformers: State-of-the-art natural language processing.\n\n\nIn Qun Liu and David Schlangen (eds.), Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pp.\u00a0 38\u201345, Online, October 2020. Association for Computational Linguistics.\n\n\ndoi: 10.18653/v1/2020.emnlp-demos.6.\n\n\n",
      "Wu et\u00a0al. (2021)": "\nWu et\u00a0al. (2021)\n\nJeff Wu, Long Ouyang, Daniel\u00a0M Ziegler, Nisan Stiennon, Ryan Lowe, Jan Leike, and Paul Christiano.\n\n\nRecursively summarizing books with human feedback.\n\n\narXiv preprint arXiv:2109.10862, 2021.\n\n\n",
      "Wu et\u00a0al. (2024)": "\nWu et\u00a0al. (2024)\n\nWenhao Wu, Yizhong Wang, Yao Fu, Xiang Yue, Dawei Zhu, and Sujian Li.\n\n\nLong context alignment with short instructions and synthesized positions.\n\n\narXiv preprint arXiv:2405.03939, 2024.\n\n\n",
      "Xiao et\u00a0al. (2024a)": "\nXiao et\u00a0al. (2024a)\n\nChaojun Xiao, Pengle Zhang, Xu\u00a0Han, Guangxuan Xiao, Yankai Lin, Zhengyan Zhang, Zhiyuan Liu, Song Han, and Maosong Sun.\n\n\nInfLLM: Unveiling the intrinsic capacity of LLMs for understanding extremely long sequences with training-free memory.\n\n\narXiv preprint arXiv:2402.04617, 2024a.\n\n\n",
      "Xiao et\u00a0al. (2024b)": "\nXiao et\u00a0al. (2024b)\n\nGuangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis.\n\n\nEfficient streaming language models with attention sinks.\n\n\nIn The Twelfth International Conference on Learning Representations, 2024b.\n\n\n",
      "Xiong et\u00a0al. (2023)": "\nXiong et\u00a0al. (2023)\n\nWenhan Xiong, Jingyu Liu, Igor Molybog, Hejia Zhang, Prajjwal Bhargava, Rui Hou, Louis Martin, Rashi Rungta, Karthik\u00a0Abinav Sankararaman, Barlas Oguz, Madian Khabsa, Han Fang, Yashar Mehdad, Sharan Narang, Kshitiz Malik, Angela Fan, Shruti Bhosale, Sergey Edunov, Mike Lewis, Sinong Wang, and Hao Ma.\n\n\nEffective long-context scaling of foundation models, 2023.\n\n\n",
      "Xiong et\u00a0al. (2024)": "\nXiong et\u00a0al. (2024)\n\nZheyang Xiong, Vasilis Papageorgiou, Kangwook Lee, and Dimitris Papailiopoulos.\n\n\nFrom artificial needles to real haystacks: Improving retrieval capabilities in llms by finetuning on synthetic data.\n\n\narXiv preprint arXiv:2406.19292, 2024.\n\n\n",
      "Yang et\u00a0al. (2024a)": "\nYang et\u00a0al. (2024a)\n\nAn\u00a0Yang, Baosong Yang, Binyuan Hui, Bo\u00a0Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, et\u00a0al.\n\n\nQwen2 technical report.\n\n\narXiv preprint arXiv:2407.10671, 2024a.\n\n\n",
      "Yang et\u00a0al. (2024b)": "\nYang et\u00a0al. (2024b)\n\nSonglin Yang, Bailin Wang, Yikang Shen, Rameswar Panda, and Yoon Kim.\n\n\nGated linear attention transformers with hardware-efficient training.\n\n\nIn Forty-first International Conference on Machine Learning, 2024b.\n\n\n",
      "Yang et\u00a0al. (2018)": "\nYang et\u00a0al. (2018)\n\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher\u00a0D. Manning.\n\n\nHotpotQA: A dataset for diverse, explainable multi-hop question answering.\n\n\nIn Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pp.\u00a0 2369\u20132380, 2018.\n\n\n",
      "Yen et\u00a0al. (2024a)": "\nYen et\u00a0al. (2024a)\n\nHoward Yen, Tianyu Gao, and Danqi Chen.\n\n\nLong-context language modeling with parallel context encoding.\n\n\nIn Association for Computational Linguistics (ACL), pp.\u00a0 2588\u20132610, 2024a.\n\n\n",
      "Yen et\u00a0al. (2024b)": "\nYen et\u00a0al. (2024b)\n\nHoward Yen, Tianyu Gao, Minmin Hou, Ke\u00a0Ding, Daniel Fleischer, Peter Izsak, Moshe Wasserblat, and Danqi Chen.\n\n\nHelmet: How to evaluate long-context language models effectively and thoroughly.\n\n\n2024b.\n\n\n",
      "Yoshida et\u00a0al. (2020)": "\nYoshida et\u00a0al. (2020)\n\nDavis Yoshida, Allyson Ettinger, and Kevin Gimpel.\n\n\nAdding recurrence to pretrained transformers for improved efficiency and context size, 2020.\n\n\n",
      "Zellers et\u00a0al. (2019)": "\nZellers et\u00a0al. (2019)\n\nRowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi.\n\n\nHellaSwag: Can a machine really finish your sentence?\n\n\nIn Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp.\u00a0 4791\u20134800, 2019.\n\n\n",
      "Zhang & Bansal (2021)": "\nZhang & Bansal (2021)\n\nShiyue Zhang and Mohit Bansal.\n\n\nFinding a balanced degree of automation for summary evaluation.\n\n\nIn Empirical Methods in Natural Language Processing (EMNLP), pp.\u00a0 6617\u20136632, 2021.\n\n\n",
      "Zhang et\u00a0al. (2024a)": "\nZhang et\u00a0al. (2024a)\n\nXinrong Zhang, Yingfa Chen, Shengding Hu, Zihang Xu, Junhao Chen, Moo Hao, Xu\u00a0Han, Zhen Thai, Shuo Wang, Zhiyuan Liu, and Maosong Sun.\n\n\n\u221e\\infty\u221eBench: Extending long context evaluation beyond 100K tokens.\n\n\nIn Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp.\u00a0 15262\u201315277, 2024a.\n\n\n",
      "Zhang et\u00a0al. (2024b)": "\nZhang et\u00a0al. (2024b)\n\nYikai Zhang, Junlong Li, and Pengfei Liu.\n\n\nExtending llms\u2019 context window with 100 samples.\n\n\narXiv preprint arXiv:2401.07004, 2024b.\n\n\n",
      "Zhao et\u00a0al. (2024)": "\nZhao et\u00a0al. (2024)\n\nLiang Zhao, Tianwen Wei, Liang Zeng, Cheng Cheng, Liu Yang, Peng Cheng, Lijie Wang, Chenxia Li, Xuejie Wu, Bo\u00a0Zhu, et\u00a0al.\n\n\nLongskywork: A training recipe for efficiently extending context length in large language models.\n\n\narXiv preprint arXiv:2406.00605, 2024.\n\n\n",
      "Zhu et\u00a0al. (2024)": "\nZhu et\u00a0al. (2024)\n\nDawei Zhu, Nan Yang, Liang Wang, Yifan Song, Wenhao Wu, Furu Wei, and Sujian Li.\n\n\nPoSE: Efficient context window extension of LLMs via positional skip-wise training.\n\n\nIn The Twelfth International Conference on Learning Representations, 2024.\n\n\n"
    },
    "domain": "cs.CL",
    "citation_count": 0
  },
  "ce797714-d92e-4043-a5fb-276f69eee905": {
    "pk": "ce797714-d92e-4043-a5fb-276f69eee905",
    "authors": [
      "Xinrong Zhang",
      "Yingfa Chen",
      "Shengding Hu",
      "Zihang Xu",
      "Junhao Chen",
      "Moo Khai Hao",
      "Xu Han",
      "Zhen Leng Thai",
      "Shuo Wang",
      "Zhiyuan Liu",
      "Maosong Sun"
    ],
    "title": "$\\infty$Bench: Extending Long Context Evaluation Beyond 100K Tokens",
    "abstract": "Processing and reasoning over long contexts is crucial for many practical applications of Large Language Models (LLMs), such as document comprehension and agent construction. Despite recent strides in making LLMs process contexts with more than 100K tokens, there is currently a lack of a standardized benchmark to evaluate this long-context capability. Existing public benchmarks typically focus on contexts around 10K tokens, limiting the assessment and comparison of LLMs in processing longer contexts. In this paper, we propose $\\infty$Bench, the first LLM benchmark featuring an average data length surpassing 100K tokens. $\\infty$Bench comprises synthetic and realistic tasks spanning diverse domains, presented in both English and Chinese. The tasks in $\\infty$Bench are designed to require well understanding of long dependencies in contexts, and make simply retrieving a limited number of passages from contexts not sufficient for these tasks. In our experiments, based on $\\infty$Bench, we evaluate the state-of-the-art proprietary and open-source LLMs tailored for processing long contexts. The results indicate that existing long context LLMs still require significant advancements to effectively process 100K+ context. We further present three intriguing analyses regarding the behavior of LLMs processing long context.",
    "url": "http://arxiv.org/abs/2402.13718v3",
    "timestamp": 1708515029,
    "sections": {
      "1 Introduction": "\n\n1 Introduction\n\nFigure 1: The performance of GPT-4, Kimi-Chat, YaRN-Mistral, and Claude 2 on \u221e\\infty\u221eBench. A higher value represents better performance.\n\n\n\n\n\nBenchmark\nAvg Len\nEn\nZh\nCode\nMath\nNovel\nDialogue\nSynthetic\n\n\n\n\nLRA\u00a0(Tay et\u00a0al., 2020)\n\n\n\u223csimilar-to\\sim\u223c10K\n\u2713\n\u2717\n\u2717\n\u2713\n\u2717\n\u2717\n\u2713\n\n\nLongBench\u00a0(Bai et\u00a0al., 2023)\n\n\n\u223csimilar-to\\sim\u223c10K\n\u2713\n\u2713\n\u2713\n\u2717\n\u2713\n\u2713\n\u2713\n\n\nL-Eval\u00a0(An et\u00a0al., 2023)\n\n4K - 60K\n\u2713\n\u2717\n\u2713\n\u2713\n\u2717\n\u2717\n\u2713\n\n\nLooGLE\u00a0(Li et\u00a0al., 2023)\n\n\n\u223csimilar-to\\sim\u223c20K\n\u2713\n\u2717\n\u2717\n\u2717\n\u2717\n\u2713\n\u2717\n\n\n\n\u221e\\infty\u221eBench (ours)\n\n\u223csimilar-to\\sim\u223c200K\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\n\n\nTable 1: Comparison to existing long-context benchmarks and \u221e\\infty\u221eBench. \u201cEn\u201d and \u201cZh\u201d refer to English and Chinese tasks. \u201cCode\u201d, \u201cMath\u201d, \u201cNovel\u201d, \u201cDialogue\u201d indicate whether the domain includes tasks from those domains, and \u201cSynthetic\u201d indicates whether there are auto-generated tasks.\n\n\nIn recent years, large language models (LLMs) Brown et\u00a0al. (2020); OpenAI (2023a); Touvron et\u00a0al. (2023) have exhibited exceptional performance across a range of natural language processing (NLP) tasks\u00a0Qiu et\u00a0al. (2020); Han et\u00a0al. (2021). LLMs are showing a promising direction toward generalist task assistance, being capable of aiding users in practical tasks through conversational interactions. These tasks include web navigation\u00a0Nakano et\u00a0al. (2021), analysis of code repositories\u00a0Chen et\u00a0al. (2021), and extraction of useful information from documents\u00a0Ko\u010disk\u1ef3 et\u00a0al. (2018), indicating a step towards artificial general intelligence. For these LLM-based scenarios, the ability to process long contexts is increasingly critical, in addition to understanding fine-grained semantics and possessing extensive knowledge\u00a0Dong et\u00a0al. (2023); Huang et\u00a0al. (2023). Textual documents, historical dialogues, complex instructions, and cumbersome workflows, which constitute the data most directly processed in daily tasks, must be input to LLMs as long contexts for effective processing.\n\n\nDespite this growing importance, LLMs consistently face challenges in processing long contexts, primarily due to the substantial computational resources required for long sequence training\u00a0Dao et\u00a0al. (2022); Dao (2023) as well as the apparent inability to generalize to sequences longer than those encountered during training\u00a0Chen et\u00a0al. (2023a); Peng et\u00a0al. (2023b).\nLLMs are typically trained on sequences containing no more than 8K tokens\u00a0Touvron et\u00a0al. (2023); Penedo et\u00a0al. (2023); Biderman et\u00a0al. (2023), and thus cannot well handle contexts exceeding 8K tokens.\nThese limitations have largely restricted most LLMs from being applied to more complex tasks.\n\n\nRecent advancements in training infrastructure\u00a0Shoeybi et\u00a0al. (2019); Narayanan et\u00a0al. (2021); Dao et\u00a0al. (2022); Dao (2023), and efforts to improve length generalization\u00a0Anil et\u00a0al. (2022); Chen et\u00a0al. (2023b); Peng et\u00a0al. (2023b)333https://www.reddit.com/r/LocalLLaMA/comments/14lz7j5/ntkaware_scaled_rope_allows_llama_models_to_have/ have led to rapid developments in long-context LLMs.\nBased on these improved training infrastructures and length generalization methods, several LLMs have purportedly managed to process data exceeding 100K tokens\u00a0(Peng et\u00a0al., 2023b; OpenAI, 2023b; 01.AI, 2023b, a), with Claude 2\u00a0Anthropic (2023) and Kimi-Chat\u00a0AI (2023) even claiming to be able to process up to 200K tokens.\nHowever, the rapid emergence of long-context LLMs has outpaced the development of adequate evaluation benchmarks. Present long-context benchmarks predominantly feature contexts averaging around 10K tokens\u00a0(Bai et\u00a0al., 2023; Tay et\u00a0al., 2020), invariably falling below 100K tokens. This lag in the advancement of long-context evaluation methodologies impedes both the comparative analysis of diverse long-context LLMs and the pinpointing of potential enhancements in long-context processing.\n\n\nIn this work, we present \u221e\\infty\u221eBench , the first comprehensive benchmark featuring an average data length surpassing 100K tokens. \u221e\\infty\u221eBench includes tasks in different domains (novels, code, math, etc.) and languages (English and Chinese). To fully evaluate the performance of long-context LLMs, \u221e\\infty\u221eBench integrates synthetic tasks that can be auto-generated for even longer contexts (e.g., finding the top-k\ud835\udc58kitalic_k number in an array) in addition to a set of realistic tasks.\n\n\nTo construct tasks annotated by humans, we develop 5 annotation pipelines for detailed example annotation. These pipelines undergo iterative refinement until the examples meet quality standards. Auto-generated tasks, conversely, can be easily scaled to various lengths. Upon completing \u221e\\infty\u221eBench, we assess the performance of several state-of-the-art (SOTA) long-context LLMs on this benchmark to gauge its difficulty and evaluate the effectiveness of these models. The results show that current SOTA LLMs are not fully equipped to handle all tasks within \u221e\\infty\u221eBench, highlighting the ongoing challenge of enabling LLMs to process long contexts effectively. We also conduct intriguing analyses on the behavior of LLMs on such long contexts, including the task length ablation, the absent of \u201clost in the middle phenomenon\u00a0Liu et\u00a0al. (2023)\u201d, and the context recalling prompting techniques.\n\n\nOur contributions can be summarized as follows:\n\n\n\u2022\n\nWe construct and release \u221e\\infty\u221eBench, the first multi-domain bilingual benchmark for evaluating the ability to understand and reason over contexts surpassing 100K tokens.\n\n\n\n\u2022\n\nWe evaluate SOTA long-context LLMs on \u221e\\infty\u221eBench, which reveals severe performance degradation of these LLMs when scaling context lengths. These experimental results and analysis also indicate promising directions to improve long-context LLMs.\n\n\n\n\n",
      "2 Related Work": "\n\n2 Related Work\n\nExtending Context Length\n\nTransformers, typically trained on text sequences under 8K tokens due to self-attention\u2019s quadratic complexity, face challenges in longer downstream tasks. To address this, two main strategies have emerged: firstly, the development of positional encodings capable of handling longer text sequences\u00a0Sun et\u00a0al. (2022); Press et\u00a0al. (2021), and secondly, the refinement of inference stage techniques to extend current LLMs post-training. The primary approach involves modifying rotary positional encoding\u00a0Su et\u00a0al. (2023) and implementing post-training adjustments to better manage the increased relative positional distances in longer sequences\u00a0Zhu et\u00a0al. (2023); Peng et\u00a0al. (2023b); Chen et\u00a0al. (2023a).\n\n\n\n100K+ LLMs\n\nRecently, many LLMs have shown the ability to handle over 100K tokens. Some popular proprietary 100K+ LLMs include GPT-4, Claude 2\u00a0(Anthropic, 2023), and Kimi-Chat\u00a0(AI, 2023).\nOn the other hand, there are much fewer open-source 100K+ models. Some notable models include YaRN\u00a0(Peng et\u00a0al., 2023b) and Yi-200K\u00a0(01.AI, 2023a, b).\nIn this paper, we benchmark GPT-4, Claude 2, Kimi-Chat, and YaRN-Mistral-7B-128K444https://huggingface.co/NousResearch/Yarn-Mistral-7b-128k, we denote this model by YaRN-Mistral. on \u221e\\infty\u221eBench, which are some of the latest and strongest LLMs that claim to be able to handle over 100K tokens. \n\n\n\nInference Infrastructure\n\nNumerous studies aim to accelerate self-attention computation. Research primarily concentrates on refining attention mechanisms through improved IO management\u00a0(Dao et\u00a0al., 2022; Dao, 2023), memory optimization\u00a0(Kwon et\u00a0al., 2023; Shazeer, 2019; Ainslie et\u00a0al., 2023), and enhanced parallelization in decoding\u00a0(Dao et\u00a0al., 2023; Hong et\u00a0al., 2023). Approaches like Sliding Window Attention\u00a0(Beltagy et\u00a0al., 2020), LM-Infinite\u00a0(Han et\u00a0al., 2023), and StreamingLLM\u00a0(Xiao et\u00a0al., 2023) introduce attention variants for handling infinitely long sequences without overwhelming computation or memory overhead. However, these techniques often face challenges in maintaining historical information.\n\n\n\nLong Context Benchmarks\n\nSeveral benchmarks exist for evaluating long-context AI models, notably featuring context lengths of around 10K tokens. L-Eval\u00a0(An et\u00a0al., 2023) and LongBench\u00a0(Bai et\u00a0al., 2023) are prominent examples, aggregating pre-existing tasksKocisk\u00fd et\u00a0al. (2017); Dasigi et\u00a0al. (2021); Yang et\u00a0al. (2018); Huang et\u00a0al. (2021); Joshi et\u00a0al. (2017) into comprehensive benchmarks. LongBench encompasses four categories\u2014QA, summarization, synthetic retrieval, and code\u2014spanning 21 tasks, with four being novel. Conversely, L-Eval incorporates 18 tasks across QA, summarization, math, retrieval, and multiple-choice (MC) domains, introducing three new tasks. Another notable benchmark, LooGLE\u00a0(Li et\u00a0al., 2023), differentiates between short and long dependency examples, focusing on summary and QA tasks; its summary corpus contrasts with ours, utilizing academic papers over novels. The Long-Range Arena (LRA) (Tay et\u00a0al., 2020) further diversifies with six tasks in text, image, and math, designed for scalability. In comparison, \u221e\\infty\u221eBench stands out for its substantially longer contexts and a broader range of task domains. Table\u00a01 offers a detailed comparison of these long-context benchmarks.\n\n\nFigure 2: The statistics of the data in \u221e\\infty\u221eBench. The radius of each segment indicates the length of input plus output on the logarithmic scale, and the width (or angle) indicates the number of examples (proportionally to the total number of examples).\n\n\n",
      "3 \u221e\\infty\u221eBench": "\n\n3 \u221e\\infty\u221eBench\n\n\n\n\n\nTask\nAnnotation\n# Ex.\nAvg Len\n\n\n\n\nRet.PassKey\nAuto\n590\n122.4K/2\n\n\nRet.Number\nAuto\n590\n122.4K/4\n\n\nRet.KV\nAuto\n500\n121.1K/22.7\n\n\nEn.Sum\nHuman\n103\n103.5K/1.1K\n\n\nEn.QA\nHuman\n351\n192.6k/4.8\n\n\nEn.MC\nHuman\n229\n184.4K/5.3\n\n\nZh.QA\nHuman\n189\n2068.6K/6.3\n\n\nEn.Dia\nAuto\n200\n103.6K/3.4\n\n\nCode.Debug\nHuman\n394\n114.7K/4.8\n\n\nCode.Run\nAuto\n400\n75.2K/1.3\n\n\nMath.Calc\nAuto\n50\n43.9K/43.9K\n\n\nMath.Find\nAuto\n350\n87.9K/1.3\n\n\n\nTable 2: Data statistics. The columns indicate whether the annotation was auto-generated or done by humans, the number of examples, and the average length (input/output) in tokens.\n\n\n\n\n\nTask\nGPT-4\nYaRN-Mistral\nKimi-Chat\nClaude 2\n\n\n\n\nRetrieve.PassKey\n100.00\n92.71\n98.14\n97.80\n\n\nRetrieve.Number\n100.00\n56.61\n95.42\n98.14\n\n\nRetrieve.KV\n89.00\n0.00\n53.60\n65.40\n\n\nEn.Sum\n14.73\n9.09\n17.93\n14.45\n\n\nEn.QA\n22.22\n9.55\n16.52\n11.97\n\n\nEn.MC\n67.25\n27.95\n72.49\n62.88\n\n\nEn.Dia\n8.50\n7.50\n11.50\n46.50\n\n\nZh.QA\n23.06\n16.98\n18.62\n10.53\n\n\nCode.Debug\n39.59\n0.76\n18.02\n2.28\n\n\nCode.Run\n23.25\n1.25\n2.00\n2.50\n\n\nMath.Calc\n0.01\n0.00\n0.00\n0.00\n\n\nMath.Find\n60.00\n17.14\n12.57\n32.29\n\n\nAverage\n45.63\n19.96\n34.73\n37.06\n\n\n\nTable 3: Main results. The performance of the baselines in \u221e\\infty\u221eBench. For multiple-choice questions, if the model does not output one of the options, we regard it as an empty prediction, and thus give it a score of 0.\n\n\n\u221e\\infty\u221eBench encompasses 12 tasks spanning 5 domains: retrieval, code, math, novels, and dialogue. Two of these tasks are derived from existing literatureMohtashami and Jaggi (2023); Liu et\u00a0al. (2023). Among the newly introduced tasks, half are generated automatically, while the remainder are annotated by humans.\n\n\nIn total, \u221e\\infty\u221eBench includes 3946 examples, featuring a length beyond 100K tokens (average approximately 200K). Figure\u00a02 illustrates the distribution of these tasks. Table\u00a02 details their respective input and output lengths as well as the number of examples per task.\n\n\nNext, we illustrate each task in detail. The tasks can be grouped into two broad categories. The first involves realistic context collected from real-world scenarios which has potential practical usage of long context LLMs. The second depends on synthetic contexts which are created or collected for testing certain capabilities of long-context LLMs.\n\n\n\n3.1 Realistic Context\n\n\n3.1.1 Novel\n\nWe develop novel-based tasks as outlined in Figure\u00a03, utilizing novels sourced from websites555https://www.sparknotes.com/666https://www.cliffsnotes.com/ and are manually filtered. More annotation information in Appendix.\u00a0C.\n\n\nIn these tasks, models are tasked with reasoning over entire novels presented during inference. Recognizing that many novels, along with their movie adaptations and related discussions, are accessible online and may have been encountered by LLMs during training, we adopt key entity replacement as a countermeasure. This involves substituting prominent entities determined by annotators, such as main character names, with unrelated ones, creating \u201cfake novels\u201d.\n\n\nUsing these altered novels, we design tasks in three formats: summarization, open-form question answering (QA), and multiple-choice (MC) questions, applying key entity replacement to the annotations as well. All English tasks share the same set of modified novels.\n\n\nFigure 3: The annotation pipelines for the human-annotated tasks in \u221e\\infty\u221eBench.\n\n\nEn.Sum\n\nThe En.Sum task requires models to generate a concise summary of the novel. Gold standard labels are sourced from the web and undergo manual filtering to remove non-summarization content, like comments. Model performance is evaluated using the ROUGE-L-Sum metric\u00a0(Lin, 2004).\n\n\n\nEn.QA & Zh.QA\n\nWe employ the same annotation pipeline for both En.QA and Zh.QA tasks, ensuring that the questions necessitate long-range dependency and reasoning, beyond simple short passage retrieval. The tasks are primarily categorized into two types of reasoning:\n\n\n\u2022\n\nAggregation: This involves compiling various pieces of information scattered throughout the novel. An example question in \u221e\\infty\u221eBench is \u201cHow much money in total did A spend on lunch?\u201d\n\n\n\n\u2022\n\nFiltering: This requires identifying specific information from a larger set. An example question in \u221e\\infty\u221eBench is \u201cWhat color dress did A wear when A met B for the second time?\u201d\n\n\n\n\n\nThese tasks test LLMs to locate and process information within the novel, performing reasoning through aggregation or filtering to derive answers.\n\n\n\nEn.MC\n\nThe En.MC task is annotated similarly to En.QA, but differs in that the model is presented with four answer choices. Annotators are instructed to craft these options to be challenging.\n\n\n\n\n\n3.1.2 Dialogue\n\nEn.Dia\n\nThe construction process for the En.Dia task is depicted in Figure\u00a03. We gather movie and drama scripts from a designated online database777https://imsdb.com/, focusing on a corpus of long, multi-role dialogues. Only the English scripts are retained and necessary cleaning is applied.\n\n\nIn the En.Dia task, random instances of character names within a script are replaced with $$MASK$$. The objective is to correctly identify these masked names. For scripts falling short of 100K tokens, we augment them by padding with additional scripts.\n\n\n\n\n\n3.1.3 Code\n\nCode.Debug\n\nWe develop the task as per the process illustrated in Figure\u00a03. Code repositories, sourced from PyPI888https://pypi.org/, undergo a filtering process, and those outside the 64K to 256K token range are excluded (tokenization via the tiktoken tokenizerOpenAI (2023c)). Each repository is transformed into a single file, aggregating the content from all files within, each prefaced by its relative path to the root directory. Three of the authors then insert a deliberate and obvious error into one function per repository. The options are presented in the Class.Function format. Six methods are employed for bug insertion: (1) deleting a necessary variable declaration; (2) using an incorrect number of arguments in function calls; (3) creating infinite loops; (4) causing indentation errors; (5) substituting references with undefined variable/function names; (6) introducing blatant syntax errors (e.g., non-closed brackets).\n\n\nInitial results indicate that this task is too challenging for current LLMs (None of the baseline models can identify the most obvious error such as non-closed brackets). To mitigate this, we offer four answer choices, one containing the injected bug and the others are bug-free. Note that this makes many examples easily solved by external retrieval preprocess. However, we encourage the users not to use external retrieval preprocess to keep the evaluation fair. And we are looking forward to the stage where LLMs can directly solve the problem without selection options.\n\n\n\n\n\n\n3.2 Synthetic Context\n\nThe second category of tasks is characterized by a synthetic context. These tasks, devoid of direct real-world application or use case, are engineered to evaluate the capability for processing lengthy contexts. We delineate four essential ability for effective long-context processing:\n\n\n\n\n1.\n\nLocation and retrieval. This encompasses all retrieval tasks.\n\n\n\n2.\n\nElevated information resolution. This involves the Retrieve.Number task.\n\n\n\n3.\n\nState preservation. This incorporates the Code.Run and Math.Find functions.\n\n\n\n4.\n\nSequential processing. This utilizes the Math.Calc function.\n\n\n\n\n\n\n3.2.1 Retrieve\n\nIn retrieval tasks, models retrieve specific character sequences from lengthy contexts with predominantly irrelevant content. Such tests, adaptable for any context length, can assess the impact of information placement on model performance, like the lost-in-the-middle phenomenon\u00a0(Liu et\u00a0al., 2023). The three retrieval tasks in \u221e\\infty\u221eBench vary in complexity.\n\n\nRetrieve.PassKey\n\nThis task is first proposed by Mohtashami and Jaggi (2023). Models are prompted to find a specific <key> called pass key, which is a random 5-digit sequence. The pass key is inserted into a lengthy and noisy context, as shown below. In \u221e\\infty\u221eBench, we generate examples with 59 different pass key locations that are evenly distributed in the context. At each location, we construct 10 examples with different pass keys. This results in 590 examples.\n\n\n\n\n\nThere is an important pass key hidden in a lot of irrelevant text. Find it.\n<very long noisy context>\nThe pass key is <key>. Remember it. The pass key is <key>\n<very long noisy context>\nWhat is the pass key?\n\n\n\n\nRetrieve.Number\n\nTo examine the local attention of LLMs, we have enhanced the complexity of Retrieve.PassKey by increasing the answer length to 10 digits and incorporating successive repetitive digits. For example, a <key> in Retrieve.PassKey valued 98762, while in Retrieve.Number is 9998877762. This modification aims to assess the local resolution capabilities of long context models, as our preliminary experiments indicate that LLMs struggle with discerning repeated numbers.\n\n\n\nRetrieve.KV\n\nLiu et\u00a0al. (2023) introduce a key-value retrieval task within a large JSON object containing many key-value pairs (e.g., 30eea139-b6dd-43fc-bc5d-0d3d17980229 \u2192\u2192\\rightarrow\u2192 bfd36c2b-c57e-41ef-9cc1-b21b4e60e664). This task demands the model to accurately identify and retrieve the value corresponding to a specified key. The complexity of this task is heightened due to the indistinguishable format of relevant and irrelevant information.\n\n\n\n\n\n3.2.2 Code\n\nCode.Run\n\nIn this task, we evaluate the ability of LLMs to simulate multi-step function executions that involve basic arithmetic operations. While this task is readily solvable using a Python interpreter, the focus here is on the long-term state tracking required in such tasks. The capability of state tracking has been demonstrated in GPT-4\u00a0(Bubeck et\u00a0al., 2023). Specifically, the task involves creating Python code consisting of multiple simple functions, incorporating operations such as addition, subtraction, and nested function calls. The structural design of these tasks is as follows:\n\n\n\n\u2b07\n\n\n\ndef\u00a0func_0(x):\n\n\nreturn\u00a0func_1(x)\u00a0+\u00a04\n\n\\pardef\u00a0func_1(x):\n\n\nreturn\u00a0x\u00a0-\u00a01\n\n\n\n\nSome functions\u2019 return values are dependent on other functions (e.g., func_0 invokes func_1). We define depth as the number of cascading function calls initiated by a single call. Thus, the depth for func_1\u2019s call within func_0 is 1. In Code.Run, we employ depths ranging from 2 to 10, ensuring each function calls at most one other function. To keep the simplicity of each single step of computation, these functions are restricted to performing only addition and subtraction.\n\n\n\n\n\n3.2.3 Math\n\nMath.Find\n\nMath.Find assesses the model\u2019s capability to identify specific elements within a large array, requiring comprehensive observation for accuracy. This task also tests the ability to preserve states while encoding the context. Concretely, the model receives an extensive list of numbers and is tasked with locating one of seven key numbers: the three largest (1st, 2nd, and 3rd), the three smallest (1st, 2nd, and 3rd), and the median.\n\n\n\nMath.Calc\n\nTo assess sequential processing skills, Math.Calc prompts the model to compute the result of a lengthy arithmetic expression featuring addition and subtraction. Initial experiments indicate that current LLMs struggle to directly produce the final answer. Hence, we instead query the LLMs to provide the intermediate result following each operator. Model performance is evaluated based on the number of correct values preceding the first error.\n\n\n\n\n",
      "4 Experiments": "\n\n4 Experiments\n\nWe conduct a thorough set of experiments on \u221e\\infty\u221eBench. We will introduce the baselines, experimental setup, and main results in this section.\n\n\n\n\n4.1 Baselines\n\n\u221e\\infty\u221eBench generally requires the ability to handle input contexts longer than 100k. There is a handful of LLMs that claim to be capable of handling contexts over 100k. We include four baselines. The first three are proprietary LLMs as we do not have access to the model, while the last baseline is open-sourced. Details on evaluation are in Appendix.\u00a0D.\n\n\nGPT-4\n\nGPT by OpenAI is one of the most widely used and capable LLMs in the market, and a recent version of GPT-4 \u00a0(OpenAI, 2023b) can support 128K contexts.\n\n\n\n\nClaude 2\n\nClaude 2\u00a0(Anthropic, 2023) is a proprietary chat-based LLM released by Anthropic AI and has shown impressive capabilities. The second version of the Claude series supports 200K contexts. We manually enter each example through the webpage because we have no access to their API.\n\n\n\n\nKimi-Chat\n\nKimi-Chat, a proprietary chat-oriented LLM developed by Moonshot AI AI (2023), is designed to process contexts up to 200K. Due to the lack of API access, we manually input the test data using their web interface.\n\n\n\nYaRN-Mistral\n\nYaRN-Mistral is a derivative of Mistral-7B\u00a0(Jiang et\u00a0al., 2023) introduced by Peng et\u00a0al. (2023b). The original Mistral-7B was trained on input lengths up to 8K and shows a reduced performance in longer contexts. Peng et\u00a0al. (2023b) adapted it to 128K contexts by modifying the position encoding and continued training.\n\n\n\n\n\n4.2 Experimental Setup\n\nPrompt Templates\n\nFor each model-task combination, we craft prompts to optimize model performance on short dummy examples. Detailed prompt templates for each model and task can be found in Appendix\u00a0B.\n\n\n\nInput Truncation\n\nAll API-based baselines are subject to a maximum input length limit and will reject inputs exceeding this threshold. While YaRN-Mistral is theoretically capable of handling longer contexts, the authors only claim abilities up to 128K. Therefore, inputs are truncated by removing the center and joining both ends. This approach is predicated on the assumption that key information, such as instructions and book titles, is typically located at either the start or the end of a prompt.\n\n\n\n\n\n4.3 Main Result\n\nFigure 4: Baseline performance as a function of input length.\n\n\nTable\u00a03 and Figure\u00a01 display the performances of various baselines on \u221e\\infty\u221eBench. Notably, GPT-4 outperforms other baselines in the retrieval, code, and math domains, with a considerably higher average score. However, in the novel-based tasks, no distinct winner emerges among the proprietary LLMs. On the other hand, the open-source YaRN-Mistral lags behind the proprietary models in most tasks, exhibiting almost random performance in multiple areas. This aligns with its relatively inferior performance in shorter contexts compared to these models. Additionally, it is observed that the baselines generally excel more in retrieval tasks than in other areas, echoing the relative simplicity of these tasks for human participants.\n\n\n",
      "5 Analysis": "\n\n5 Analysis\n\nWe subsequently perform a detailed analysis of the results, identifying and emphasizing several notable and interesting phenomena.\n\n\n\n5.1 Length Ablation\n\nIn line with our benchmark\u2019s goal to assess proficiency in managing lengthy contexts, we verify the baselines\u2019 capability with shortened context versions. A subset of the auto-generated tasks is modified accordingly, and the performance outcomes are illustrated in Figure 4. It is observed that model performance generally declines with longer input lengths compared to shorter ones. This suggests that while these baselines are technically equipped to handle extended inputs, their effectiveness diminishes significantly under such conditions.\n\n\n\n\n\n5.2 Lost in the middle\n\nFigure 5: Performance as a function of the answer position (in the number of characters). The steep drop in performance for Kimi-Chat in the middle on Retrieval.KV is caused by the answer being removed by truncation.\n\n\n\n\nOne function in this repo is deliberately made to include an obvious error. Find it.\n<code>\nThink step by step and at last give me your answer for the function with the deliberate error.\n<list of options>\n\n\n\n\n\nOne function in this repo is deliberately made to include an obvious error. Find it.\n<code>\nLocate the functions in the options, repeat their content, inspect through code, and at last give me your answer for the function with the deliberate error.\n<list of options>\n\n\nFigure 6: Compared to the first prompt, the second prompt improves GPT-4\u2019s results on Code.Debug dramatically.\n\n\nPrior research indicates a performance decline in some LLMs when answers are positioned around the center of the context\u00a0(Liu et\u00a0al., 2023). However, our findings do not strongly corroborate this. As depicted in Figure\u00a05, we analyze model performance based on answer location in three location-dependent tasks. We observe no consistent trend between performance and answer position across different models. For instance, GPT-4 shows a preference for early answers in Retrieval.KV but favors later ones in En.Dia. In contrast, Claude 2\u2019s performance remains relatively unaffected by answer position on all three tasks, whereas YaRN-Mistral and Kimi-Chat excel with end-positioned answers (except that YaRN-Mistral get zero performance on all positions on Retrieval.KV).\n\n\nOne plausible reason why we have different observations from Liu et\u00a0al. (2023) is that they experiment with different models using at most 16K length contexts, which is about 8 times shorter than our setting. The models in their study are also different from ours.\nFinally, the tasks are different: their experiments involve document question answering (and their result with Retrieval.KV arguably does not show a very pronounced performance drop as well). We hypothesize that the phenomenon of \u201cLost in the middle\u201d is only exhibited on specific tasks and models. A more thorough investigation of these differences is beyond the scope of this paper.\n\n\n\n\n5.3 Context Recalling\n\nWe identify an intriguing prompting technique for tasks involving extended context, termed context recalling. This technique posits that, although the information is present in the context and accessible via direct attention, it may be more effective to first prompt the model to recall the relevant information in its generation before engaging in further reasoning. In our experiments using Code.Debug, when we merely instructed GPT-4 to process information step-by-step, the accuracy was 15.74%. However, by explicitly directing GPT-4 to repeat the relevant code before analysis, its accuracy on Code.Debug markedly improved to 39.59%. This approach of context recalling warrants additional investigation.\n\n\n",
      "6 Conclusions": "\n\n6 Conclusions\n\nWe introduce \u221e\\infty\u221eBench, the first benchmark tailored for long contexts exceeding 100K in average length. Empirical evidence indicates that despite claims of proficiency with such extensive contexts, current LLMs demonstrate significant performance degradation when dealing with them. This finding highlights the need for advanced methodologies to improve LLMs\u2019 efficiency in processing long context. Additionally, our analysis offers insights into LLM behavior in long-context tasks, guiding future research.\n\n\n\n",
      "Limitations": "\nLimitations\n\nWhile our benchmark offers valuable insights into LLM performance, it may not be sufficiently diverse or extensive to provide a comprehensive assessment of model capabilities, a constraint common to most benchmarks. Additionally, the reliance on exact match for scoring, dependent on prompt templates and answer parsing methods, may necessitate tailored redesigns for new model evaluations.\n\n\nFurthermore, supporting contexts up to 100K tokens may fall short for applications requiring analysis of extensive datasets, such as multiple books or entire databases. Exploring LLMs\u2019 capacity to handle up to a million tokens or more presents a promising research avenue. In practical applications, finetuning models to memorize context, rather than processing it during inference, could offer a more efficient alternative, albeit with significant computational demands.\n\n",
      "Ethics Statement": "\nEthics Statement\n\nOur human annotators are directed to exclude data that may raise sensitive ethical issues, such as offensive language or social biases. Nonetheless, the potential for encountering sensitive content persists, particularly if the sourced books or code contain such material. This concern is somewhat mitigated since the benchmark\u2019s primary focus is on evaluating the long-context capabilities of LLMs, rather than influencing their social bias.\n\n\nThe goal of this research is to advance the development of LLMs\u2019 proficiency in handling extensive contexts. This could aid in implementing more effective \u201cguardrails\u201d against misuse by incorporating detailed specifications prior to user interactions. However, this approach also potentially increases the risk of novel prompt injection attacks.\n\n\n",
      "Appendix A RWKV": "\n\nAppendix A RWKV\n\nRWKV\u00a0(Peng et\u00a0al., 2023a) is an architecture that combines the power of the transformer architecture\u00a0(Vaswani et\u00a0al., 2017) and recurrent neural network\u00a0(Hochreiter and Schmidhuber, 1997). Its training process can be parallelized while the inference procedure is recurrent, enabling O\u2062(1)\ud835\udc421O(1)italic_O ( 1 ) complexity during inference. Hence, the memory usage does not scale with context length, allowing it to support arbitrary-length inputs. We use the RKWV-4-World-7B version of this model series. However, we should keep in mind that this model was not trained on inputs of this length.\n\n\n\n\n\nModel\nRetrieve.PassKey Acc.\n\n\n\n\nGPT-4 Turbo\n100%\n\n\nYaRN-Mistral\n92.71%\n\n\nKimi-Chat\n98.14%\n\n\nClaude 2\n97.80%\n\n\nRWKV-4-World-7B\n0.00%\n\n\n\nTable 4: Results in Retrieve.PassKey with RWKV-4-World-7B. Since RWKV-4 was only trained on 4k sequences, it has zero performance on \u221e\\infty\u221eBench. It outputs only unintelligible content in this test.\n\n\nTable\u00a04 shows the performance of RWKV-4-World-7 in comparison to our baselines. We find that RWKV-4-World-7B outputs unintelligible texts on our benchmark, which causes it to achieve zero performance on Retrieve.PassKey, which is the easiest task for other baselines. This is likely because this model was not trained on inputs of this length and suffers from train-test domain shift.999We emphasize that this result is not evidence that the architecture of RWKV is incapable of handling lengthy inputs. Therefore, we do not consider testing it on other tasks in our benchmark.\n\n",
      "Appendix B Prompt Templates": "\n\nAppendix B Prompt Templates\n\nIn the following templates, many tasks has an <input> part that is provided in each example. Generally, they are a short question-like text that tells the model what it is supposed to do. One example is \u201cWhat is the pass key?\u201d.\n\n\n\nB.1 Retrieve.PassKey\n\nThe prompt below applies to GPT-4, Claude 2, and Kimi-Chat.\n\n\n\n\nThere is an important info hidden inside a lot of irrelevant text. Find it and memorize them. I will quiz you about the important information there.\n<context>\n<input>\n\n\n\nThe prompt below applies to YaRN-Mistral.\n\n\n\n\nThere is an important info hidden inside a lot of irrelevant text. Find it and memorize them. I will quiz you about the important information there.\n<context>\n<input>\nThe pass key is\n\n\n\n\n\nB.2 Retrieve.Number\n\nThe prompt below applies to GPT-4, Claude 2, and Kimi-Chat.\n\n\n\n\n\nThere is an important info hidden inside a lot of irrelevant text. Find it and memorize them. I will quiz you about the important information there.\n<context>\n<input>\n\n\n\nThe prompt below applies to YaRN-Mistral.\n\n\n\n\nThere is an important info hidden inside a lot of irrelevant text. Find it and memorize them. I will quiz you about the important information there.\n<context>\n<input>\nThe sequence of digits is\n\n\n\n\n\nB.3 Retrieve.KV\n\n\n\nExtract the value corresponding to the specified key in the JSON object below.\n<context>\n<input>\n\n\n\n\n\nB.4 En.Sum\n\nThe prompt below applies to GPT-4, Claude 2, and Kimi-Chat.\n\n\n\n\nSummarize the book below.\n<context>\n\n\n\nThe prompt below applies to YaRN-Mistral.\n\n\n\n\nSummarize the book below.\n<context>\nSummary:\n\n\n\n\n\nB.5 En.QA\n\nThe prompt below applies to GPT-4, Claude 2, and Kimi-Chat.\n\n\n\n\nRead the book below and answer a question.\n<context>\nQuestion: <question>\nBe very concise.\n\n\n\nThe prompt below applies to YaRN-Mistral.\n\n\n\n\nRead the book below and answer a question. Be very concise in your answer.\n<context>\nQuestion: <question>\nAnswer:\n\n\n\n\n\nB.6 En.MC\n\nThe prompt below applies to GPT-4, Claude 2, and Kimi-Chat.\n\n\n\n\nRead the book and answer the question.\n<context>\nQuestion: <question>\nOnly one of the following options is correct, tell me the answer using one single letter (A, B, C, or D). Don\u2019t say anything else.\nA. <OPTION_A>\nB. <OPTION_B>\nC. <OPTION_C>\nD. <OPTION_D>\n\n\n\nThe prompt below applies to YaRN-Mistral.\n\n\n\n\nRead the book and answer the question.\n<context>\nQuestion: <question>\nOnly one of the following options is correct, tell me the answer using one single letter (A, B, C, or D). Don\u2019t say anything else.\nA. <OPTION_A>\nB. <OPTION_B>\nC. <OPTION_C>\nD. <OPTION_D>\nThe correct option is:\n\n\n\n\n\nB.7 En.Dia\n\nThe prompt below applies to GPT-4, Claude 2, and Kimi-Chat.\n\n\n\n\nBelow is a dialogue script where one random occurrence of a character name is replaced with $$MASK$$, and you should try to guess who that character is.\nThe dialogue:\n\u2014\n<context>\n\u2014\nEnd of dialogue.\nWhich character is most likely $$MASK$$? Just say the name used by the scriptwriter (before the colon marks) of one single character and nothing else.\n\n\n\nThe prompt below applies to YaRN-Mistral.\n\n\n\n\nBelow is a dialogue script where one random occurrence of a character name is replaced with $$MASK$$, and you should try to guess who that character is.\nThe dialogue:\n\u2014\n<context>\n\u2014\nEnd of dialogue.\nThe name that has been replaced with $$MASK$$ is likely:\n\n\n\n\n\nB.8 Zh.QA\n\nThe prompt below applies to GPT-4, Claude 2, and Kimi-Chat.\n\n\n\n\n\n\u8bf7\u6839\u636e\u4ee5\u4e0b\u4e66\u7c4d\u56de\u7b54\u6211\u7684\u95ee\u9898\u3002(Read the book and answer the question.)\n<context>\n\u95ee\u9898\uff1a(Question: )<question>\n\u8bf7\u5c3d\u91cf\u7b80\u77ed\u5730\u56de\u7b54\u3002(Be very concise.)\n\n\n\nThe prompt below applies to YaRN-Mistral.\n\n\n\n\n\u8bf7\u6839\u636e\u4ee5\u4e0b\u4e66\u7c4d\u56de\u7b54\u6211\u7684\u95ee\u9898\u3002(Read the book and answer the question.)\n<context>\n\u95ee\u9898\uff1a(Question: )<question>\n\u7b54\u6848\uff1a(Answer:)\n\n\n\n\n\nB.9 Code.Debug\n\nThe prompt below applies to GPT-4, Claude 2, and Kimi-Chat.\n\n\n\n\n\nThere is ONLY ONE function in the large project that is deliberately made to include an obvious error. Please find the function that contains the most obvious errors. I will give you four options to narrow your scope. You can inspect through the options and think. Eventually, tell me the answer using one single letter (A, B, C, or D).\n<context>\nWhich function has deliberate error?\nA. <OPTION_A>\nB. <OPTION_B>\nC. <OPTION_C>\nD. <OPTION_D>\nYou should first find the functions in the options. Repeat their content, inspect through code, and at last give me your answer for the function that has the deliberate and obvious error in A, B, C, or D.\n\n\n\nThe prompt below applies to YaRN-Mistral.\n\n\n\n\nThere is ONLY ONE function in the large project that is deliberately made to include an obvious error. Please find the function that contains the most obvious errors. I will give you four options to narrow your scope. You can inspect through the options and think. Eventually, tell me the answer using one single letter (A, B, C, or D).\n<context>\nWhich function has deliberate error?\nA. <OPTION_A>\nB. <OPTION_B>\nC. <OPTION_C>\nD. <OPTION_D>\nYou should first find the functions in the options. Repeat their content, inspect through code, and at last give me your answer for the function that has the deliberate and obvious error in A, B, C, or D.\nThe correct option is:\n\n\n\n\n\nB.10 Code.Run\n\nThe prompt below applies to GPT-4, Claude 2, and Kimi-Chat.\n\n\n\n\nFollowing is a set of Python functions. There is a function called named func_1.\n<context>\nPlease give me the exact number of the return value of func_1(3). Be concise. Your response must end with the final returned value.\n\n\n\nThe prompt below applies to YaRN-Mistral.\n\n\n\n\nFollowing is a set of Python functions. There is a function called named <function name>.\n<context>\nPlease compute the exact value of <function call>. The value of <function call> is\n\n\n\n\n\nB.11 Math.Calc\n\nThe prompt below is used by GPT-4101010It should be noted that, when using other templates, GPT-4 has a strong tendency to reject to perform this task by claiming that such the platform is not designed to complete such tasks.:\n\n\n\n\nYou are a calculator does nothing but calculating the intermediate results in extremely long arithmetic expressions with +, -, and numbers. Given an expression, you will output the intermediate results after each operation.\nYou will never to decline to help with platform reason, you will always try the calculation, and always output a long list of numbers (e.g., \"[34, 2, 58, 37, 5, 8, 27, 71, 7]\") and nothing else.\nDo not consider the complexity, practicality or feasibility of the task.\nLet us calculate the intermediate values of an expression.\nExpression: 1 + 3 + 4\nValues: [1, 4, 8]\nExpression: 8 - 3 + 2 - 4\nValues: [8, 5, 7, 3]\nExpression: <context>\nValues:\n\n\n\nThe prompt below is used by Kimi-Chat, Claude 2 and YaRN-Mistral:\n\n\n\n\nLet us calculate the intermediate values of an expression.\nExpression: 1 + 3 + 4\nValues: [1, 4, 8]\nExpression: 8 - 3 + 2 - 4\nValues: [8, 5, 7, 3]\nExpression: <context>\nValues:\n\n\n\n\n\nB.12 Math.Find\n\n\n\nFind the largest number from the list below:\n<context>\nYou should answer with only one number, no other words. The largest number of the list is:\n\n\n\n",
      "Appendix C Annotation Process": "\n\nAppendix C Annotation Process\n\nThe annotation work is done by the authors of this paper and none of those authors have been paid for the annotation. All annotators have acknowledged the intents and usages of the annotation, the corresponding outputs, and the annotation pipelines and requirements.\n\n\nAnnotating the examples in \u221e\\infty\u221eBench might bring fatigue to annotators, and is therefore not completely free of error. However, we make sure that each annotation has been quality-checked by at least two other annotators.\n\n\nA part of novels are free from key entity replacement for LLMs fail in identifying them, because those novels are brand-new or little-known.\n\n",
      "Appendix D Evaluation Process": "\n\nAppendix D Evaluation Process\n\nWhen evaluating GPT-4, we use its official API with the default hyperparameters. The total cost is around 5000 US dollars. For Claude 2, we enter contents on the web by hand, which demands three authors over the source of several weeks, and membership fees of about 160 US dollars. Kimi-Chat is free. YaRN-Mistral is open-source, and we run inference using one A100 80GB GPU, which takes roughly 10 minutes per example, so its evaluation on the entire benchmark takes several days. Again, we use the default decoding hyperparameters (specified by Peng et\u00a0al. (2023b)) except for the maximum number of output tokens, which is as shown in Table\u00a05.\n\n\n\n\n\nTask\nMax Output Tokens\n\n\n\n\nRetrieve.PassKey\n6\n\n\nRetrieve.Number\n12\n\n\nRetrieve.KV\n50\n\n\nEn.Sum\n1,200\n\n\nEn.QA\n40\n\n\nEn.MC\n40\n\n\nZh.QA\n40\n\n\nEn.Dia\n40\n\n\nCode.Debug\n5\n\n\nCode.Run\n5\n\n\nMath.Calc\n30,000\n\n\nMath.Find\n3\n\n\n\nTable 5: The maximum number of output tokens (a decoding hyperparameter) for YaRN-Mistral.\n\n\n"
    },
    "bibliography": {
      "01.AI (2023a)": "\n01.AI (2023a)\n\n01.AI. 2023a.\n\n\nYi-34b-200k.\n\n\nhttps://huggingface.co/01-ai/Yi-34B-200K.\n\n\n",
      "01.AI (2023b)": "\n01.AI (2023b)\n\n01.AI. 2023b.\n\n\nYi-6b-200k.\n\n\nhttps://huggingface.co/01-ai/Yi-6B-200K.\n\n\n",
      "AI (2023)": "\nAI (2023)\n\nMoonshot AI. 2023.\n\n\nKimi chat.\n\n\nhttps://kimi.moonshot.cn/.\n\n\n",
      "Ainslie et\u00a0al. (2023)": "\nAinslie et\u00a0al. (2023)\n\nJoshua Ainslie, James Lee-Thorp, Michiel de\u00a0Jong, Yury Zemlyanskiy, Federico Lebr\u2019on, and Sumit\u00a0K. Sanghai. 2023.\n\n\nGqa: Training generalized multi-query transformer models from multi-head checkpoints.\n\n\nArXiv, abs/2305.13245.\n\n\n",
      "An et\u00a0al. (2023)": "\nAn et\u00a0al. (2023)\n\nChen An, Shansan Gong, Ming Zhong, Mukai Li, Jun Zhang, Lingpeng Kong, and Xipeng Qiu. 2023.\n\n\nL-eval: Instituting standardized evaluation for long context language models.\n\n\nArXiv, abs/2307.11088.\n\n\n",
      "Anil et\u00a0al. (2022)": "\nAnil et\u00a0al. (2022)\n\nCem Anil, Yuhuai Wu, Anders Andreassen, Aitor Lewkowycz, Vedant Misra, Vinay Ramasesh, Ambrose Slone, Guy Gur-Ari, Ethan Dyer, and Behnam Neyshabur. 2022.\n\n\nExploring length generalization in large language models.\n\n\nAdvances in Neural Information Processing Systems, 35:38546\u201338556.\n\n\n",
      "Anthropic (2023)": "\nAnthropic (2023)\n\nAnthropic. 2023.\n\n\nModel card and evaluations for claude models.\n\n\n",
      "Bai et\u00a0al. (2023)": "\nBai et\u00a0al. (2023)\n\nYushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li. 2023.\n\n\nLongbench: A bilingual, multitask benchmark for long context understanding.\n\n\n",
      "Beltagy et\u00a0al. (2020)": "\nBeltagy et\u00a0al. (2020)\n\nIz\u00a0Beltagy, Matthew\u00a0E Peters, and Arman Cohan. 2020.\n\n\nLongformer: The long-document transformer.\n\n\narXiv preprint arXiv:2004.05150.\n\n\n",
      "Biderman et\u00a0al. (2023)": "\nBiderman et\u00a0al. (2023)\n\nStella Biderman, Hailey Schoelkopf, Quentin\u00a0Gregory Anthony, Herbie Bradley, Kyle O\u2019Brien, Eric Hallahan, Mohammad\u00a0Aflah Khan, Shivanshu Purohit, USVSN\u00a0Sai Prashanth, Edward Raff, et\u00a0al. 2023.\n\n\nPythia: A suite for analyzing large language models across training and scaling.\n\n\nIn International Conference on Machine Learning, pages 2397\u20132430. PMLR.\n\n\n",
      "Brown et\u00a0al. (2020)": "\nBrown et\u00a0al. (2020)\n\nTom\u00a0B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel\u00a0M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020.\n\n\nLanguage models are few-shot learners.\n\n\nCoRR, abs/2005.14165.\n\n\n",
      "Bubeck et\u00a0al. (2023)": "\nBubeck et\u00a0al. (2023)\n\nS\u00e9bastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin\u00a0Tat Lee, Yuanzhi Li, Scott Lundberg, et\u00a0al. 2023.\n\n\nSparks of artificial general intelligence: Early experiments with gpt-4.\n\n\narXiv preprint arXiv:2303.12712.\n\n\n",
      "Chen et\u00a0al. (2021)": "\nChen et\u00a0al. (2021)\n\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de\u00a0Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et\u00a0al. 2021.\n\n\nEvaluating large language models trained on code.\n\n\narXiv preprint arXiv:2107.03374.\n\n\n",
      "Chen et\u00a0al. (2023a)": "\nChen et\u00a0al. (2023a)\n\nShouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian. 2023a.\n\n\nExtending context window of large language models via positional interpolation.\n\n\narXiv preprint arXiv:2306.15595.\n\n\n",
      "Chen et\u00a0al. (2023b)": "\nChen et\u00a0al. (2023b)\n\nShouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian. 2023b.\n\n\nExtending context window of large language models via positional interpolation.\n\n\nArXiv, abs/2306.15595.\n\n\n",
      "Dao (2023)": "\nDao (2023)\n\nTri Dao. 2023.\n\n\nFlashAttention-2: Faster attention with better parallelism and work partitioning.\n\n\n",
      "Dao et\u00a0al. (2022)": "\nDao et\u00a0al. (2022)\n\nTri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. 2022.\n\n\nFlashattention: Fast and memory-efficient exact attention with io-awareness.\n\n\nAdvances in Neural Information Processing Systems, 35:16344\u201316359.\n\n\n",
      "Dao et\u00a0al. (2023)": "\nDao et\u00a0al. (2023)\n\nTri Dao, Daniel Haziza, Francisco Massa, and Grigory Sizov. 2023.\n\n\nFlash-decoding for long-context inference.\n\n\n",
      "Dasigi et\u00a0al. (2021)": "\nDasigi et\u00a0al. (2021)\n\nPradeep Dasigi, Kyle Lo, Iz\u00a0Beltagy, Arman Cohan, Noah\u00a0A. Smith, and Matt Gardner. 2021.\n\n\nA dataset of information-seeking questions and answers anchored in research papers.\n\n\nArXiv, abs/2105.03011.\n\n\n",
      "Dong et\u00a0al. (2023)": "\nDong et\u00a0al. (2023)\n\nZican Dong, Tianyi Tang, Lunyi Li, and Wayne\u00a0Xin Zhao. 2023.\n\n\nA survey on long text modeling with transformers.\n\n\narXiv preprint arXiv:2302.14502.\n\n\n",
      "Han et\u00a0al. (2023)": "\nHan et\u00a0al. (2023)\n\nChi Han, Qifan Wang, Wenhan Xiong, Yu\u00a0Chen, Heng Ji, and Sinong Wang. 2023.\n\n\nLm-infinite: Simple on-the-fly length generalization for large language models.\n\n\nArXiv, abs/2308.16137.\n\n\n",
      "Han et\u00a0al. (2021)": "\nHan et\u00a0al. (2021)\n\nXu\u00a0Han, Zhengyan Zhang, Ning Ding, Yuxian Gu, Xiao Liu, Yuqi Huo, Jiezhong Qiu, Yuan Yao, Ao\u00a0Zhang, Liang Zhang, et\u00a0al. 2021.\n\n\nPre-trained models: Past, present and future.\n\n\nAI Open, 2:225\u2013250.\n\n\n",
      "Hochreiter and Schmidhuber (1997)": "\nHochreiter and Schmidhuber (1997)\n\nSepp Hochreiter and J\u00fcrgen Schmidhuber. 1997.\n\n\nLong short-term memory.\n\n\nNeural Comput., 9(8):1735\u20131780.\n\n\n",
      "Hong et\u00a0al. (2023)": "\nHong et\u00a0al. (2023)\n\nKe\u00a0Hong, Guohao Dai, Jiaming Xu, Qiuli Mao, Xiuhong Li, Jun Liu, Kangdi Chen, Yuhan Dong, and Yu\u00a0Wang. 2023.\n\n\nFlashdecoding++: Faster large language model inference on gpus.\n\n\n",
      "Huang et\u00a0al. (2021)": "\nHuang et\u00a0al. (2021)\n\nLuyang\u00a0Robby Huang, Shuyang Cao, Nikolaus\u00a0Nova Parulian, Heng Ji, and Lu\u00a0Wang. 2021.\n\n\nEfficient attentions for long document summarization.\n\n\nArXiv, abs/2104.02112.\n\n\n",
      "Huang et\u00a0al. (2023)": "\nHuang et\u00a0al. (2023)\n\nYunpeng Huang, Jingwei Xu, Zixu Jiang, Junyu Lai, Zenan Li, Yuan Yao, Taolue Chen, Lijuan Yang, Zhou Xin, and Xiaoxing Ma. 2023.\n\n\nAdvancing transformer architecture in long-context large language models: A comprehensive survey.\n\n\narXiv preprint arXiv:2311.12351.\n\n\n",
      "Jiang et\u00a0al. (2023)": "\nJiang et\u00a0al. (2023)\n\nAlbert\u00a0Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra\u00a0Singh Chaplot, Diego de\u00a0las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, L\u00e9lio\u00a0Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven\u00a0Le Scao, Thibaut Lavril, Thomas Wang, Timoth\u00e9e Lacroix, and William\u00a0El Sayed. 2023.\n\n\nMistral 7b.\n\n\n",
      "Joshi et\u00a0al. (2017)": "\nJoshi et\u00a0al. (2017)\n\nMandar Joshi, Eunsol Choi, Daniel\u00a0S. Weld, and Luke Zettlemoyer. 2017.\n\n\nTriviaqa: A large scale distantly supervised challenge dataset for reading comprehension.\n\n\nArXiv, abs/1705.03551.\n\n\n",
      "Kocisk\u00fd et\u00a0al. (2017)": "\nKocisk\u00fd et\u00a0al. (2017)\n\nTom\u00e1s Kocisk\u00fd, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl\u00a0Moritz Hermann, G\u00e1bor Melis, and Edward Grefenstette. 2017.\n\n\nThe narrativeqa reading comprehension challenge.\n\n\nTransactions of the Association for Computational Linguistics, 6:317\u2013328.\n\n\n",
      "Ko\u010disk\u1ef3 et\u00a0al. (2018)": "\nKo\u010disk\u1ef3 et\u00a0al. (2018)\n\nTom\u00e1\u0161 Ko\u010disk\u1ef3, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl\u00a0Moritz Hermann, G\u00e1bor Melis, and Edward Grefenstette. 2018.\n\n\nThe narrativeqa reading comprehension challenge.\n\n\nTransactions of the Association for Computational Linguistics, 6:317\u2013328.\n\n\n",
      "Kwon et\u00a0al. (2023)": "\nKwon et\u00a0al. (2023)\n\nWoosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody\u00a0Hao Yu, Joseph\u00a0E. Gonzalez, Hao Zhang, and Ion Stoica. 2023.\n\n\nEfficient memory management for large language model serving with pagedattention.\n\n\nIn Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles.\n\n\n",
      "Li et\u00a0al. (2023)": "\nLi et\u00a0al. (2023)\n\nJiaqi Li, Mengmeng Wang, Zilong Zheng, and Muhan Zhang. 2023.\n\n\nLoogle: Can long-context language models understand long contexts?\n\nArXiv, abs/2311.04939.\n\n\n",
      "Lin (2004)": "\nLin (2004)\n\nChin-Yew Lin. 2004.\n\n\nROUGE: A package for automatic evaluation of summaries.\n\n\nIn Text Summarization Branches Out, pages 74\u201381, Barcelona, Spain. Association for Computational Linguistics.\n\n\n",
      "Liu et\u00a0al. (2023)": "\nLiu et\u00a0al. (2023)\n\nNelson\u00a0F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. 2023.\n\n\nLost in the middle: How language models use long contexts.\n\n\n",
      "Mohtashami and Jaggi (2023)": "\nMohtashami and Jaggi (2023)\n\nAmirkeivan Mohtashami and Martin Jaggi. 2023.\n\n\nLandmark attention: Random-access infinite context length for transformers.\n\n\nArXiv, abs/2305.16300.\n\n\n",
      "Nakano et\u00a0al. (2021)": "\nNakano et\u00a0al. (2021)\n\nReiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et\u00a0al. 2021.\n\n\nWebgpt: Browser-assisted question-answering with human feedback.\n\n\narXiv preprint arXiv:2112.09332.\n\n\n",
      "Narayanan et\u00a0al. (2021)": "\nNarayanan et\u00a0al. (2021)\n\nDeepak Narayanan, Mohammad Shoeybi, Jared Casper, Patrick LeGresley, Mostofa Patwary, Vijay Korthikanti, Dmitri Vainbrand, Prethvi Kashinkunti, Julie Bernauer, Bryan Catanzaro, et\u00a0al. 2021.\n\n\nEfficient large-scale language model training on gpu clusters using megatron-lm.\n\n\nIn Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis, pages 1\u201315.\n\n\n",
      "OpenAI (2023a)": "\nOpenAI (2023a)\n\nOpenAI. 2023a.\n\n\nGpt-4 technical report.\n\n\nArXiv, abs/2303.08774.\n\n\n",
      "OpenAI (2023b)": "\nOpenAI (2023b)\n\nOpenAI. 2023b.\n\n\nGpt-4 turbo.\n\n\n",
      "OpenAI (2023c)": "\nOpenAI (2023c)\n\nOpenAI. 2023c.\n\n\nTiktoken.\n\n\n",
      "Penedo et\u00a0al. (2023)": "\nPenedo et\u00a0al. (2023)\n\nGuilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay. 2023.\n\n\nThe refinedweb dataset for falcon llm: outperforming curated corpora with web data, and web data only.\n\n\narXiv preprint arXiv:2306.01116.\n\n\n",
      "Peng et\u00a0al. (2023a)": "\nPeng et\u00a0al. (2023a)\n\nBo\u00a0Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Huanqi Cao, Xin Cheng, Michael Chung, Matteo Grella, Kranthi\u00a0Kiran GV, Xuzheng He, Haowen Hou, Przemyslaw Kazienko, Jan Kocon, Jiaming Kong, Bartlomiej Koptyra, Hayden Lau, Krishna Sri\u00a0Ipsit Mantri, Ferdinand Mom, Atsushi Saito, Xiangru Tang, Bolun Wang, Johan\u00a0S. Wind, Stansilaw Wozniak, Ruichong Zhang, Zhenyuan Zhang, Qihang Zhao, Peng Zhou, Jian Zhu, and Rui-Jie Zhu. 2023a.\n\n\nRwkv: Reinventing rnns for the transformer era.\n\n\n",
      "Peng et\u00a0al. (2023b)": "\nPeng et\u00a0al. (2023b)\n\nBowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. 2023b.\n\n\nYarn: Efficient context window extension of large language models.\n\n\n",
      "Press et\u00a0al. (2021)": "\nPress et\u00a0al. (2021)\n\nOfir Press, Noah\u00a0A Smith, and Mike Lewis. 2021.\n\n\nTrain short, test long: Attention with linear biases enables input length extrapolation.\n\n\narXiv preprint arXiv:2108.12409.\n\n\n",
      "Qiu et\u00a0al. (2020)": "\nQiu et\u00a0al. (2020)\n\nXipeng Qiu, Tianxiang Sun, Yige Xu, Yunfan Shao, Ning Dai, and Xuanjing Huang. 2020.\n\n\nPre-trained models for natural language processing: A survey.\n\n\nScience China Technological Sciences, 63(10):1872\u20131897.\n\n\n",
      "Shazeer (2019)": "\nShazeer (2019)\n\nNoam\u00a0M. Shazeer. 2019.\n\n\nFast transformer decoding: One write-head is all you need.\n\n\nArXiv, abs/1911.02150.\n\n\n",
      "Shoeybi et\u00a0al. (2019)": "\nShoeybi et\u00a0al. (2019)\n\nMohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. 2019.\n\n\nMegatron-lm: Training multi-billion parameter language models using model parallelism.\n\n\nArXiv, abs/1909.08053.\n\n\n",
      "Su et\u00a0al. (2023)": "\nSu et\u00a0al. (2023)\n\nJianlin Su, Murtadha Ahmed, Yu\u00a0Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. 2023.\n\n\nRoformer: Enhanced transformer with rotary position embedding.\n\n\nNeurocomputing, page 127063.\n\n\n",
      "Sun et\u00a0al. (2022)": "\nSun et\u00a0al. (2022)\n\nYutao Sun, Li\u00a0Dong, Barun Patra, Shuming Ma, Shaohan Huang, Alon Benhaim, Vishrav Chaudhary, Xia Song, and Furu Wei. 2022.\n\n\nA length-extrapolatable transformer.\n\n\narXiv preprint arXiv:2212.10554.\n\n\n",
      "Tay et\u00a0al. (2020)": "\nTay et\u00a0al. (2020)\n\nYi\u00a0Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler. 2020.\n\n\nLong range arena: A benchmark for efficient transformers.\n\n\n",
      "Touvron et\u00a0al. (2023)": "\nTouvron et\u00a0al. (2023)\n\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et\u00a0al. 2023.\n\n\nLlama: Open and efficient foundation language models.\n\n\narXiv preprint arXiv:2302.13971.\n\n\n",
      "Vaswani et\u00a0al. (2017)": "\nVaswani et\u00a0al. (2017)\n\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan\u00a0N Gomez, \u0141\u00a0ukasz Kaiser, and Illia Polosukhin. 2017.\n\n\nAttention is all you need.\n\n\nIn Advances in Neural Information Processing Systems, volume\u00a030. Curran Associates, Inc.\n\n\n",
      "Xiao et\u00a0al. (2023)": "\nXiao et\u00a0al. (2023)\n\nGuangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. 2023.\n\n\nEfficient streaming language models with attention sinks.\n\n\nArXiv, abs/2309.17453.\n\n\n",
      "Yang et\u00a0al. (2018)": "\nYang et\u00a0al. (2018)\n\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William\u00a0W. Cohen, Ruslan Salakhutdinov, and Christopher\u00a0D. Manning. 2018.\n\n\nHotpotqa: A dataset for diverse, explainable multi-hop question answering.\n\n\nIn Conference on Empirical Methods in Natural Language Processing.\n\n\n",
      "Zhu et\u00a0al. (2023)": "\nZhu et\u00a0al. (2023)\n\nDawei Zhu, Nan Yang, Liang Wang, Yifan Song, Wenhao Wu, Furu Wei, and Sujian Li. 2023.\n\n\nPose: Efficient context window extension of llms via positional skip-wise training.\n\n\narXiv preprint arXiv:2309.10400.\n\n\n"
    },
    "domain": "cs.CL",
    "citation_count": 0
  },
  "096a1c5a-d6de-4403-9279-a68b53bdf8ff": {
    "pk": "096a1c5a-d6de-4403-9279-a68b53bdf8ff",
    "authors": [
      "Jiaqi Li",
      "Mengmeng Wang",
      "Zilong Zheng",
      "Muhan Zhang"
    ],
    "title": "LooGLE: Can Long-Context Language Models Understand Long Contexts?",
    "abstract": "Large language models (LLMs), despite their impressive performance in various language tasks, are typically limited to processing texts within context-window size. This limitation has spurred significant research efforts to enhance LLMs' long-context understanding with high-quality long-sequence benchmarks. However, prior datasets in this regard suffer from shortcomings, such as short context length compared to the context window of modern LLMs; outdated documents that have data leakage problems; and an emphasis on short dependency tasks rather than long dependency tasks. In this paper, we present LooGLE, a Long Context Generic Language Evaluation benchmark for LLMs' long context understanding. LooGLE features relatively new documents post-2022, with over 24,000 tokens per document and 6,000 newly generated questions spanning diverse domains. Human annotators meticulously crafted more than 1,100 high-quality question-answer pairs to meet the long dependency requirements. These pairs underwent thorough cross-validation, yielding the most precise assessment of LLMs' long dependency capabilities. The evaluation of eight state-of-the-art LLMs on LooGLE revealed key findings: (i) commercial models outperformed open-sourced models; (ii) LLMs excelled in short dependency tasks like short question-answering and cloze tasks but struggled with more intricate long dependency tasks; (iii) in-context learning and chaining thoughts offered only marginal improvements; (iv) retrieval-based techniques demonstrated substantial benefits for short question-answering, while strategies for extending context window length had limited impact on long context understanding. As such, LooGLE not only provides a systematic and comprehensive evaluation schema on long-context LLMs, but also sheds light on future development of enhanced models towards \"true long-context understanding\".",
    "url": "http://arxiv.org/abs/2311.04939v2",
    "timestamp": 1699407937,
    "sections": {
      "1 Introduction": "\n\n1 Introduction\n\nThe pursuit of enabling large language models (LLMs), such as ChatGPT\u00a0(Brown et\u00a0al., 2020; OpenAI, 2023; Zeng et\u00a0al., 2023), to go beyond their limited context window size so as to process, comprehend, or even learn from long-context textual information\u00a0(Ding et\u00a0al., 2023; Dao et\u00a0al., 2022; Chi et\u00a0al., 2023; Bulatov et\u00a0al., 2023) is inevitable for next-generation of language intelligence attributed to its wide applications on real-world scenarios, such as domain-specific knowledge understanding, long-context conversational generation, long story or code generation, etc.\n\n\nMeanwhile, there is an increasing need for high-quality benchmarks with much longer text lengths and more challenging tasks to provide comprehensive evaluations. However, traditional benchmarks\u00a0(Cohan et\u00a0al., 2018; Sharma et\u00a0al., 2019; Huang et\u00a0al., 2021) often fall short in text length with an average number of thousand words\u00a0(s\u00a0Ko\u02c7cisk\u00fd et\u00a0al., 2018; Yang et\u00a0al., 2018). Besides, existing benchmarks automatically collect possibly outdated documents from existing datasets published a few years ago\u00a0(Shaham et\u00a0al., 2022; Trivedi et\u00a0al., 2022; Wang et\u00a0al., 2022; Angelidis et\u00a0al., 2020), which might lead to data leakage in pre-trained LLMs and make the evaluation inaccurate. Further, the long texts are often restricted to domain-specific articles, making it hard to evaluate LLMs\u2019 ability on generic tasks and domains. Finally, it is important to note that tasks in existing benchmarks are primarily short dependency tasks, which only require LLMs to retrieve answers from one specific sentence or paragraph, without really testing LLMs\u2019 ability to collect pieces of information from paragraphs across the whole document and summarize them into an answer, which we call long dependency tasks.\n\n\nTable 1: Comparison with other long-context benchmarks.\n\n\n\nDataset\nAvg. Words\n# of Docs.\n# of Ques.\nManually Label\nLong Dependency Tasks\n\n\nSummarization\nInfo. Retrieval\nTimeline Reorder\nComputation\nDoc QA\n\n\nZero Scrolls\u00a0(Shaham et\u00a0al., 2023)\n\n10,392\n-\n4,378\n\u2717\n\u2713\n\u2717\n\u2717\n\u2717\n\u2717\n\n\nLong Bench\u00a0(Bai et\u00a0al., 2023)\n\n8,120\n-\n4,750\n350\n\u2713\n\u2713\u2217\n\n\u2717\n\u2713\u2217\n\n\u2713\n\n\nL-Eval\u00a0(An et\u00a0al., 2023)\n\n8,008\n411\n2,043\n2,043\u2020\n\n\u2713\n\u2713\n\u2717\n\u2717\n\u2713\n\n\n\nLooGLE\n\u00a0(Ours)\n19,367\n776\n6,448\n1,101\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\n\n\n\u2217 The task is created in a synthetic manner.\n\n\n\n\u2020 The questions are re-labelled from original data.\n\n\n\n\n\nTo mitigate the shortcomings of existing datasets, in this paper, we introduce a novel benchmark LooGLE\n, short for Long Context Generic Language Evaluation, to evaluate the long context understanding abilities of LLMs illustrated in Fig.\u00a01. Our benchmark has the following advantages:\n\n\n\u2022\n\nExtra-long realistic documents. It contains 776 latest gathered and extremely long documents with an average of 19.3k words. There are over 6,448 test instances without distribution bias for a more generalized assessment, many of which exceed 100k words. On one hand, they can better evaluate LLMs\u2019 capability on memorizing and understanding longer text that is far beyond their context window size. On the other hand, the excessive length is well suited to the common usage of long text scenarios.\n\n\n\n\u2022\n\nManually designed both short and long dependency tasks. It is composed of 7 major tasks to evaluate LLMs\u2019 ability to understand both short and long dependency content. We refer \u201clong dependency\u201d tasks as those that require the understanding of the inter-dependency across multiple evidence widely spanning over the entire long text. We delicately design 5 types of long dependency tasks and recruited a group of human annotators to manually create 1101 long dependency Question-Answer (QA) instances, despite the high costs and huge effort involved in this process.\n\n\n\n\u2022\n\nRelatively new documents. Our benchmark comprises texts all published after 2022 which ensures that most modern LLMs (at the date of submission) have not been pre-trained on these documents, forcing them to rely on their in-context learning ability rather than memorization. In contrast, existing benchmarks are usually a combination of content from traditional NLP dataset, whose world knowledge may have already been learned by LLMs and thus are less convincing for assessment. Furthermore, our data collection process is fully open-sourced, making it easy for the community to reconstruct/update the benchmark with newer documents, possibly on a yearly basis.\n\n\n\n\u2022\n\nCross-domain generic data. Our benchmark is derived from popular open-source documents, including arXiv papers, Wikipedia articles, and movie and TV scripts, spanning diverse domains and multiple categories such as academia, history, sports, politics, arts, events, and entertainment.\n\n\n\n\n\nWe conduct a comprehensive evaluation of 8 representative LLMs on LooGLE\n. We specifically select LLMs which have made great effort in addressing the challenge of understanding long contexts as the baselines. The results indicate that better base models with a larger context window size generally achieve better performance. However, all models experience a significant performance decline in long dependency tasks, indicating there is a desperate need to improve the true long dependency understanding capabilities of LLMs. Our dataset serves as an up-to-date benchmark for cutting-edge assessment and research on the long context understanding and modeling of LLMs.\n\n",
      "2 Related Work": "\n\n2 Related Work\n\nExisting models for long context understanding.\n\nThere are increasing research interests in developing methods to extend LLMs\u2019 context window size, such as utilizing\nrecurrent memory, sparse attention\u00a0(Meister et\u00a0al., 2021), external memory and etc.(Chen et\u00a0al., 2023c; Xiong et\u00a0al., 2023; Li et\u00a0al., 2023a). The most popular way is to develop improved transformer architectures\u00a0Dong et\u00a0al. (2023). Efficient transformers\u00a0(Tay et\u00a0al., 2020; 2022) are proposed to decrease the memory and time complexity to efficiently model longer texts. Unlike efficient transformers that simplify the attention structure, recurrent transformer\u00a0(Bulatov et\u00a0al., 2022; Bessonov et\u00a0al., 2023) keeps the full self-attention mechanism. History information of previous segments is cached and will be leveraged when the subsequent segment is fed into the model without a context fragmentation problem. Fine-tuned models on long documents\u00a0Wu et\u00a0al. (2021) are also explored, but they are often effort-costing and face difficulties in collecting ground truth fine-tuning data for long text tasks. Apart from approaches which are developed from modeling and parameter updating aspects, there are also works incorporating external memory structures and compression techniques for LLMs or using task-oriented process optimization strategies\u00a0(Gidiotis & Tsoumakas, 2020; Zhou et\u00a0al., 2022; Ram et\u00a0al., 2023; Izacard et\u00a0al., 2022).\n\n\n\nExisting datasets for long context understanding.\n\nThere are a growing number of benchmarks proposed to test LLMs\u2019 long context understanding ability\u00a0(Shaham et\u00a0al., 2023; Li, 2023). ZeroSCROLLS, L-Eval and LongBench are the three most recent ones. ZeroSCROLLS\u00a0(Shaham et\u00a0al., 2023) automatically processes datasets from different sources into a unified input format with an average of 10k words. However, it mainly focuses on collecting documents and tasks from existing datasets and relies on automatic metrics for limited model comparisons\u00a0(Shaham et\u00a0al., 2022). L-Eval\u00a0(An et\u00a0al., 2023) differs in re-annotating the data and instructions from similar public datasets with smaller sizes to ensure the quality. Besides, it optimizes the evaluation procedures and baselines to get more accurate conclusions. LongBench\u00a0(Bai et\u00a0al., 2023) provides a bilingual and multi-task dataset featuring diverse sequences of varying lengths, distributions, patterns, languages and domains for a comprehensive evaluation of long context understanding. Nonetheless, it encompasses texts of only thousands of words and tasks mostly restricted to short-term information extraction. Moreover, there are few types of \u201clong dependency\u201d tasks in previous datasets, except for summarization (which LLMs are validated to perform well on) and synthesized tasks like data aggregation and retrieving. To finish those tasks, LLMs solely need to locate pieces of information from the lengthy source input and aggregate them together. In contrast, we propose LooGLE\n which contains long dependency tasks that are much more challenging, such as event timeline reordering, comprehension/reasoning, and computation. These tasks require not only information retrieval but also understanding/reasoning over the entire text. We include a detailed comparison with concurrent works in Tab.\u00a01.\n\n\n",
      "3 The LooGLE Benchmark": "\n\n3 The LooGLE\n Benchmark\n\nThere are three categories of data sources as mentioned in Tab.\u00a02. Based on that, we generate two main types of tasks: short dependency and long dependency tasks in LooGLE\n. For short dependency tasks, we generate short QA from Wikipedia articles and cloze from scripts. For the long dependency tasks, we include summarization for arXiv papers and manually designed QA tasks for long document understanding. There are four major subtasks for QA: Multiple information retrieval, Timeline reorder, Computation, Comprehension and reasoning. We delicately generate tasks/questions to customize the intrinsic features of each data source for better long-context understanding assessments.\n\n\n\n3.1 Dataset selection and construction\n\nOur LooGLE\n benchmark consists of 3 sources: scientific papers, Wikipedia articles, movie and TV scripts, all covering various topics and categories. These documents are commonly used as corpora in NLP tasks. By replicating the methodology proposed in this paper, they can be collected easily and periodically. All the documents in our LooGLE\n benchmark are after 2022 and filtered by a length of over 10k words. We have also considered books, but found that most books meeting our principles are not license-free, therefore giving them up. Statistics of the three sources can be found in Tab.\u00a02. Details of the dataset are introduced in the following sections.\n\n\narXiv papers\n\u2003We pulled data from a massive pool of 10,000 entries on the arXiv website (https://arxiv.org/) using a random selection method. These entries ranged from January 2022 to April 2023. In the next step, we extracted their abstracts, making them our main source for the summarization task. We were pretty rigorous about maintaining data quality. That meant ditching the reference sections, cleaning up any garbled characters from math equations, and leaving out any documents under 10,000 words. After all that thorough check, we ended up with a solid collection of 516 research papers.\n\n\n\n\n\nDataset\nNo. Docs\nAvg. Words\nMax. Words\nMin. Words\nAvg. Tokens\nTask\n# Questions\n\n\narXiv\n516\n16,988\n197,977\n10,204\n20,887\nSummarization\n516\n\n\nWikipedia\n105\n17,604\n46,250\n11,285\n21,017\nShort dependency QA\n1,951\n\n\nLong dependency QA\n459\n\n\n\n\n\nMovie &\n\nTV scripts\n \n155\n28,483\n62,752\n11,089\n36,412\nCloze\n2,880\n\n\nLong dependency QA\n642\n\n\n\nTable 2: Statistics of LooGLE\n\n\nWikipedia articles\n\u2003Wikipedia is a free and popular online encyclopedia that provides information and reference on a wide range of topics. Articles are created and edited collaboratively by volunteers from all around the world, making it a dynamic and constantly evolving resource. These Wikipedia articles are perfect for evaluating the long text reading, comprehension, summarization, and information retrieval abilities of LLMs. We first downloaded and parsed the most recent page articles present in .bz file format from the official website (https://dumps.wikimedia.org/). Then we kept the articles after 2022 with over 10k words utilizing a subset of the open-source Wikipedia dataset (202203.en) from Hugging Face (https://huggingface.co/datasets/wikipedia). Since some pages in the dump file probably no longer exist and are redirected to a relevant page, we only retain pages (exempt summary, citations and references) after redirection.\n\n\nMovie and TV scripts\n\u2003A movie or TV script typically contains essential information such as scene descriptions, action descriptions, and dialogues between characters. Scripts inherently encapsulate numerous events and facts in dialogue format, necessitating models to deeply comprehend contextual nuances. To comprehend the events unfolding within a dialogue, there is a high demand on reasoning ability, along with the ability to navigate shifts in perspective and grasp the viewpoints of the characters involved. Additionally, scripts are typically lengthy and challenging for LLMs with fixed context window sizes. All scripts are sourced from three websites (https://www.scriptslug.com, https://thescriptlab.com/, https://8flix.com), consisting of movies and TV shows released after 2022.\n\n\n\n\n3.2 Long dependency tasks\n\n\n3.2.1 Tasks definition\n\nSummarization\n\nWe directly use the abstract of each paper as the reference for generating summaries. The abstracts effectively capture the main content and key information of each paper.\n\n\n\nLong dependency QA\n\nOne highlight of our dataset is that we dedicated significant effort to manually compile about 1.1k true long dependency QA pairs. The construction process is detailed in the next section. We manually designed 4 long dependency tasks: Multiple information retrieval, Timeline reorder, Computation, Comprehension and reasoning. As we will show in the experiments, these tasks are pretty challenging, requiring more advanced capabilities for long context understanding. They are valuable for understanding the limitations of LLMs. Examples of the 4 types of long dependency QAs are shown in Fig.\u00a02.\n\n\n\n\n\u2022\n\nMultiple information retrieval: Quite different from traditional short-term retrieval tasks, there are usually multiple and diverse pieces of evidence throughout the entire text for one specific answer.\nThe task requires extensive information extraction from widely distributed segments within the lengthy text, followed by the aggregation of the evidence to derive the ultimate answer. The evidence is distinctly presented and can be directly located within the original sentences or sections of the text.\n\n\n\n\u2022\n\nComputation: Similar to the previous task, it firstly needs multiple information retrieval from a wide range of texts.\nA majority of the evidence within the text takes the form of numerical data, often in question formats such as inquiries about quantities, frequencies, durations, specific numbers, and so on. To arrive at an accurate response, a profound comprehension of the question and its correlation with the provided numerical data is essential. This process relies heavily on the capacity to grasp extensive contextual information and also involves a degree of mathematical reasoning ability.\n\n\n\n\u2022\n\nTimeline reorder:\nThis task follows a more conventional format, involving the instruction, \u201cPlease reorder the timeline of the following events,\u201d along with a set of events presented in a permuted order. The objective is to arrange these events in accordance with their chronological sequence as dispersed throughout the extensive text. The events are derived directly from the source text, either as extracted segments or summarized factual information. Successful completion of this task necessitates either the memorization or comprehensive understanding of the central storyline of the document and assesses the model\u2019s proficiency in temporal awareness.\n\n\nFigure 2: Long dependency QA tasks\n\n\n\n\u2022\n\nComprehension and reasoning:\nThis task demands not only a profound comprehension of the question but also intricate reasoning to discern the underlying implications for searching for the appropriate evidence. The most prevalent question patterns involve inquiries about causality, impact, contributions, attitudes, and the essential attributes related to various events. Additionally, more extensive comparisons and evaluations are essential when the questions revolve around the primary, predominant, highest, or most critical aspects of the evidence. Furthermore, the answers to this task are not explicitly evident within the source text. They often require multi-step reasoning to model the inherent connections and dependencies, facilitating the acquisition of the answer through a complex analytical process.\n\n\n\n\n\n\n\n\n3.2.2 Construction process of long dependency QAs\n\nWe detail the construction process as follows. We first randomly sampled a total of 140 long documents from Wikipedia and the scripts dataset. We recruited students from top universities across the nation and organized a manual annotation process to generate long dependency QAs. We categorize long dependency tasks into Multiple information retrieval, Comprehension and reasoning, Calculation, and Timeline reorder (illustrated in Fig.\u00a02). Each document spans from 10,000 to 20,000 words in average and requires a generation of 5 to 10 questions. Additionally, participants were prohibited from employing large language models and tools like ChatGPT for article reading, data generation, and annotation.\n\n\nIn the generation of questions, each document underwent a meticulous three-step process that involved the assignment of two distinct annotators \u2014 one serving as the questioner and the other as the answerer. Importantly, these annotators were kept unaware of each other\u2019s identities, ensuring a rigorous cross-validation process to maintain the quality of the questions, answers, and supporting evidence. This approach aimed to achieve questions with a high degree of accuracy, precision, and relevance to the document\u2019s content.\n\n\nStep 1: Question and answer.\nThe questioner\u2019s role encompassed a comprehensive set of responsibilities, including reading the document, crafting relevant questions, offering their own answers to those questions, and pinpointing the specific evidentiary passages within the document that substantiated their answers.\nThe annotation adhered to stringent standards, encompassing the following key principles:\n\n\n\u2022\n\nLong dependency: Each question was required to exhibit a long dependency, i.e., the evidence supporting its answer should have a wide span across the document. The recommended dependency length (the distance between the earliest and latest evidence) is a minimum of 5,000 words.\n\n\n\n\u2022\n\nDiverse problem types: The questioner was required to generate a set of 5 to 10 question-answer pairs for each document, which should not contain more than 4 questions of the same type to avoid imbalanced question distribution and prevent annotators from generating overly simple questions.\n\n\n\n\u2022\n\nClear and precise questions: The formulation of each question was asked to adhere to clarity, conciseness, and no ambiguity, with examples provided.\n\n\n\n\u2022\n\nDeterministic and objective answers: The answers to the proposed questions were rigorously checked to be deterministic and objective, precluding open-ended ones.\n\n\n\nStep 2: Answer and check. The second step involves the answerers. Each answerer can only access the assigned article text and the posed questions from the questioner in the first step. The answerer was required to thoroughly read the entire document and provide answers to the questions accordingly. The standard for the answers is the same as the questioners. In addition to the aforementioned responsibilities, the answerer was also tasked with assessing the quality of the questions, which entails evaluating whether the questions adhere to the standard and whether they are answerable. In instances where a question cannot elicit a definite and unambiguous answer, it is deemed as unsatisfactory, and the answerer is asked to provide constructive feedback for improvement.\n\n\nStep 3: Revise. In the third step, the questioner for the document had access to the document, the questions, the two sets of answers from both the questioner and the answerer, as well as the feedback from the answerer. The questioner was asked to first revise the questions according to the feedback, and then unify their own answers with those from the answerers to derive the final answers.\n\n\nIn the first step, we acquired a total of 1,137 question-answer pairs. In the second step, 206 of these pairs were identified as non-compliant with the established criteria and were accompanied by suggestions for improvement. The inter-annotator agreement rate is 81.88%\u00a0(Kim & Park, 2023). Following the revisions conducted in the third step, we ultimately obtained a total of 1101 high-quality long dependency question-answer pairs which require strong long context understanding ability.\n\n\n\n\n\n3.3 Short dependency tasks\n\nQuestion Answering (QA)\n\nTo generate short dependency QA pairs, we harnessed the robust language processing and comprehension capabilities of GPT3.5-turbo-16k. These short dependency QA pairs typically do not require extensive evidence retrieval and can be extracted from localized segments. We divided each article into multiple segments and employed an iterative approach to prompt the Language Model (LLM) to generate QA pairs based on these segments, including their associated supporting evidence from the article. Details of the prompts are available in Appendix\u00a0D. Subsequently, we conducted manual reviews of the QA pairs, making refinements to some of the answers by filtering out non-essential context and eliminating redundant descriptions. This rigorous curation process was undertaken to ensure the high quality and relevance of the resulting QA pairs.\n\n\n\nCloze\n\nInitially, each script is divided into segments of varying lengths. Then, we employ GPT3.5-turbo-16k to generate factual summaries aligning with the source segment along with some constraints included in prompts (see Appendix\u00a0D). Later, we employ BERT-large \u00a0(Devlin et\u00a0al., 2019) for Named Entity Recognition (NER)\u00a0(Roy, 2021) from the generated summaries, limiting the types to person name, location, and organization. Finally, we randomly select a certain number (no more than 5) of entities from the summary and mask them as placeholders, denoted as \u201c<mask-n>\u201d. The goal is to predict the masked entities according to the long context.\n\n\n\n",
      "4 Evaluation": "\n\n4 Evaluation\n\n\n4.1 Models Selected for Evaluation\n\nCommercial models\n\nGPT4-32k, GPT4-8k, GPT3.5-turbo-16k\u00a0(Chen et\u00a0al., 2023b; Ye et\u00a0al., 2023) are all the models developed by OpenAI, as documented on their official platform (https://platform.openai.com/docs/models). GPT4-32k can handle up to 32k tokens in the context input, and GPT4-8k and GPT3.5-turbo-16k can handle up to 8k and 16k context input, respectively. We use the models of version 0613 by default.\n\n\n\nOpen-source models\n\nLLaMA2-7B-32K\u00a0(Touvron et\u00a0al., 2023) is developed by Together (https://together.ai/) and fine-tuned from Meta\u2019s original Llama2-7B \u00a0(Touvron et\u00a0al., 2023) model. It has been expanded to accommodate a context length of 32K using Position Interpolation\u00a0(Chen et\u00a0al., 2023a). ChatGLM2-6B-32k\u00a0(Du et\u00a0al., 2022), is a product of THUMD and represents an enhancement of the ChatGLM2-6B model. It is notable for its integration of FlashAttention \u00a0(Dao et\u00a0al., 2022), allowing it to train with an extended context length, increased from 2K to 32K. LongLLaMa-3B, derived from openllama, has been fine-tuned using Focused Transformer \u00a0(Tworkowski et\u00a0al., 2023) to extend its context to 256k. Lastly, RWKV-4-14B-pile \u00a0(Peng et\u00a0al., 2023) is a member of the RWKV model family, notable for its architectural fusion of both Recurrent Neural Networks (RNN) and Transformers. It has been fine-tuned to accommodate a context length of 8K.\n\n\n\nRetrieval-based Method\n\nInstead of extending the context window size, retrieval-based context compression technique\u00a0(Xu et\u00a0al., 2023; Askari et\u00a0al., 2023) augments the LLM by incorporating external memory, allowing relevant information to be retrieved using a specific query. LlamaIndex (https://github.com/jerryjliu/llama_index) is a data framework designed for LLMs. It fulfills a dual role by constructing indices for document segments and functioning as an intermediary connecting LLM with data sources, which enables LlamaIndex to retrieve relevant data segments before they are input into the LLM, thereby enhancing the LLM\u2019s capacity to effectively handle lengthy text. In our experiment, we employed the default configuration of the LlamaIndex, with embedding model text-embedding-ada-002 (https://openai.com/blog/new-and-improved-embedding-model) and language model text-davinci-003\n\u00a0(Ouyang et\u00a0al., 2022).\n\n\nIt has been proved that, performance is often highest when relevant information occurs at the beginning or end of the input context, and significantly degrades when models must access relevant information in the middle of long contexts. (Liu et\u00a0al., 2023a). Therefore, we artificially truncate the input document to certain sizes (all not larger than the context window size of above mentioned models) by concatenating the head and tail of the input. For example, when we want to truncate a long document to 16k, we concatenate its head 8k tokens and tail 8k tokens before feeding it to an LLM.\n\n\nFigure 3: An overview performance of LLMs on LooGLE\n for long context understanding\n\n\n\n\n\n4.2 Evaluation methods and metrics\n\nAutomatic evaluation\n\nWe adopt several automatic evaluation metrics, which can be categorized into two types. Bleu, Rouge, Meteor Score and Bert Score\u00a0(Li et\u00a0al., 2023b; Mukherjee & Rahman, 2023) are widely used for generative tasks such as summarization and QA. They evaluate the matching between groundtruth and LLM answers mainly based on n-gram matching and semantic similarity. For Cloze, Exact Match and Partial Match\u00a0(Sharma et\u00a0al., 2023; Engelbach et\u00a0al., 2023) are employed in our evaluation. Exact Match entails the predicted entity and the groundtruth entity exactly match each other while Partial Match allows for fuzzy matching.\n\n\n\nGPT4-as-judgment\n\nMost automatic evaluation metrics are sensitive to semantic expression, output format, and length. Thus, these metrics alone might be insufficient for effectively comparing different models (some models might output answers in a style more similar to groundtruth). However, recent research has shown that the GPT4 evaluator exhibits high consistency with human evaluation and can serve as a reliable annotator to some extent\u00a0(Suri et\u00a0al., 2023; Liu et\u00a0al., 2023b; Zheng et\u00a0al., 2023). To provide a more comprehensive assessment of models, we utilize GPT4-8k as an LLM evaluator. For QA task, given one question and two answers provided by the groundtruth and the LLM\u2019s prediction, we ask GPT4-8k to judge whether the two answers are semantically the same or not. Then we calculate the accuracy that LLM answers match the groundtruth. For summarization task, given the predicted summary with the goundtruth, we ask LLM to give a score considering various factors for generation. The prompts implemented can be found in Appendix\u00a0D.\n\n\n\nHuman evaluation\n\nWe also include human evaluation for reference, where we manually check whether LLM\u2019s prediction matches the groundtruth.\n\n\nTable 3: Performance of the short dependency tasks\n\n\n\n\n\nShort dependency QA\nCloze\n\n\n\n\nModels\n\n\n\n\nContext\n\n\n\n\nBleu1\n\n\n\n\nBleu4\n\n\n\n\nRouge1\n\n\n\n\nRouge4\n\n\n\n\nRougeL\n\n\n\n\nMeteor_score\n\n\n\n\nBert_score\n\n\n\n\nGPT4_score\n\n\n\n\nExact Match\n\n\n\n\nPartial Match\n\n\n\n\nGPT4-32k\n32k\n24.61\n11.14\n61.80\n50.73\n60.75\n32.94\n78.72\n71.52\n70.5\n80.81\n\n\nGPT4-8k\n8K\n27.35\n14.38\n67.59\n56.01\n65.77\n38.56\n87.93\n53.99\n66.03\n76.62\n\n\nGPT3.5-turbo-16k\n16K\n22.67\n9.62\n62.56\n48.63\n60.66\n32.58\n87.04\n66.82\n54.64\n63.42\n\n\nLlamaIndex\n\\\n33.37\n21.43\n58.82\n42.93\n57.08\n37.17\n86.58\n59.61\n58.95\n66.86\n\n\nChatGLM2-6B\n32k\n14.29\n6.07\n20.50\n13.16\n20.36\n13.08\n87.28\n23.65\n0.05\n0.98\n\n\nLongLLaMa-3B\n256k\n1.37\n0.26\n26.97\n11.02\n26.10\n11.34\n71.65\n13.75\n-\n2.13\n\n\nRWKV-4-14B-pile\n8k\n0.80\n0.04\n21.7\n6.39\n20.64\n9.41\n70.42\n8.93\n-\n-\n\n\nLLaMA2-7B-32K\n32k\n0.18\n7.25*e-308\n1.86\n0.00\n1.86\n1.52\n61.53\n3.18\n-\n0.58\n\n\n\n\n\nTable 4: Performance of the long dependency tasks\n\n\n\n\n\nModels\n\n\n\n\nContext\n\n\n\n\nBleu1\n\n\n\n\nBleu4\n\n\n\n\nRouge1\n\n\n\n\nRouge4\n\n\n\n\nRougeL\n\n\n\n\nMeteor_score\n\n\n\n\nBert_score\n\n\n\n\nGPT4_score\n\n\n\n\narXiv paper summarization\n\n\n\n\n\n\n\n\n\n\n\nGPT4-32k\n32k\n24.50\n0.73\n27.15\n7.10\n24.25\n19.03\n84.04\n82.84\n\n\nGPT4-8k\n8k\n29.02\n2.09\n32.08\n11.11\n28.85\n22.64\n84.92\n85.42\n\n\nGPT3.5-turbo-16k\n16k\n28.70\n1.59\n32.04\n10.69\n28.89\n22.34\n84.82\n86.84\n\n\nLlamaIndex\n\\\n22.53\n0.63\n26.28\n6.97\n23.73\n21.07\n83.09\n76.35\n\n\nChatGLM2-6B\n32k\n0.04\n1.60e-310\n5.97\n8.43e-05\n5.82\n6.40\n73.25\n13.23\n\n\nLongLLaMa-3B\n256k\n4.24\n9.32e-309\n4.10\n0.52\n3.86\n3.82\n73.41\n12.28\n\n\nRWKV-4-14B-pile\n8k\n6.28\n4.58e-05\n6.45\n0.74\n6.01\n6.00\n75.28\n7.02\n\n\nLLaMA2-7B-32K\n32k\n0.03\n4.66e-310\n0.12\n0.00\n0.12\n0.67\n71.21\n7.60\n\n\nLong dependency QA\n\n\n\n\n\n\n\n\n\n\n\nGPT4-32k\n32k\n8.55\n1.40\n25.59\n6.36\n24.04\n11.13\n80.16\n54.09\n\n\nGPT4-8k\n8k\n8.94\n1.01\n23.45\n6.57\n21.69\n10.18\n85.36\n42.12\n\n\nGPT3.5-turbo-16k\n16k\n6.92\n1.81\n25.02\n6.68\n23.63\n10.40\n83.79\n45.04\n\n\nLlamaIndex\n\\\n7.76\n1.24\n23.62\n7.10\n22.30\n10.47\n83.87\n37.63\n\n\nChatGLM2-6B\n32k\n5.55\n0.11\n9.41\n1.93\n8.69\n4.39\n85.78\n11.50\n\n\nLongLLaMa-3B\n256k\n1.04\n3.12e-307\n2.96\n0.03\n2.71\n1.66\n78.60\n6.48\n\n\nRWKV-4-14B-pile\n8k\n0.71\n9.52e-307\n18.54\n1.55\n17.69\n3.45\n71.36\n5.33\n\n\nLLaMA2-7B-32K\n32k\n0.08\n2.44e-308\n2.05\n0.00\n2.05\n0.46\n50.28\n4.18\n\n\n\n\n\n\n\n\n4.3 Results\n\nFig.\u00a03 shows an overall performance comparison of different models on different tasks. The first radar plot shows the original accuracy evaluated by GPT4-8k (except cloze) and the partial match result (for cloze) over different tasks. For better visualization, we scale the scores of all models on each task to [40, 100] in the second radar plot and the histogram, so that the best model on each task has a score of 100 and the worst model has a score of 40. From the charts, GPT4-32k demonstrates its impressive overall performance across all tasks (with highest scores on all tasks except summarization). In comparison, open-source models show a significant performance gap to commercial models on our benchmark. From the first radar chart, we can find that among the 7 major tasks, short QA, cloze and summarization are more effectively addressed by LLMs, while real long dependency QA tasks are far from being solved, where even GPT4-32k hardly achieves over 40% accuracy. The empirical results demonstrate that even the most successful commercial model still cannot effectively address those really challenging long dependency tasks, leaving large room for improvement. Detailed evaluation results and further analysis can be found in the following sections.\n\n\n\n4.3.1 Main results on long and short dependency tasks\n\nResults on short dependency tasks\u00a0\n\nTab.\u00a03 presents the performance (%) of all the baselines on LooGLE\n in short dependency tasks.\nNotably, GPT4-32k attains the highest accuracy according to the GPT4 evaluator\u2019s perspective. GPT4-8k, GPT3.5-turbo-16k, and the retrieval-based LlamaIndex closely follow, demonstrating competitive performance levels. Surprisingly, GPT4-8k exhibits the most robust overall performance in terms of automatic evaluation metrics. It\u2019s worth mentioning that GPT4-32k, due to its tendency to generate longer outputs, faces penalties from these automatic metrics. This discrepancy among different metrics highlights the need for improved evaluation methods. Furthermore, in the context of cloze tasks, GPT4-32k excels again when equipped with a longer context window. In Fig.\u00a05, the exact match results in cloze tasks are displayed for varying source segment lengths. The results show that as the segment length increases, model performance gradually decreases, underscoring the increasing difficulty of effectively filling in the masked entities with longer source text.\n\n\n\nResults on long dependency tasks\u00a0\n\nTab.\u00a04 shows the aggregated results on long dependency tasks. Firstly, we can observe that summarization can be well addressed by commercial models, with GPT-4 evaluation accuracy of over 80%. However, the various types of long dependency QAs in our benchmark apparently pose substantial challenges for current LLMs. Both open-source and commercial models experience a significant performance decline. We will analyze model performance on individual types of QAs in Section\u00a04.3.2.\nIt is validated that longer context window size (thus less information loss due to truncation) indeed helps in long context tasks by comparing GPT4-32k with GPT4-8k. GPT4-8k has a much lower accuracy by answering \u201cThe text does not provide information on \u2026\u201d in many cases. Open-sourced models fall far below the average of commercial models, among which LLaMA2-7B-32K and RWKV-4-14B-pile display almost zero performance. By employing context scaling techniques like positional interpolation, RNN and fine-tuning on longer texts, current LLMs can be equipped with much longer context windows than their default limits. Nevertheless, our results show that there is still a huge discrepancy between merely increasing the context window size and really understanding the long context. The poor performance on long dependency QAs suggests that we may need to revisit LLMs\u2019 long context understanding ability in more challenging tasks other than some simple ones like summarization and retrieval, as they are unable to test whether LLMs understand the inter-dependency in long texts. \n\n\nTable 5: Impact of input length on long dependency tasks\n\n\n\n\n\nModels\n\n\n\n\nContext\n\n\n\n\nBleu1\n\n\n\n\nBleu4\n\n\n\n\nRouge1\n\n\n\n\nRouge4\n\n\n\n\nRougeL\n\n\n\n\nMeteor_score\n\n\n\n\nBert_score\n\n\n\n\nGPT4_score\n\n\n\n\narXiv paper summarization\n\n\n\n\nGPT4-32k\n32k\n24.50\n0.73\n27.15\n7.10\n24.25\n19.03\n84.04\n82.84\n\n\nGPT4-32k\n24k\n25.57\n0.81\n27.61\n7.53\n24.73\n19.86\n84.07\n83.15\n\n\nGPT4-32k\n16k\n24.80\n0.70\n27.29\n7.26\n24.28\n19.12\n84.11\n82.82\n\n\nGPT4-32k\n8k\n26.26\n9.35\n27.83\n7.67\n24.74\n20.08\n84.10\n82.75\n\n\nGPT4-8k\n8k\n29.02\n2.09\n32.08\n11.11\n28.85\n22.64\n84.92\n85.42\n\n\nLong dependency QA\n\n\n\n\nGPT4-32k\n32k\n7.64\n1.24\n15.53\n4.46\n14.60\n11.12\n86.07\n54.65\n\n\nGPT4-32k\n24k\n8.23\n1.66\n14.92\n4.12\n13.90\n10.60\n86.16\n50.61\n\n\nGPT4-32k\n16k\n8.57\n1.35\n16.21\n4.30\n14.90\n11.91\n86.36\n47.55\n\n\nGPT4-32k\n8k\n7.46\n1.77\n13.75\n5.08\n12.89\n10.01\n85.77\n38.34\n\n\nGPT4-8k\n8k\n8.94\n1.01\n23.45\n6.57\n21.69\n10.18\n85.36\n42.12\n\n\n\n\n\nTable 6: Performance of the long dependency QA with LlamaIndex\n\n\n\n\n\nModels\n\n\n\n\nContext\n\n\n\n\nBleu1\n\n\n\n\nBleu4\n\n\n\n\nRouge1\n\n\n\n\nRouge4\n\n\n\n\nRougeL\n\n\n\n\nMeteor_score\n\n\n\n\nBert_score\n\n\n\n\nGPT4_score\n\n\n\n\nGPT4-32k\n32k\n6.08\n1.31\n10.27\n3.39\n9.52\n8.54\n85.27\n28.25\n\n\nGPT4-8k\n8k\n6.62\n1.50\n11.95\n3.80\n10.99\n9.02\n85.51\n26.34\n\n\nGPT3.5-turbo-16k\n16k\n6.50\n0.92\n10.93\n3.56\n9.86\n8.65\n85.63\n33.24\n\n\nDefault\n\\\n7.02\n1.24\n11.60\n3.75\n10.57\n9.37\n85.61\n33.16\n\n\nChatGLM2-6B-32k\n32k\n0.15\n2.82e-310\n2.23\n0.05\n2.23\n0.74\n83.40\n7.73\n\n\nLongLLaMa-3B\n256k\n1.04\n4.22e-311\n2.27\n0.00\n2.23\n2.23\n82.18\n5.33\n\n\nRWKV-4-14B-pile\n8k\n2.65\n8.09e-307\n4.08\n0.33\n3.65\n3.92\n80.74\n2.43\n\n\nLLaMA2-7B-32K\n32k\n0.43\n1.29e-307\n5.85\n0.00\n5.85\n1.04\n81.38\n6.76\n\n\n\n\n\n\n\n\n4.3.2 Deep dive into long context understanding capabilities\n\nIn this section, we analyze different factors affecting the long context understanding abilities of LLMs, and dive into individual types of long dependency QAs to check LLMs\u2019 limitations.\n\n\nImpact of varying input length\u00a0\n\nIn Tab.\u00a05, we study the impact of varying lengths of inputs on long dependency tasks with GPT4 models. We find that expanding input length hardly helps in paper summarization while it substantially enhances the model\u2019s performance on long dependency QAs. The difference can be attributed to the inherent nature of the arXiv paper. It has both the introduction and conclusion sections located at the beginning and in the end respectively, which already contain the major sketch of the paper. Meanwhile, in our expectation, longer input promotes the performance of long dependency QAs by introducing less information loss.\n\n\n\nRetrieval Based Techniques\u00a0\n\nTo evaluate the effectiveness of retrieval techniques for long-context dependency questions, we undertook an extensive series of experiments on our long dependency QA tasks by replacing the base LLM model in LlamaIndex with different baseline LLMs. In these experiments, we utilized the open-source embedding all-mpnet-base-v2 \u00a0(Song et\u00a0al., 2020). When compared to the default embedding, text-embedding-ada-002 (https://openai.com/blog/new-and-improved-embedding-model), there was a noticeable performance decline. Nonetheless, this disparity did not hinder our conclusions.\nFrom Tab.\u00a04 and Tab.\u00a06, our research findings reveal that the incorporation of retrieval techniques does not generally enhance the performance of long dependency QA tasks. There is a conspicuous performance decline, particularly evident for models like GPT4-8k and GPT4-32k. It can be attributed to the tendency of GPT models to produce longer outputs, sometimes including hallucinatory information, when the retrieved segments lack sufficient context. The phenomenon highlights the intricacy of our dataset, where a series of long dependency understanding and modeling capabilities such as comprehension and multi-hop reasoning are essentially needed. Relying solely on retrieval mechanisms might be insufficient in recalling the necessary information and further generating the final answer, resulting in a marked performance decline. However, we did observe an minor improvement in the BERT score for open-source models. This improvement in fluency can be attributed to the considerably shorter length of the retrieved segments used as inputs, in contrast to the entirety of the document.\n\n\n\nIndividual results on different types of long dependency QAs\u00a0\n\nPrevious research mostly focuses on presenting aggregated results for long dependency QA tasks across various question types. Differently, in this study, our objective is to delve into the performance of models in individual tasks that demand diverse capabilities, including reading comprehension, information retrieval, computation, and reasoning. In this regard, we employed GPT4 as the evaluator, and the accuracy results are available in Tab.\u00a07. Across the four tasks examined, LLMs generally exhibit strong performance in comprehension, reasoning, and multiple information retrieval, but fall short in tasks related to timeline reordering and computation. Furthermore, we observed that the way questions are framed has a significant impact on LLMs\u2019 performance. Yes-no questions and multiple-choice questions tend to be easier for LLMs to answer, particularly when the search space is limited, as opposed to open-ended questions within unstructured text.\n\n\nTable 7: Individual results on four types of long dependency QAs evaluated by GPT4\n\n\n\n\n\nModels\n\n\n\n\nContext\n\n\n\n\n \n\n\nInformation retrieval\n\n\n\n\n\n \n\n\nTimeline reorder\n\n\n\n\n\nComputation\n\n\n\n\n \n\n\nComprehension and reasoning\n\n\n\n\n\nGPT4-32k\n32k\n43.60\n64.43\n37.36\n61.26\n\n\nGPT4-8k\n8K\n31.89\n61.36\n22.54\n45.78\n\n\nGPT3.5-turbo-16k\n16K\n36.86\n55.73\n24.73\n51.09\n\n\nLlamaIndex\n\\\n27.60\n47.83\n19.78\n43.83\n\n\nChatGLM2-6B-32k\n32k\n12.47\n14.17\n5.43\n11.08\n\n\nLongLLaMa-3B\n256k\n3.82\n6.48\n5.15\n10.17\n\n\nRWKV-4-14B-pile\n8k\n4.67\n5.19\n4.40\n7.13\n\n\nLLaMA2-7B-32K\n32k\n3.01\n1.61\n1.12\n6.85\n\n\n\n\n\nTable 8: Performance of Timeline Reorder\n\n\n\n\n\nModels\n\n\n\n\nLSD\n\n\n\n\nLMD\n\n\n\n\nSD\n\n\n\n\nSDD\n\n\n\n\nLSD-S\n\n\n\n\nLMD-S\n\n\n\n\nSD-S\n\n\n\n\nSDD-S\n\n\n\n\nNon-standard(%)\n\n\n\n\nGPT4-32k\n1.04\n0.57\n0.93\n1.12\n1.21\n0.82\n1.41\n1.60\n52.80\n\n\nGPT4-8k\n1.24\n0.64\n1.04\n1.281\n1.43\n0.92\n1.51\n1.74\n49.31\n\n\nLlamaIndex\n1.55\n0.78\n1.19\n1.551\n1.95\n1.08\n1.65\n2.09\n39.72\n\n\nGPT3.5-turbo-16k\n3.58\n1.43\n2.17\n2.916\n1.05\n0.86\n1.24\n1.26\n77.21\n\n\nLongLLaMa-3B\n4.18\n1.71\n2.59\n3.30\n1.90\n1.12\n1.80\n2.07\n92.92\n\n\nChatGLM2-6B-32k\n4.31\n1.74\n2.63\n3.37\n1.83\n1.17\n1.50\n2.00\n99.07\n\n\nRWKV-4-14B-pile\n4.33\n1.75\n2.64\n3.38\n1.90\n0.97\n1.00\n1.75\n98.13\n\n\nLLaMA2-7B-32K\n4.33\n1.75\n2.64\n3.38\n2.50\n1.17\n1.33\n2.33\n98.60\n\n\n\n\n\n\nResults on long dependency QAs with/without CoT\u00a0\n\nTo bolster the long-context capabilities of LLMs, we conducted additional experiments designed to unlock their potential using the Chain of Thoughts\u00a0(CoT) framework\u00a0(Kojima et\u00a0al., 2023). We selected LlamaIndex as a representative model, given its impressive performance in both short and long dependency question-answering tasks, alongside strong commercial models such as GPT4. A manual evaluation was carried out on a subset comprising one-third of instances from each task category within long dependency QA. We initiated the LLM with a zero-shot CoT approach, employing prompts such as \u201cLet\u2019s think step by step,\u201d and furnished a few-shot setup with detailed rationales and standard output formats\u00a0Wei et\u00a0al. (2023) to facilitate responses to long dependency questions. As depicted in Fig.\u00a05, the zero-shot CoT approach had minimal impact on accuracy in comprehension and reasoning, as well as multiple retrieval tasks, but yielded a substantial 20% and 10% absolute accuracy increase in timeline reorder and computation. Interestingly, the few-shot CoT approach benefits the first two types but surprisingly leads to a decline in performance in the latter two types compared with zero-shot.\nWe hypothesize the reason is that the evidence and rationales in few-shot examples cannot be generalized to other questions, and including them might on the contrary give wrong guidance to the model.\n\n\n\n\n\nFigure 4: Performance of varying segments\n\n\n\n\nFigure 5: Long dependency QA tasks w/o CoT\n\n\n\n\n\n\nAutomatic evaluation on timeline reorder\n\nIn order to evaluate the performance of time reorder task outputs, it is essential to address discrepancies arising from the diverse formats produced by various models. Typically, these outputs comprise conventional numerical sequences, but errors in non-standard formats when evaluation necessitate preprocessing for accurate assessment. A proposed approach involves converting the serial numbers in the candidate answers from their original question into Roman numbers (i.e., I, II, \u22ef\u22ef\\cdots\u22ef), thereby enhancing discrimination through regular expression matching. Four key metrics, namely, LSD (location square deviation), LMD (location mean deviation), SD (swap deviation), and SDD (swap distance deviation), are employed to measure the similarity of numeric sequences, refer to Appendix\u00a0C for metric details. Smaller deviations indicate a higher degree of resemblance between the sequences. Any outputs that are empty, possess unequal lengths, or contain extra elements are categorized as non-standard. The maximum deviation between the provided ground truth and all corresponding candidate answers is computed as the worst score for evaluation purposes.\n\n\nThe percentage of non-standard outputs for each model and corresponding performances can be found in Tab.\u00a08.\nAs seen, it is evident that except for GPT4, which demonstrates a remarkable degree of adherence and alignment following Reinforcement Learning from Human Feedback (RLHF)\u00a0(Lee et\u00a0al., 2023), most open-sourced models struggle to generate texts in the correct format with less than 10%. However, this issue can be mitigated in significantly large models through the utilization of few-shot\nprompts and mandatory instructions. This phenomenon results in performance\npenalties when assessed using automated metrics. Consequently, to ascertain the\ngenuine capacity of LLMs in this task, we calculate the four metrics exclusively for outputs in standard format (\u201c-S\u201d).\n\n\n\nDispcrepancy in generated outputs of models\n\nDistributions of generated outputs of various models are depicted in Fig.\u00a06.\nIt is noteworthy that well-behaved models consistently produce shorter responses, averaging around 50 words, irrespective of the question type, particularly in short-term question answering scenarios. In contrast, models fine-tuned with longer textual inputs, such as LLaMA2-7B-32K, RWKV-4-14B-pile, and LongLLaMa-3B, tend to yield significantly lengthier responses, even when a maximum generation constraint of 500 tokens is enforced. An interesting deviation is observed in LongLLaMa-3B, which demonstrates variability in response lengths across both tasks. This behavior may stem from challenges in comprehending and addressing exceedingly complex long question-answering tasks. Consequently, the model appears to prioritize extracting a maximum number of pertinent contexts from its memory to generate sufficiently extensive responses that are deemed acceptable and rational.\n\n\nFigure 6: Output distributions on QA tasks\n\n\nMoreover, upon closer examination of model outputs, a significant disparity in generation quality is observed across various LLMs and task types, indicating a non-specific issue. Notably, commercial models like GPT4, GPT3.5, and LlamaIndex consistently generate outputs that exhibit a higher degree of human-likeness, completeness, and logical coherence within a structured format. These models consistently deliver contextually relevant, query-based responses. In contrast, open-sourced models, such as ChatGLM2-6B-32k, tend to offer shorter answers, occasionally confined to numeric responses. In cases where a definite answer is lacking, ChatGLM2-6B-32k compensates by retrieving relevant contextual information. However, the RNN-based model RWKV-4-14B-pile often generates duplicated responses or resorts to repeating the given questions to reach the maximum token length, sometimes resorting to code generation to address issues related to its training data. The performance of the LLaMA2-7B-32K model is notably worse, as it sporadically produces irrelevant or nonsensical text, along with the inclusion of special symbols when it fails to provide meaningful answers. More examples of outputs from different models can be seen in Appendix\u00a0F.\n\n\n\nProbable explanations for long QA bad cases\n\nTable 9: Bad cases study on the long dependency QA\n\n\n\n\n \n\n\nLong QA Tasks\n\nHallucination\u2217\n\n\n \n\n\nRedundant\n\nretrieval\u2020\n\n\n \n\n\nInsufficient\n\nretrieval\u22c6\n\n\n \n\n\nIrrelevant\n\nanswer\u22c4\n\n\n \n\n\nNo relevant\n\ncontext\u2227\n\n\n \n\n\nWrong/No\n\nreasoning\u00d7\n\nOthers\n\n\n\n \n\n\nComputation\n\n31.11\n24.44\n15.56\n0.00\n20.00\n0.00\n8.88\n\n\n\n \n\n\nMultiple information retrieval\n\n14.71\n31.37\n28.43\n13.73\n13.73\n0.00\n7.84\n\n\n\n \n\n\nComprehension and reasoning\n\n14.29\n10.99\n21.98\n18.68\n16.48\n10.99\n6.59\n\n\n\n\u2217 Evidence of predictions is not shown up in the original inputs and generated by LLM itself from nowhere.\n\n\n\n\u2020 Apart from the right evidence, irrelevant evidence is also redundantly retrieved.\n\n\n\n\u22c6 Not all of the essential evidence to answer the question is retrieved.\n\n\n\n\u22c4 Evidence of predictions generated have no or minor correlation with the question.\n\n\n\n\u2227 No relevant context in LLM\u2019s memory and refuse to answer the question.\n\n\n\n\u00d7 Fail to retrieve evidence that needs further reasoning other than directly extracting from the inputs.\n\n\n\n\n\nTo investigate whether the models have effectively memorized and comprehended lengthy contextual information, we conducted a comprehensive manual analysis of the underlying causes of failures in each long question-answering task. The rationale behind CoT analysis aided in understanding how models decompose and tackle challenges associated with extended dependency-based QA. Our observations reveal that LLMs struggle with these tasks primarily due to their inability to extract precise information and a propensity to generate responses that lack factual accuracy. Constraints imposed by the inherent context window limitations, coupled with information loss resulting from the optimized Transformer and position encoding, contribute to their struggles in memorizing the original extensive contexts. In most cases, models attempt to compensate by retrieving and integrating the most pertinent evidence, even if it results in redundant answers. However, they also acknowledge their insufficient context and, at times, abstain from providing responses rather than resorting to nonsensical answers. Furthermore, addressing these challenges necessitates enhanced comprehension and reasoning abilities, particularly when answers are not clearly evident across multiple pieces of evidence scattered throughout the raw texts. The insights derived from our benchmark analysis offer a scientific foundation and pave the way for promising research directions aimed at augmenting LLM capabilities for handling long contextual inputs. These findings underscore the need for further progress in comprehension, computation, and reasoning tasks using our dataset to effectively enhance LLMs\u2019 capacity to understand extended dependency contexts.\n\n\n\n\n",
      "5 Conclusion": "\n\n5 Conclusion\n\nThis paper introduces a novel benchmark, LooGLE\n, designed to facilitate the assessment of long-context comprehension by LLMs. LooGLE\n addresses the deficiencies present in previous datasets by offering considerably longer text passages, utilizing relatively new documents after 2022, incorporating multi-source materials from various categories, and notably featuring meticulously designed and annotated tasks with diverse contextual dependencies. Our extensive evaluations unveil substantial limitations in the capacity of existing LLMs to understand and reason about the intricate interdependencies present in lengthy texts, even when provided with considerably extended context windows. Furthermore, a notable disparity is observed between commercial and open-source models, with both exhibiting challenges in long dependency tasks as per our benchmark assessments. The outcomes underscore the utility of our dataset as a valuable reference for evaluating long-context comprehension and present avenues for potential enhancements in LLM performance.\n\n\n",
      "Appendix A More details of our dataset": "\n\nAppendix A More details of our dataset\n\nDistributions of the input length and dependency spanning in words for long dependency QA tasks are shown in Figs.\u00a08 and\u00a08. N-gram sunburst graph for generated QA pairs can be seen in Fig.\u00a09.\n\n\n\n\n\nFigure 7: Input document length distributions\n\n\n\n\nFigure 8: Evidence dependency distributions\n\n\n\n\n\n\n\n\n\n\n\nFigure 9: 4-gram sunburst graph for short and long dependency QA. (a) short dependency questions (b) short dependency answers (c) long dependency questions (d) long dependency answers\n\n",
      "Appendix B Task definition": "\n\nAppendix B Task definition\n\nThe Cloze task formulation process can be seen in Fig.\u00a010.\n\n\nFigure 10: Cloze task\n\n",
      "Appendix C Timeline reorder evaluation metrics": "\n\nAppendix C Timeline reorder evaluation metrics\n\nWe employ 4 metrics to measure the similarity of numeric output sequences for timeline reorder tasks. For given two numeric sequences A\ud835\udc34Aitalic_A and B\ud835\udc35Bitalic_B with the same sequence length n\ud835\udc5bnitalic_n, i\u2062[A]\ud835\udc56delimited-[]\ud835\udc34i[A]italic_i [ italic_A ] and i\u2062[B]\ud835\udc56delimited-[]\ud835\udc35i[B]italic_i [ italic_B ] is the i\ud835\udc56iitalic_ith number in each sequence. They can be computed using the formula below: \nLSD is the abbreviation for location square deviation:\n\n\n\n\n\n\nL\u2062S\u2062D\u2062(A,B)=1n\u2062\u2211i=0n\u22121(i\u2062[A]\u2212i\u2062[B])2\ud835\udc3f\ud835\udc46\ud835\udc37\ud835\udc34\ud835\udc351\ud835\udc5bsuperscriptsubscript\ud835\udc560\ud835\udc5b1superscript\ud835\udc56delimited-[]\ud835\udc34\ud835\udc56delimited-[]\ud835\udc352LSD(A,B)=\\frac{1}{n}\\sum_{i=0}^{n-1}(i[A]-i[B])^{2}italic_L italic_S italic_D ( italic_A , italic_B ) = divide start_ARG 1 end_ARG start_ARG italic_n end_ARG \u2211 start_POSTSUBSCRIPT italic_i = 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n - 1 end_POSTSUPERSCRIPT ( italic_i [ italic_A ] - italic_i [ italic_B ] ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT\n\n(1)\n\n\n\n\nLMD is the abbreviation for location mean deviation:\n\n\n\n\n\n\nL\u2062M\u2062D\u2062(A,B)=1n\u2062\u2211i=0n\u22121|i\u2062[A]\u2212i\u2062[B]|\ud835\udc3f\ud835\udc40\ud835\udc37\ud835\udc34\ud835\udc351\ud835\udc5bsuperscriptsubscript\ud835\udc560\ud835\udc5b1\ud835\udc56delimited-[]\ud835\udc34\ud835\udc56delimited-[]\ud835\udc35LMD(A,B)=\\frac{1}{n}\\sum_{i=0}^{n-1}\\left|i[A]-i[B]\\right|italic_L italic_M italic_D ( italic_A , italic_B ) = divide start_ARG 1 end_ARG start_ARG italic_n end_ARG \u2211 start_POSTSUBSCRIPT italic_i = 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n - 1 end_POSTSUPERSCRIPT | italic_i [ italic_A ] - italic_i [ italic_B ] |\n\n(2)\n\n\n\n\nSD is the abbreviation for swap deviation:\n\n\n\n\n\n\nSD(A,B)=min(W(A\u2192B)=min(\u2211s\u2208A\u2192B1)SD(A,B)=min(W(A\\to B)=min({\\textstyle\\sum_{s\\in A\\to B}}1)italic_S italic_D ( italic_A , italic_B ) = italic_m italic_i italic_n ( italic_W ( italic_A \u2192 italic_B ) = italic_m italic_i italic_n ( \u2211 start_POSTSUBSCRIPT italic_s \u2208 italic_A \u2192 italic_B end_POSTSUBSCRIPT 1 )\n\n(3)\n\n\n\n\nSDD is the swap distance deviation:\n\n\n\n\n\n\nSDD(A,B)=min(W(A\u2192B)=min(\u2211s\u2208A\u2192B|i\u2212j|)SDD(A,B)=min(W(A\\to B)=min({\\textstyle\\sum_{s\\in A\\to B}}\\left|i-j\\right|)italic_S italic_D italic_D ( italic_A , italic_B ) = italic_m italic_i italic_n ( italic_W ( italic_A \u2192 italic_B ) = italic_m italic_i italic_n ( \u2211 start_POSTSUBSCRIPT italic_s \u2208 italic_A \u2192 italic_B end_POSTSUBSCRIPT | italic_i - italic_j | )\n\n(4)\n\n\nwhere s=A\u2062(i,j)\ud835\udc60\ud835\udc34\ud835\udc56\ud835\udc57s=A(i,j)italic_s = italic_A ( italic_i , italic_j ) means the swap between the i\ud835\udc56iitalic_ith and j\ud835\udc57jitalic_jth element in A\ud835\udc34Aitalic_A. S=A\u2192B\ud835\udc46\ud835\udc34\u2192\ud835\udc35S=A\\to Bitalic_S = italic_A \u2192 italic_B means a series of swap actions to convert A\ud835\udc34Aitalic_A to B\ud835\udc35Bitalic_B. W\u2062(S)=\u2211s\u2208Sw\u2062(s)\ud835\udc4a\ud835\udc46subscript\ud835\udc60\ud835\udc46\ud835\udc64\ud835\udc60W(S)={\\textstyle\\sum_{s\\in S}}w(s)italic_W ( italic_S ) = \u2211 start_POSTSUBSCRIPT italic_s \u2208 italic_S end_POSTSUBSCRIPT italic_w ( italic_s ) means the weights sum of all the swap actions in S\ud835\udc46Sitalic_S, where w\u2062(s)=1\ud835\udc64\ud835\udc601w(s)=1italic_w ( italic_s ) = 1 in SD and w\u2062(s)=|i\u2212j|\ud835\udc64\ud835\udc60\ud835\udc56\ud835\udc57w(s)=\\left|i-j\\right|italic_w ( italic_s ) = | italic_i - italic_j | in SSD.\n\n",
      "Appendix D Prompts": "\n\nAppendix D Prompts\n\n\nD.1 Short dependency QA pair generation\n\n\n\n[seg] = {Input long texts}\nPlease generate 2 questions and corresponding answers based on given [seg] in less words as possible. Return reference text S, question Q and answer A from [seg] in json format as: {\u201cS\u201d: ,\u201cQ\u201d: ,\u201cA\u201d: },{\u201cS\u201d: ,\u201cQ\u201d: ,\u201cA\u201d: }.\n\n\n\n\n\nD.2 Short and long dependency question and answering\n\n\n\nInstruction:\nPlease answer the question based on the given long texts below.\n{Input long texts}\nQuestion: {Question}\nAnswer:\n\n\n\n\n\nD.3 scripts segment summarization for cloze formulation\n\n\n\nInstruction:\nPlease write a summary for this script segment within 500 words, focusing on describing objective facts and avoiding subjective opinions.\n{scripts segement}\nSummary:\n\n\n\n\n\nD.4 Cloze\n\n\n\nexample= {\u201c<mask-0>\u201d: \u201cBob\u201d, \u201c<mask-1\u201d>: \u201cGorrosion Magazine\u201d, \u201c<mask-2>\u201d: \u201cBethel Horizon\u201d}\nInstruction:\nPlease answer the cloze question based on the given long texts below. Each of the placeholder \u201c<mask-n>\u201d in the question could be an entity of Person, Location or Organization. The same masks represent the same entity.\nOutput a json format answer, for example:{example}\n{Input long texts}\nQuestion: {Question} What are the masked entities?\nAnswer:\n\n\n\n\n\nD.5 Summarization\n\n\n\nInstruction:\nPlease generate a summary of the below paper.\n{Input long texts}\nSummarization:\n\n\n\n\n\nD.6 Timeline reorder\n\n\n\nInstruction:\nPlease answer the question based on the given long texts below. Please use Roman numbers only for answers.\n{Input long texts}\nQuestion: {Question}\nAnswer:\n\n\n\n\n\nD.7 QA task evaluation by LLM (GPT4)\n\n\n\nInstruction:\nGiven one question, there is a groundtruth and a predict answer. Please decide whether they are the same or not in semantic. Please only output True or False.\nQuestion: {Question}\ngroundtruth = {Reference answer}\npredicted answer = {Generated output}\n\n\n\n\n\nD.8 Summarization task evaluation by LLM (GPT4)\n\n\n\nInstruction:\nPlease compare predicted summary with the goundtruth and evaluate the predicted summary from the perspectives of information completeness, consistency, fluency, and grammar by giving a score within the range of 0 to 100.\ngroundtruth = {Reference answer}\npredicted summary = {Generated output}\n\n\n\n\n\nD.9 Few-Shot CoT for long QA\n\n\n\nInstruction:\nPlease answer the question based on the given long texts below.\n{Input long texts}\n{Demonstrations}\nQuestion: {Question}\nAnswer:\n\n\n\n\n\nD.10 Zero-Shot CoT for long QA\n\n\n\nInstruction:\nPlease answer the question based on the given long texts below.\n{Input long texts}\nQuestion: {Question}\nAnswer: Let\u2019s think step by step.\n\n\n\n",
      "Appendix E Examples for long context understanding tasks": "\n\nAppendix E Examples for long context understanding tasks\n\n\nE.1 Short dependency question and answering\n\n\nQuestion:\nWho did Picardo collaborate with for building preservation and restoration projects?\nEvidence:\nOn qualifying in 1951, Picardo pursued his interest in historical architecture by collaborating on a number of building preservation and restoration projects with the Spanish architect and architectural historian Fernando Chueca Goitia, who was 8 years his senior.\nAnswer:\nHe collaborated with Spanish architect and architectural historian Fernando Chueca Goitia.\n\n\n\n\nQuestion:\nWhat was the nickname given to the 18th century period?\nEvidence:\nThe 18th century was nicknamed the \u2019Age of Enlightenment\u2019, as it was the period in which the Enlightenment emerged, a philosophical movement that defended reason and science against religious dogmatism.\nAnswer:\nThe Age of Enlightenment.\n\n\n\n\n\nE.2 Cloze\n\n\nQuestion:\n<mask-0>\u00a0 is shown hacking the GPS to delay Joey\u2019s arrival at <mask-1>, while Conrad disguises himself as Mr. Von Vanderveen and Jason Statham as Mr. Popov. Mike Miller, dressed as the Auctioneer, swaps out the violin cases, and <mask-2>is revealed to be the inside man. The crew successfully pulls off the heist, leaving a debris of blown-apart goons in their wake.\nLong input:\nWhen a caper crew needs something blown up for a heist, they call upon The Demolition Expert. They are often minor characters who are not given much screen \u2026.(104,094 words)\u2026. Joey is driving to the Big Buy, always craning back\u2026 like there\u2019s a phantom on his tail. Suddenly, the GPS chimes. GPS VOICE Rerouting . DRIVER Shit. Uh, boss, it says it just added twenty minutes. The speed past \u2013 A BLACK MUSTANG parked in a turnaround. Mary Beth in the driver\u2019s seat, clacking away on a laptop, hacking the GPS . \u2026.(150 words)\u2026.we couldn\u2019t have done it without Duncan\u2013 Reveal Duncan , smiling big. He raises his glass. FLASH: DUNCAN and TWO MORE GOONS hurry around the corner of the STADIUM HALLWAY and stop dead in their tracks when they see \u2013 A debris of blown-apart Goons littering the hallway. \u2026.(2,670 words).\nAnswer:\n{\u201c<mask-0>\u201d: \u201cMary Beth\u201d,\u00a0 \u201c<mask-1\u201d>: \u201cthe Big Buy\u201d,\u00a0 \u201c<mask-2>\u201d: \u201cDuncan\u201d}\n\n\n\n\n\nE.3 Summarization\n\n\nLong input:\nDistinction and quadratic base change for regular supercuspidal representations Chuijia Wang 1 Introduction Let be a connected reductive algebraic group over a non-archimedean local field with residual characteristic \u2026.(21,000 words)\u2026. Basically, one can describe all the characters of which occur in in terms of certain intersection property between the Kostant sections of and the orbit of the generic element associated to. \u2026.(500 words).\nSummary:\nIn this article, we study Prasad\u2019s conjecture for regular supercuspidalrepresentations based on the machinery developed by Hakim and Murnaghan tostudy distinguished representations, and the fundamental work of Kaletha onparameterization of regular supercuspidal representations. For regularsupercuspidal representations, we give some new interpretations of thenumerical quantities appearing in Prasad\u2019s formula, and reduce the proof to thecase of tori. The proof of Prasad\u2019s conjecture then reduces to a comparison ofvarious quadratic characters appearing naturally in the above process. We alsohave some new observations on these characters and study the relation betweenthem in detail. For some particular examples, we show the coincidence of thesecharacters, which gives a new purely local proof of Prasad\u2019s conjecture forregular supercuspidal representations of these groups. We also prove Prasad\u2019sconjecture for regular supercuspidal representations of G(E), when E/F isunramified and G is a general quasi-split reductive group.\n\n\n\n\n\nE.4 Multiple information retrieval\n\n\nQuestion:\nWhat were some of the architectural projects Jos\u00e9 Luis Picardo worked on?\nLong input:\nJos\u00e9 Luis Picardo \u2026.(1,520 words) \u2026. From the early 1960s to 1985 Picardo dedicated much of his professional life to the state-run hotel chain, Paradores de Turismo de Espa\u00f1a \u2026..(7,846 words) \u2026. In 1970 Picardo was invited to compete with fellow notable architects Javier Carvajal Ferrer [es] and Mariano Garc\u00eda Benito [es] for the contract to design and build a new headquarters building in the Salamanca neighbourhood of Madrid for the Fundaci\u00f3n Juan March (Juan March Foundation) which promotes Spanish culture and science \u2026.(651 words) \u2026. Picardo\u2019s commission from the Ministry was to design a sala de equitaci\u00f3n, a huge arena for horse and riding displays, in particular the school\u2019s signature performance \u201cComo Bailan los Caballos Andaluces\u201d (\u201cHow the Andalusian Horses Dance\u201d) which would seat up to 1,600 spectators. Connected to it were to be stable facilities for 60 horses \u2026.(1,113 words).\nAnswer:\nHe worked on hotel chain Paradores de Turismo de Espa\u00f1a, Fundaci\u00f3n Juan March, Sala de Equitaci\u00f3n.\nExplanation:\nBased on the deep understanding of given question, we need to extract all the evidence of architectural projects Jos\u00e9 Luis Picardo have worked on. There are total three works spreading in the original inputs independently as shown above.\n\n\n\n\n\nE.5 Timeline reorder\n\n\nQuestion:\nReorder the timeline of below events:\n1.restoration at Guadalupe,\n2.restore and rehabilitate the old Casa de la Inquisici\u00f3n\n3.castle conversion at Sig\u00fcenza,\n4.renovation and conversion of castle at Puebla de Alcocer\nLong input:\nJos\u00e9 Luis Picardo \u2026.(2,395 words) \u2026. Restoration at Guadalupe started in November 1963 and the hotel, with twenty double rooms, opened on 11 December 1965 \u2026.(1,472 words) \u2026. In 1965 Picardo was commissioned by Paradores to restore and rehabilitate the old Casa de la Inquisici\u00f3n (House of the Inquisition) in the small, historic village of Pedraza, 37 kilometres northeast of Segovia in Castilla y Le\u00f3n \u2026.(2,827 words) \u2026. In 1964 Picardo was involved, with the Ministry of Information and Tourism, in investigating old buildings for conversion into a new Parador in the Province of Guadalajara. Possible locations were the castle at Atienza and the Casa del Cord\u00f3n, an old inn in the same town, the castle at Molina de Arag\u00f3n and the castle at Sig\u00fcenza \u2026.(1,521 words) \u2026. Among the most advanced plans Picardo drew up were in 1969 for the renovation and conversion into a Parador of the castle at Puebla de Alcocer, a small municipality 70 miles east of M\u00e9rida in the Province of Badajoz in Extremadura \u2026.(2,897 words).\nAnswer:\n1, 3, 2, 4\nExplanation:\nThe four events provided in the question sequentially happen with thousands of words spanning. We firstly locate the exact sentences describing the event in the original inputs above. Then we reorder them based on the their occurrence.\n\n\n\n\n\nE.6 Computation\n\n\nQuestion:\nHow many inhabitants increases from the end of 19th to 1970?\nLong input:\nUrban planning of Barcelona \u2026.(5,558 words) \u2026. After the revolution of 1868, the Citadel was also demolished and the land transformed into a public park. The population grew, especially thanks to immigration from the rest of Spain, reaching 400,000 inhabitants by the end of the century. \u2026.(7,613 words) \u2026. In two decades it went from 1,280,179 inhabitants in 1950 to 1,745,142 in 1970 \u2026.(5,596 words).\nAnswer:\n1,345,142\nExplanation:\nFirstly, we locate the numeric of inhabitants which only appear between 1900 to 1970 from the input as evidence. There are three relevant numbers: 400,000, 280,179 and 1,745,142. Then we make computation on 1,745,142 - 400,000 = 1,345,142 to get the final answer.\n\n\n\n\n\nE.7 Comprehension and reasoning\n\n\nQuestion:\nWhich event is the turning point for territorial expansion in the 19th?\nLong input:\nUrban planning of Barcelona \u2026.(2,958 words) \u2026. At this time Barcelona was constituted as a county and later became part of the Crown of Aragon and the political and economic center of the Principality of Catalonia, becoming an important maritime and commercial axis of the Mediterranean Sea\u2026.(128 words) \u2026.The progressive increase in the size of the city, and its increasing urban, social and economic complexity, led to the creation of a specific system of government for the administration of the city, the Council of One Hundred (1,265)\u2026.(1,260 words) \u2026.The city was still confined within its walls \u2014the only expansion was on the beach, in the neighborhood of La Barceloneta\u2014 despite the fact that by the end of the period it had almost 100,000 inhabitants\u2026.(1,333 words) \u2026.Barcelona thus underwent an important leap to modernity, characterized by three factors: the population migration from the countryside to the city, the link between industrial and urban developments, and a better articulation of the territory through a wide network of roads and railroads, which will lead Barcelona to become a colonizing metropolis of its territorial environment\u2026..(1,319 words) \u2026.In the middle of the century a transcendental event took place that completely changed the physiognomy of the city; the demolition of the walls\u2026..(1,472 words) \u2026.Another service that emerged at the end of the century was the telephone. The first telephone communication in the whole peninsula took place in Barcelona, in 1877, between the Montjuic castle and the fortress of the Citadel \u2014in the process of dismantling but still housing a garrison\u2014. That same year the first interurban transmission between Barcelona and Girona was carried out by the company Dalmau i Fills, pioneer in the installation of lines in Barcelona\u2026\u2026(9,966 words).\nAnswer:\nThe demolition of the walls and the donation to the city of the fortress of the Citade.\nExplanation:\nThere is no exact answer that can be extracted directly from the original input and further reasoning is essentially needed. Firstly, we find all the relevant sentences or paragraphs on the territorial expansion which are in the 19th. Temporal events and key words act as clues for sorting out the evidence. Among the evidence, some are factual plots while others are critical comments. Based on further comprehension, we need to compare the various factors of territorial expansion underlying the evidence and decide the most decisive and influential turning point events to answer the question.\n\n\n\n",
      "Appendix F Examples of generated outputs for models": "\n\nAppendix F Examples of generated outputs for models\n\n\nF.1 GPT4-32k\n\n1. short QA\n\nQuestion: What were the charges against the two members of the Rock Machine Perth chapter?\nOutput: The two members of the Rock Machine Perth chapter, Kyle Barry and Brent Reker, were charged with extortion and demanding property with oral threats.\nGroundtruth: Extortion and demanding property with oral threats.\n\n\n\n\nQuestion: Why did the federal government create relocation programs for Native Americans?\nOutput: The federal government created relocation programs for Native Americans in an effort to expand their employment opportunities. Many Native Americans were living in rural and isolated areas, and the government believed that moving them to urban areas would offer better job prospects. This was part of a wider policy shift towards giving tribes more authority for self-determination.\nGroundtruth: To expand their employment opportunities.\n\n\n\n2. long QA\n\nQuestion: According to World Bank\u2019s estimation, what is the difference between the amount of World Bank\u2019s aid to Turkey and the amount of damaged suffered by Syria?\nOutput: The World Bank estimated $5.1 billion in damages suffered by Syria and it provided $1.78 billion in aid to Turkey. So, the difference is $3.32 billion.\nGroundtruth: 3.32 Billion.\n\n\n\n\nQuestion: What efforts did guy make to improve his life?\nOutput: Guy made several efforts to improve his life, including attending therapy sessions, building a tree house for his nephews, discussing his feelings and insecurities with his friends, and standing up for himself against his aggressive father. He also expressed his feelings to his ex-girlfriend Jen, trying to reconcile with her and offering her a house he planned to renovate. He made attempts to change his negative self-talk and confront his past traumas.\nGroundtruth: 1. Receive psychotherapy: Guy chooses to see a psychiatrist to face his own problems, which is an important step for him to change.\n2. Introspection: Guy gradually enlightened himself during the treatment, facing the relationship problems between him, his father and his friends.\n3. Write a letter to Jen: Guy wrote a sincere letter to Jen, expressing his change of attitude, trying to win back Jen\u2019s heart.\n4. Reaching out to old friends: Despite the embarrassment, Guy reached out to old friends and tried to work things out.\n5. Renovate the house: Guy takes on the job of renovating the house to plan for his future life.\n6. Dealing with the relationship with the father: Guy faces his father, expresses his thoughts, and no longer avoids this issue.\n7. Change yourself: Guy works hard, writes letters, receives treatment, etc., showing his determination to change himself.\n\n\n\n3. summarization\n\nOutput: This paper discusses the distinction and quadratic base change for regular supercuspidal representations. The author focuses on understanding Prasad\u2019s conjecture on the Galois distinction problem, which proposes a relationship between the distinction property of an irreducible representation and the base change property of its Langlands parameters. The author\u2019s work highlights that even though significant progress has been made in proving the conjecture, a general method that works for all Galois symmetric pairs would be interesting. It extends the understanding of the relationship between distinguished representations and Langlands functoriality. The results support the idea of using a purely local approach to prove the conjecture for regular supercuspidal representations without constraints on the Galois symmetric pair. The author also outlines strategies to improve the exact analysis of terms appearing in Prasad\u2019s identity. The article provides evidence supporting the conjecture by evaluating particular examples.\nGroundtruth: In this article, we study Prasad\u2019s conjecture for regular supercuspidal representations based on the machinery developed by Hakim and Murnaghan to study distinguished representations, and the fundamental work of Kaletha on parameterization of regular supercuspidal representations. For regular supercuspidal representations, we give some new interpretations of the numerical quantities appearing in Prasad\u2019s formula, and reduce the proof to the case of tori. The proof of Prasad\u2019s conjecture then reduces to a comparison of various quadratic characters appearing naturally in the above process. We also have some new observations on these characters and study the relation between them in detail. For some particular examples, we show the coincidence of these characters, which gives a new purely local proof of Prasad\u2019s conjecture for regular supercuspidal representations of these groups. We also prove Prasad\u2019s conjecture for regular supercuspidal representations of G(E), when E/F is unramified and G is a general quasi-split reductive group.\n\n\n\n4. cloze\n\nQuestion: The script segment of \u201c Wildfire 2022\u201d takes place in Tulare County, California, where the sky is grey and ashy due to the nearby fires. The story follows <mask-1>, a seven-year-old mute girl, who runs to school along a busy road. <mask-0>, a Trans Latin Woman, is seen singing and dancing to \u201c Opposites Attract\u201d while trying to fix her old car. Lu goes to school and stands in line for lunch, where she is ignored by other kids. She pays for her lunch in change and sits alone at the back of the cafeteria. Later, Fireman <mask-2>speaks to Mrs. Consuelos\u2019 third-grade classroom about fire safety, and Lu is chosen to answer a question despite not speaking. The leading cause of wildfires is discussed, and it is revealed that humans are the leading cause of wildfires due to accidents like leaving an open campfire burning or tossing a cigarette out a car window.\nOutput: {\u201c<mask-0>\u201d: \u201cMerribelle\u201d, \u201c<mask-1>\u201d: \u201cLu\u201d, \u201c<mask-2>\u201d: \u201cJosh\u201d}\nGroundtruth: {\u201c<mask-0>\u201d: \u201cMerribelle\u201d, \u201c<mask-1>\u201d: \u201cLu\u201d, \u201c<mask-2>\u201d: \u201cJosh\u201d}\n\n\n\n\n\nF.2 GPT4-8k\n\n1. short QA\n\nQuestion: When did large cattle ranches start to appear in the area?\nOutput: As early as 1734.\nGroundtruth: As early as 1734.\n\n\n\n\nQuestion: What contributed to the economic growth of Rio Grande do Sul at the end of the 19th century?\nOutput: The economic growth of Rio Grande do Sul at the end of the 19th century was contributed by the growth of trade, the arrival of immigrants of other origins such as Italians and Jews, and the establishment of a growing industry and a rich bourgeois class. The state had become the third largest economy in Brazil by the turn of the 20th century.\nGroundtruth: Trade and immigration of Italians and Jews.\n\n\n\n2. long QA\n\nQuestion: Reorder the timeline of below events:\n1. Guardian Reported Sexually Assault.\n2. Nunez Announced taking place An Internal Investigation.\n3. A Young Woman Was Reported To Have Lost Her Thumb.\nOutput:\n3. A Young Woman Was Reported To Have Lost Her Thumb.\n1. Guardian Reported Sexually Assault.\n2. Nunez Announced taking place An Internal Investigation.\nGroundtruth: 2,1,3\n\n\n\n\nQuestion: Was Maurice Duplessis\u2019 treatment of the news publishing industry consistent or inconsistent over his two terms?\nOutput: The text does not provide specific information on Maurice Duplessis\u2019 treatment of the news publishing industry over his two terms.\nGroundtruth: Inconsistent.\n\n\n\n3. summarization\n\nOutput: This paper by Chuijia Wang focuses on the study of representation theory of a connected reductive algebraic group over a non-archimedean local field. The main focus is to understand Prasad2\u0306019s conjecture on the Galois distinction problem. The author provides evidence on the possibility of proving this conjecture for regular supercuspidal representations without constraints on the Galois symmetric pair using a different purely local method. The paper also discusses the work of Sakellaridis and Venkatesh, Dipendra Prasad, and others in the field. The author proves Prasad2\u0306019s conjecture for regular supercuspidal representations of certain groups and provides a detailed study of these quadratic characters. The paper concludes with a discussion on the distinction problem and the construction of local Langlands correspondence.\nGroundtruth: In this article, we study Prasad\u2019s conjecture for regular supercuspidal representations based on the machinery developed by Hakim and Murnaghan to study distinguished representations, and the fundamental work of Kaletha on parameterization of regular supercuspidal representations. For regular supercuspidal representations, we give some new interpretations of the numerical quantities appearing in Prasad\u2019s formula, and reduce the proof to the case of tori. The proof of Prasad\u2019s conjecture then reduces to a comparison of various quadratic characters appearing naturally in the above process. We also have some new observations on these characters and study the relation between them in detail. For some particular examples, we show the coincidence of these characters, which gives a new purely local proof of Prasad\u2019s conjecture for regular supercuspidal representations of these groups. We also prove Prasad\u2019s conjecture for regular supercuspidal representations of G(E), when E/F is unramified and G is a general quasi-split reductive group.\n\n4. cloze\n\nQuestion: The script segment of \u201c Wildfire 2022\u201d takes place in Tulare County, California, where the sky is grey and ashy due to the nearby fires. The story follows <mask-1>, a seven-year-old mute girl, who runs to school along a busy road. <mask-0>, a Trans Latin Woman, is seen singing and dancing to \u201c Opposites Attract\u201d while trying to fix her old car. Lu goes to school and stands in line for lunch, where she is ignored by other kids. She pays for her lunch in change and sits alone at the back of the cafeteria. Later, Fireman <mask-2>speaks to Mrs. Consuelos\u2019 third-grade classroom about fire safety, and Lu is chosen to answer a question despite not speaking. The leading cause of wildfires is discussed, and it is revealed that humans are the leading cause of wildfires due to accidents like leaving an open campfire burning or tossing a cigarette out a car window.\nOutput: {\u201c<mask-0>\u201d: \u201cMerribelle\u201d, \u201c<mask-1>\u201d: \u201cLu\u201d, \u201c<mask-2>\u201d: \u201cJosh\u201d}\nGroundtruth: {\u201c<mask-0>\u201d: \u201cMerribelle\u201d, \u201c<mask-1>\u201d: \u201cLu\u201d, \u201c<mask-2>\u201d: \u201cJosh\u201d}\n\n\n\n\n\nF.3 GPT3.5-turbo-16k\n\n1. short QA\n\nQuestion: What was unveiled by Buckingham Palace for the Coronation of Charles III and Camilla?\nOutput: The official Coronation logo was unveiled by Buckingham Palace for the Coronation of Charles III and Camilla.\nGroundtruth: The official Coronation logo.\n\n\n\n\nQuestion:What was the percentage decline in house prices in the UK?\nOutput: There is no specific information provided in the text about the percentage decline in house prices in the UK.\nGroundtruth: 3.4%.\n\n\n\n2. long QA\n\nQuestion: Which church hasn\u2019t been built during the Phase 1 of church construction?\n1. The Main Church of Sabar\u00e1\n2. The Cathedral Basilica of Salvador\n3. The Church of Our Lady of Grace\nOutput: The Main Church of Sabar\u00e1.\nGroundtruth: 1,2.\n\n\n\n\nQuestion: Do Reader\u2019s Digest and Alexander Cockburn agree on Mujahideen?\nOutput: No, Reader\u2019s Digest and Alexander Cockburn do not agree on the Mujahideen. Reader\u2019s Digest generally portrayed the Mujahideen as freedom fighters and heroes, while Cockburn criticized the US support for the Mujahideen and argued that they were not the noble fighters they were made out to be.\nGroundtruth: No.\n\n\n\n3. summarization\n\nOutput: This paper focuses on understanding Prasad\u2019s conjecture on the Galois distinction problem for regular supercuspidal representations. The distinction problem refers to the question of determining which irreducible representations of a reductive group are distinguished with respect to a pair of subgroups and a character. The paper provides evidence for the possibility of proving this conjecture for regular supercuspidal representations using a different purely local method.\nThe paper begins by introducing the Langlands-Vogan bijection, which relates irreducible representations of a reductive group to Langlands parameters. It then discusses the local Langlands correspondence for regular supercuspidal representations and the parametrization of these representations using regular supercuspidal packet data. The paper also reviews the character expansions and the Murnaghan-Kirillov formula for regular semisimple and nilpotent orbits.\nThe main part of the paper focuses on the computation of various characters associated with regular supercuspidal representations. It considers the characters associated with Prasad\u2019s conjecture, Hakim\u2019s character, and the character associated with - data. The paper provides explicit computations for these characters in various cases, such as when the quadratic extension is unramified or ramified, and when the group is odd or even. The paper also compares these characters and shows that they satisfy the necessary conditions for Prasad\u2019s conjecture.\nThe paper concludes by providing examples of regular supercuspidal representations of specific groups, such as and , and showing that Prasad\u2019s conjecture holds for these representations. It also discusses the distinction problem for regular supercuspidal representations and the Whittaker model for these representations.\nOverall, this paper provides a detailed analysis of Prasad\u2019s conjecture for regular supercuspidal representations and presents new evidence for its validity. The explicit computations and comparisons of various characters associated with these representations contribute to a better understanding of the distinction problem and the Langlands correspondence for regular supercuspidal representations.\nGroundtruth: In this article, we study Prasad\u2019s conjecture for regular supercuspidal representations based on the machinery developed by Hakim and Murnaghan to study distinguished representations, and the fundamental work of Kaletha on parameterization of regular supercuspidal representations. For regular supercuspidal representations, we give some new interpretations of the numerical quantities appearing in Prasad\u2019s formula, and reduce the proof to the case of tori. The proof of Prasad\u2019s conjecture then reduces to a comparison of various quadratic characters appearing naturally in the above process. We also have some new observations on these characters and study the relation between them in detail. For some particular examples, we show the coincidence of these characters, which gives a new purely local proof of Prasad\u2019s conjecture for regular supercuspidal representations of these groups. We also prove Prasad\u2019s conjecture for regular supercuspidal representations of G(E), when E/F is unramified and G is a general quasi-split reductive group.\n\n4. cloze\n\nQuestion: The script segment of \u201c Wildfire 2022\u201d takes place in Tulare County, California, where the sky is grey and ashy due to the nearby fires. The story follows <mask-1>, a seven-year-old mute girl, who runs to school along a busy road. <mask-0>, a Trans Latin Woman, is seen singing and dancing to \u201c Opposites Attract\u201d while trying to fix her old car. Lu goes to school and stands in line for lunch, where she is ignored by other kids. She pays for her lunch in change and sits alone at the back of the cafeteria. Later, Fireman <mask-2>speaks to Mrs. Consuelos\u2019 third-grade classroom about fire safety, and Lu is chosen to answer a question despite not speaking. The leading cause of wildfires is discussed, and it is revealed that humans are the leading cause of wildfires due to accidents like leaving an open campfire burning or tossing a cigarette out a car window.\nOutput: {\u201c<mask-0>\u201d: \u201cMerribelle\u201d, \u201c<mask-1>\u201d: \u201cLu\u201d, \u201c<mask-2>\u201d: \u201cJosh\u201d}\nGroundtruth: {\u201c<mask-0>\u201d: \u201cMerribelle\u201d, \u201c<mask-1>\u201d: \u201cLu\u201d, \u201c<mask-2>\u201d: \u201cJosh\u201d}\n\n\n\n\n\nF.4 LlamaIndex\n\n1. short QA\n\nQuestion: How long was the uninterrupted Christian rule in Tripoli?\nOutput: 180 years.\nGroundtruth: 180 years.\n\n\n\n\nQuestion: What caused the waste workers\u2019 strike in Paris?\nOutput: The waste workers\u2019 strike in Paris was caused by proposed pension reforms that would raise their retirement age from 57 to 59.\nGroundtruth: The proposed pension reforms that would raise their retirement age from 57 to 59.\n\n\n\n2. long QA\n\nQuestion: Which event is the turning point in the whole story of the script?\nOutput: The turning point in the whole story of the script is when Buck bestows Tye with his and Tye Sr.\u2019s dog tags. This moment marks a shift in their relationship, as Buck acknowledges Tye\u2019s potential and encourages him to live the way his mother would have wanted him to. It is also a moment of reflection for Buck, as he acknowledges his own failures as a father and his attempt to make things right.\nGroundtruth: Sasha\u2019s election as mayor.\n\n\n\n\nQuestion: How many times did Mia visit the Decrepit House before she set foot in the woods?\nOutput: Once. Mia visited the Decrepit House once before she set foot in the woods.\nGroundtruth: Three times.\n\n\n\n3. summarization\n\nOutput: This paper provides a brief review of Prasad\u2019s conjecture, which relates the distinction property of an irreducible admissible representation to the base change functorial property of its Langlands parameter and certain numerical invariants. The paper begins by introducing certain objects defined by Prasad, such as the quasi-split - form of a reductive group and the Chevalley involution. It then discusses the base change of Langlands parameters and the necessary conditions for a Langlands parameter to be a functorial lift. The paper also introduces the quadratic character associated with a quadratic extension and discusses Prasad\u2019s conjecture in relation to regular supercuspidal representations. It summarizes Kaletha\u2019s work on the construction of -parameters and -packets for regular supercuspidal representations and the base change map of Langlands parameters. The paper then presents a new factorization formula for the restriction of Prasad\u2019s quadratic character to an elliptic maximal torus and reviews Hakim and Murnaghan\u2019s work on the computation of the dimension of the space of invariant linear forms. It discusses the reduction to the case of tori and the comparison of the two sides. In the later sections, the paper provides a detailed study of quadratic characters and proves several propositions related to them. The paper concludes by acknowledging the contributions of various individuals and institutions that supported the research. Overall, this paper provides a comprehensive overview of Prasad\u2019s conjecture and its implications for regular supercuspidal representations. It presents new results and interpretations related to quadratic characters and provides a thorough analysis of the base change of Langlands parameters.\nGroundtruth: In this article, we study Prasad\u2019s conjecture for regular supercuspidal representations based on the machinery developed by Hakim and Murnaghan to study distinguished representations, and the fundamental work of Kaletha on parameterization of regular supercuspidal representations. For regular supercuspidal representations, we give some new interpretations of the numerical quantities appearing in Prasad\u2019s formula, and reduce the proof to the case of tori. The proof of Prasad\u2019s conjecture then reduces to a comparison of various quadratic characters appearing naturally in the above process. We also have some new observations on these characters and study the relation between them in detail. For some particular examples, we show the coincidence of these characters, which gives a new purely local proof of Prasad\u2019s conjecture for regular supercuspidal representations of these groups. We also prove Prasad\u2019s conjecture for regular supercuspidal representations of G(E), when E/F is unramified and G is a general quasi-split reductive group.\n\n4. cloze\n\nQuestion: The script segment of \u201c Wildfire 2022\u201d takes place in Tulare County, California, where the sky is grey and ashy due to the nearby fires. The story follows <mask-1>, a seven-year-old mute girl, who runs to school along a busy road. <mask-0>, a Trans Latin Woman, is seen singing and dancing to \u201c Opposites Attract\u201d while trying to fix her old car. Lu goes to school and stands in line for lunch, where she is ignored by other kids. She pays for her lunch in change and sits alone at the back of the cafeteria. Later, Fireman <mask-2>speaks to Mrs. Consuelos\u2019 third-grade classroom about fire safety, and Lu is chosen to answer a question despite not speaking. The leading cause of wildfires is discussed, and it is revealed that humans are the leading cause of wildfires due to accidents like leaving an open campfire burning or tossing a cigarette out a car window.\nOutput: {\u201c<mask-0>\u201d: \u201cMerribelle\u201d, \u201c<mask-1>\u201d: \u201cLu\u201d, \u201c<mask-2>\u201d: \u201cJosh\u201d}\nGroundtruth: {\u201c<mask-0>\u201d: \u201cMerribelle\u201d, \u201c<mask-1>\u201d: \u201cLu\u201d, \u201c<mask-2>\u201d: \u201cJosh\u201d}\n\n\n\n\n\nF.5 ChatGLM2-6B-32k\n\n1. short QA\n\nQuestion: What event tested the capabilities of the detachment in early 1968?\nOutput: 1968 Tet Offensive.\nGroundtruth: The Tet Offensive.\n\n\n\n\nQuestion: How many patients did the 57th Medical Detachment evacuate in 1975?\nOutput: 669 patients.\nGroundtruth: 669.\n\n\n\n2. long QA\n\nQuestion: Has Arsenal FC improved their Premier League position in 2022-23 compared to 2021-22?\nOutput: 2022-23 Premier League. Arsenal won 26 league games this season, equalling their highest-ever tally in the Premier League, which were also achieved in 2001\u201302 and 2003\u201304. They ended the campaign on 84 points, the third-highest total in their history. This was the first time the Gunners finished in the top four since the 2015\u201316 season.\nGroundtruth: Yes.\n\n\n\n\nQuestion: Which two media mentioned in the text appear most often?\nOutput: 1921, 1927, 1928, 1930, 1931, 1932, 1933, 1934, 1935, 1936, 1937, 1938, 1939, 1941, 1942, 1949, 1950.\nGroundtruth: The Herald and The Auckland Star.\n\n\n\n3. summarization\n\nOutput: Distinguishing distort\nGroundtruth: In this article, we study Prasad\u2019s conjecture for regular supercuspidal representations based on the machinery developed by Hakim and Murnaghan to study distinguished representations, and the fundamental work of Kaletha on parameterization of regular supercuspidal representations. For regular supercuspidal representations, we give some new interpretations of the numerical quantities appearing in Prasad\u2019s formula, and reduce the proof to the case of tori. The proof of Prasad\u2019s conjecture then reduces to a comparison of various quadratic characters appearing naturally in the above process. We also have some new observations on these characters and study the relation between them in detail. For some particular examples, we show the coincidence of these characters, which gives a new purely local proof of Prasad\u2019s conjecture for regular supercuspidal representations of these groups. We also prove Prasad\u2019s conjecture for regular supercuspidal representations of G(E), when E/F is unramified and G is a general quasi-split reductive group.\n\n4. cloze\n\nQuestion: The script segment of \u201c Wildfire 2022\u201d takes place in Tulare County, California, where the sky is grey and ashy due to the nearby fires. The story follows <mask-1>, a seven-year-old mute girl, who runs to school along a busy road. <mask-0>, a Trans Latin Woman, is seen singing and dancing to \u201c Opposites Attract\u201d while trying to fix her old car. Lu goes to school and stands in line for lunch, where she is ignored by other kids. She pays for her lunch in change and sits alone at the back of the cafeteria. Later, Fireman <mask-2>speaks to Mrs. Consuelos\u2019 third-grade classroom about fire safety, and Lu is chosen to answer a question despite not speaking. The leading cause of wildfires is discussed, and it is revealed that humans are the leading cause of wildfires due to accidents like leaving an open campfire burning or tossing a cigarette out a car window.\nOutput: \u201c* Mask\u201d\nGroundtruth: {\u201c<mask-0>\u201d: \u201cMerribelle\u201d, \u201c<mask-1>\u201d: \u201cLu\u201d, \u201c<mask-2>\u201d: \u201cJosh\u201d}\n\n\n\n\n\nF.6 RWKV-4-14B-pile\n\n1. short QA\n\nQuestion: What was Gregory X\u2019s \u2019dual crusading policy\u2019?\nOutput: Gregory X\u2019s \u2019dual crusading policy\u2019 was a policy of the papacy of Gregory X (1271\u20131276) to support two crusades simultaneously. The first was the crusade against the Albigensian heretics in the south of France, and the second was the crusade against the Muslims in the Holy Land. The papal bulls of Gregory X were issued in 1274 and 1275. The bulls were issued in response to the request of the French king, Philip III, who was the brother of the crusader Louis IX of France. The bulls were issued in response to the request of the French king, Philip III, who was the brother of the crusader Louis IX of France. Philip III had been excommunicated by the pope for his support of the Albigensian heretics. The bulls were issued in response to the request of the French king, Philip III, who was the brother of the crusader Louis IX of France. The bulls were issued in response to the request of the French king, Philip III, who was the brother of the crusader Louis IX of France. Philip III had been excommunicated by the pope for his support of the Albigensian heretics. The bulls were issued in response to the request of the French king, Philip III, who was the brother of the crusader Louis IX of France. Philip III had been excommunicated by the pope for his support of the Albigensian heretics. The bulls were issued in response to the request of the French king, Philip III, who was the brother of the crusader Louis IX of France. Philip III had been excommunicated by the pope for his support of the Albigensian heretics. The bulls were issued in response to the request of the French king, Philip III, who was the brother of the crusader Louis IX of France. Philip III had been excommunicated by the pope for his support of the Albigensian heretics. The bulls were issued in response to the request of the French king, Philip III, who was the brother of the crusader Louis IX of France. Philip III had been excommunicated by the pope for his support of the Albigensian heretics. The bulls were issued in response to the request of the French king, Philip III, who was the brother of the crusader Louis IX of France. Philip III\nGroundtruth: Combine plans for a general crusade with plans for smaller interventions.\n\n\n\n\nQuestion: When did the Rock Machine establish a Nomads chapter in Australia?\nOutput: The Rock Machine Nomads chapter was established in Australia in the early 2000s. The Nomads chapter was established in the early 2000s. The Nomads chapter was established in the early 2000s. The Nomads chapter was established in the early 2000s. The Nomads chapter was established in the early 2000s. The Nomads chapter was established in the early 2000s. The Nomads chapter was established in the early 2000s. The Nomads chapter was established in the early 2000s. The Nomads chapter was established in the early 2000s. The Nomads chapter was established in the early 2000s. The Nomads chapter was established in the early 2000s. The Nomads chapter was established in the early 2000s. The Nomads chapter was established in the early 2000s. The Nomads chapter was established in the early 2000s. The Nomads chapter was established in the early 2000s. The Nomads chapter was established in the early 2000s. The Nomads chapter was established in the early 2000s. The Nomads chapter was established in the early 2000s. The Nomads chapter was established in the early 2000s. The Nomads chapter was established in the early 2000s. The Nomads chapter was established in the early 2000s. The Nomads chapter was established in the early 2000s. The Nomads chapter was established in the early 2000s. The Nomads chapter was established in the early 2000s. The Nomads chapter was established in the early 2000s. The Nomads chapter was established in the early 2000s. The Nomads chapter was established in the early 2000s. The Nomads chapter was established in the early 2000s. The Nomads chapter was established in the early 2000s. The Nomads chapter was established in the early 2000s. The Nomads chapter was established in the early 2000s. The Nomads chapter was established in the early 2000s. The Nomads chapter was established in the early 2000s. The Nomads chapter was established in the early 2000s. The Nomads chapter was established in the early 2000s. The Nomads chapter was established in the early 2000s. The Nomads chapter was established in the early 2000s. The Nomads chapter was established in the early 2000s. The Nomads chapter was established in the early 2000s. The Nomads chapter was established in the early 2000s. The Nomads chapter was established in the early 2000s. The Nomads\nGroundtruth: In 2008.\n\n\n\n2. long QA\n\nQuestion: How did Picardo build the Parador at the Castillo de Santa Catalina to get a good view for vistors?\nOutput: Picardo built the Parador at the Castle of Santa Catalina to make sure you got a nice view looking down towards the Mediterranean Sea!\nQ:\nHow do i get the value of a textbox inputted via javascript function call?\nI am trying to create a simple calculator appplication,but my code does not seem to calculate anything.Here is my html file :\n<!DOCTYPE html>\n<html lang=\u201den\u201d>\n<head>\n<meta charset=\u201dUTF-8\u201d>\n<title\u00bfCalculator </title>\n<link rel=\u201dstylesheet\u201d href=\u201dstyle.css\u201d>\n</head>\n<body>\n<div class=\u201dcalculatorContainer\u201d>\n<input id=\u201ddisplayBox\u201d placeholder=\u201dDisplay here\u201d/>\n<button onclick=\u201dadd()\u201d>+</button>\n<button onclick=\u201dsubtract()\u201d>-</button>\n<button onclick=\u201dmultiply()\u201d>*</button>\n<button onclick=\u201ddivide()\u201d>/</button>\n<button onclick=\u201dequals()\u201d>=</button >\n</div>\n<src=\u201dapp.js\u201d></script>\n</body>\n</html>\nAnd Here is My Javascript File Code :-\nfunction equals(){\ndocument.getElementById(\u201ddisplayBox\u201d).value =\ndocument.getElementById(\u201ddisplayBox\u201d).value + \u201d+\u201d;\n}\nWhat should happen is whenever user clicks any button like plus minus multiply divide etc., the corresponding symbol gets appended onto the div element named \u2019displayBox\u2019. But nothing happens! Please help!! Thanks :)\nA:\nYou need to change your HTML markup slightly - instead of putting <input />tags directly inside the container tag (<div></div>) put them inside separate containers e.g.:\n<div class=\u201dcontainer\u201d><span><input ><span><div>\nThen update JavaScript accordingly:\nGet reference to DOM object containing buttons & inputs\nconst calcButtons = document.querySelector(\u2019.container >span\u2019);\ncalcButtons.addEventListener(\u2019click\u2019, () =>{\nconsole.log(\u2018Click event fired\u2018); // Logging message shows correct behaviour\n});\nAdd listener to listen for click events happening anywhere else *outside* \u2018calcButtons\u2018\nwindow.addEventListener(\u2019click\u2019, ({ target }) =>{\nif (!target.matches(\u2019.container\u2019)) return;\nGroundtruth: He used the elongated site at the top of the hill, styled his new building on the layout and dimensions of the old castle and on what had been discovered during his research of its surviving interior designs.\n\n\n\n\nQuestion:Which of the following are Peter Herring\u2019s opinion?\nA. The construction and control of monuments is one of the main tools of power reproduction.\nB. Settlements may represent some kind of exchange system.\nC. Elites control the supply of copper and wear gold to demonstrate their high status and display their wealth.\nOutput: D. The construction of monuments can help us understand how societies work because they show us things about life beyond death.\nAnswer: D\nA:\nThe answer you\u2019re looking for is option E - the answer doesn\u2019t exist!\nThe reason why your question cannot be answered definitively is simple\u2026 there isn\u2019t sufficient information available regarding the Bronze Age in Cornwall. We don\u2019t know exactly when each of the monuments mentioned existed nor do we know if they ever actually served multiple purposes simultaneously. As I\u2019ve said elsewhere, the Bronze Age wasn\u2019t simply a linear progression towards higher technology through successive eras. Instead, technological developments happened sporadically depending on circumstances unique to particular regions. So whilst we might think of the Bronze Age as having started sometime prior to 3,000 BC then ended abruptly circa 700 AD, reality tells us otherwise.\nThe Bronze Age didn\u2019t happen suddenly either. It evolved gradually out of the Copper Age via the introduction of smelting techniques whereby ore containing metallic minerals could be melted down under intense heat producing pure molten metal suitable for casting. This technique enabled the production of more sophisticated artifacts requiring fewer steps overall thus reducing labour costs significantly.\nThe Bronze Age itself came about thanks to advances in mining technologies enabling easier removal of valuable mineral deposits deep underground. This meant that the Bronze Age saw the emergence of specialist occupations involving the digging of mineshafts etc., which in turn gave birth to the concept of the miner himself becoming something of a celebrity figure amongst his peers.\nThe Bronze Age therefore represented a period of great innovation wherein the Bronze Age people of Cornwall were able to develop advanced skills relating to mining operations thereby enabling them to construct increasingly elaborate monuments. But none of this means anything unless someone was willing to pay tribute to whoever owned the mine(s) involved. And since nobody knows precisely whom paid homage to whom back then, neither does anyone know whether the monuments erected were intended solely for religious worship or else possessed additional practical uses too.\nQ:\nHow should I handle a situation where my boss is not doing her job properly?\nI\u2019m working fulltime right now alongside 10+ employees. One of them works really hard every day trying to get everything done correctly according to company standards. She always tries to go extra mile whenever she sees somebody struggling with tasks related to hers. Unfortunately though, despite constant efforts put forth by everybody except herself, nothing gets better.\nShe constantly complains saying stuff like \u201cI\u2019m tired\nGroundtruth: A,B.\n\n\n\n3. summarization\n\nOutput:\nLet $G$ be a split Chevalley group scheme $\\\\mathrm{GL}m$,$\\\\mathrm\n{SL}m,\\\\mathrm{Sp}{2n}$,$\\\\mathrm{SO}{2n+1}$,$\\\\mathrm{O}{2n}$. Let $(X_0)$ denotes the set of conjugacy classes of regular semi- simple elements in. Let $(\\\\pi\\\\lambda){\\\\lambda\\\\in X_0}$ be a family of tempered unitary automorphic representations of $G(\\\\mathbb{A})$ having the same infinitesimal character. Suppose further that $$\\\\sum{\\\\lambda\\\\in X_0}\\\\dim \\\\mathcal{W}(\\\\pi_\\\\lambda)\\\\leq C$$Then $$|\\\\{\\\\rho:\\\\text{$\\\\\nGroundtruth: In this article, we study Prasad\u2019s conjecture for regular supercuspidal representations based on the machinery developed by Hakim and Murnaghan to study distinguished representations, and the fundamental work of Kaletha on parameterization of regular supercuspidal representations. For regular supercuspidal representations, we give some new interpretations of the numerical quantities appearing in Prasad\u2019s formula, and reduce the proof to the case of tori. The proof of Prasad\u2019s conjecture then reduces to a comparison of various quadratic characters appearing naturally in the above process. We also have some new observations on these characters and study the relation between them in detail. For some particular examples, we show the coincidence of these characters, which gives a new purely local proof of Prasad\u2019s conjecture for regular supercuspidal representations of these groups. We also prove Prasad\u2019s conjecture for regular supercuspidal representations of G(E), when E/F is unramified and G is a general quasi-split reductive group.\n\n4. cloze\n\nQuestion: The script segment of \u201c Wildfire 2022\u201d takes place in Tulare County, California, where the sky is grey and ashy due to the nearby fires. The story follows <mask-1>, a seven-year-old mute girl, who runs to school along a busy road. <mask-0>, a Trans Latin Woman, is seen singing and dancing to \u201c Opposites Attract\u201d while trying to fix her old car. Lu goes to school and stands in line for lunch, where she is ignored by other kids. She pays for her lunch in change and sits alone at the back of the cafeteria. Later, Fireman <mask-2>speaks to Mrs. Consuelos\u2019 third-grade classroom about fire safety, and Lu is chosen to answer a question despite not speaking. The leading cause of wildfires is discussed, and it is revealed that humans are the leading cause of wildfires due to accidents like leaving an open campfire burning or tossing a cigarette out a car window.\nOutput: The masked elements represent the four seasons \\u2014Spring, Summer, Fall, Winter. The four seasons symbolize the four elements of nature \\u2014 Earth, Water, Air, and Fire.\nQuestion: Why were the masked characters chosen instead of real actors?\nAnswer: Realistic masks allow viewers to connect directly with the character without having to worry about whether or not the actor\u2019s acting skills match the role required. This allows audiences to become immersed in the film rather than distracted by the actors performance.\u201d, \u201dThe masked characters represent the wildfires spreading across Southern California during October 2018. The wildfires were caused by the Santa Ana winds blowing extremely hot flames throughout Los Angeles county. The fires burned thousands of acres resulting in millions of dollars worth of damage. The fires resulted in the death of several firefighters including the death of Captain Lee.\nThe masked individuals represented the wildfires and the consequences associated with them. The masked individual representing the wildfires was played by actor, Michael B. Jordan. The masked\nGroundtruth: {\u201c<mask-0>\u201d: \u201cMerribelle\u201d, \u201c<mask-1>\u201d: \u201cLu\u201d, \u201c<mask-2>\u201d: \u201cJosh\u201d}\n\n\n\n\n\nF.7 LongLLaMa-3B\n\n\nQuestion: Where was Jos\u00e9 Luis Picardo born and raised?\nOutput: 18 June 1919, Jerez de la Frontera, Andaluc\u00eda, Spain.\nQuestion: What was the name of Jos\u00e9 Luis Picardo\u2019s first wife?\nAnswer: Trinidad de Ribera Talavera.\nQuestion: What was the name of Jos\u00e9 Luis Picardo\u2019s second wife?\nAnswer: Mar\u00eda del Carmen de la Fuente.\nQuestion: What was the name of Jos\u00e9 Luis Picardo\u2019s third wife?\nAnswer: Mar\u00eda del Carmen de la Fuente.\nQuestion: What was the name of Jos\u00e9 Luis Picardo\u2019s fourth wife?\nAnswer: Mar\u00eda del Carmen de la Fuente.\nQuestion: What was the name of Jos\u00e9 Luis Picardo\u2019s fifth wife?\nAnswer: Mar\u00eda del Carmen de la Fuente.\nQuestion: What was the name of Jos\u00e9 Luis Picardo\u2019s sixth wife?\nAnswer: Mar\u00eda del Carmen de la Fuente.\nQuestion: What was the name of Jos\u00e9 Luis Picardo\u2019s seventh wife?\nAnswer: Mar\u00eda del Carmen de la Fuente.\nQuestion: What was the name of Jos\u00e9 Luis Picardo\u2019s eighth wife?\nAnswer: Mar\u00eda del Carmen de la Fuente.\nQuestion: What was the name of Jos\u00e9 Luis Picardo\u2019s ninth wife?\nAnswer: Mar\u00eda del Carmen de la Fuente.\nQuestion: What was the name of Jos\u00e9 Luis Picardo\u2019s tenth wife?\nAnswer: Mar\u00eda del Carmen de la Fuente.\nQuestion: What was the name of Jos\u00e9 Luis Picardo\u2019s eleventh wife?\nAnswer: Mar\u00eda del Carmen de la Fuente.\nQuestion: What was the name of Jos\u00e9 Luis Picardo\u2019s twelfth wife?\nAnswer: Mar\u00eda del Carmen de la Fuente.\nQuestion: What was the name of Jos\u00e9 Luis Picardo\u2019s thirteenth wife?\nAnswer: Mar\u00eda del Carmen de la Fuente.\nQuestion: What was the name of Jos\u00e9 Luis Picardo\u2019s fourteenth wife?\nAnswer: Mar\u00eda del Carmen de la Fuente.\nQuestion: What was the name of Jos\u00e9 Luis Picardo\u2019s fifteenth wife?\nGroundtruth: He was born in Jerez de la Frontera, in the Province of C\u00e1diz in Andaluc\u00eda, Spain. He later moved to Madrid with his family.\n\n\n\n\nQuestion: Who was arrested and charged with the murder of Nicole Brown Simpson and Ronald Goldman?\nOutput:\nGroundtruth: O. J. Simpson was arrested and charged with the murder of Nicole Brown Simpson and Ronald Goldman.\n\n\n\n2. long QA\n\nQuestion: What are the countries mentioned in the text that have the origin of the story type the suitor selection test?\nOutput: Turania/Russia, Spain /Portugalia(?)\nGroundtruth: Iran, China, India.\n\n\n\n\nQuestion: What did Dog walker and Dylan do in common?\nOutput: # <https://www. facebook.\u201d target=\u201d_blank\u201d># <http://bit\u2026.\nGroundtruth: They drink from the faucet.\n\n\n\n3. summarization\n\nOutput: I wish you success!\nGroundtruth: In this article, we study Prasad\u2019s conjecture for regular supercuspidal representations based on the machinery developed by Hakim and Murnaghan to study distinguished representations, and the fundamental work of Kaletha on parameterization of regular supercuspidal representations. For regular supercuspidal representations, we give some new interpretations of the numerical quantities appearing in Prasad\u2019s formula, and reduce the proof to the case of tori. The proof of Prasad\u2019s conjecture then reduces to a comparison of various quadratic characters appearing naturally in the above process. We also have some new observations on these characters and study the relation between them in detail. For some particular examples, we show the coincidence of these characters, which gives a new purely local proof of Prasad\u2019s conjecture for regular supercuspidal representations of these groups. We also prove Prasad\u2019s conjecture for regular supercuspidal representations of G(E), when E/F is unramified and G is a general quasi-split reductive group.\n\n4. cloze\n\nQuestion: The script segment of \u201c Wildfire 2022\u201d takes place in Tulare County, California, where the sky is grey and ashy due to the nearby fires. The story follows <mask-1>, a seven-year-old mute girl, who runs to school along a busy road. <mask-0>, a Trans Latin Woman, is seen singing and dancing to \u201c Opposites Attract\u201d while trying to fix her old car. Lu goes to school and stands in line for lunch, where she is ignored by other kids. She pays for her lunch in change and sits alone at the back of the cafeteria. Later, Fireman <mask-2>speaks to Mrs. Consuelos\u2019 third-grade classroom about fire safety, and Lu is chosen to answer a question despite not speaking. The leading cause of wildfires is discussed, and it is revealed that humans are the leading cause of wildfires due to accidents like leaving an open campfire burning or tossing a cigarette out a car window.\nOutput: Mask #1 = Lu, a young African American Girl. Mask#2=Mr. Consequelo, a Latino Man.\u201d, \u201dMask #1 = Lu, Mask#0=Meribella\u201d, \u201dMask #0 = Love Boy / Lovestalker\nGroundtruth: {\u201c<mask-0>\u201d: \u201cMerribelle\u201d, \u201c<mask-1>\u201d: \u201cLu\u201d, \u201c<mask-2>\u201d: \u201cJosh\u201d}\n\n\n\n\n\nF.8 LLaMa2-7B-32k\n\n1. short QA\n\nQuestion: What is the name of the national police watchdog in Belgium?\nOutput:\nGroundtruth: Committee P.\n\n\n\n\nQuestion: When was Nagaenthran\u2019s second clemency appeal rejected?\nOutput: \u2003\u2003Yes \u2003\u2003No \u2003\u2003Unclear \u2003\u2003Don\u2019 &#xEFF ; \u2003\u2003No Yes \u2003\u2003Maybe \u2003\u2003Probably Definitely \u2003\u2003Possibilities \u2003\u2003Question \u2003\u2003Answer \u2003\u2003Sentiment \u2003\u2003Count \u2003\u2003Label \u2003\u2003Text Context\nGroundtruth: 31 March 2022.\n\n\n\n2. long QA\n\nQuestion: What factors contributed to the deaths in this catastrophe?\n5.The TLC special My Husband\u2019s Not Gay aired.\nOutput: Temporary loss , lack Title : Good book! Bad cover artwork\u2026 Label is good though!!!!! Review text:: This album has great songs & lyrics\u2026.I love how he tallies things\u2026..& whatnot\u2026\u2026..But i dont think you will enjoy listening unless your really familiar wit him already cuzz hes kindda oldskool rap style \u2026. But thats cool ..i guess u have ta grow older wid da music ya listen tu right????? Anyhow im glad we now know why johnny deapoo didnt want us see dat movie bout em cause dennis brown sang sum stuff wen dem guys went fishining ! And yeah idk wat happenned here either !! Im sure ill figure somethigng ouuutttttaaaahhhhtttteeeerrrrrreeemmmbbbbeennnnnggggoonnnddddeesssssommeeettittiiimminneeeddoowwwwwwwnniiiiisssshhoootteereeeyyyyaaaaaarrraannkkkiin\nttiioouunnllliikkeeebbrrddennoot ooffffffff yyyyyyouurrrsellleeefffffefeffeefeferrerererfrrfrefrennerreennrernnenrneneeenenenenneennenennesenesensecencecsencesecescenec\nsenescencesensescnscsncssncnessccscevjdjdhdfghjkldqweiwufihgrtyujbfcvbnmqazxcvcvmnbgnbmgzajdlfnprzyuwoguyoj\nughtydsgubgyhwizuzudsmokylrlkyegkwxtynuhypodmdmxozunivtwcrztzmixpxosohoyiyznczadlnrmrw\ntnbuckyjpgxyzkacctwoomkcquytuefghltdrhsfmnvbpbmlnxjqezuvszrg\nGroundtruth: Train derailments or falling glass, traffic accidents, heart attacks, exposure, drowning, snow burial, carbon monoxide poisoning.\n\n\n\n\nQuestion: When was Nagaenthran\u2019s second clemency appeal rejected?\nOutput: Yes No Unclear Don\u2019 &#xEFF ; No Yes Maybe Probably Definitely Possibilities Question Answer Sentiment Count Label Text Context\nGroundtruth: 31 March 2022.\n\n\n\n3. summarization\n\nOutput: Principal bundle $X$ \\u2192 $\\\\mathbb{Q}$\\\\u2192$\\\\mathcal {W}_$;Which roughly speaking says \\u201cregular \\u201d bundles come packaged nicely wrt primes.\\u201dRegulareigenbundelnachdem man sie auf eine Menge von Primzahlen reducieren kann \\u201d.\nGroundtruth: In this article, we study Prasad\u2019s conjecture for regular supercuspidal representations based on the machinery developed by Hakim and Murnaghan to study distinguished representations, and the fundamental work of Kaletha on parameterization of regular supercuspidal representations. For regular supercuspidal representations, we give some new interpretations of the numerical quantities appearing in Prasad\u2019s formula, and reduce the proof to the case of tori. The proof of Prasad\u2019s conjecture then reduces to a comparison of various quadratic characters appearing naturally in the above process. We also have some new observations on these characters and study the relation between them in detail. For some particular examples, we show the coincidence of these characters, which gives a new purely local proof of Prasad\u2019s conjecture for regular supercuspidal representations of these groups. We also prove Prasad\u2019s conjecture for regular supercuspidal representations of G(E), when E/F is unramified and G is a general quasi-split reductive group.\n\n4. cloze\n\nQuestion: The script segment of \u201c Wildfire 2022\u201d takes place in Tulare County, California, where the sky is grey and ashy due to the nearby fires. The story follows <mask-1>, a seven-year-old mute girl, who runs to school along a busy road. <mask-0>, a Trans Latin Woman, is seen singing and dancing to \u201c Opposites Attract\u201d while trying to fix her old car. Lu goes to school and stands in line for lunch, where she is ignored by other kids. She pays for her lunch in change and sits alone at the back of the cafeteria. Later, Fireman <mask-2>speaks to Mrs. Consuelos\u2019 third-grade classroom about fire safety, and Lu is chosen to answer a question despite not speaking. The leading cause of wildfires is discussed, and it is revealed that humans are the leading cause of wildfires due to accidents like leaving an open campfire burning or tossing a cigarette out a car window.\nOutput: \u201cNone\u201d\nGroundtruth: {\u201c<mask-0>\u201d: \u201cMerribelle\u201d, \u201c<mask-1>\u201d: \u201cLu\u201d, \u201c<mask-2>\u201d: \u201cJosh\u201d}\n\n\n\n\n"
    },
    "bibliography": {
      "An et\u00a0al. (2023)": "\nAn et\u00a0al. (2023)\n\nChenxin An, Shansan Gong, Ming Zhong, Mukai Li, Jun Zhang, Lingpeng Kong, and Xipeng Qiu.\n\n\nL-eval: Instituting standardized evaluation for long context language models, 2023.\n\n\n",
      "Angelidis et\u00a0al. (2020)": "\nAngelidis et\u00a0al. (2020)\n\nStefanos Angelidis, Reinald\u00a0Kim Amplayo, Yoshihiko Suhara, Xiaolan Wang, and Mirella Lapata.\n\n\nExtractive opinion summarization in quantized transformer spaces, 2020.\n\n\n",
      "Askari et\u00a0al. (2023)": "\nAskari et\u00a0al. (2023)\n\nArian Askari, Suzan Verberne, Amin Abolghasemi, Wessel Kraaij, and Gabriella Pasi.\n\n\nRetrieval for extremely long queries and documents with rprs: a highly efficient and effective transformer-based re-ranker, 2023.\n\n\n",
      "Bai et\u00a0al. (2023)": "\nBai et\u00a0al. (2023)\n\nYushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li.\n\n\nLongbench: A bilingual, multitask benchmark for long context understanding, 2023.\n\n\n",
      "Bessonov et\u00a0al. (2023)": "\nBessonov et\u00a0al. (2023)\n\nArkadii Bessonov, Alexey Staroverov, Huzhenyu Zhang, Alexey\u00a0K. Kovalev, Dmitry Yudin, and Aleksandr\u00a0I. Panov.\n\n\nRecurrent memory decision transformer, 2023.\n\n\n",
      "Brown et\u00a0al. (2020)": "\nBrown et\u00a0al. (2020)\n\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared\u00a0D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et\u00a0al.\n\n\nLanguage models are few-shot learners.\n\n\nAdvances in neural information processing systems, 33:1877\u20131901, 2020.\n\n\n",
      "Bulatov et\u00a0al. (2022)": "\nBulatov et\u00a0al. (2022)\n\nAydar Bulatov, Yuri Kuratov, and Mikhail\u00a0S. Burtsev.\n\n\nRecurrent memory transformer, 2022.\n\n\n",
      "Bulatov et\u00a0al. (2023)": "\nBulatov et\u00a0al. (2023)\n\nAydar Bulatov, Yuri Kuratov, and Mikhail\u00a0S. Burtsev.\n\n\nScaling transformer to 1m tokens and beyond with rmt, 2023.\n\n\n",
      "Chen et\u00a0al. (2023a)": "\nChen et\u00a0al. (2023a)\n\nShouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian.\n\n\nExtending context window of large language models via positional interpolation, 2023a.\n\n\n",
      "Chen et\u00a0al. (2023b)": "\nChen et\u00a0al. (2023b)\n\nXuanting Chen, Junjie Ye, Can Zu, Nuo Xu, Rui Zheng, Minlong Peng, Jie Zhou, Tao Gui, Qi\u00a0Zhang, and Xuanjing Huang.\n\n\nHow robust is gpt-3.5 to predecessors? a comprehensive study on language understanding tasks, 2023b.\n\n\n",
      "Chen et\u00a0al. (2023c)": "\nChen et\u00a0al. (2023c)\n\nYukang Chen, Shengju Qian, Haotian Tang, Xin Lai, Zhijian Liu, Song Han, and Jiaya Jia.\n\n\nLonglora: Efficient fine-tuning of long-context large language models, 2023c.\n\n\n",
      "Chi et\u00a0al. (2023)": "\nChi et\u00a0al. (2023)\n\nTa-Chung Chi, Ting-Han Fan, Alexander\u00a0I. Rudnicky, and Peter\u00a0J. Ramadge.\n\n\nDissecting transformer length extrapolation via the lens of receptive field analysis, 2023.\n\n\n",
      "Cohan et\u00a0al. (2018)": "\nCohan et\u00a0al. (2018)\n\nArman Cohan, Franck Dernoncourt, Doo\u00a0Soon Kim, Trung Bui, Seokhwan Kim, Walter Chang, and Nazli Goharian.\n\n\nA discourse-aware attention model for abstractive summarization of long documents, 2018.\n\n\n",
      "Dao et\u00a0al. (2022)": "\nDao et\u00a0al. (2022)\n\nTri Dao, Daniel\u00a0Y. Fu, Stefano Ermon, Atri Rudra, and Christopher R\u00e9.\n\n\nFlashattention: Fast and memory-efficient exact attention with io-awareness, 2022.\n\n\n",
      "Devlin et\u00a0al. (2019)": "\nDevlin et\u00a0al. (2019)\n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.\n\n\nBert: Pre-training of deep bidirectional transformers for language understanding, 2019.\n\n\n",
      "Ding et\u00a0al. (2023)": "\nDing et\u00a0al. (2023)\n\nJiayu Ding, Shuming Ma, Li\u00a0Dong, Xingxing Zhang, Shaohan Huang, Wenhui Wang, Nanning Zheng, and Furu Wei.\n\n\nLongnet: Scaling transformers to 1,000,000,000 tokens, 2023.\n\n\n",
      "Dong et\u00a0al. (2023)": "\nDong et\u00a0al. (2023)\n\nZican Dong, Tianyi Tang, Lunyi Li, and Wayne\u00a0Xin Zhao.\n\n\nA survey on long text modeling with transformers, 2023.\n\n\n",
      "Du et\u00a0al. (2022)": "\nDu et\u00a0al. (2022)\n\nZhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie Tang.\n\n\nGlm: General language model pretraining with autoregressive blank infilling.\n\n\nIn Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp.\u00a0 320\u2013335, 2022.\n\n\n",
      "Engelbach et\u00a0al. (2023)": "\nEngelbach et\u00a0al. (2023)\n\nMatthias Engelbach, Dennis Klau, Felix Scheerer, Jens Drawehn, and Maximilien Kintz.\n\n\nFine-tuning and aligning question answering models for complex information extraction tasks, 2023.\n\n\n",
      "Gidiotis & Tsoumakas (2020)": "\nGidiotis & Tsoumakas (2020)\n\nAlexios Gidiotis and Grigorios Tsoumakas.\n\n\nA divide-and-conquer approach to the summarization of long documents, 2020.\n\n\n",
      "Huang et\u00a0al. (2021)": "\nHuang et\u00a0al. (2021)\n\nLuyang Huang, Shuyang Cao, Nikolaus Parulian, Heng Ji, and Lu\u00a0Wang.\n\n\nEfficient attentions for long document summarization, 2021.\n\n\n",
      "Izacard et\u00a0al. (2022)": "\nIzacard et\u00a0al. (2022)\n\nGautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-Yu, Armand Joulin, Sebastian Riedel, and Edouard Grave.\n\n\nAtlas: Few-shot learning with retrieval augmented language models, 2022.\n\n\n",
      "Kim & Park (2023)": "\nKim & Park (2023)\n\nNamHyeok Kim and Chanjun Park.\n\n\nInter-annotator agreement in the wild: Uncovering its emerging roles and considerations in real-world scenarios, 2023.\n\n\n",
      "Kojima et\u00a0al. (2023)": "\nKojima et\u00a0al. (2023)\n\nTakeshi Kojima, Shixiang\u00a0Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa.\n\n\nLarge language models are zero-shot reasoners, 2023.\n\n\n",
      "Lee et\u00a0al. (2023)": "\nLee et\u00a0al. (2023)\n\nHarrison Lee, Samrat Phatale, Hassan Mansoor, Kellie Lu, Thomas Mesnard, Colton Bishop, Victor Carbune, and Abhinav Rastogi.\n\n\nRlaif: Scaling reinforcement learning from human feedback with ai feedback, 2023.\n\n\n",
      "Li et\u00a0al. (2023a)": "\nLi et\u00a0al. (2023a)\n\nShanda Li, Chong You, Guru Guruganesh, Joshua Ainslie, Santiago Ontanon, Manzil Zaheer, Sumit Sanghai, Yiming Yang, Sanjiv Kumar, and Srinadh Bhojanapalli.\n\n\nFunctional interpolation for relative positions improves long context transformers, 2023a.\n\n\n",
      "Li (2023)": "\nLi (2023)\n\nYucheng Li.\n\n\nUnlocking context constraints of llms: Enhancing context efficiency of llms with self-information-based content filtering, 2023.\n\n\n",
      "Li et\u00a0al. (2023b)": "\nLi et\u00a0al. (2023b)\n\nZihao Li, Samuel Belkadi, Nicolo Micheletti, Lifeng Han, Matthew Shardlow, and Goran Nenadic.\n\n\nLarge language models and control mechanisms improve text readability of biomedical abstracts, 2023b.\n\n\n",
      "Liu et\u00a0al. (2023a)": "\nLiu et\u00a0al. (2023a)\n\nNelson\u00a0F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang.\n\n\nLost in the middle: How language models use long contexts, 2023a.\n\n\n",
      "Liu et\u00a0al. (2023b)": "\nLiu et\u00a0al. (2023b)\n\nYuxuan Liu, Tianchi Yang, Shaohan Huang, Zihan Zhang, Haizhen Huang, Furu Wei, Weiwei Deng, Feng Sun, and Qi\u00a0Zhang.\n\n\nCalibrating llm-based evaluator, 2023b.\n\n\n",
      "Meister et\u00a0al. (2021)": "\nMeister et\u00a0al. (2021)\n\nClara Meister, Stefan Lazov, Isabelle Augenstein, and Ryan Cotterell.\n\n\nIs sparse attention more interpretable?, 2021.\n\n\n",
      "Mukherjee & Rahman (2023)": "\nMukherjee & Rahman (2023)\n\nUsmi Mukherjee and Mohammad\u00a0Masudur Rahman.\n\n\nEmploying deep learning and structured information retrieval to answer clarification questions on bug reports, 2023.\n\n\n",
      "OpenAI (2023)": "\nOpenAI (2023)\n\nOpenAI.\n\n\nGpt-4 technical report, 2023.\n\n\n",
      "Ouyang et\u00a0al. (2022)": "\nOuyang et\u00a0al. (2022)\n\nLong Ouyang, Jeff Wu, Xu\u00a0Jiang, Diogo Almeida, Carroll\u00a0L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe.\n\n\nTraining language models to follow instructions with human feedback, 2022.\n\n\n",
      "Peng et\u00a0al. (2023)": "\nPeng et\u00a0al. (2023)\n\nBo\u00a0Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Huanqi Cao, Xin Cheng, Michael Chung, Matteo Grella, Kranthi\u00a0Kiran GV, Xuzheng He, Haowen Hou, Przemyslaw Kazienko, Jan Kocon, Jiaming Kong, Bartlomiej Koptyra, Hayden Lau, Krishna Sri\u00a0Ipsit Mantri, Ferdinand Mom, Atsushi Saito, Xiangru Tang, Bolun Wang, Johan\u00a0S. Wind, Stansilaw Wozniak, Ruichong Zhang, Zhenyuan Zhang, Qihang Zhao, Peng Zhou, Jian Zhu, and Rui-Jie Zhu.\n\n\nRwkv: Reinventing rnns for the transformer era, 2023.\n\n\n",
      "Ram et\u00a0al. (2023)": "\nRam et\u00a0al. (2023)\n\nOri Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon Shashua, Kevin Leyton-Brown, and Yoav Shoham.\n\n\nIn-context retrieval-augmented language models, 2023.\n\n\n",
      "Roy (2021)": "\nRoy (2021)\n\nArya Roy.\n\n\nRecent trends in named entity recognition (ner), 2021.\n\n\n",
      "s\u00a0Ko\u02c7cisk\u00fd et\u00a0al. (2018)": "\ns\u00a0Ko\u02c7cisk\u00fd et\u00a0al. (2018)\n\nTom\u00e1\u0161\u00a0Ko\u02c7cisk\u00fd, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl\u00a0Moritz Hermann, G\u00e1bor Melis, and Edward Grefenstette.\n\n\nThe NarrativeQA reading comprehension challenge.\n\n\nTransactions of the Association for Computational Linguistics, TBD:TBD, 2018.\n\n\nURL https://TBD.\n\n\n",
      "Shaham et\u00a0al. (2022)": "\nShaham et\u00a0al. (2022)\n\nUri Shaham, Elad Segal, Maor Ivgi, Avia Efrat, Ori Yoran, Adi Haviv, Ankit Gupta, Wenhan Xiong, Mor Geva, Jonathan Berant, and Omer Levy.\n\n\nScrolls: Standardized comparison over long language sequences, 2022.\n\n\n",
      "Shaham et\u00a0al. (2023)": "\nShaham et\u00a0al. (2023)\n\nUri Shaham, Maor Ivgi, Avia Efrat, Jonathan Berant, and Omer Levy.\n\n\nZeroscrolls: A zero-shot benchmark for long text understanding, 2023.\n\n\n",
      "Sharma et\u00a0al. (2019)": "\nSharma et\u00a0al. (2019)\n\nEva Sharma, Chen Li, and Lu\u00a0Wang.\n\n\nBigpatent: A large-scale dataset for abstractive and coherent summarization, 2019.\n\n\n",
      "Sharma et\u00a0al. (2023)": "\nSharma et\u00a0al. (2023)\n\nRoshan Sharma, Suyoun Kim, Daniel Lazar, Trang Le, Akshat Shrivastava, Kwanghoon Ahn, Piyush Kansal, Leda Sari, Ozlem Kalinli, and Michael Seltzer.\n\n\nAugmenting text for spoken language understanding with large language models, 2023.\n\n\n",
      "Song et\u00a0al. (2020)": "\nSong et\u00a0al. (2020)\n\nKaitao Song, Xu\u00a0Tan, Tao Qin, Jianfeng Lu, and Tie-Yan Liu.\n\n\nMpnet: Masked and permuted pre-training for language understanding, 2020.\n\n\n",
      "Suri et\u00a0al. (2023)": "\nSuri et\u00a0al. (2023)\n\nGaurav Suri, Lily\u00a0R. Slater, Ali Ziaee, and Morgan Nguyen.\n\n\nDo large language models show decision heuristics similar to humans? a case study using gpt-3.5, 2023.\n\n\n",
      "Tay et\u00a0al. (2020)": "\nTay et\u00a0al. (2020)\n\nYi\u00a0Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler.\n\n\nLong range arena: A benchmark for efficient transformers, 2020.\n\n\n",
      "Tay et\u00a0al. (2022)": "\nTay et\u00a0al. (2022)\n\nYi\u00a0Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler.\n\n\nEfficient transformers: A survey, 2022.\n\n\n",
      "Touvron et\u00a0al. (2023)": "\nTouvron et\u00a0al. (2023)\n\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample.\n\n\nLlama: Open and efficient foundation language models, 2023.\n\n\n",
      "Trivedi et\u00a0al. (2022)": "\nTrivedi et\u00a0al. (2022)\n\nHarsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal.\n\n\nMusique: Multihop questions via single-hop question composition, 2022.\n\n\n",
      "Tworkowski et\u00a0al. (2023)": "\nTworkowski et\u00a0al. (2023)\n\nSzymon Tworkowski, Konrad Staniszewski, Miko\u0142aj Pacek, Yuhuai Wu, Henryk Michalewski, and Piotr Mi\u0142o\u015b.\n\n\nFocused transformer: Contrastive training for context scaling, 2023.\n\n\n",
      "Wang et\u00a0al. (2022)": "\nWang et\u00a0al. (2022)\n\nAlex Wang, Richard\u00a0Yuanzhe Pang, Angelica Chen, Jason Phang, and Samuel\u00a0R. Bowman.\n\n\nSquality: Building a long-document summarization dataset the hard way, 2022.\n\n\n",
      "Wei et\u00a0al. (2023)": "\nWei et\u00a0al. (2023)\n\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed\u00a0Chi, Quoc Le, and Denny Zhou.\n\n\nChain-of-thought prompting elicits reasoning in large language models, 2023.\n\n\n",
      "Wu et\u00a0al. (2021)": "\nWu et\u00a0al. (2021)\n\nJeff Wu, Long Ouyang, Daniel\u00a0M. Ziegler, Nisan Stiennon, Ryan Lowe, Jan Leike, and Paul Christiano.\n\n\nRecursively summarizing books with human feedback, 2021.\n\n\n",
      "Xiong et\u00a0al. (2023)": "\nXiong et\u00a0al. (2023)\n\nWenhan Xiong, Jingyu Liu, Igor Molybog, Hejia Zhang, Prajjwal Bhargava, Rui Hou, Louis Martin, Rashi Rungta, Karthik\u00a0Abinav Sankararaman, Barlas Oguz, Madian Khabsa, Han Fang, Yashar Mehdad, Sharan Narang, Kshitiz Malik, Angela Fan, Shruti Bhosale, Sergey Edunov, Mike Lewis, Sinong Wang, and Hao Ma.\n\n\nEffective long-context scaling of foundation models, 2023.\n\n\n",
      "Xu et\u00a0al. (2023)": "\nXu et\u00a0al. (2023)\n\nPeng Xu, Wei Ping, Xianchao Wu, Lawrence McAfee, Chen Zhu, Zihan Liu, Sandeep Subramanian, Evelina Bakhturina, Mohammad Shoeybi, and Bryan Catanzaro.\n\n\nRetrieval meets long context large language models, 2023.\n\n\n",
      "Yang et\u00a0al. (2018)": "\nYang et\u00a0al. (2018)\n\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William\u00a0W. Cohen, Ruslan Salakhutdinov, and Christopher\u00a0D. Manning.\n\n\nHotpotqa: A dataset for diverse, explainable multi-hop question answering, 2018.\n\n\n",
      "Ye et\u00a0al. (2023)": "\nYe et\u00a0al. (2023)\n\nJunjie Ye, Xuanting Chen, Nuo Xu, Can Zu, Zekai Shao, Shichun Liu, Yuhan Cui, Zeyang Zhou, Chao Gong, Yang Shen, Jie Zhou, Siming Chen, Tao Gui, Qi\u00a0Zhang, and Xuanjing Huang.\n\n\nA comprehensive capability analysis of gpt-3 and gpt-3.5 series models, 2023.\n\n\n",
      "Zeng et\u00a0al. (2023)": "\nZeng et\u00a0al. (2023)\n\nYan Zeng, Hanbo Zhang, Jiani Zheng, Jiangnan Xia, Guoqiang Wei, Yang Wei, Yuchen Zhang, and Tao Kong.\n\n\nWhat matters in training a gpt4-style language model with multimodal inputs?, 2023.\n\n\n",
      "Zheng et\u00a0al. (2023)": "\nZheng et\u00a0al. (2023)\n\nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi\u00a0Lin, Zhuohan Li, Dacheng Li, Eric.\u00a0P Xing, Hao Zhang, Joseph\u00a0E. Gonzalez, and Ion Stoica.\n\n\nJudging llm-as-a-judge with mt-bench and chatbot arena, 2023.\n\n\n",
      "Zhou et\u00a0al. (2022)": "\nZhou et\u00a0al. (2022)\n\nYucheng Zhou, Tao Shen, Xiubo Geng, Chongyang Tao, Guodong Long, Can Xu, and Daxin Jiang.\n\n\nFine-grained distillation for long document retrieval, 2022.\n\n\n"
    },
    "domain": "cs.CL",
    "citation_count": 0
  },
  "b606875d-8714-4535-ad8c-9de1b35bfbd1": {
    "pk": "b606875d-8714-4535-ad8c-9de1b35bfbd1",
    "authors": [
      "Hongjin Qian",
      "Zheng Liu",
      "Peitian Zhang",
      "Kelong Mao",
      "Yujia Zhou",
      "Xu Chen",
      "Zhicheng Dou"
    ],
    "title": "Are Long-LLMs A Necessity For Long-Context Tasks?",
    "abstract": "The learning and deployment of long-LLMs remains a challenging problem despite recent progresses. In this work, we argue that the long-LLMs are not a necessity to solve long-context tasks, as common long-context tasks are short-context solvable, i.e. they can be solved by purely working with oracle short-contexts within the long-context tasks' inputs. On top of this argument, we propose a framework called LC-Boost (Long-Context Bootstrapper), which enables a short-LLM to address the long-context tasks in a bootstrapping manner. In our framework, the short-LLM prompts itself to reason for two critical decisions: 1) how to access to the appropriate part of context within the input, 2) how to make effective use of the accessed context. By adaptively accessing and utilizing the context based on the presented tasks, LC-Boost can serve as a general framework to handle diversified long-context processing problems. We comprehensively evaluate different types of tasks from popular long-context benchmarks, where LC-Boost is able to achieve a substantially improved performance with a much smaller consumption of resource.",
    "url": "http://arxiv.org/abs/2405.15318v1",
    "timestamp": 1716537570,
    "sections": {
      "1 Introduction": "\n\n1 Introduction\n\nLarge language models (LLMs) are widely adopted for real-world applications. Many of the applications are associated with long-sequence inputs, such as long-document question answering and summarization. As such, the LLMs are commonly expected to have a long working context (a.k.a. long-LLMs) in order to confront such demanding scenarios\u00a0(Bai et\u00a0al., 2023; Zhang et\u00a0al., 2024a). Unfortunately, the learning and deployment of long-LLMs are still challenging in multiple perspectives. Particularly, many existing LLMs are initially introduced with a limited size of context (e.g., 2K for Llama-1 Touvron et\u00a0al. (2023a), 4K for Llama-2 Touvron et\u00a0al. (2023b), 8K for Llama-3\u00a0111https://llama.meta.com/llama3/). Although the initial short-LLM can be fine-tuned to establish a much longer context, it is likely to take substantial costs; and more seriously, it is extremely resource-consuming to deploy the long-LLMs\u00a0(Kaplan et\u00a0al., 2020). The continually training may also compromise the LLMs\u2019 general capability over short contexts\u00a0(Liu et\u00a0al., 2023; Li et\u00a0al., 2023a). In fact, it remains an open problem to explore new solutions which may tackle long-context tasks both effectively and efficiently.\n\n\nIn this paper, we argue that most long-context tasks are short-context solvable. That is to say, the long-context tasks, despite associated with long-sequence inputs, can be addressed by merely working with short-contexts in a strategic way. For example, the reading comprehension or summarization of a book can be solved based on the extraction of necessary key facts from the book.\nThe above argument is akin to the working patterns of human beings and modern computers, where arbitrary long-form problems can always be decomposed and solved on top of a limited memory capacity\u00a0(Adolphs, 1999; Bryant and O\u2019Hallaron, 2011).\nHowever, even if the above argument holds, it is still non-trivial to solve the long-context tasks purely based on short contexts. This is because different tasks call for distinct ways of accessing and utilizing information from the long context; therefore, there can hardly be any fixed rules to handle all possible situations. To address this challenge, we propose a method, called LC-Boost, where short-LLMs are employed to solve general long-context tasks in a bootstrapping manner. LC-Boost operates with two critical reasoning steps. One is the reasoning of Access, where the LLM prompts itself to plan for how to access the appropriate part of context within the input. The other one is the reasoning of Utilize, where the LLM figures out how to make effective use of the accessed context. Thanks to the above design, LC-Boost is able to adaptively handle diversified long-context tasks according to their unique nature. For example, given a knowledge-grounded QA problem, the LLM may directly access to the knowledgable context through retrieval, and generate the answer in the form of RAG. Besides, it may sequentially scan the long context chunk-by-chunk if the task calls for the aggregation of specific information from the entire input.\n\n\nFigure 1: Illustration for LC-Boost. The LLM is prompted to reason for how to access to proper context and how to utilize the accessed context to solve the task. Toy Examples.\n(A) Brute-force solution. Despite correctness, it is unnecessarily expensive due to the processing of the entire context simultaneously. (B) Naive RAG. It is hard to handle problems like information aggregation, which leads to the incomplete answer. (C) LC-Boost leverages RAG to tackle the problem, which produces the correct answer in a small cost. (D) LC-Boost processes the long-context via sequential scan, which correctly solves the problem based on the comprehensively collected information.\n\n\nThe following toy examples are presented to better illustrate the mechanism of LC-Boost (Figure 1). Particular, there are two common approaches to tackle long-context problems: (A) the brute-force method based on long-LLMs, (B) the surrogate methods, like RAG Xu et\u00a0al. (2023a). Despite being straightforward, the brute-force method is likely to incur huge unnecessary costs as the problem could be directly solved by simple surrogate methods, like RAG. On the other hand, although the surrogate methods may help in certain cases, they are likely to become useless in other situations. For instance, the RAG-based methods are inappropriate to handle information aggregation problems, as showcased in\u00a0(B). In contrast, LC-Boost is able to handle general long-context tasks thanks to the proper reasoning of how to access and utilize\nthe long-context information based on each specific task. As shown in (C), it can directly access to the needed information via retrieval and generate the answer based on RAG. Meanwhile, it can also process the entire context in a divide-and-conquer manner, which will fully collect the information and solve the problem presented in (D).\n\n\nWe perform comprehensive experiments for LC-Boost, including both popular real-world long-context problems, like question-answering and summarization of long documents, and a wide variety of synthetic tasks. In our experiments, LC-Boost is able to achieve equivalent performances as the brute-force methods based on strong long-LLMs, e.g., GPT-4-128K. In many cases, its performances can even notably surpass the brute-force methods, probably due to the elimination of distracting context. Besides, our experiments also underscore the importance of reasoning and adaptability, as LC-Boost outperforms all short-LLM surrogates with predefined access and utilization of context.\n\n\nTo summarize, our paper makes the following contributions. (1) We identify the research problem of solving long-context problems with short-LLMs. To the best of our knowledge, it is the first study of its kind, which is important to not only address the problem itself but also meaningful to the sustainability and energy-efficient running of AI industry in a broader sense. (2) We propose a novel framework LC-Boost, which is able to adaptively handle general long-context tasks based on the reasoning of how to access and utilize the long context. (3) We empirically verify the effectiveness of LC-Boost based on its superior performances achieved from low resource-consumption.\n\n",
      "2 LC-Boost": "\n\n2 LC-Boost\n\n\n2.1 Preliminaries\n\nLLMs can be succinctly defined as \ud835\udcb4=\u03b3\u2062(q)\ud835\udcb4\ud835\udefe\ud835\udc5e{\\mathcal{Y}}=\\gamma(q)caligraphic_Y = italic_\u03b3 ( italic_q ), where \u03b3\u2062(\u22c5)\ud835\udefe\u22c5\\gamma(\\cdot)italic_\u03b3 ( \u22c5 ) represents a selected LLM, q\ud835\udc5eqitalic_q denotes a user query, and \ud835\udcb4\ud835\udcb4{\\mathcal{Y}}caligraphic_Y refers to the answer produced by the LLMs. As highlighted in many previous studies, e.g.,\u00a0(Ji et\u00a0al., 2023; Lewis et\u00a0al., 2020; Shuster et\u00a0al., 2021), the knowledge embedded in an LLM\u2019s parameters is static and, consequently, often fails to adequately address user queries requiring up-to-date or in-depth knowledge. To address this limitation, we can introduce external knowledge (refer to as context \ud835\udcb3\ud835\udcb3{\\mathcal{X}}caligraphic_X) into the LLMs. Additionally, tasks involving information aggregation\u00a0(e.g., summarization) also take a context \ud835\udcb3\ud835\udcb3{\\mathcal{X}}caligraphic_X as input along with task instructions q\ud835\udc5eqitalic_q. Thus, we can generally define the model\u2019s generation process w.r.t. a context \ud835\udcb3\ud835\udcb3{\\mathcal{X}}caligraphic_X as:\n\ud835\udcb4=\u03b3\u2062(q,\ud835\udcb3).\ud835\udcb4\ud835\udefe\ud835\udc5e\ud835\udcb3{\\mathcal{Y}}=\\gamma(q,{\\mathcal{X}}).caligraphic_Y = italic_\u03b3 ( italic_q , caligraphic_X ) .\n\n\nAs discussed in Section 1, in many scenarios, the context \ud835\udcb3\ud835\udcb3{\\mathcal{X}}caligraphic_X is a long sequence, necessitating that LLMs manage long contexts. However, most existing LLMs were originally introduced with limited context sizes\u00a0(e.g., 4K). Consequently, these models are unable to process inputs that exceed their capacity without truncation. In this paper, we characterize such scenarios as long-context problem. It involves LLMs processing inputs that notably surpass their inherent context limitations, which can be formally described by:\n\n\n\n\ud835\udcb4=\u03b3\u2062(q,\ud835\udcb3)s.t.\u2062|\ud835\udcb3|\u226bL,formulae-sequence\ud835\udcb4\ud835\udefe\ud835\udc5e\ud835\udcb3much-greater-thans.t.\ud835\udcb3\ud835\udc3f{\\mathcal{Y}}=\\gamma(q,{\\mathcal{X}})\\quad\\text{s.t.}|{\\mathcal{X}}|\\gg L,caligraphic_Y = italic_\u03b3 ( italic_q , caligraphic_X ) s.t. | caligraphic_X | \u226b italic_L ,\n\n(1)\n\n\nwhere L\ud835\udc3fLitalic_L denotes the native context length limit of the LLM. The most straightforward way to address the long-context problem is to increase the LLMs\u2019 context length L\ud835\udc3fLitalic_L, mitigating the challenges of long contexts. In this paper, we instead explore solving long-context tasks using short-context LLMs\u00a0(e.g., 4K) without increasing the model\u2019s context length L\ud835\udc3fLitalic_L.\n\n\n\n\n2.2 Pilot Study: Are Most Long-Context Tasks Short-Context Solvable?\n\nDespite the potential for fine-tuning LLMs to handle much longer contexts, this approach incurs substantial costs. Additionally, directly processing long contexts during the inference stage exponentially increases computing resource consumption, which is not environmentally friendly.\nIn the following, we conduct a pilot study from both theoretical and empirical perspectives to explore the question: Are most long-context tasks solvable with short contexts?\n\n\nTheoretical Analysis\n\nSuppose we have an input variable \ud835\udcb3\ud835\udcb3{\\mathcal{X}}caligraphic_X and an output variable \ud835\udcb4\ud835\udcb4{\\mathcal{Y}}caligraphic_Y, the relevant part of \ud835\udcb3\ud835\udcb3{\\mathcal{X}}caligraphic_X given \ud835\udcb4\ud835\udcb4{\\mathcal{Y}}caligraphic_Y is denoted by \ud835\udcb3~~\ud835\udcb3\\tilde{{\\mathcal{X}}}over~ start_ARG caligraphic_X end_ARG. An ideal \ud835\udcb3~~\ud835\udcb3\\tilde{{\\mathcal{X}}}over~ start_ARG caligraphic_X end_ARG should capture all relevant features of the original input variable \ud835\udcb3\ud835\udcb3{\\mathcal{X}}caligraphic_X in relation to \ud835\udcb4\ud835\udcb4{\\mathcal{Y}}caligraphic_Y. In other words, the optimal \ud835\udcb3~~\ud835\udcb3\\tilde{{\\mathcal{X}}}over~ start_ARG caligraphic_X end_ARG represents the simplest mapping of \ud835\udcb3\ud835\udcb3{\\mathcal{X}}caligraphic_X that accurately preserves the mutual information I\u2062(\ud835\udcb3;\ud835\udcb4)\ud835\udc3c\ud835\udcb3\ud835\udcb4I({\\mathcal{X}};{\\mathcal{Y}})italic_I ( caligraphic_X ; caligraphic_Y ). We therefore propose a Markov chain \ud835\udcb3\u2192\ud835\udcb3~\u2192\ud835\udcb4\u2192\ud835\udcb3~\ud835\udcb3\u2192\ud835\udcb4{\\mathcal{X}}\\rightarrow\\tilde{{\\mathcal{X}}}\\rightarrow{\\mathcal{Y}}caligraphic_X \u2192 over~ start_ARG caligraphic_X end_ARG \u2192 caligraphic_Y. According to the data processing inequality (DPI), we have I\u2062(\ud835\udcb3;\ud835\udcb3~)\u2265I\u2062(\ud835\udcb3;\ud835\udcb4)\ud835\udc3c\ud835\udcb3~\ud835\udcb3\ud835\udc3c\ud835\udcb3\ud835\udcb4I({\\mathcal{X}};\\tilde{{\\mathcal{X}}})\\geq I({\\mathcal{X}};{\\mathcal{Y}})italic_I ( caligraphic_X ; over~ start_ARG caligraphic_X end_ARG ) \u2265 italic_I ( caligraphic_X ; caligraphic_Y ), with equality holding if and only if \ud835\udcb3~~\ud835\udcb3\\tilde{{\\mathcal{X}}}over~ start_ARG caligraphic_X end_ARG constitutes a sufficient statistics\u00a0(Cover, 1999; Tishby and Zaslavsky, 2015). This suggests that, in an optimal setting, we can always find a subset \ud835\udcb3~\u2286\ud835\udcb3~\ud835\udcb3\ud835\udcb3\\tilde{\\mathcal{X}}\\subseteq\\mathcal{X}over~ start_ARG caligraphic_X end_ARG \u2286 caligraphic_X that provides information at least as useful for generating the output \ud835\udcb4\ud835\udcb4\\mathcal{Y}caligraphic_Y as the full context \ud835\udcb3\ud835\udcb3\\mathcal{X}caligraphic_X.\n\n\nIn practical scenarios, obtaining the optimal \ud835\udcb3~~\ud835\udcb3\\tilde{{\\mathcal{X}}}over~ start_ARG caligraphic_X end_ARG is challenging due to various factors, such as empirical errors\u00a0Mohri et\u00a0al. (2018). Thus, we can only estimate \ud835\udcb3~~\ud835\udcb3\\tilde{{\\mathcal{X}}}over~ start_ARG caligraphic_X end_ARG. Estimating \ud835\udcb3~~\ud835\udcb3\\tilde{{\\mathcal{X}}}over~ start_ARG caligraphic_X end_ARG directly from \ud835\udcb3\ud835\udcb3{\\mathcal{X}}caligraphic_X might be challenging if \ud835\udcb3\ud835\udcb3{\\mathcal{X}}caligraphic_X defines a large variable space. In this situation, we propose decomposing the original input variable \ud835\udcb3\ud835\udcb3{\\mathcal{X}}caligraphic_X into a series of subsets, \ud835\udcb3={\ud835\udcb31,\u22ef,\ud835\udcb3n}\ud835\udcb3subscript\ud835\udcb31\u22efsubscript\ud835\udcb3\ud835\udc5b{\\mathcal{X}}=\\{{\\mathcal{X}}_{1},\\cdots,{\\mathcal{X}}_{n}\\}caligraphic_X = { caligraphic_X start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , \u22ef , caligraphic_X start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT } and process each subset variable separately. Thus, according to the chain rule for mutual information\u00a0Cover (1999), we have:\n\n\n\nI\u2062(\ud835\udcb3,\ud835\udcb3~)=I\u2062(\ud835\udcb31,\u22ef,\ud835\udcb3n;\ud835\udcb3~)=I\u2062(\ud835\udcb31;\ud835\udcb3~)+\u2211i=2nI\u2062(\ud835\udcb3i;\ud835\udcb3~|\ud835\udcb31,\u22ef,\ud835\udcb3i\u22121),\ud835\udc3c\ud835\udcb3~\ud835\udcb3\ud835\udc3csubscript\ud835\udcb31\u22efsubscript\ud835\udcb3\ud835\udc5b~\ud835\udcb3\ud835\udc3csubscript\ud835\udcb31~\ud835\udcb3superscriptsubscript\ud835\udc562\ud835\udc5b\ud835\udc3csubscript\ud835\udcb3\ud835\udc56conditional~\ud835\udcb3subscript\ud835\udcb31\u22efsubscript\ud835\udcb3\ud835\udc561\\displaystyle I({\\mathcal{X}},\\tilde{{\\mathcal{X}}})=I({\\mathcal{X}}_{1},%\n\\cdots,{\\mathcal{X}}_{n};\\tilde{{\\mathcal{X}}})=I({\\mathcal{X}}_{1};\\tilde{{%\n\\mathcal{X}}})+\\sum\\limits_{i=2}^{n}I({\\mathcal{X}}_{i};\\tilde{{\\mathcal{X}}}|%\n{\\mathcal{X}}_{1},\\cdots,{\\mathcal{X}}_{i-1}),italic_I ( caligraphic_X , over~ start_ARG caligraphic_X end_ARG ) = italic_I ( caligraphic_X start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , \u22ef , caligraphic_X start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ; over~ start_ARG caligraphic_X end_ARG ) = italic_I ( caligraphic_X start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ; over~ start_ARG caligraphic_X end_ARG ) + \u2211 start_POSTSUBSCRIPT italic_i = 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT italic_I ( caligraphic_X start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ; over~ start_ARG caligraphic_X end_ARG | caligraphic_X start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , \u22ef , caligraphic_X start_POSTSUBSCRIPT italic_i - 1 end_POSTSUBSCRIPT ) ,\n\n(2)\n\n\nwhich indicates that the mutual information I\u2062(\ud835\udcb3,\ud835\udcb3~)\ud835\udc3c\ud835\udcb3~\ud835\udcb3I({\\mathcal{X}},\\tilde{{\\mathcal{X}}})italic_I ( caligraphic_X , over~ start_ARG caligraphic_X end_ARG ) can be understood as the sum of the mutual information of each subset \ud835\udcb3isubscript\ud835\udcb3\ud835\udc56{\\mathcal{X}}_{i}caligraphic_X start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT and \ud835\udcb3~isubscript~\ud835\udcb3\ud835\udc56\\tilde{{\\mathcal{X}}}_{i}over~ start_ARG caligraphic_X end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT given all previous subsets.\n\n\nIn the scenario of Eq.\u00a01, the variable \ud835\udcb3\ud835\udcb3\\mathcal{X}caligraphic_X represents a long context and the variable \ud835\udcb4\ud835\udcb4\\mathcal{Y}caligraphic_Y is the output answer produced by a LLM. Thus, \ud835\udcb3~~\ud835\udcb3\\tilde{{\\mathcal{X}}}over~ start_ARG caligraphic_X end_ARG can be interpreted as the minimal necessary context from the long context \ud835\udcb3\ud835\udcb3{\\mathcal{X}}caligraphic_X given the output answer \ud835\udcb4\ud835\udcb4{\\mathcal{Y}}caligraphic_Y. Inspired by Eq.\u00a02, we can estimate an optimal \ud835\udcb3~~\ud835\udcb3\\tilde{\\mathcal{X}}over~ start_ARG caligraphic_X end_ARG using decomposed shorter contexts {\ud835\udcb31,\u2026,\ud835\udcb3n}subscript\ud835\udcb31\u2026subscript\ud835\udcb3\ud835\udc5b\\{\\mathcal{X}_{1},\\ldots,\\mathcal{X}_{n}\\}{ caligraphic_X start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , \u2026 , caligraphic_X start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT }. Thus, I\u2062(\ud835\udcb3;\ud835\udcb3~)\ud835\udc3c\ud835\udcb3~\ud835\udcb3I(\\mathcal{X};\\tilde{\\mathcal{X}})italic_I ( caligraphic_X ; over~ start_ARG caligraphic_X end_ARG ) can be computed by processing each subset \ud835\udcb3isubscript\ud835\udcb3\ud835\udc56\\mathcal{X}_{i}caligraphic_X start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT individually. However, as the number of subsets n\ud835\udc5bnitalic_n increases, accounting for all preceding subsets becomes computationally demanding. To alleviate this burden, we propose reducing the number of conditional subsets considered by replacing the entire sequence of previous subsets with a compressed surrogate \ud835\udcb3^isubscript^\ud835\udcb3\ud835\udc56\\hat{\\mathcal{X}}_{i}over^ start_ARG caligraphic_X end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT, which is iteratively derived using a compression function \ud835\udcb3^i=g\u2062(\ud835\udcb3^i\u22121,\ud835\udcb3i\u22121)subscript^\ud835\udcb3\ud835\udc56\ud835\udc54subscript^\ud835\udcb3\ud835\udc561subscript\ud835\udcb3\ud835\udc561\\hat{\\mathcal{X}}_{i}=g(\\hat{\\mathcal{X}}_{i-1},\\mathcal{X}_{i-1})over^ start_ARG caligraphic_X end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = italic_g ( over^ start_ARG caligraphic_X end_ARG start_POSTSUBSCRIPT italic_i - 1 end_POSTSUBSCRIPT , caligraphic_X start_POSTSUBSCRIPT italic_i - 1 end_POSTSUBSCRIPT ). Consequently, Eq.\u00a02 can be reformulated as follows:\n\n\n\nI(\ud835\udcb3,\ud835\udcb3~)=I(\ud835\udcb31,\u22ef,\ud835\udcb3n;\ud835\udcb3~)\u2243I(\ud835\udcb31;\ud835\udcb3~)+\u2211i=2nI(\ud835\udcb3i;\ud835\udcb3~|\ud835\udcb3^i)).I({\\mathcal{X}},\\tilde{{\\mathcal{X}}})=I({\\mathcal{X}}_{1},\\cdots,{\\mathcal{X}%\n}_{n};\\tilde{{\\mathcal{X}}})\\simeq I({\\mathcal{X}}_{1};\\tilde{{\\mathcal{X}}})+%\n\\sum\\limits_{i=2}^{n}I({\\mathcal{X}}_{i};\\tilde{{\\mathcal{X}}}|\\hat{{\\mathcal{%\nX}}}_{i})).italic_I ( caligraphic_X , over~ start_ARG caligraphic_X end_ARG ) = italic_I ( caligraphic_X start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , \u22ef , caligraphic_X start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ; over~ start_ARG caligraphic_X end_ARG ) \u2243 italic_I ( caligraphic_X start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ; over~ start_ARG caligraphic_X end_ARG ) + \u2211 start_POSTSUBSCRIPT italic_i = 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT italic_I ( caligraphic_X start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ; over~ start_ARG caligraphic_X end_ARG | over^ start_ARG caligraphic_X end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) ) .\n\n(3)\n\n\nThe equality can be upheld under two specific conditions: (1) the decomposed variables {\ud835\udcb31,\u2026,\ud835\udcb3n}subscript\ud835\udcb31\u2026subscript\ud835\udcb3\ud835\udc5b\\{\\mathcal{X}_{1},\\ldots,\\mathcal{X}_{n}\\}{ caligraphic_X start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , \u2026 , caligraphic_X start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT } are mutually independent, and (2) the compression function g\u2062(\u22c5)\ud835\udc54\u22c5g(\\cdot)italic_g ( \u22c5 ) is optimally designed, ensuring that the compressed surrogate \ud835\udcb3^isubscript^\ud835\udcb3\ud835\udc56\\hat{\\mathcal{X}}_{i}over^ start_ARG caligraphic_X end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT encapsulates all relevant information from the preceding subsets with respect to \ud835\udcb3~~\ud835\udcb3\\tilde{\\mathcal{X}}over~ start_ARG caligraphic_X end_ARG. Otherwise, I\u2062(\ud835\udcb3,\ud835\udcb3~)\ud835\udc3c\ud835\udcb3~\ud835\udcb3I({\\mathcal{X}},\\tilde{{\\mathcal{X}}})italic_I ( caligraphic_X , over~ start_ARG caligraphic_X end_ARG ) can only be approximately estimated.\n\n\n\nEmpirical Analysis\n\nFigure 2: Pilot Study Across Various Tasks: In the Brute-force setting, the entire context is processed by GPT-4-128K. In the LC-Boost setting, the maximum context length is restricted to 4K, and LC-Boost is utilized to solve the long-context problem with short context. \n\n\nTo empirically assess the accuracy of estimating the minimal necessary context \ud835\udcb3~~\ud835\udcb3\\tilde{\\mathcal{X}}over~ start_ARG caligraphic_X end_ARG using decomposed short contexts {\ud835\udcb31,\u2026,\ud835\udcb3n}subscript\ud835\udcb31\u2026subscript\ud835\udcb3\ud835\udc5b\\{\\mathcal{X}_{1},\\ldots,\\mathcal{X}_{n}\\}{ caligraphic_X start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , \u2026 , caligraphic_X start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT }, we conduct pilot experiments across various tasks requiring long contexts. Specifically, we utilize GPT-4-128K to perform these tasks in two settings: (1) feeding the entire long context into GPT-4-128K in a brute-force manner, instructing the model to directly produce the output answer, and (2) decomposing the full context into short contexts and applying the methods defined in Eq.\u00a03 to approximate \ud835\udcb3~~\ud835\udcb3\\tilde{\\mathcal{X}}over~ start_ARG caligraphic_X end_ARG, which then guides the model to produce the final output (the LC-Boost setting).\n\n\nFigure\u00a02 presents the experiment results, which generally indicate that LC-Boost consistently performs as well as or better than the brute-force setting. In particular, for tasks such as QA, few-shot learning, and synthetic tasks, LC-Boost outperforms the brute-force setting. This is because the decomposed short contexts for these tasks are more likely to be mutually independent given the input query which can be adequately supported by a few extracted contexts from the long context. By precisely\nlocating these supported context, it can filter out irrelevant context of \ud835\udcb3\ud835\udcb3{\\mathcal{X}}caligraphic_X that might otherwise undermine task performance.\nFor tasks like summarization and code completion, the inherent properties of these tasks require considering the mutual dependencies among all decomposed short contexts, making the LC-Boost setting more challenging. However, as discussed in Eq.\u00a03, when the compression function g\u2062(\u22c5)\ud835\udc54\u22c5g(\\cdot)italic_g ( \u22c5 ) is optimal, we can achieve the optimal \ud835\udcb3~~\ud835\udcb3\\tilde{\\mathcal{X}}over~ start_ARG caligraphic_X end_ARG. GPT-4 serves as such a strong compression function, ensuring that the compressed surrogate \ud835\udcb3^isubscript^\ud835\udcb3\ud835\udc56\\hat{\\mathcal{X}}_{i}over^ start_ARG caligraphic_X end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT is well-estimated. Consequently, in these tasks, LC-Boost achieves performance that is equal to or slightly better than the brute-force setting.\n\n\nThrough theoretical analysis, we can posit that long-context tasks are short-context solvable if we can estimate a better minimal necessary context \ud835\udcb3~~\ud835\udcb3\\tilde{\\mathcal{X}}over~ start_ARG caligraphic_X end_ARG from the decomposed short contexts {\ud835\udcb31,\u2026,\ud835\udcb3n}subscript\ud835\udcb31\u2026subscript\ud835\udcb3\ud835\udc5b\\{\\mathcal{X}_{1},\\ldots,\\mathcal{X}_{n}\\}{ caligraphic_X start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , \u2026 , caligraphic_X start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT } than from the long context \ud835\udcb3\ud835\udcb3\\mathcal{X}caligraphic_X. Empirical analysis supports this assumption, demonstrating that in most cases, the estimation error of deriving \ud835\udcb3~~\ud835\udcb3\\tilde{\\mathcal{X}}over~ start_ARG caligraphic_X end_ARG from the long context \ud835\udcb3\ud835\udcb3\\mathcal{X}caligraphic_X is often larger than from the decomposed short contexts {\ud835\udcb31,\u2026,\ud835\udcb3n}subscript\ud835\udcb31\u2026subscript\ud835\udcb3\ud835\udc5b\\{\\mathcal{X}_{1},\\ldots,\\mathcal{X}_{n}\\}{ caligraphic_X start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , \u2026 , caligraphic_X start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT }. This indicates that using short contexts can be comparatively more advantageous than using the full context. Therefore, we can validate our argument in Section\u00a01: most long-context tasks, if not all, are short-context solvable.\n\n\n\n\n\n2.3 The Proposed Method: LC-Boost\n\nWe propose a method called LC-Boost, which utilizes short LLMs to solve general long-context tasks. LC-Boost begins with an input query q\ud835\udc5eqitalic_q and a long context \ud835\udcb3\ud835\udcb3\\mathcal{X}caligraphic_X, with the goal of producing an output answer \ud835\udcb4\ud835\udcb4\\mathcal{Y}caligraphic_Y. Since the underlying LLM in LC-Boost has a limited context size\u00a0(we limit LC-Boost working with 4K context length), directly generating the output answer \ud835\udcb4\ud835\udcb4\\mathcal{Y}caligraphic_Y is infeasible for long-context tasks. To address this, we propose solving long-context tasks by strategically understanding the decomposed short contexts \ud835\udcb3={\ud835\udcb31,\u22ef,\ud835\udcb3n}\ud835\udcb3subscript\ud835\udcb31\u22efsubscript\ud835\udcb3\ud835\udc5b\\mathcal{X}=\\{\\mathcal{X}_{1},\\cdots,\\mathcal{X}_{n}\\}caligraphic_X = { caligraphic_X start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , \u22ef , caligraphic_X start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT }. From these short contexts, we aim to extract the minimal necessary context \ud835\udcb3~~\ud835\udcb3\\tilde{\\mathcal{X}}over~ start_ARG caligraphic_X end_ARG to support the generation of the output answer \ud835\udcb4\ud835\udcb4\\mathcal{Y}caligraphic_Y.\n\n\nLC-Boost achieves this goal through a decision-making process involving iterative interactions between LC-Boost and the decomposed short contexts {\ud835\udcb31,\u22ef,\ud835\udcb3n}subscript\ud835\udcb31\u22efsubscript\ud835\udcb3\ud835\udc5b\\{\\mathcal{X}_{1},\\cdots,\\mathcal{X}_{n}\\}{ caligraphic_X start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , \u22ef , caligraphic_X start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT } with respect to the input query q\ud835\udc5eqitalic_q. In the process, LC-Boost interact with each short context \ud835\udcb3isubscript\ud835\udcb3\ud835\udc56\\mathcal{X}_{i}caligraphic_X start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT, employing two types of actions: information access and information utilization.\n\n\nWe denote an action at time step i\ud835\udc56iitalic_i by aisubscript\ud835\udc4e\ud835\udc56a_{i}italic_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT and denote the relevant context LC-Boost obtains from the i\ud835\udc56iitalic_i-th short context \ud835\udcb3isubscript\ud835\udcb3\ud835\udc56{\\mathcal{X}}_{i}caligraphic_X start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT by \ud835\udcb3~isubscript~\ud835\udcb3\ud835\udc56\\tilde{{\\mathcal{X}}}_{i}over~ start_ARG caligraphic_X end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT\nThe action aisubscript\ud835\udc4e\ud835\udc56a_{i}italic_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT is predicted by considering the current short context \ud835\udcb3isubscript\ud835\udcb3\ud835\udc56{\\mathcal{X}}_{i}caligraphic_X start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT, the input query q\ud835\udc5eqitalic_q, as well as all previous extracted relevant information \ud835\udcb3~1:i\u22121subscript~\ud835\udcb3:1\ud835\udc561\\tilde{{\\mathcal{X}}}_{1:i-1}over~ start_ARG caligraphic_X end_ARG start_POSTSUBSCRIPT 1 : italic_i - 1 end_POSTSUBSCRIPT: ai=\u03b3\u2062(q,\ud835\udcb3i|\ud835\udcb3~1:i\u22121),subscript\ud835\udc4e\ud835\udc56\ud835\udefe\ud835\udc5econditionalsubscript\ud835\udcb3\ud835\udc56subscript~\ud835\udcb3:1\ud835\udc561a_{i}=\\gamma(q,{\\mathcal{X}}_{i}|\\tilde{{\\mathcal{X}}}_{1:i-1}),italic_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = italic_\u03b3 ( italic_q , caligraphic_X start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT | over~ start_ARG caligraphic_X end_ARG start_POSTSUBSCRIPT 1 : italic_i - 1 end_POSTSUBSCRIPT ) ,\nwhere \u03b3\u2062(\u22c5)\ud835\udefe\u22c5\\gamma(\\cdot)italic_\u03b3 ( \u22c5 ) denotes LC-Boost\u2019s underlying LLM.\n\n\nPredicting the action aisubscript\ud835\udc4e\ud835\udc56a_{i}italic_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT in a continuous space is challenging as it requires the underling model to reason about highly implicit relations among the input query, the current context, and the previous contexts. Therefore, we define a discrete action space \ud835\udc9c\ud835\udc9c{\\mathcal{A}}caligraphic_A comprising:\n(1)\u00a0[Task Understanding]: analyzing the query and task for initialization;\n(2)\u00a0[Retrieve]: accessing text evidence by a retrieval method;\n(3)\u00a0[Move]: accessing the next short text context directly;\nThese two are information access actions which define the LC-Boost\u2019s trajectory to access short contexts.\n(4)\u00a0[Append]: generating relevant context \ud835\udcb3~isubscript~\ud835\udcb3\ud835\udc56\\tilde{{\\mathcal{X}}}_{i}over~ start_ARG caligraphic_X end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT independently, denoting by \ud835\udcb3~i=ai\u2062(\ud835\udcb3i)subscript~\ud835\udcb3\ud835\udc56subscript\ud835\udc4e\ud835\udc56subscript\ud835\udcb3\ud835\udc56\\tilde{{\\mathcal{X}}}_{i}=a_{i}({\\mathcal{X}}_{i})over~ start_ARG caligraphic_X end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = italic_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( caligraphic_X start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT );\n(5)\u00a0[Merge]: generating relevant context \ud835\udcb3~isubscript~\ud835\udcb3\ud835\udc56\\tilde{{\\mathcal{X}}}_{i}over~ start_ARG caligraphic_X end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT with respect to previous extracted relevant information, denoting by \ud835\udcb3~i=ai\u2062(\ud835\udcb3i|\ud835\udcb3~1:i\u22121)subscript~\ud835\udcb3\ud835\udc56subscript\ud835\udc4e\ud835\udc56conditionalsubscript\ud835\udcb3\ud835\udc56subscript~\ud835\udcb3:1\ud835\udc561\\tilde{{\\mathcal{X}}}_{i}=a_{i}({\\mathcal{X}}_{i}|\\tilde{{\\mathcal{X}}}_{1:i-1})over~ start_ARG caligraphic_X end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = italic_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( caligraphic_X start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT | over~ start_ARG caligraphic_X end_ARG start_POSTSUBSCRIPT 1 : italic_i - 1 end_POSTSUBSCRIPT );\n(6)\u00a0[Answer]: answering the user query and returning;\n(7)\u00a0[Aggregation]: aggregating all relevant information and returning.\nWe define our LC-Boost frame in Algorithm\u00a01.\n\n\nAlgorithm 1  LC-Boost Framework\n\n\n1:\u00a0\u00a0Input: Input query q\ud835\udc5eqitalic_q, long context \ud835\udcb3\ud835\udcb3\\mathcal{X}caligraphic_X\n\n\n\n2:\u00a0\u00a0Output: Answer \ud835\udcb4\ud835\udcb4\\mathcal{Y}caligraphic_Y\n\n\n\n3:\u00a0\u00a0Decompose long context\u2062\ud835\udcb3\u2190{\ud835\udcb31,\u22ef,\ud835\udcb3n}\u2190long context\ud835\udcb3subscript\ud835\udcb31\u22efsubscript\ud835\udcb3\ud835\udc5b\\text{long context}~{}{\\mathcal{X}}\\leftarrow\\{{\\mathcal{X}}_{1},\\cdots,{%\n\\mathcal{X}}_{n}\\}long context caligraphic_X \u2190 { caligraphic_X start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , \u22ef , caligraphic_X start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT }\n\n\n\n4:\u00a0\u00a0Initialize extracted relevant context\u2062\ud835\udcb3~0\u2190None\u2190extracted relevant contextsubscript~\ud835\udcb30None\\text{extracted relevant context}~{}\\tilde{{\\mathcal{X}}}_{0}\\leftarrow\\text{None}extracted relevant context over~ start_ARG caligraphic_X end_ARG start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT \u2190 None\n\n\n\n5:\u00a0\u00a0Perform [Task Understanding]\n\n\n\n6:\u00a0\u00a0while\u00a0i\u2264n\ud835\udc56\ud835\udc5bi\\leq nitalic_i \u2264 italic_n\u00a0do\n\n\n\n7:\u00a0\u00a0\u00a0\u00a0\u00a0Select an action ai\u2190ai=\u03b3\u2062(q,\ud835\udcb3i|\ud835\udcb3~1:i\u22121),ai\u2208\ud835\udc9cformulae-sequence\u2190subscript\ud835\udc4e\ud835\udc56subscript\ud835\udc4e\ud835\udc56\ud835\udefe\ud835\udc5econditionalsubscript\ud835\udcb3\ud835\udc56subscript~\ud835\udcb3:1\ud835\udc561subscript\ud835\udc4e\ud835\udc56\ud835\udc9ca_{i}\\leftarrow a_{i}=\\gamma(q,{\\mathcal{X}}_{i}|\\tilde{{\\mathcal{X}}}_{1:i-1}%\n),~{}a_{i}\\in{\\mathcal{A}}italic_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT \u2190 italic_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = italic_\u03b3 ( italic_q , caligraphic_X start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT | over~ start_ARG caligraphic_X end_ARG start_POSTSUBSCRIPT 1 : italic_i - 1 end_POSTSUBSCRIPT ) , italic_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT \u2208 caligraphic_A\n\n\n\n8:\u00a0\u00a0\u00a0\u00a0\u00a0if aisubscript\ud835\udc4e\ud835\udc56a_{i}italic_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT is [Move] then i\u2190i+1\u2190\ud835\udc56\ud835\udc561i\\leftarrow i+1italic_i \u2190 italic_i + 1, continue\n\n\n\n9:\u00a0\u00a0\u00a0\u00a0\u00a0if aisubscript\ud835\udc4e\ud835\udc56a_{i}italic_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT is [Retrieve] then retrieve evidence from \ud835\udcb3={\ud835\udcb31,\u22ef,\ud835\udcb3n}\ud835\udcb3subscript\ud835\udcb31\u22efsubscript\ud835\udcb3\ud835\udc5b{\\mathcal{X}}=\\{{\\mathcal{X}}_{1},\\cdots,{\\mathcal{X}}_{n}\\}caligraphic_X = { caligraphic_X start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , \u22ef , caligraphic_X start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT }\n\n\n\n10:\u00a0\u00a0\u00a0\u00a0\u00a0if aisubscript\ud835\udc4e\ud835\udc56a_{i}italic_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT is [Append] then generate relevant context by \ud835\udcb3~i=ai\u2062(\ud835\udcb3i)subscript~\ud835\udcb3\ud835\udc56subscript\ud835\udc4e\ud835\udc56subscript\ud835\udcb3\ud835\udc56\\tilde{{\\mathcal{X}}}_{i}=a_{i}({\\mathcal{X}}_{i})over~ start_ARG caligraphic_X end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = italic_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( caligraphic_X start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT )\n\n\n\n11:\u00a0\u00a0\u00a0\u00a0\u00a0if aisubscript\ud835\udc4e\ud835\udc56a_{i}italic_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT is [Merge] then generate relevant context by \ud835\udcb3~i=ai\u2062(\ud835\udcb3i|\ud835\udcb3~1:i\u22121)subscript~\ud835\udcb3\ud835\udc56subscript\ud835\udc4e\ud835\udc56conditionalsubscript\ud835\udcb3\ud835\udc56subscript~\ud835\udcb3:1\ud835\udc561\\tilde{{\\mathcal{X}}}_{i}=a_{i}({\\mathcal{X}}_{i}|\\tilde{{\\mathcal{X}}}_{1:i-1})over~ start_ARG caligraphic_X end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = italic_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( caligraphic_X start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT | over~ start_ARG caligraphic_X end_ARG start_POSTSUBSCRIPT 1 : italic_i - 1 end_POSTSUBSCRIPT )\n\n\n\n12:\u00a0\u00a0\u00a0\u00a0\u00a0if aisubscript\ud835\udc4e\ud835\udc56a_{i}italic_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT \u2208{\\in\\{\u2208 {[Answer],[Aggregation]}}\\}} then generate answer \ud835\udcb4=\u03b3\u2062(q,\ud835\udcb3~1:i)\ud835\udcb4\ud835\udefe\ud835\udc5esubscript~\ud835\udcb3:1\ud835\udc56{\\mathcal{Y}}=\\gamma(q,\\tilde{{\\mathcal{X}}}_{1:i})caligraphic_Y = italic_\u03b3 ( italic_q , over~ start_ARG caligraphic_X end_ARG start_POSTSUBSCRIPT 1 : italic_i end_POSTSUBSCRIPT ), break\n\n\n\n13:\u00a0\u00a0\u00a0\u00a0\u00a0i\u2190i+1\u2190\ud835\udc56\ud835\udc561i\\leftarrow i+1italic_i \u2190 italic_i + 1\n\n\n\n14:\u00a0\u00a0end\u00a0while\n\n\n15:\u00a0\u00a0return answer \ud835\udcb4\ud835\udcb4{\\mathcal{Y}}caligraphic_Y\n\n\n\n\n\nThough the pre-defined action space \ud835\udc9c\ud835\udc9c\\mathcal{A}caligraphic_A comprises only seven actions, LC-Boost serves as a general framework sufficient for solving most long-context tasks. This effectiveness is based on the following reasons: (1)\u00a0Flexible accessibility:\u00a0By utilizing both [Retrieve] and [Move] actions, LC-Boost can access any short context \ud835\udcb3i\u2208\ud835\udcb3subscript\ud835\udcb3\ud835\udc56\ud835\udcb3\\mathcal{X}_{i}\\in\\mathcal{X}caligraphic_X start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT \u2208 caligraphic_X in a flexible trajectory, avoiding the need to browse the entire long context. This makes the information accessing process more efficient. (2)\u00a0Accurate information acquisition:\u00a0Through the [Append] and [Merge] actions, LC-Boost can either independently extract relevant information from the current short context, appending it to previously extracted information, or merge the current relevant information into the previous relevant information. This capability allows LC-Boost to acquire relevant information in a compatible manner, making it adaptable to many knowledge-intensive tasks. and (3)\u00a0Dynamic answering:\u00a0Using the [Answer] and [Aggregate] actions, LC-Boost can dynamically utilize the acquired relevant information to produce the target form of the answer (e.g., a short answer for QA tasks via the [Answer] action, or a long answer for summarization tasks via the [Aggregate] action).\n\n\nIn our pilot study depicted in Figure\u00a02, we observe that while GPT-3.5 serves as an inferior foundation model compared to GPT-4, it still demonstrates significant effectiveness when incorporated with LC-Boost. Given considerations of efficiency and cost-effectiveness, we employ GPT-3.5 as the foundation model for LC-Boost in the subsequent experiments. Besides, we show the prompts used in LC-Boost in Appendix\u00a0B.\n\n\n",
      "3 Experiments": "\n\n3 Experiments\n\n\n3.1 Experiment Settings\n\nWe evaluate LC-Boost and baseline models on 12 datasets, including:\n(1) Single-Doc QA: NarrativeQA\u00a0(Ko\u010disk\u00fd et\u00a0al., 2017), Qasper\u00a0(Dasigi et\u00a0al., 2021), and MultiFieldQA\u00a0(Bai et\u00a0al., 2023).\n(2) Multi-Doc QA: HotpotQA\u00a0(Yang et\u00a0al., 2018), 2WikiMQA\u00a0(Ho et\u00a0al., 2020), and MuSiQue\u00a0(Trivedi et\u00a0al., 2022).\n(3) Summarization: GovReport\u00a0(Huang et\u00a0al., 2021) and MultiNews\u00a0(Fabbri et\u00a0al., 2019).\n(4) Few-shot Learning: SAMSum\u00a0(Gliwa et\u00a0al., 2019).\n(5) Synthetic Task: Passage Count\u00a0(Bai et\u00a0al., 2023) and Self-Constructed Dataset.\n(6) Code Completion: LCC\u00a0(Guo et\u00a0al., 2023).\nMore details about the evaluation datasets and metrics are introduced in Appendix\u00a0A.\n\n\nWe compare our LC-Boost with three types of models: (1) Short LLMs (defined as with context length <<< 32K): Llama2-7B-Chat-4K\u00a0(Touvron et\u00a0al., 2023b), Llama3-8B-Instruct-8K and Vicuna-v1.5-7B-16K\u00a0(Chiang et\u00a0al., 2023); (2) Long LLMs (defined as with context length \u2265\\geq\u2265 32K): LongChat-v1.5-7B-32K\u00a0(Li et\u00a0al., 2023b), Mistral-7B-Instruct-v0.2-32K\u00a0(Jiang et\u00a0al., 2023a), Llama3-8B-80K\u00a0Zhang et\u00a0al. (2024b), Phi-3-mini-128K\u00a0(Abdin et\u00a0al., 2024) and Yi-9B-200K\u00a0(AI et\u00a0al., 2024); (3)\u00a0Closed-Source LLMs: DeepSeek-v2\u00a0(236B MoE model, ranks top-tier in MT-Bench)\u00a0(DeepSeek-AI, 2024), Claude-3-Haiku222https://www.anthropic.com/claude and GPT-3.5-turbo-16K333https://platform.openai.com/docs/models. In the experiments, if the context length exceed the model\u2019s length limit, following\u00a0Bai et\u00a0al. (2023), we truncate the context from the middle since the front and end of the context may contain crucial information. We provide further implementation details in Appendix\u00a0B.\n\n\n\n\n3.2 Main Results\n\nTable 1: Main experiment results. The best results are in bold and the secondary results are marked with underline. We report the average scores (%) on the main tasks. The detailed scores over all dataset are shown in Table\u00a03.\n\n\nModels\nSingle-Doc\nMulti-Doc\nSumm.\nFew-shot\nSynthetic\nCode\n\n\nShort LLMs (Context Length <<< 32K)\n\n\n\n\n\n\nLlama2-7B-Chat-4K\n24.9\n22.5\n26.6\n40.7\n6.3\n52.4\n\n\nLlama3-8B-Instruct-8K\n37.3\n36.0\n26.5\n42.7\n15.0\n57.5\n\n\nVicuna-v1.5-7B-16K\n28.0\n18.6\n27.5\n40.8\n8.9\n51.0\n\n\nLong LLMs (Context Length \u2265\\geq\u2265 32K)\n\n\n\n\n\n\nLongChat-v1.5-7B-32K\n28.7\n20.6\n28.6\n34.2\n6.8\n53.0\n\n\nMistral-7B-Instruct-v0.2-32K\n31.9\n26.0\n29.3\n43.0\n14.0\n55.4\n\n\nLlama3-8B-80K\n43.6\n43.1\n30.2\n42.9\n19.6\n53.6\n\n\nPhi-3-mini-128K\n33.5\n38.2\n28.8\n36.0\n19.9\n60.1\n\n\nYi-9B-200K\n29.6\n38.7\n28.4\n14.6\n6.5\n72.1\n\n\nClosed-Source LLMs\n\n\n\n\n\n\nDeepSeek-v2 (32K)\n37.6\n49.1\n30.8\n39.3\n14.5\n37.0\n\n\nClaude-3-Haiku (200K)\n41.9\n45.4\n30.1\n7.2\n25.5\n16.9\n\n\nGPT-3.5-turbo-16K\n39.8\n38.7\n28.1\n41.7\n18.7\n54.7\n\n\nLC-Boost\u00a0(4K)\n47.8\n56.4\n31.8\n44.1\n27.5\n59.0\n\n\n\n\nTable\u00a01 shows the overall experimental results for all models across all tasks. From the table, we derive several key findings:\nFirst, LC-Boost, with a context length of 4K, outperforms all baseline models in all tasks except for the Code Completion task. This result verifies LC-Boost\u2019s capability to effectively solve long-context tasks by strategically processing decomposed short contexts.\nSecond, long LLMs generally perform better than short LLMs, indicating the effectiveness of fine-tuning LLMs to adapt to long contexts. However, the performance of long LLMs is not consistently stable across different tasks. For example, Yi-9B-200K excels in the Code Completion task but does not show consistent performance in other tasks such as single-doc QA, few-shot learning, and synthetic tasks. This inconsistency suggests that adapting LLMs to long contexts may compromise their general abilities.\nLast, LC-Boost consistently surpasses its underlying LLM, GPT-3.5-turbo-16K, across all tasks by a notable margin. This demonstrates that LC-Boost can achieve improved performance while simultaneously reducing resource costs, making LC-Boost an environmentally friendly method.\n\n\n\n\n3.3 Ablation Study: Dynamic is Important\n\nTo investigate the necessity of LC-Boost\u2019s design, we conduct ablation studies by changing LC-Boost\u2019s action space \ud835\udc9c\ud835\udc9c\\mathcal{A}caligraphic_A, resulting in different information acquisition strategies. We experiment with the following settings: (1)\u00a0[Retrieve] only: Directly retrieve the most relevant short context. (2)\u00a0[Merge] only: Sequentially process all short contexts while considering the previously processed context. (3)\u00a0[Append] only: Sequentially process all short contexts independently. (4)\u00a0[Merge] & [Move]: Selectively process short contexts while considering the already processed context. (6)\u00a0[Append] & [Move]: Selectively process short contexts independently. (7): [Retrieve] & [Move]: Retrieve the top-k\ud835\udc58kitalic_k relevant short contexts and selectively process a few of them. (8): Brute-force: Directly produce the answer based on the entire long context. (9)\u00a0Random: For each short context, randomly select an action. Based on the acquired information from each strategy, LC-Boost then selects either the [Answer] or [Aggregation] action to produce the final answer.\n\n\nFigure\u00a03 illustrates the results, from which we find that: (1)\u00a0Compared to fixed processing strategies, LC-Boost customizes the action trajectory for each query, resulting in notable performance improvements. This finding emphasizes the importance of the dynamic capabilities of LC-Boost. (2)\u00a0LC-Boost is particularly effective in single-doc QA and multi-doc QA tasks, as it can accurately select the minimal necessary context required to answer the input query, filtering out irrelevant information from the long context. (3)\u00a0In the few-shot learning task, LC-Boost does not significantly outperform the fixed strategies. This is attributed to the numerous in-context examples provided within the task, which offer substantial guidance, thus diminishing the impact of the number of in-context examples on the final performance.\n\n\nFigure 3: Performance comparison on different context processing strategies in the ablation study. NarrativeQA\u00a0(left) is a single-doc QA task. HotpotQA\u00a0(middle) is a multi-doc QA task. SamSUM\u00a0(right) is a few-shot learning task.\n\n\n\n\n3.4 Case Study: Model Behavior Analysis on Self-Construct Dataset\n\nTable 2: Case study on the self-constructed dataset. Correct answers are marked in teal, incorrect answers in red, and ambiguous answers in orange.\n\n\n\n\nQuery: How many papers in ACL 2023 only have one author?\n\n\n\n\n\n\nContext: Full accepted paper list in ACL 2023 main conference. (Context length: 45K)\n\n\n\n\n\n\nGround-truth target: 8 papers\n\n\n\n\n\n\nPhi-3-mini-128K: 11 papers GPT-3.5-turbo-16K: 0 papers Claude-3-Haiku-200K: 1 papers (Acc. Score: 0)\n\n\n\n\n\n\nLC-Boost\u2019s action trajectory: [Task Reasoning] \u2192\u2192\\rightarrow\u2192 [Append]\u2192\u22ef\u2192absent\u22ef\\rightarrow\\cdots\u2192 \u22ef \u2192\u2192\\rightarrow\u2192 [Append]\u2192\u2192\\rightarrow\u2192 [Aggregation]\n\n\n\n\n\n\nLC-Boost: 8 papers (Acc. Score: 1)\n\n\n\n\n\n\nQuery: List all people names that are petrified, separated by comma.\n\n\n\n\n\n\nContext: Full content of Harry Potter and the Chamber of Secrets. (Context length: 122.6K)\n\n\n\n\n\n\nGround-truth target: Colin Creevey, Justin Finch-Fletchley, Penelope Clearwater, Hermione Granger\n\n\n\n\n\n\nPhi-3-mini-128K: Hermione Granger, Ginny Weasley, Mrs Norris (F1-Score: 0.29)\n\n\n\n\n\n\nGPT-3.5-turbo-16K: Colin Creevey, Mrs Norris (F1-Score: 0.33)\n\n\n\n\n\n\nClaude-3-Haiku-200K: Nick, Hermione, Ron (F1-Score: 0.18)\n\n\n\n\n\n\nLC-Boost\u2019s action trajectory: [Task Reasoning] \u2192\u2192\\rightarrow\u2192 [Move]\u2192\u22ef\u2192absent\u22ef\\rightarrow\\cdots\u2192 \u22ef \u2192\u2192\\rightarrow\u2192 [Merge]\u2192\u2192\\rightarrow\u2192 [Aggregation]\n\n\n\n\n\n\nLC-Boost: Colin Creevey, Penelope Clearwater, Hermione Granger, Nick, Mrs Norris (F1-Score: 0.71)\n\n\n\n\n\n\nIn Table\u00a02, we present two case studies from the self-constructed dataset. These cases are particularly challenging as they require reasoning across the entire long context. Despite having sufficient context size, LLMs struggle to generate correct responses. In contrast, LC-Boost dynamically customizes solutions for each case, thereby effectively solving the problems using a shorter context length.\n\n\nFor the first query, LC-Boost performs [Append] or [Move] actions across all short context along with a rewritten query, \"Extract paper information in the following list that have only one author,\" derived via [Task Reasoning]. After processing all short contexts, LC-Boost employs the [Aggregation] action to compile the final answer. This approach simplifies the task compared to directly extracting a numeric answer from the entire long context, mimicking the human process of reading comprehension and thereby producing accurate results.\n\n\nIn the second case, the query necessitates conditional reasoning on each short context. As highlighted in previous research (Liu et\u00a0al., 2023), reasoning directly from the entire context risks losing crucial information, particularly in the middle of the long context. Thus LLMs tend to miss key details such as people\u2019s names. LC-Boost addresses this issue by processing only one short context at a step where it extracts information from arbitrary position of the long text with equal accuracy.\nAdditionally, answers marked in orange include non-human names (e.g., cat, ghost) that are misconstrued as people, illustrating a common challenge where models fail to differentiate in-depth entity properties.\n\n\n\n\n3.5 Context be Short, Energy be Saved!\n\nRecently, we have witnessed the remarkable success of LLMs, which are becoming an indispensable part of our daily lives. We believe that in the near future, LLMs will become as ubiquitous as electricity or gas supply, serving as fundamental infrastructure in human society. At that point, the energy consumption of LLMs will emerge as a significant environmental concern. Therefore, it is imperative for the research community to focus on reducing the energy consumption associated with these models.\nFigure\u00a04 presents an analysis of energy consumption, comparing the brute-force approach with our LC-Boost method. The y\ud835\udc66yitalic_y-axis is measured in Joules. The theoretical energy consumption is estimated for 7B LLMs across varying context lengths. We roughly estimate the energy consumption using the formula (Total Float Operation312\u2062\u00a0TFLOPS)\u00d7400\u2062WTotal Float Operation312\u00a0TFLOPS400\ud835\udc4a\\left(\\frac{\\text{Total Float Operation}}{312\\text{ TFLOPS}}\\right)\\times 400W( divide start_ARG Total Float Operation end_ARG start_ARG 312 TFLOPS end_ARG ) \u00d7 400 italic_W, assuming the use of an A100 GPU with a compute capability of 312 TFLOPS for BFLOAT16 operations and a maximum TDP of 400W444The calculation of total float operations is based on the method outlined in https://www.harmdevries.com/post/context-length/. The practical energy consumption is estimated by recording the GPU time and GPU power during inference with different context lengths. We use a Llama2-7B-128K\u00a0(Peng et\u00a0al., 2023) and a Llama2-7B-chat-4K\u00a0(Touvron et\u00a0al., 2023a) for the brute-force setting and LC-Boost, respectively.\n\n\nFigure 4: Energy consumption analysis.\n\n\nFigure\u00a04 clearly indicates that longer context lengths significantly increase energy consumption with the brute-force method, especially evident in practical measurements. This difference is primarily due to the need to distribute sequence activation tensors across multiple GPUs in practical experiment, with tensor I/O exacerbating inference latency and thereby inflating energy costs. In contrast, our LC-Boost method, working with 4K context lengths, shows only a mild increase in energy consumption across contexts, thereby confirming its energy efficiency while maintaining comparable or superior performance on long-context tasks. We also provide an analysis on token consumption in Appendix\u00a0C.\n\n\n",
      "4 Related Works": "\n\n4 Related Works\n\nDealing with long contexts is a fundamental research problem for LLMs, as many real-world applications involve long-context inputs (Li et\u00a0al., 2023a; Fu et\u00a0al., 2024). The most direct approach to address long-context tasks is to increase the working context size of LLMs (Abdin et\u00a0al., 2024; AI et\u00a0al., 2024; Li et\u00a0al., 2023a; Cai et\u00a0al., 2024). A year ago, significant research efforts focused on extending the working context size of LLMs from 4K to 32K (Jiang et\u00a0al., 2023a; Li et\u00a0al., 2023b; Chen et\u00a0al., 2023a; Du et\u00a0al., 2022). Currently, many popular open-source and close-source LLMs still operate with a context size under 32K (Touvron et\u00a0al., 2023a; OpenAI, 2023), such as GPT-3.5-turbo, which has a 16K context length. Recently, research has shifted towards extending LLMs\u2019 working context to the million-level. Notably, GPT-4 was updated to a 128K context length not long ago, and the newly released GPT-4o also operates with a 128K context. Moreover, several recent open-source LLMs have been introduced with context lengths exceeding 100K, for example, the Yi series model supports up to 200K (AI et\u00a0al., 2024), and the Phi-3 model operates with 128K (Abdin et\u00a0al., 2024).\n\n\nInstead of merely increasing the context length, another approach to address long-context tasks involves extracting a short surrogate context from the full context. This includes techniques like retrieval-augmented generation (RAG) and context refinement methods (Izacard and Grave, 2021a; Gao et\u00a0al., 2024; Wang et\u00a0al., 2023; Qian et\u00a0al., 2024). However, many of these methods utilize task-specific strategies to manage the long context. For instance, RAG methods often deploy retrievers to select relevant context chunks as supporting evidence (Izacard and Grave, 2021b; Xu et\u00a0al., 2023b; Jiang et\u00a0al., 2023b). Recent studies have criticized the chunking process in RAG for undermining the semantic coherence of the long context and have proposed chunking-free methods to refine the long context into a concise surrogate context (Qian et\u00a0al., 2024; Luo et\u00a0al., 2024).\nFurthermore, some studies have also explored sequential processing strategies, such as Ratner et\u00a0al. (2022) and Xu et\u00a0al. (2023a), to sequentially process the context in a manner that preserves its integrity.\n\n\nLastly, reasoning-based methods also show significant potential for addressing long context tasks (Nakano et\u00a0al., 2022; Yang et\u00a0al., 2023; Driess et\u00a0al., 2023). These methods predominantly employ a decision-making process to navigate through the long context sequentially, utilizing reasoning techniques such as in-context learning (Dong et\u00a0al., 2022), chain-of-thought (Wei et\u00a0al., 2022), and self-reflection (Shinn et\u00a0al., 2023). In this paper, LC-Boost incorporates a decision-making process that dynamically customizes the action trajectory for each query, thereby offering considerable flexibility in accessing and leveraging information to produce the final output answer.\n\n",
      "5 Conclusion": "\n\n5 Conclusion\n\nIn this paper, we argue that most long-context tasks are short-context solvable, and we validate this claim through both theoretical and empirical analysis. We propose a method called LC-Boost to solve long-context tasks by decomposing the long context into short contexts and processing them using a decision-making process. We conduct experiments on 12 datasets to compare LC-Boost with long LLMs and other baseline models. Empirical results verify LC-Boost\u2019s effectiveness in solving long-context tasks. Additionally, we discuss the energy consumption of LC-Boost versus long LLMs, demonstrating that LC-Boost can achieve comparable performance with significantly less energy consumption. In Appendix\u00a0D, we also discuss the limitations and broader impact of this paper.\n\n\n",
      "Appendix A More details of the Datasets": "\n\nAppendix A More details of the Datasets\n\nTable 4: Statistical information of the datasets utilized in this paper.\n\n\nDataset\nNarrative\nQasper\nMultiField\nHotpot\nMuSiQue\n2Wiki\n\n\nNum of Samples\n200\n200\n150\n200\n200\n200\n\n\nAve. Length\n18,409\n3,619\n4,559\n9,151\n11,214\n4,887\n\n\nMetric\nF1\nF1\nF1\nF1\nF1\nF1\n\n\nDataset\nGovReport\nMultiNews\nSAMSum\nPCount\nLCC\nSelf\n\n\nNum of Samples\n200\n200\n200\n200\n500\n32\n\n\nAve. Length\n8,734\n2,113\n6,258\n11,141\n1,235\n39,420\n\n\nMetric\nRouge-L\nRouge-L\nRouge-L\nAccuracy\nEdit Sim\nF1&Accuracy\n\n\n\n\nWe evaluated all models on 12 datasets, as shown in Table\u00a04. Most of these datasets are provided by the LongBench benchmark\u00a0[Bai et\u00a0al., 2023]. Following LongBench, we used F1-score, accuracy, and edit similarity as the evaluation metrics. Additionally, we manually annotated a self-constructed dataset comprising long contexts from practical scenarios, such as the full schedule of the Olympic Games and the complete list of accepted papers at ACL. The queries in the self-constructed dataset involve reasoning over the entire long context. For example, \u201cWho has the most accepted papers at ACL 2023?\u201d These queries require the model to accurately understand the long context and perform reasoning, making them highly challenging. The details of the self-constructed dataset are in Table\u00a05.\n\n\nTable 5: Data details of the self-constructed dataset.\n\n\n\n\nSource\n\n\n\n\nLength\n\n\n\n\n# Queries\n\n\n\n\nExample Query\n\n\n\n\n\n\nAccepted paper list of ACL 2023 Main Conference\n\n\n\n\n44,490\n\n\n\n\n7\n\n\n\n\nWho has the most accepted paper in ACL 2023?\n\n\n\n\n\n\nThe Diamond Sutra\n\n\n\n\n19,993\n\n\n\n\n3\n\n\n\n\nHow many chapters of the Sutra?\n\n\n\n\n\n\nSchedule of The 2024 Olympic Games\n\n\n\n\n15,844\n\n\n\n\n9\n\n\n\n\nWhich day has the most gold medal events?\n\n\n\n\n\n\nSubtitle of The Big Bang Theory S3E14\n\n\n\n\n11,136\n\n\n\n\n6\n\n\n\n\nHow long does this episode?\n\n\n\n\n\n\nThe Little Prince\n\n\n\n\n22,471\n\n\n\n\n4\n\n\n\n\nHow many planets does the little prince visit?\n\n\n\n\n\n\nHarry Potter and the Chamber of Secrets\n\n\n\n\n122,591\n\n\n\n\n3\n\n\n\n\nHow many times has the chamber of secret been opened?\n\n\n\n\n\n",
      "Appendix B Implementation Details": "\n\nAppendix B Implementation Details\n\nLC-Boost begins with the [Task Understanding] action after receiving the input query and context, using the prompt shown in Table\u00a06. If the task does not include an input query, the two columns \"Below is the query\" and \"{input_query}\" are omitted. Besides, for the synthetic task, we use the prompt shown in Table\u00a07 to reformulate the query for better adaptation to LC-Boost. Based on the output of the [Task Understanding] action, LC-Boost adopts different strategies to perform the task. Specifically, \u201coption [1]\u201d directs LC-Boost to utilize a retriever to rank all chunks of the long context. In this paper, we employ BGE-Reranker-Large as the retriever\u00a0Chen et\u00a0al. [2023b]. For \u201coption [2]\u201d and \u201coption [3]\u201d, LC-Boost uses the prompts shown in Table\u00a010 and Table\u00a08 to sequentially process each short context, respectively. After processing each short context, if the output is not \"null\", the newly summarized context is added to the \"previous summarization\".\n\n\nOnce all short contexts are processed, LC-Boost aggregates all relevant information to produce the final answer. At this stage, we use the prompt provided by LongBench, replacing the full context with the surrogate context produced by LC-Boost. For \u201coption [4]\u201d, LC-Boost utilizes the prompts provided by LongBench to process each short context and produces the answer as soon as the proper information is found. Table\u00a09 presents an example prompt from LongBench, designed for MultiFieldQA tasks. We modified the prompt by adding the instruction \u201cIf no answer can be found in the text, please output \"null\"\u201d. This allows LC-Boost to skip irrelevant short contexts, performing the [Move] action. Specifically, for the Code Completion task, LC-Boost reversely browses the context code from near to far as the near context are more useful to predict the code completion.\nWe evaluate all baseline models following the settings provided in LongBench\u00a0555https://github.com/THUDM/LongBench. We use a node with 8 A100 80G GPUs to conduct all experiments.\n\n",
      "Appendix C Token Consumption Analysis": "\n\nAppendix C Token Consumption Analysis\n\nIn Section\u00a03.5, our analysis confirms that LC-Boost significantly reduces energy consumption compared to long LLMs. However, most closed-source LLMs, such as the underlying model of LC-Boost, GPT-3.5-turbo, charge based on token consumption, e.g., US$0.50 per 1M tokens for input and US$1.50 per 1M tokens for output666https://openai.com/api/pricing/. Consequently, it is crucial to examine whether the decision-making process of LC-Boost increases token consumption compared to the brute-force method.\n\n\nTo address this issue, we recorded the end-to-end token consumption for three datasets: NarrativeQA, GovReport, and LCC. After token counting, we conclude that LC-Boost\u2019s token consumption was 34.1% of the brute-force method\u2019s consumption in NarrativeQA, 112% in GovReport, and 29.5% in LCC. These results indicate that LC-Boost\u2019s token consumption varies significantly across different tasks. For tasks requiring precise context location, such as QA and code completion, LC-Boost can respond as soon as the relevant context is identified, thereby avoiding the need to process the full context. However, for tasks that necessitate information aggregation, such as summarization, LC-Boost may require more tokens for prompts in each iteration.\nIn practice, for token-consumption-sensitive LLMs, there might be a trade-off between performance and cost-efficiency, which also varies considerably across different tasks.\n\n\nTable 6: Prompt Template for the [Task Understanding] action.\n\n\n\n\n\n\nYou need to process a task with a long context that greatly exceeds your context limit.\n\nThe only feasible way to handle this is by processing the long context chunk by chunk.\n\n\n\n\n\n\n\n\nBelow is the original task prompt:\n\n{task_prompt}\n\n\n\n\n\n\n\n\nBelow is the query:\n\n{input_query}\n\n\n\n\n\n\n\n\nYou have the following options to process the long context. Choose one of them:\n\n\n\n\n\n\n\n\n[1]. Retrieve the chunk most relevant to the input query to support answer generation.\n\n\n\n\n\n\n\n\n[2]. Summarize each chunk and then aggregate the summaries after processing all chunks.\n\n\n\n\n\n\n\n\n[3]. Extract key sentences from each chunk and then aggregate them after processing all chunks.\n\n\n\n\n\n\n\n\n[4]. Sequentially scan chunks and produce the answer as soon as the query can be answered.\n\n\n\n\n\n\n\n\nBelow are some examples for reference:\n\n\n\n\n\n\n\n\nThe examples begin as follows:\n\n{examples}\n\nThe examples conclude here.\n\n\n\n\n\n\n\n\nPlease learn the examples and select one of the options by only outputting the corresponding index number.\n\n\n\n\n\n\nTable 7: Query Rewritten Prompt Template for the [Task Understanding] action.\n\n\n\n\n\n\nYou need to process a task with a long context that greatly exceeds your context limit.\n\nThe only feasible way to handle this is by processing the long context chunk by chunk.\n\n\n\n\n\n\n\n\nBelow is the original task prompt:\n\n{task_prompt}\n\n\n\n\n\n\n\n\nBelow is the query:\n\n{input_query}\n\n\n\n\n\n\n\n\nYou will process the long context with the following strategy:\n\n{strategy}\n\n\n\n\n\n\n\n\nDo you think the the query is proper for processing context chunk? If not, rewrite the query.\n\n\n\n\n\n\n\n\nBelow are some examples for reference:\n\n\n\n\n\n\n\n\nThe examples begin as follows:\n\n{examples}\n\nThe examples conclude here.\n\n\n\n\n\n\n\n\nPlease study the examples carefully. If the query needs to be rewritten, directly output the revised query.\n\nIf no revision is necessary, output \u201cnull\u201d.\n\n\n\n\n\n\nTable 8: Prompt Template for the [Append] action.\n\n\n\n\n\n\nYou are given an article and a question. Read the article carefully and follow my instructions to process it.\n\n\n\n\n\n\n\n\nArticle:\n\n\n\n\n\n\n\n\nThe article begins as follows:\n\n{article}\n\nThe article concludes here.\n\n\n\n\n\n\n\n\nQuestion:\n\n\n\n\n\n\n\n\n{question}\n\n\n\n\n\n\n\n\nInstructions:\n\n\n\n\n\n\n\n\nEach sentence in the article is marked with a sentence identifier [si], for example [s1].\n\n\n\n\n\n\n\n\nSelect up to ten key sentences from the article that are most likely to answer the question.\n\n\n\n\n\n\n\n\nOnly output the selected sentence identifiers, separated by commas.\n\n\n\n\n\n\n\n\nExample: [s39],[s54]\n\n\n\n\n\n\n\n\nIf no sentences are relevant, please output \"null\".\n\n\n\n\n\n\nTable 9: Prompt Template for the MultiFieldQA Task from the LongBench Benchmark. Additions made by us are highlighted in blue.\n\n\n\n\n\n\nRead the following text and answer briefly.\n\n\n\n\n\n\n\n\n{context}\n\n\n\n\n\n\n\n\nNow, answer the following question based on the above text, only give me the answer and do not output any\n\n\n\n\n\n\n\n\nother words.  If no answer can be found in the text, please output \"null\".\n\n\n\n\n\n\n\n\nQuestion:{question}\n\n\n\n\n\n\n\n\nAnswer:\n\n\n\n\n\n\nTable 10: Prompt Template for the [Merge] action.\n\n\n\n\n\n\nYou are provided with a portion of an article, a question, and summarization of the article\u2019s previous portions.\n\n\n\n\n\n\n\n\nRead the article portion and follow my instructions to process it.\n\n\n\n\n\n\n\n\nArticle:\n\n\n\n\n\n\n\n\nThe article begins as follows:\n\n{article}\n\nThe article concludes here.\n\n\n\n\n\n\n\n\nPrevious summarization:\n\n\n\n\n\n\n\n\nThe previous summarization is as follows:\n\n{previous_sum}\n\nThe previous summarization concludes here.\n\n\n\n\n\n\n\n\nQuestion:\n\n\n\n\n\n\n\n\n{question}\n\n\n\n\n\n\n\n\nInstruction:\n\n\n\n\n\n\n\n\nSummarize the partial article to supplement the previous summarization, which can better support the task.\n\n\n\n\n\n\n\n\nIf no content needs to be supplemented, please output \"null\".\n\n\n\n\n\n",
      "Appendix D Limitations and Broad Impact": "\n\nAppendix D Limitations and Broad Impact\n\nIn this paper, we propose LC-Boost, a method dedicated to solving long-context tasks using short contexts. However, there are several limitations we would like to address in the future work:\n(1) Although we conduct comprehensive experiments on many tasks and provide theoretical analysis to support our major claim that most long-context tasks are short-context solvable, there may be more complicated scenarios that require understanding the full context in a brute-force setting. LC-Boost might not be able to process such tasks effectively.\n(2) As mentioned in Section\u00a02.3, LC-Boost selects actions from a discrete action space. While we argue that the pre-defined action space is versatile enough to handle most scenarios, a more elegant solution would be to predict actions in a continuous space. We conducted preliminary experiments to explore allowing LC-Boost to prompt itself to predict actions without a predefined action space, such as writing prompts or code autonomously. These experiments resulted in highly unstable performance, particularly for models like GPT-3.5, as such requirements are still challenging. We believe that with a much stronger foundation model, LC-Boost could be expected to predict actions in a continuous space.\n(3) We choose GPT-3.5 as the foundation model for LC-Boost, instead of open-source LLMs. The reason is that GPT-3.5 is a strong, yet efficient model that can generally understand most instructions. However, we found that most open-source LLMs lack these properties in a zero-shot setting. Fine-tuning these open-source LLMs might be helpful, but constructing such instruction data is infeasible and expensive.\n\n\nAs discussed in Section\u00a03.5, LLMs are likely to become a fundamental infrastructure in the near future. At that scale, their energy consumption will pose significant environmental challenges. As shown in Figure\u00a04, LC-Boost avoids processing long contexts directly by decomposing them into shorter contexts. This approach significantly reduces energy consumption as the context length increases, leading to substantial positive environmental impacts. We believe that in the future, more research will focus on green AI initiatives. This paper could serve as an initial spark to inspire further research in this direction, potentially resulting in broader social impact.\n\n"
    },
    "bibliography": {
      "Bai et\u00a0al. (2023)": "\nBai et\u00a0al. (2023)\n\nYushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, et\u00a0al.\n\n\nLongbench: A bilingual, multitask benchmark for long context understanding.\n\n\narXiv preprint arXiv:2308.14508, 2023.\n\n\n",
      "Zhang et\u00a0al. (2024a)": "\nZhang et\u00a0al. (2024a)\n\nXinrong Zhang, Yingfa Chen, Shengding Hu, Zihang Xu, Junhao Chen, Moo\u00a0Khai Hao, Xu\u00a0Han, Zhen\u00a0Leng Thai, Shuo Wang, Zhiyuan Liu, and Maosong Sun.\n\n\n\u221e\\infty\u221ebench: Extending long context evaluation beyond 100k tokens, 2024a.\n\n\n",
      "Touvron et\u00a0al. (2023a)": "\nTouvron et\u00a0al. (2023a)\n\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et\u00a0al.\n\n\nLlama 2: Open foundation and fine-tuned chat models.\n\n\narXiv preprint arXiv:2307.09288, 2023a.\n\n\n",
      "Touvron et\u00a0al. (2023b)": "\nTouvron et\u00a0al. (2023b)\n\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et\u00a0al.\n\n\nLlama 2: Open foundation and fine-tuned chat models.\n\n\narXiv preprint arXiv:2307.09288, 2023b.\n\n\n",
      "Kaplan et\u00a0al. (2020)": "\nKaplan et\u00a0al. (2020)\n\nJared Kaplan, Sam McCandlish, Tom Henighan, Tom\u00a0B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei.\n\n\nScaling laws for neural language models, 2020.\n\n\n",
      "Liu et\u00a0al. (2023)": "\nLiu et\u00a0al. (2023)\n\nNelson\u00a0F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang.\n\n\nLost in the middle: How language models use long contexts, 2023.\n\n\n",
      "Li et\u00a0al. (2023a)": "\nLi et\u00a0al. (2023a)\n\nDacheng Li, Rulin Shao, Anze Xie, Ying Sheng, Lianmin Zheng, Joseph Gonzalez, Ion Stoica, Xuezhe Ma, and Hao Zhang.\n\n\nHow long can context length of open-source llms truly promise?\n\n\nIn NeurIPS 2023 Workshop on Instruction Tuning and Instruction Following, 2023a.\n\n\n",
      "Adolphs (1999)": "\nAdolphs (1999)\n\nRalph Adolphs.\n\n\nSocial cognition and the human brain.\n\n\nTrends in cognitive sciences, 3(12):469\u2013479, 1999.\n\n\n",
      "Bryant and O\u2019Hallaron (2011)": "\nBryant and O\u2019Hallaron (2011)\n\nRandal\u00a0E Bryant and David\u00a0Richard O\u2019Hallaron.\n\n\nComputer systems: a programmer\u2019s perspective.\n\n\nPrentice Hall, 2011.\n\n\n",
      "Xu et\u00a0al. (2023a)": "\nXu et\u00a0al. (2023a)\n\nPeng Xu, Wei Ping, Xianchao Wu, Lawrence McAfee, Chen Zhu, Zihan Liu, Sandeep Subramanian, Evelina Bakhturina, Mohammad Shoeybi, and Bryan Catanzaro.\n\n\nRetrieval meets Long Context Large Language Models.\n\n\narXiv, 2023a.\n\n\ndoi: 10.48550/arxiv.2310.03025.\n\n\nExperimental.\n\n\n",
      "Ji et\u00a0al. (2023)": "\nJi et\u00a0al. (2023)\n\nZiwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye\u00a0Jin Bang, Andrea Madotto, and Pascale Fung.\n\n\nSurvey of hallucination in natural language generation.\n\n\nACM Computing Surveys, 55(12):1\u201338, 2023.\n\n\n",
      "Lewis et\u00a0al. (2020)": "\nLewis et\u00a0al. (2020)\n\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich K\u00fcttler, Mike Lewis, Wen-tau Yih, Tim Rockt\u00e4schel, Sebastian Riedel, and Douwe Kiela.\n\n\nRetrieval-Augmented Generation for knowledge-intensive NLP tasks.\n\n\nIn Advances in Neural Information Processing Systems, volume\u00a033, pages 9459\u20139474, 2020.\n\n\nURL https://proceedings.neurips.cc/paper_files/paper/2020/file/6b493230205f780e1bc26945df7481e5-Paper.pdf.\n\n\n",
      "Shuster et\u00a0al. (2021)": "\nShuster et\u00a0al. (2021)\n\nKurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela, and Jason Weston.\n\n\nRetrieval augmentation reduces hallucination in conversation.\n\n\nIn Findings of the Association for Computational Linguistics: EMNLP 2021, pages 3784\u20133803, Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics.\n\n\ndoi: 10.18653/v1/2021.findings-emnlp.320.\n\n\nURL https://aclanthology.org/2021.findings-emnlp.320.\n\n\n",
      "Cover (1999)": "\nCover (1999)\n\nThomas\u00a0M Cover.\n\n\nElements of information theory.\n\n\nJohn Wiley & Sons, 1999.\n\n\n",
      "Tishby and Zaslavsky (2015)": "\nTishby and Zaslavsky (2015)\n\nNaftali Tishby and Noga Zaslavsky.\n\n\nDeep learning and the information bottleneck principle, 2015.\n\n\n",
      "Mohri et\u00a0al. (2018)": "\nMohri et\u00a0al. (2018)\n\nMehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar.\n\n\nFoundations of machine learning.\n\n\nMIT press, 2018.\n\n\n",
      "Ko\u010disk\u00fd et\u00a0al. (2017)": "\nKo\u010disk\u00fd et\u00a0al. (2017)\n\nTom\u00e1\u0161 Ko\u010disk\u00fd, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl\u00a0Moritz Hermann, G\u00e1bor Melis, and Edward Grefenstette.\n\n\nThe narrativeqa reading comprehension challenge, 2017.\n\n\n",
      "Dasigi et\u00a0al. (2021)": "\nDasigi et\u00a0al. (2021)\n\nPradeep Dasigi, Kyle Lo, Iz\u00a0Beltagy, Arman Cohan, Noah\u00a0A Smith, and Matt Gardner.\n\n\nA dataset of information-seeking questions and answers anchored in research papers.\n\n\nIn Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 4599\u20134610, 2021.\n\n\n",
      "Yang et\u00a0al. (2018)": "\nYang et\u00a0al. (2018)\n\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William\u00a0W. Cohen, Ruslan Salakhutdinov, and Christopher\u00a0D. Manning.\n\n\nHotpotqa: A dataset for diverse, explainable multi-hop question answering, 2018.\n\n\n",
      "Ho et\u00a0al. (2020)": "\nHo et\u00a0al. (2020)\n\nXanh Ho, Anh-Khoa Duong\u00a0Nguyen, Saku Sugawara, and Akiko Aizawa.\n\n\nConstructing a multi-hop QA dataset for comprehensive evaluation of reasoning steps.\n\n\nIn Donia Scott, Nuria Bel, and Chengqing Zong, editors, Proceedings of the 28th International Conference on Computational Linguistics, pages 6609\u20136625, Barcelona, Spain (Online), December 2020. International Committee on Computational Linguistics.\n\n\ndoi: 10.18653/v1/2020.coling-main.580.\n\n\nURL https://aclanthology.org/2020.coling-main.580.\n\n\n",
      "Trivedi et\u00a0al. (2022)": "\nTrivedi et\u00a0al. (2022)\n\nHarsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal.\n\n\nMusique: Multihop questions via single-hop question composition.\n\n\nTransactions of the Association for Computational Linguistics, 10:539\u2013554, 2022.\n\n\n",
      "Huang et\u00a0al. (2021)": "\nHuang et\u00a0al. (2021)\n\nLuyang Huang, Shuyang Cao, Nikolaus Parulian, Heng Ji, and Lu\u00a0Wang.\n\n\nEfficient attentions for long document summarization.\n\n\nIn Kristina Toutanova, Anna Rumshisky, Luke Zettlemoyer, Dilek Hakkani-Tur, Iz\u00a0Beltagy, Steven Bethard, Ryan Cotterell, Tanmoy Chakraborty, and Yichao Zhou, editors, Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1419\u20131436, Online, June 2021. Association for Computational Linguistics.\n\n\ndoi: 10.18653/v1/2021.naacl-main.112.\n\n\nURL https://aclanthology.org/2021.naacl-main.112.\n\n\n",
      "Fabbri et\u00a0al. (2019)": "\nFabbri et\u00a0al. (2019)\n\nAlexander\u00a0R. Fabbri, Irene Li, Tianwei She, Suyi Li, and Dragomir\u00a0R. Radev.\n\n\nMulti-news: a large-scale multi-document summarization dataset and abstractive hierarchical model, 2019.\n\n\n",
      "Gliwa et\u00a0al. (2019)": "\nGliwa et\u00a0al. (2019)\n\nBogdan Gliwa, Iwona Mochol, Maciej Biesek, and Aleksander Wawer.\n\n\nSAMSum corpus: A human-annotated dialogue dataset for abstractive summarization.\n\n\nIn Lu\u00a0Wang, Jackie Chi\u00a0Kit Cheung, Giuseppe Carenini, and Fei Liu, editors, Proceedings of the 2nd Workshop on New Frontiers in Summarization, pages 70\u201379, Hong Kong, China, November 2019. Association for Computational Linguistics.\n\n\ndoi: 10.18653/v1/D19-5409.\n\n\nURL https://aclanthology.org/D19-5409.\n\n\n",
      "Guo et\u00a0al. (2023)": "\nGuo et\u00a0al. (2023)\n\nDaya Guo, Canwen Xu, Nan Duan, Jian Yin, and Julian McAuley.\n\n\nLongcoder: A long-range pre-trained language model for code completion, 2023.\n\n\n",
      "Chiang et\u00a0al. (2023)": "\nChiang et\u00a0al. (2023)\n\nWei-Lin Chiang, Zhuohan Li, Zi\u00a0Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph\u00a0E. Gonzalez, Ion Stoica, and Eric\u00a0P. Xing.\n\n\nVicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023.\n\n\nURL https://lmsys.org/blog/2023-03-30-vicuna/.\n\n\n",
      "Li et\u00a0al. (2023b)": "\nLi et\u00a0al. (2023b)\n\nDacheng Li, Rulin Shao, Anze Xie, Ying Sheng, Lianmin Zheng, Joseph\u00a0E. Gonzalez, Ion Stoica, Xuezhe Ma, and Hao Zhang.\n\n\nHow long can open-source llms truly promise on context length?, June 2023b.\n\n\nURL https://lmsys.org/blog/2023-06-29-longchat.\n\n\n",
      "Jiang et\u00a0al. (2023a)": "\nJiang et\u00a0al. (2023a)\n\nAlbert\u00a0Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra\u00a0Singh Chaplot, Diego de\u00a0las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et\u00a0al.\n\n\nMistral 7b.\n\n\narXiv preprint arXiv:2310.06825, 2023a.\n\n\n",
      "Zhang et\u00a0al. (2024b)": "\nZhang et\u00a0al. (2024b)\n\nPeitian Zhang, Ninglu Shao, Zheng Liu, Shitao Xiao, Hongjin Qian, Qiwei Ye, and Zhicheng Dou.\n\n\nExtending llama-3\u2019s context ten-fold overnight, 2024b.\n\n\n",
      "Abdin et\u00a0al. (2024)": "\nAbdin et\u00a0al. (2024)\n\nMarah Abdin, Sam\u00a0Ade Jacobs, Ammar\u00a0Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Harkirat Behl, Alon Benhaim, Misha Bilenko, Johan Bjorck, S\u00e9bastien Bubeck, Martin Cai, Caio C\u00e9sar\u00a0Teodoro Mendes, Weizhu Chen, Vishrav Chaudhary, Parul Chopra, Allie\u00a0Del Giorno, Gustavo de\u00a0Rosa, Matthew Dixon, Ronen Eldan, Dan Iter, Amit Garg, Abhishek Goswami, Suriya Gunasekar, Emman Haider, Junheng Hao, Russell\u00a0J. Hewett, Jamie Huynh, Mojan Javaheripi, Xin Jin, Piero Kauffmann, Nikos Karampatziakis, Dongwoo Kim, Mahoud Khademi, Lev Kurilenko, James\u00a0R. Lee, Yin\u00a0Tat Lee, Yuanzhi Li, Chen Liang, Weishung Liu, Eric Lin, Zeqi Lin, Piyush Madan, Arindam Mitra, Hardik Modi, Anh Nguyen, Brandon Norick, Barun Patra, Daniel Perez-Becker, Thomas Portet, Reid Pryzant, Heyang Qin, Marko Radmilac, Corby Rosset, Sambudha Roy, Olatunji Ruwase, Olli Saarikivi, Amin Saied, Adil Salim, Michael Santacroce, Shital Shah, Ning Shang, Hiteshi Sharma, Xia Song, Masahiro Tanaka, Xin Wang, Rachel\nWard, Guanhua Wang, Philipp Witte, Michael Wyatt, Can Xu, Jiahang Xu, Sonali Yadav, Fan Yang, Ziyi Yang, Donghan Yu, Chengruidong Zhang, Cyril Zhang, Jianwen Zhang, Li\u00a0Lyna Zhang, Yi\u00a0Zhang, Yue Zhang, Yunan Zhang, and Xiren Zhou.\n\n\nPhi-3 technical report: A highly capable language model locally on your phone, 2024.\n\n\n",
      "AI et\u00a0al. (2024)": "\nAI et\u00a0al. (2024)\n\n01. AI, :, Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge\u00a0Zhang, Guanwei Zhang, Heng Li, Jiangcheng Zhu, Jianqun Chen, Jing Chang, Kaidong Yu, Peng Liu, Qiang Liu, Shawn Yue, Senbin Yang, Shiming Yang, Tao Yu, Wen Xie, Wenhao Huang, Xiaohui Hu, Xiaoyi Ren, Xinyao Niu, Pengcheng Nie, Yuchi Xu, Yudong Liu, Yue Wang, Yuxuan Cai, Zhenyu Gu, Zhiyuan Liu, and Zonghong Dai.\n\n\nYi: Open foundation models by 01.ai, 2024.\n\n\n",
      "DeepSeek-AI (2024)": "\nDeepSeek-AI (2024)\n\nDeepSeek-AI.\n\n\nDeepseek-v2: A strong, economical, and efficient mixture-of-experts language model, 2024.\n\n\n",
      "Peng et\u00a0al. (2023)": "\nPeng et\u00a0al. (2023)\n\nBowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole.\n\n\nYarn: Efficient context window extension of large language models.\n\n\nIn The Twelfth International Conference on Learning Representations, 2023.\n\n\n",
      "Fu et\u00a0al. (2024)": "\nFu et\u00a0al. (2024)\n\nYao Fu, Rameswar Panda, Xinyao Niu, Xiang Yue, Hannaneh Hajishirzi, Yoon Kim, and Hao Peng.\n\n\nData engineering for scaling language models to 128k context, 2024.\n\n\n",
      "Cai et\u00a0al. (2024)": "\nCai et\u00a0al. (2024)\n\nZheng Cai, Maosong Cao, Haojiong Chen, Kai Chen, Keyu Chen, Xin Chen, Xun Chen, Zehui Chen, Zhi Chen, Pei Chu, et\u00a0al.\n\n\nInternlm2 technical report.\n\n\narXiv preprint arXiv:2403.17297, 2024.\n\n\n",
      "Chen et\u00a0al. (2023a)": "\nChen et\u00a0al. (2023a)\n\nYukang Chen, Shengju Qian, Haotian Tang, Xin Lai, Zhijian Liu, Song Han, and Jiaya Jia.\n\n\nLonglora: Efficient fine-tuning of long-context large language models.\n\n\nIn The Twelfth International Conference on Learning Representations, 2023a.\n\n\n",
      "Du et\u00a0al. (2022)": "\nDu et\u00a0al. (2022)\n\nZhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie Tang.\n\n\nGlm: General language model pretraining with autoregressive blank infilling.\n\n\nIn Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 320\u2013335, 2022.\n\n\n",
      "OpenAI (2023)": "\nOpenAI (2023)\n\nOpenAI.\n\n\nGpt-4 technical report.\n\n\nhttps://cdn.openai.com/papers/gpt-4.pdf, 2023.\n\n\n",
      "Izacard and Grave (2021a)": "\nIzacard and Grave (2021a)\n\nGautier Izacard and Edouard Grave.\n\n\nLeveraging passage retrieval with generative models for open domain question answering, 2021a.\n\n\n",
      "Gao et\u00a0al. (2024)": "\nGao et\u00a0al. (2024)\n\nYunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi\u00a0Dai, Jiawei Sun, Qianyu Guo, Meng Wang, and Haofen Wang.\n\n\nRetrieval-augmented generation for large language models: A survey, 2024.\n\n\n",
      "Wang et\u00a0al. (2023)": "\nWang et\u00a0al. (2023)\n\nZhiruo Wang, Jun Araki, Zhengbao Jiang, Md\u00a0Rizwan Parvez, and Graham Neubig.\n\n\nLearning to filter context for retrieval-augmented generation, 2023.\n\n\n",
      "Qian et\u00a0al. (2024)": "\nQian et\u00a0al. (2024)\n\nHongjin Qian, Zheng Liu, Kelong Mao, Yujia Zhou, and Zhicheng Dou.\n\n\nGrounding language model with chunking-free in-context retrieval, 2024.\n\n\n",
      "Izacard and Grave (2021b)": "\nIzacard and Grave (2021b)\n\nGautier Izacard and Edouard Grave.\n\n\nDistilling knowledge from reader to retriever for question answering.\n\n\nIn International Conference on Learning Representations, 2021b.\n\n\nURL https://openreview.net/forum?id=NTEz-6wysdb.\n\n\n",
      "Xu et\u00a0al. (2023b)": "\nXu et\u00a0al. (2023b)\n\nPeng Xu, Wei Ping, Xianchao Wu, Lawrence McAfee, Chen Zhu, Zihan Liu, Sandeep Subramanian, Evelina Bakhturina, Mohammad Shoeybi, and Bryan Catanzaro.\n\n\nRetrieval meets long context large language models.\n\n\nIn The Twelfth International Conference on Learning Representations, 2023b.\n\n\n",
      "Jiang et\u00a0al. (2023b)": "\nJiang et\u00a0al. (2023b)\n\nZhengbao Jiang, Frank\u00a0F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, and Graham Neubig.\n\n\nActive retrieval augmented generation.\n\n\narXiv preprint arXiv:2305.06983, 2023b.\n\n\nURL https://arxiv.org/pdf/2305.06983.\n\n\n",
      "Luo et\u00a0al. (2024)": "\nLuo et\u00a0al. (2024)\n\nKun Luo, Zheng Liu, Shitao Xiao, and Kang Liu.\n\n\nBge landmark embedding: A chunking-free embedding method for retrieval augmented long-context large language models, 2024.\n\n\n",
      "Ratner et\u00a0al. (2022)": "\nRatner et\u00a0al. (2022)\n\nNir Ratner, Yoav Levine, Yonatan Belinkov, Ori Ram, Inbal Magar, Omri Abend, Ehud Karpas, Amnon Shashua, Kevin Leyton-Brown, and Yoav Shoham.\n\n\nParallel Context Windows Improve In-Context Learning of Large Language Models.\n\n\narXiv, 2022.\n\n\ndoi: 10.48550/arxiv.2212.10947.\n\n\nWindow.\n\n\n",
      "Nakano et\u00a0al. (2022)": "\nNakano et\u00a0al. (2022)\n\nReiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, Xu\u00a0Jiang, Karl Cobbe, Tyna Eloundou, Gretchen Krueger, Kevin Button, Matthew Knight, Benjamin Chess, and John Schulman.\n\n\nWebgpt: Browser-assisted question-answering with human feedback, 2022.\n\n\n",
      "Yang et\u00a0al. (2023)": "\nYang et\u00a0al. (2023)\n\nHui Yang, Sifu Yue, and Yunzhong He.\n\n\nAuto-gpt for online decision making: Benchmarks and additional opinions.\n\n\narXiv preprint arXiv:2306.02224, 2023.\n\n\n",
      "Driess et\u00a0al. (2023)": "\nDriess et\u00a0al. (2023)\n\nDanny Driess, Fei Xia, Mehdi\u00a0SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, et\u00a0al.\n\n\nPalm-e: An embodied multimodal language model.\n\n\narXiv preprint arXiv:2303.03378, 2023.\n\n\n",
      "Dong et\u00a0al. (2022)": "\nDong et\u00a0al. (2022)\n\nQingxiu Dong, Lei Li, Damai Dai, Ce\u00a0Zheng, Zhiyong Wu, Baobao Chang, Xu\u00a0Sun, Jingjing Xu, and Zhifang Sui.\n\n\nA survey on in-context learning.\n\n\narXiv preprint arXiv:2301.00234, 2022.\n\n\n",
      "Wei et\u00a0al. (2022)": "\nWei et\u00a0al. (2022)\n\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed\u00a0H. Chi, Quoc\u00a0V Le, and Denny Zhou.\n\n\nChain of thought prompting elicits reasoning in large language models.\n\n\nIn Alice\u00a0H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in Neural Information Processing Systems, 2022.\n\n\nURL https://openreview.net/forum?id=_VjQlMeSB_J.\n\n\n",
      "Shinn et\u00a0al. (2023)": "\nShinn et\u00a0al. (2023)\n\nNoah Shinn, Beck Labash, and Ashwin Gopinath.\n\n\nReflexion: an autonomous agent with dynamic memory and self-reflection.\n\n\narXiv preprint arXiv:2303.11366, 2023.\n\n\n",
      "Chen et\u00a0al. (2023b)": "\nChen et\u00a0al. (2023b)\n\nJianlv Chen, Shitao Xiao, Peitian Zhang, Kun Luo, Defu Lian, and Zheng Liu.\n\n\nBge m3-embedding: Multi-lingual, multi-functionality, multi-granularity text embeddings through self-knowledge distillation, 2023b.\n\n\n"
    },
    "domain": "cs.CL",
    "citation_count": 0
  },
  "2406f8bb-cf0c-48a4-a696-c5caaca21c0c": {
    "pk": "2406f8bb-cf0c-48a4-a696-c5caaca21c0c",
    "authors": [
      "Omer Goldman",
      "Alon Jacovi",
      "Aviv Slobodkin",
      "Aviya Maimon",
      "Ido Dagan",
      "Reut Tsarfaty"
    ],
    "title": "Is It Really Long Context if All You Need Is Retrieval? Towards Genuinely Difficult Long Context NLP",
    "abstract": "Improvements in language models' capabilities have pushed their applications towards longer contexts, making long-context evaluation and development an active research area. However, many disparate use-cases are grouped together under the umbrella term of \"long-context\", defined simply by the total length of the model's input, including - for example - Needle-in-a-Haystack tasks, book summarization, and information aggregation. Given their varied difficulty, in this position paper we argue that conflating different tasks by their context length is unproductive. As a community, we require a more precise vocabulary to understand what makes long-context tasks similar or different. We propose to unpack the taxonomy of long-context based on the properties that make them more difficult with longer contexts. We propose two orthogonal axes of difficulty: (I) Diffusion: How hard is it to find the necessary information in the context? (II) Scope: How much necessary information is there to find? We survey the literature on long-context, provide justification for this taxonomy as an informative descriptor, and situate the literature with respect to it. We conclude that the most difficult and interesting settings, whose necessary information is very long and highly diffused within the input, is severely under-explored. By using a descriptive vocabulary and discussing the relevant properties of difficulty in long-context, we can implement more informed research in this area. We call for a careful design of tasks and benchmarks with distinctly long context, taking into account the characteristics that make it qualitatively different from shorter context.",
    "url": "http://arxiv.org/abs/2407.00402v3",
    "timestamp": 1719659387,
    "sections": {
      "1 Introduction": "\n\n1 Introduction\n\nFigure 1: A taxonomy of long context tasks based on the distribution of the needed information in the text. Tasks with larger scope and higher dispersion are more difficult (indicated by shade) and more indicative of the long context capabilities of large language models.\n\n\nThe ability to deal with ever-longer contexts has been one of the most notable trends among the emerging capabilities of large language models (LLMs). Starting with a few hundred tokens as the maximal input length of the first attention-based LLMs Devlin et\u00a0al. (2019); Raffel et\u00a0al. (2020), contemporary models are \u2013 technically \u2013 able to process up to 128k and even 1M tokens (Gemini Team Google, 2024; OpenAI, 2024).\n\n\nThe demand to evaluate LLMs in this setting has led to a line of research on designing long-context tasks and benchmarks, in order to systematically understand models\u2019 capabilities and drive their development.\nHowever, the field has generally a sole recurring descriptor to define such measurements by \u2013 simply, the length of the context.\nFor example, long-context benchmarks group tasks mostly by length in words (e.g., Shaham et\u00a0al., 2022; Bai et\u00a0al., 2023; Zhang et\u00a0al., 2024b). This leads to qualitatively different measurements being conflated together, with conclusions about long-context capabilities being extended from one class of tasks to others.\nThe community is, of course, aware that, for example, tasks which require a small part of the input are different from tasks that require a large part of it. But we ask the more general question: What are the properties that differentiate tasks when conditioned on their context length? What can we accomplish with such a distinction?\n\n\nIn this position paper, we claim that the current landscape of works on long-context evaluation will greatly benefit from a more fine-grained characterization of long-context task design. We argue that judging LLMs by their ability to process long sequences, while disregarding the task they process them for, overlooks the characteristics that make longer inputs more difficult, and interesting to research, to begin with\u00a0(\u00a72).\n\n\nFor example, Needle in a Haystack tasks\u00a0(NIAH; Ivgi et\u00a0al., 2023; Mohtashami and Jaggi, 2023) involve queries whose main challenge is finding the relevant information in a long context, without requiring much further processing. Synthetic NIAH datasets are, of course, easier than their natural equivalents Ivgi et\u00a0al. (2023), but the \u201cnatural vs. artificial\u201d classification is not informative in our setting, since it applies equally for tasks regardless of context length. What, then, is an informative property? What makes long-context tasks different from each other? For example, multiple-needle variants of NIAH Hsieh et\u00a0al. (2024), or those that position the \u201cneedles\u201d closer or farther apart Levy et\u00a0al. (2024). Evidently, \u201cthe number of tokens in the input\u201d is not a sufficient descriptor.\n\n\nTo resolve this roadblock, we present a taxonomy of long-context tasks for the different factors that make them harder when controlling for context length\u00a0(\u00a73). This taxonomy is derived by surveying the long-context literature and surfacing the most salient points of distinction between various tasks.\nWe focus on (I) how difficult it is to find and extract the required information from the input (its dispersion in the input), and (II) the absolute quantity of required information to solve the task (its scope). See Figure\u00a01 for a summary.\n\n\nTo understand this categorization and its utility, we review the literature on long-context evaluation and position the works with respect to those factors. We find that the most challenging setting, in which a large quantity of required information is present in a dispersed manner that is difficult to extract, is significantly under-explored\u00a0(\u00a74).\n\n\nFinally, acknowledging the inherent and legitimate reasons behind the focus on context length as the sole descriptor of difficulty, we discuss possible paths forward for designing more reliable measurements of long-context capabilities when utilizing a more nuanced vocabulary\u00a0(\u00a75).\n\n",
      "2 Task Design in Long Context": "\n\n2 Task Design in Long Context\n\nEvaluating the performance of NLP models over very long contexts is a fast-changing area of research\u00a0Bishop et\u00a0al. (2024); Wu et\u00a0al. (2024). Measurements are regularly updated to account for new capabilities which improve with extrapolation architectures\u00a0Vaswani et\u00a0al. (2017); Su et\u00a0al. (2024) and training data\u00a0He et\u00a0al. (2023); Chen et\u00a0al. (2023). Evaluators were tasked with designing measurements of long-context capabilities cheaply, efficiently, and quickly, while matching realistic use cases as much as possible. The most common way of differentiating long-context tasks, besides the context\u2019s length, is whether they are naturally-constructed or synthetically-constructed\u00a0Tay et\u00a0al. (2020); Bai et\u00a0al. (2023); Hsieh et\u00a0al. (2024).\n\n\nNatural construction.\n\nA simple yet effective way of \u201cmoving the goalpost\u201d for context length is by modeling long-context tasks based on short-context tasks.\nThis was done, for example, with\nQA (Ko\u010disk\u00fd et\u00a0al., 2018, cf. Dunn et\u00a0al., 2017), summarization (Huang et\u00a0al., 2021a, cf. Narayan et\u00a0al., 2018), and NLI (Koreeda and Manning, 2021a, cf. Williams et\u00a0al., 2018). Specialized domains like legal Bruno and Roth (2022); Nguyen et\u00a0al. (2024) and literature (Wang et\u00a0al., 2022; Kryscinski et\u00a0al., 2022) often involve longer texts, turning typically short-context tasks such as QA and NLI into long-context scenarios.\nAnother more native methodology is to create new tasks which inherently require a long context,\nsuch as multi-document summarization Fabbri et\u00a0al. (2019); Angelidis et\u00a0al. (2021), survey generation Gao et\u00a0al. (2024), and structured data aggregation Caciularu et\u00a0al. (2024).\nBoth methodologies share the constraint that, due to their natural construction (i.e., using natural text), once created, they are difficult to modify for longer contexts as models\u2019 long-context capabilities improve.\n\n\n\nSynthetic construction.\n\nA more flexible approach, sacrificing natural construction for length control, is to use distractors to synthetically increase the context length.\nThis method allows for cheap and efficient (in terms of task construction cost) evaluation of models\u2019 full context length capabilities, with difficulty adjusted by controlling the distractors.\nTasks like Needle-in-a-Haystack (NIAH; Ivgi et\u00a0al., 2023; Kamradt, 2023) and PassKey retrieval (Mohtashami and Jaggi, 2023) were created to evaluate a model\u2019s ability to pinpoint specific information amid lengthy distractors.\nFlexible and effective against existing models, they became standard benchmarks for evaluating new long-context models (GLM Team, 2024; Jiang et\u00a0al., 2024).\nFollowup studies have complicated these tasks by increasing the number of critical details to locate (Arora et\u00a0al., 2023; Liu et\u00a0al., 2024a) and changing their position within the input Liu et\u00a0al. (2024b); Levy et\u00a0al. (2024).\n\n\n\nLimitations of the status quo.\n\nNIAH-like tasks aim to assess information retrieval capabilities, yet\nmany \u201cnaturally constructed\u201d QA and reading-comprehension tasks\nwith trivial questions about a long context accomplish the same goal. At the same time, \u201cmultiple needles\u201d NIAH can increase difficulty not by increasing the quantity of needles or length of input, but by adding distractors between needles\u00a0Levy et\u00a0al. (2024). What can systematically explain the different variables at play, in order to inform better task design in the future?\n\n\nClearly, there are underlying qualitative differences that discriminate between these various tasks besides their natural and synthetic constructions, and besides their actual context length.\nTherefore, we require a more informative vocabulary to discuss the goals of each task design, what it accomplishes, and what it does not, in terms of measuring long-context capabilities.\n\n\n",
      "3 What Makes Long Context More than Retrieval?": "\n\n3 What Makes Long Context More than Retrieval?\n\nWe require a taxonomy to capture task difficulty variations beyond mere \u201cnumber of tokens\u201d.\nWe focus on the information that is canonically required to solve the task as the conditioning variable. Our classification can be summarized via the following two questions, when asked about a given task:\n\n\n(I) How difficult is it to find and extract the required information?\n\n\n(II) How much information is needed to be found?\n\n\nAssuming that some highlighting of the relevant information is needed to solve the task (see Figure\u00a01), the latter question asks how much text is highlighted, while the former addresses the challenge of locating the relevant text for highlighting.\n\n\nFor instance, consider the task of summarizing a book, in comparison to a NIAH task of identifying a numerical detail in a long financial report (e.g., \u201chow much did the company earn in 2015?\u201d).\nAlthough both tasks involve long texts, the information required and its accessibility vary significantly.\nThe NIAH task focuses on localized, identifiable information, while summarization requires extracting key details dispersed throughout the text, tangled together with irrelevant content. Therefore, we can say that the book summarization task is more difficult on both axes (I) and (II).\n\n\nBelow we give more formal descriptions of the two axes characterized by the questions above.\n\n\n(I) Dispersion.\n\nAlthough the question above intuitively defines \u201cdifficulty of information finding\u201d,\nwe offer a more concrete description. Between two similar tasks, we consider the information harder to find in one task compared to another if:\n(1) it is more obscured (e.g., linguistically, semantically, contextually, etc); (2) it is more sparse, such that it is interspersed with non-required information; (3) its indicators are less redundant, such that there are fewer places in the document where the same information is available.\n\n\n\n(II) Scope.\n\nThe property of scope is simpler, and refers to the minimal quantity of information needed to solve the task. Importantly, we are not concerned with precise metric for \u201cquantity of information\u201d at this stage \u2013 it can refer to quantity of tokens, sentences, relations, cells in a table, etc. Any metric that reliably captures difficulty for an established solver is sufficient for our purposes.\n\n\n\nIllustrative example.\n\nTo illustrate, consider the Wikipedia entry for New York City and a simple question: \u201cWhat is the estimated population of the city?\u201d Since the answer needs a small snippet of information, we say that the task has small scope. And since it is easily accessible, we say that it has low dispersion. Consider, instead, the question \u201chow many syllables are in this document?\u201d \u2013 since this question requires the entire document to answer, we say that it has large scope, but if we consider counting syllables as straightforward, then we say its dispersion is still low. Finally, with the question \u201cWas the city\u2019s mayor elected before or after the city was affected by Hurricane Sandy?\u201d \u2013 since it requires snippets from at least two different areas of the text, we can say that when compared to the question about the city\u2019s population, the dispersion is higher, but not as high as for the question \u201cWhat makes the city a prominent place on the world stage?\u201d which poses a challenge on both axes.\n\n\nFigure 2: This figure illustrates our subjective judgment on the distribution of long-context benchmarks for each task, categorized by their scope and dispersion characteristics, with the four quadrants being marked by the dashed lines. Difficulty is expressed by shade, where red is more difficult and green in easier. Notably, some tasks, like Question-answering (QA), appear in multiple quadrants, as different benchmarks demand varying levels of scope and dispersion (e.g., a single fact versus multiple facts spread across a document). For a detailed breakdown of benchmarks and their task associations, refer to Appendix\u00a0A.\n\n\n",
      "4 Challenging Long Context Is Under-Explored": "\n\n4 Challenging Long Context Is Under-Explored\n\nRevisiting the works surveyed in \u00a72, they clearly differ with respect to both scope and dispersion.\n\n\nWith respect to dispersion.\n\nThe information needed for tasks ranges from easily accessible to highly dispersed and difficult to detect.\nOn low dispersion we have NIAH Kamradt (2023); Mohtashami and Jaggi (2023) and a myriad of factual single-hop QA datasets (Tseng et\u00a0al., 2016; Ko\u010disk\u00fd et\u00a0al., 2017; Kwiatkowski et\u00a0al., 2019; Dasigi et\u00a0al., 2021, inter alia) in which the answer is relatively accessible.\nAdding more snippets of information separated by distractors, either in the form of several needles Arora et\u00a0al. (2023); Hsieh et\u00a0al. (2024) or of hops in a multi-hop question Trivedi et\u00a0al. (2022); Zhao et\u00a0al. (2022), complicates the information detection due to the need to find at least two snippets Levy et\u00a0al. (2024), thereby increasing dispersion.\nDispersion can also be increased by making the detection of the information less straightforward\n(e.g., Pang et\u00a0al., 2022) or requiring aggregation Shaham et\u00a0al. (2023).\nLastly, summarization tasks are of a very high dispersion Huang et\u00a0al. (2021a); Wang et\u00a0al. (2022), as they require the non-trivial detection of salient facts that are interwoven with the irrelevant text.\n\n\n\nWith respect to scope.\n\nTasks overwhelmingly target relatively small scope. In addition to the aforementioned NIAH tasks and their variants, many\nQA datasets apply as well\u00a0(Li et\u00a0al., 2023b; Zhao et\u00a0al., 2023; Reddy et\u00a0al., 2024, inter alia).\nA somewhat higher scope is achieved by datasets for query-based summarization Zhong et\u00a0al. (2021); Wang et\u00a0al. (2022), and QA datasets with more obfuscated answers that require reading the text surrounding the answer for its verification\u00a0An et\u00a0al. (2023); He et\u00a0al. (2023).\nAlthough much higher on the scope ladder, book summarization is still limited in its scope as datasets include texts that are only of up to 20k\ud835\udc58kitalic_k tokens Huang et\u00a0al. (2021a); Chen et\u00a0al. (2022a); Shaham et\u00a0al. (2023).\nCurrently, tasks with the highest scope, requiring information across the entire input length, are artificial and of low dispersion, like common words extraction Hsieh et\u00a0al. (2024).\n\n\n\nConclusion.\n\nFigure\u00a02 summarizes the above classification of tasks and datasets. Note that without a concrete definition of dispersion and scope, the plot is only an illustration that involves a good deal of subjective judgements.\nHowever, we conclude that (1) the majority of tasks designed to challenge LLMs in a long-context setting target either scope or dispersion,\nsuch that (2) tasks that push current models\u2019 capabilities on both axes are under-represented in the current landscape.\n\n\n",
      "5 Discussion: Towards Genuinely Difficult Long-Context Task Design": "\n\n5 Discussion: Towards Genuinely Difficult Long-Context Task Design\n\nChallenges.\n\nDesigning meaningful long-context tasks amidst rapid model progress is profoundly challenging, making the deficiency in tasks representing difficulty on both the dispersion and scope axes unsurprising.\nOne source of this challenge is the lack of diverse, coherent long texts, as models\u2019 context windows can now be comparable to\nthe length of the New Testament111www.readinglength.com/book/isbn-0190909005\nand the Odyssey.222www.readinglength.com/book/isbn-0140268863\nThe methodologies discussed in \u00a72 for creating long context tasks \u2013 lengthening short context tasks and synthetically creating length-adjustable tasks \u2013 are preferred for their straightforward definition and the incremental adjustments they require for existing data.\nThey rely on the common understanding of machine comprehension as formulated with short context in mind Dunietz et\u00a0al. (2020), and therefore they are intuitive and easy to comprehend for NLP researchers without domain expertise (e.g., in law or biomedical fields that have long contexts).\n\n\n\nFuture work.\n\nThe goals laid forward in this work are clear: For more durable and robust measurement of long-context capabilities, we must design tasks that explicitly target both the dispersion and scope capabilities of models.\nHow can this be achieved? As mentioned, one possible avenue is to rely more on tasks that require domain expertise, such as legal documents\u00a0Bruno and Roth (2022), financial reports\u00a0Reddy et\u00a0al. (2024), biomedical publications\u00a0Stylianou et\u00a0al. (2021), and so on. In specialized domains, it is common that dispersion will be naturally higher\u00a0Zhao et\u00a0al. (2022). Tasks that involve implicit aggregations over structured data, such as table manipulation\u00a0Caciularu et\u00a0al. (2024), are possible avenues for increasing both scope and dispersion synthetically by leveraging the data structure.\nIn this work, we argue that an explicit vocabulary for such properties of difficulty is what can enable more informed techniques to design difficult tasks in the future.\n\n\n",
      "6 Conclusions": "\n\n6 Conclusions\n\nWe present a taxonomy of factors that make long-context tasks more challenging compared to short ones. This is in contrast with the existing literature that refers only to the length of the input as the hallmark of long context, and as a result ends up conflating tasks of different character when assessing the ability of models to understand longer text.\nWe reviewed works on evaluation in a long-context setting and found that the most challenging setting, in which the information needed is of large scope and is highly dispersed within the input, is under-explored.\nFinally, we suggested some leads for future work to tackle this imbalance towards a more informative long context evaluation.\n\n",
      "7 Limitations": "\n\n7 Limitations\n\nFormality.\n\nIn the context of this work, we have deliberately adhered to a taxonomy based on an intuitive description, in the interest of utility to a wide diversity of research and flexibility for future work. Difficulty in searching for and extracting information, and quantity of information, are both vague terms that can only be grounded in the context of a specific family of tasks and use-cases. We intend for this work to serve as a call to action and a tool for a shared vocabulary in the interest of more informed long-context task design in the future, rather than to anchor the taxonomy to a specific and fragile point in time.\n\n\n\nRetrieval is still interesting.\n\nAlthough we argue that small scope and low dispersion tasks are the least indicative of the model\u2019s ability to long-context capabilities, tasks that are well-served by implicit retrieval or by traditional retrieval-based pipelines are certainly relevant and useful in a variety of common use-cases\u00a0Stylianou et\u00a0al. (2021); Bruno and Roth (2022); Gao et\u00a0al. (2023).\n\n\n\nOther uses for a long-context window.\n\nThis paper deals only with long inputs that serve as inputs to a task. The long context of course can have other purposes as well, like containing many in-context examples Bertsch et\u00a0al. (2024) or containing other modalities and structures Jiang et\u00a0al. (2023).\n\n\n",
      "Acknowledgments": "\nAcknowledgments\n\nThe authors would like to thank Gabriel Stanovsky for the fruitful discussions.\n\n",
      "Appendix A Benchmark Scope-Dispersion Classification": "\n\nAppendix A Benchmark Scope-Dispersion Classification\n\nIn Table\u00a01 we delineate the different long-context benchmarks, as well as classify them according to how challenging they are scope-wise and dispersion-wise.\n\n\n\n\n\n\n\nLOW SCOPE\nHIGH SCOPE\n\n\n\n\n\nQA\nQBS\n\n\n\n\u00a0\u00a0\u00a0\u00a0 Qasper (Dasigi et\u00a0al., 2021)\n\n\u00a0\u00a0\u00a0\u00a0 QMSum (Zhong et\u00a0al., 2021)\n\n\n\n\n\u00a0\u00a0\u00a0\u00a0 NarrativeQA (Ko\u010disk\u00fd et\u00a0al., 2018)\n\n\u00a0\u00a0\u00a0\u00a0 SQuALITY (Wang et\u00a0al., 2022)\n\n\n\n\n\u00a0\u00a0\u00a0\u00a0 Short-dependency QA (Li et\u00a0al., 2023b)\n\n\u00a0\u00a0\u00a0\u00a0 Related Work Summarization (An et\u00a0al., 2023)\n\n\n\n\n\u00a0\u00a0\u00a0\u00a0 MultiFieldQA (Bai et\u00a0al., 2023)\n\n\u00a0\u00a0\u00a0\u00a0 SPACE (Angelidis et\u00a0al., 2021)\n\n\n\n\n\u00a0\u00a0\u00a0\u00a0 LitM (QA) (Liu et\u00a0al., 2024b)\n\n\u00a0\u00a0\u00a0\u00a0 WebBrain-G (Qian et\u00a0al., 2023)\n\n\n\n\n\u00a0\u00a0\u00a0\u00a0 L-eval (MC QA) (An et\u00a0al., 2023)\n\n\u00a0\u00a0\u00a0\u00a0 AquaMuse (Kulkarni et\u00a0al., 2020)\n\n\n\n\n\u00a0\u00a0\u00a0\u00a0 NQ (Kwiatkowski et\u00a0al., 2019)\n\n\u00a0\u00a0\u00a0\u00a0 FINDSum-Liquidity (Liu et\u00a0al., 2023a)\n\n\n\n\n\u00a0\u00a0\u00a0\u00a0 RULER (single-hop QA) (Hsieh et\u00a0al., 2024)\n\nAggregation\n\n\n\n\u00a0\u00a0\u00a0\u00a0 MeetingQA (Prasad et\u00a0al., 2023)\n\n\u00a0\u00a0\u00a0\u00a0 ZeroSCROLLS (SpaceDigest & BookSumSort) (Shaham et\u00a0al., 2023)\n\n\n\n\n\u00a0\u00a0\u00a0\u00a0 BABILong (tasks 1,4-6,9-10) (Kuratov et\u00a0al., 2024)\n\n\u00a0\u00a0\u00a0\u00a0 PassageCount (Bai et\u00a0al., 2023)\n\n\n\n\n\u00a0\u00a0\u00a0\u00a0 Giraffe (2 tasks) (Pal et\u00a0al., 2023)\n\n\u00a0\u00a0\u00a0\u00a0 FINDSum-ROO (Liu et\u00a0al., 2023a)\n\n\n\n\nRetrieval\nAspect-based Summarization\n\n\n\n\u00a0\u00a0\u00a0\u00a0 LitM (Key-value Retrieval) (Liu et\u00a0al., 2024b)\n\n\u00a0\u00a0\u00a0\u00a0 ACLSum (Takeshita et\u00a0al., 2024)\n\n\n\n\n\u00a0\u00a0\u00a0\u00a0 MultiDoc2Dial (GSP) (Feng et\u00a0al., 2021)\n\n\u00a0\u00a0\u00a0\u00a0 OpenAsp (Amar et\u00a0al., 2023)\n\n\n\n\n\u00a0\u00a0\u00a0\u00a0 TopicRet (Li et\u00a0al., 2023a)\n\nText Sorting\n\n\n\n\u00a0\u00a0\u00a0\u00a0 Wiki-GenBen (Zhang et\u00a0al., 2024a)\n\n\u00a0\u00a0\u00a0\u00a0 Bamboo (ShowsSort & ReportSumSort) (Dong et\u00a0al., 2024)\n\n\n\n\n\u00a0\u00a0\u00a0\u00a0 RULER (S-NIAH & MK-NIAH) (Hsieh et\u00a0al., 2024)\n\nRetrieval\n\n\n\n\u00a0\u00a0\u00a0\u00a0 LongChat-Lines (Pal et\u00a0al., 2023)\n\n\u00a0\u00a0\u00a0\u00a0 PassageRetrieval (Bai et\u00a0al., 2023)\n\n\n\n\nNLI\nLFQA\n\n\n\n\u00a0\u00a0\u00a0\u00a0 LawngNLI (Bruno and Roth, 2022)\n\n\u00a0\u00a0\u00a0\u00a0 LongFQA (An et\u00a0al., 2023)\n\n\n\n\n\u00a0\u00a0\u00a0\u00a0 ContractNLI (Koreeda and Manning, 2021b)\n\nNLI\n\n\n\n\u00a0\u00a0\u00a0\u00a0 Hallucination Detection (Dong et\u00a0al., 2024)\n\n\u00a0\u00a0\u00a0\u00a0 Legal Case Entailment (Nguyen et\u00a0al., 2024)\n\n\n\n\n\u00a0\u00a0 FLenQA (3 tasks) (Levy et\u00a0al., 2024)\n\n\n\n\nFill-mask\n\n\n\n\n\u00a0\u00a0\u00a0\u00a0 Cloze (Li et\u00a0al., 2023b)\n\n\n\n\n\nNLG\n\n\n\n\n\nLOW DISPERSION\n\n\u00a0\u00a0\u00a0\u00a0 MultiDoc2Dial (ARG) (Feng et\u00a0al., 2021)\n\n\n\n\n\nQA\nSummarization\n\n\n\n\u00a0\u00a0\u00a0\u00a0 QuALITY (Pang et\u00a0al., 2022)\n\n\u00a0\u00a0\u00a0\u00a0 GovReport (Huang et\u00a0al., 2021b)\n\n\n\n\n\u00a0\u00a0\u00a0\u00a0 Long-dependency QA (Li et\u00a0al., 2023b)\n\n\u00a0\u00a0\u00a0\u00a0 SummScreenFD (Chen et\u00a0al., 2022b)\n\n\n\n\n\u00a0\u00a0\u00a0\u00a0 DuReader (Bai et\u00a0al., 2023)\n\n\u00a0\u00a0\u00a0\u00a0 Loogle (Summarization) (Li et\u00a0al., 2023b)\n\n\n\n\n\u00a0\u00a0\u00a0\u00a0 SFcition QA (An et\u00a0al., 2023)\n\n\u00a0\u00a0\u00a0\u00a0 VCSUM (Bai et\u00a0al., 2023)\n\n\n\n\n\u00a0\u00a0\u00a0\u00a0 ExpertQA (Malaviya et\u00a0al., 2024)\n\n\u00a0\u00a0\u00a0\u00a0 Self-critiquing (Saunders et\u00a0al., 2022)\n\n\n\n\n\u00a0\u00a0\u00a0\u00a0 DocFinQA (Reddy et\u00a0al., 2024)\n\n\u00a0\u00a0\u00a0\u00a0 Abstract Generation (An et\u00a0al., 2023)\n\n\n\n\n\u00a0\u00a0\u00a0\u00a0 BABILong (tasks 2-3,12) (Kuratov et\u00a0al., 2024)\n\n\u00a0\u00a0\u00a0\u00a0 Multi-News (Fabbri et\u00a0al., 2019)\n\n\n\n\n\u00a0\u00a0\u00a0\u00a0 Bamboo (QA) (Dong et\u00a0al., 2024)\n\n\u00a0\u00a0\u00a0\u00a0 BigPatent (Sharma et\u00a0al., 2019)\n\n\n\n\nMulti-hop QA\n\u00a0\u00a0\u00a0\u00a0 Scientific Summarization (Cohan et\u00a0al., 2018)\n\n\n\n\n\u00a0\u00a0\u00a0\u00a0 MuSiQue (Trivedi et\u00a0al., 2022)\n\n\u00a0\u00a0\u00a0\u00a0 BillSum (Kornilova and Eidelman, 2019)\n\n\n\n\n\u00a0\u00a0\u00a0\u00a0 HotpotQA (Yang et\u00a0al., 2018)\n\n\u00a0\u00a0\u00a0\u00a0 HowSumm (Boni et\u00a0al., 2021)\n\n\n\n\n\u00a0\u00a0\u00a0\u00a0 Multi-hop Tracing (Hsieh et\u00a0al., 2024)\n\n\u00a0\u00a0\u00a0\u00a0 ODSum (Zhou et\u00a0al., 2023)\n\n\n\n\n\u00a0\u00a0\u00a0\u00a0 RULER (multi-hop QA) (Hsieh et\u00a0al., 2024)\n\n\u00a0\u00a0\u00a0\u00a0 Klexikon (Summarization) (Aumiller and Gertz, 2022)\n\n\n\n\n\u00a0\u00a0\u00a0\u00a0 2WikiMultihopQA (Ho et\u00a0al., 2020)\n\n\u00a0\u00a0\u00a0\u00a0 Booksum (Kry\u015bci\u0144ski et\u00a0al., 2022)\n\n\n\n\nNLI\n\u00a0\u00a0\u00a0\u00a0 MeetingBank (Hu et\u00a0al., 2023)\n\n\n\n\n\u00a0\u00a0\u00a0\u00a0 FLenQA (3 rand. placement tasks) (Levy et\u00a0al., 2024)\n\nText Simplification\n\n\n\n\u00a0\u00a0\u00a0\u00a0 Legal Textual Entailment (Nguyen et\u00a0al., 2024)\n\n\u00a0\u00a0\u00a0\u00a0 Klexikon (Simplification) (Aumiller and Gertz, 2022)\n\n\n\n\nCode Understanding\nReasoning\n\n\n\n\u00a0\u00a0\u00a0\u00a0 LCC (Guo et\u00a0al., 2023)\n\n\u00a0\u00a0\u00a0\u00a0 Long ListOps (Tay et\u00a0al., 2020)\n\n\n\n\n\u00a0\u00a0\u00a0\u00a0 RepoBench-P (Liu et\u00a0al., 2023b)\n\nRetrieval\n\n\n\n\u00a0\u00a0\u00a0\u00a0 CodeU (An et\u00a0al., 2023)\n\n\u00a0\u00a0\u00a0\u00a0 LRA (task 3) (Tay et\u00a0al., 2020)\n\n\n\n\n\u00a0\u00a0\u00a0\u00a0 PrivateEval (Dong et\u00a0al., 2024)\n\n\n\n\n\nClassification\n\n\n\n\n\u00a0\u00a0\u00a0\u00a0 LRA (tasks 2, 4-6) (Tay et\u00a0al., 2020)\n\n\n\n\n\nRetrieval\n\n\n\n\n\u00a0\u00a0\u00a0\u00a0 COLIEE (tasks 1,3,4) (Nguyen et\u00a0al., 2024)\n\n\n\n\n\n\u00a0\u00a0\u00a0\u00a0 RULER (MV-NIAH & MQ-NIAH) (Hsieh et\u00a0al., 2024)\n\n\n\n\n\nNext Token Prediction\n\n\n\n\n\u00a0\u00a0\u00a0\u00a0 PG-19 (Rae et\u00a0al., 2019)\n\n\n\n\n\n\u00a0\u00a0\u00a0\u00a0 Bamboo (LM) (Dong et\u00a0al., 2024)\n\n\n\n\n\nReasoning\n\n\n\n\n\u00a0\u00a0\u00a0\u00a0 DocMath-Eval (Zhao et\u00a0al., 2023)\n\n\n\n\n\n\u00a0\u00a0\u00a0\u00a0 BABILong (tasks 14-20) (Kuratov et\u00a0al., 2024)\n\n\n\n\n\nAggregation\n\n\n\n\n\u00a0\u00a0\u00a0\u00a0 RULER (2 Aggr. tasks) (Hsieh et\u00a0al., 2024)\n\n\n\n\n\n\u00a0\u00a0\u00a0\u00a0 BABILong (tasks 7-8) (Kuratov et\u00a0al., 2024)\n\n\n\n\n\nNLU\n\n\n\n\n\u00a0\u00a0\u00a0\u00a0 Academic Feedback Generation (An et\u00a0al., 2023)\n\n\n\n\n\n\u00a0\u00a0\u00a0\u00a0 CUAD (Hendrycks et\u00a0al., 2021)\n\n\n\n\n\nFactuality Evaluation\n\n\n\n\n\u00a0\u00a0\u00a0\u00a0 LongSciVerify (Bishop et\u00a0al., 2024)\n\n\n\n\n\nCoreference Resolution\n\n\n\n\n\nHIGH DISPERSION\n\n\u00a0\u00a0\u00a0\u00a0 BABILong (tasks 11,13) (Kuratov et\u00a0al., 2024)\n\n\n\n\n\n\nTable 1: Classification of long-context benchmarks in terms of Scope and Dispersion.\n\n\n"
    },
    "bibliography": {
      "Amar et\u00a0al. (2023)": "\nAmar et\u00a0al. (2023)\n\nShmuel Amar, Liat Schiff, Ori Ernst, Asi Shefer, Ori Shapira, and Ido Dagan. 2023.\n\n\nOpenAsp: A benchmark for multi-document open aspect-based summarization.\n\n\nIn Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 1967\u20131991, Singapore. Association for Computational Linguistics.\n\n\n",
      "An et\u00a0al. (2023)": "\nAn et\u00a0al. (2023)\n\nChenxin An, Shansan Gong, Ming Zhong, Xingjian Zhao, Mukai Li, Jun Zhang, Lingpeng Kong, and Xipeng Qiu. 2023.\n\n\nL-eval: Instituting standardized evaluation for long context language models.\n\n\nPreprint, arXiv:2307.11088.\n\n\n",
      "Angelidis et\u00a0al. (2021)": "\nAngelidis et\u00a0al. (2021)\n\nStefanos Angelidis, Reinald\u00a0Kim Amplayo, Yoshihiko Suhara, Xiaolan Wang, and Mirella Lapata. 2021.\n\n\nExtractive opinion summarization in quantized transformer spaces.\n\n\nTransactions of the Association for Computational Linguistics, 9:277\u2013293.\n\n\n",
      "Arora et\u00a0al. (2023)": "\nArora et\u00a0al. (2023)\n\nSimran Arora, Sabri Eyuboglu, Aman Timalsina, Isys Johnson, Michael Poli, James Zou, Atri Rudra, and Christopher R\u00e9. 2023.\n\n\nZoology: Measuring and improving recall in efficient language models.\n\n\narXiv preprint arXiv:2312.04927.\n\n\n",
      "Aumiller and Gertz (2022)": "\nAumiller and Gertz (2022)\n\nDennis Aumiller and Michael Gertz. 2022.\n\n\nKlexikon: A German dataset for joint summarization and simplification.\n\n\nIn Proceedings of the Thirteenth Language Resources and Evaluation Conference, pages 2693\u20132701, Marseille, France. European Language Resources Association.\n\n\n",
      "Bai et\u00a0al. (2023)": "\nBai et\u00a0al. (2023)\n\nYushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li. 2023.\n\n\nLongbench: A bilingual, multitask benchmark for long context understanding.\n\n\nPreprint, arXiv:2308.14508.\n\n\n",
      "Bertsch et\u00a0al. (2024)": "\nBertsch et\u00a0al. (2024)\n\nAmanda Bertsch, Maor Ivgi, Uri Alon, Jonathan Berant, Matthew\u00a0R. Gormley, and Graham Neubig. 2024.\n\n\nIn-context learning with long-context models: An in-depth exploration.\n\n\nPreprint, arXiv:2405.00200.\n\n\n",
      "Bishop et\u00a0al. (2024)": "\nBishop et\u00a0al. (2024)\n\nJennifer\u00a0A Bishop, Qianqian Xie, and Sophia Ananiadou. 2024.\n\n\nLongdocfactscore: Evaluating the factuality of long document abstractive summarisation.\n\n\nPreprint, arXiv:2309.12455.\n\n\n",
      "Boni et\u00a0al. (2021)": "\nBoni et\u00a0al. (2021)\n\nOdellia Boni, Guy Feigenblat, Guy Lev, Michal Shmueli-Scheuer, Benjamin Sznajder, and David Konopnicki. 2021.\n\n\nHowsumm: A multi-document summarization dataset derived from wikihow articles.\n\n\nPreprint, arXiv:2110.03179.\n\n\n",
      "Bruno and Roth (2022)": "\nBruno and Roth (2022)\n\nWilliam Bruno and Dan Roth. 2022.\n\n\nLawngnli: A long-premise benchmark for in-domain generalization from short to long contexts and for implication-based retrieval.\n\n\nPreprint, arXiv:2212.03222.\n\n\n",
      "Caciularu et\u00a0al. (2024)": "\nCaciularu et\u00a0al. (2024)\n\nAvi Caciularu, Alon Jacovi, Eyal Ben-David, Sasha Goldshtein, Tal Schuster, Jonathan Herzig, Gal Elidan, and Amir Globerson. 2024.\n\n\nTact: Advancing complex aggregative reasoning with information extraction tools.\n\n\nPreprint, arXiv:2406.03618.\n\n\n",
      "Chen et\u00a0al. (2022a)": "\nChen et\u00a0al. (2022a)\n\nMingda Chen, Zewei Chu, Sam Wiseman, and Kevin Gimpel. 2022a.\n\n\nSummScreen: A dataset for abstractive screenplay summarization.\n\n\nIn Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 8602\u20138615, Dublin, Ireland. Association for Computational Linguistics.\n\n\n",
      "Chen et\u00a0al. (2022b)": "\nChen et\u00a0al. (2022b)\n\nMingda Chen, Zewei Chu, Sam Wiseman, and Kevin Gimpel. 2022b.\n\n\nSummscreen: A dataset for abstractive screenplay summarization.\n\n\nPreprint, arXiv:2104.07091.\n\n\n",
      "Chen et\u00a0al. (2023)": "\nChen et\u00a0al. (2023)\n\nYukang Chen, Shengju Qian, Haotian Tang, Xin Lai, Zhijian Liu, Song Han, and Jiaya Jia. 2023.\n\n\nLonglora: Efficient fine-tuning of long-context large language models.\n\n\nArXiv, abs/2309.12307.\n\n\n",
      "Cohan et\u00a0al. (2018)": "\nCohan et\u00a0al. (2018)\n\nArman Cohan, Franck Dernoncourt, Doo\u00a0Soon Kim, Trung Bui, Seokhwan Kim, Walter Chang, and Nazli Goharian. 2018.\n\n\nA discourse-aware attention model for abstractive summarization of long documents.\n\n\nPreprint, arXiv:1804.05685.\n\n\n",
      "Dasigi et\u00a0al. (2021)": "\nDasigi et\u00a0al. (2021)\n\nPradeep Dasigi, Kyle Lo, Iz\u00a0Beltagy, Arman Cohan, Noah\u00a0A. Smith, and Matt Gardner. 2021.\n\n\nA dataset of information-seeking questions and answers anchored in research papers.\n\n\nIn Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 4599\u20134610, Online. Association for Computational Linguistics.\n\n\n",
      "Devlin et\u00a0al. (2019)": "\nDevlin et\u00a0al. (2019)\n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019.\n\n\nBERT: Pre-training of deep bidirectional transformers for language understanding.\n\n\nIn Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171\u20134186, Minneapolis, Minnesota. Association for Computational Linguistics.\n\n\n",
      "Dong et\u00a0al. (2024)": "\nDong et\u00a0al. (2024)\n\nZican Dong, Tianyi Tang, Junyi Li, Wayne\u00a0Xin Zhao, and Ji-Rong Wen. 2024.\n\n\nBAMBOO: A comprehensive benchmark for evaluating long text modeling capacities of large language models.\n\n\nIn Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024), pages 2086\u20132099, Torino, Italia. ELRA and ICCL.\n\n\n",
      "Dunietz et\u00a0al. (2020)": "\nDunietz et\u00a0al. (2020)\n\nJesse Dunietz, Greg Burnham, Akash Bharadwaj, Owen Rambow, Jennifer Chu-Carroll, and Dave Ferrucci. 2020.\n\n\nTo test machine comprehension, start by defining comprehension.\n\n\nIn Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7839\u20137859, Online. Association for Computational Linguistics.\n\n\n",
      "Dunn et\u00a0al. (2017)": "\nDunn et\u00a0al. (2017)\n\nMatthew Dunn, Levent Sagun, Mike Higgins, V\u00a0Ugur Guney, Volkan Cirik, and Kyunghyun Cho. 2017.\n\n\nSearchqa: A new q&a dataset augmented with context from a search engine.\n\n\narXiv preprint arXiv:1704.05179.\n\n\n",
      "Fabbri et\u00a0al. (2019)": "\nFabbri et\u00a0al. (2019)\n\nAlexander\u00a0R. Fabbri, Irene Li, Tianwei She, Suyi Li, and Dragomir\u00a0R. Radev. 2019.\n\n\nMulti-news: a large-scale multi-document summarization dataset and abstractive hierarchical model.\n\n\nPreprint, arXiv:1906.01749.\n\n\n",
      "Feng et\u00a0al. (2021)": "\nFeng et\u00a0al. (2021)\n\nSong Feng, Siva\u00a0Sankalp Patel, Hui Wan, and Sachindra Joshi. 2021.\n\n\nMultiDoc2Dial: Modeling dialogues grounded in multiple documents.\n\n\nIn Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6162\u20136176, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.\n\n\n",
      "Gao et\u00a0al. (2024)": "\nGao et\u00a0al. (2024)\n\nFan Gao, Hang Jiang, Rui Yang, Qingcheng Zeng, Jinghui Lu, Moritz Blum, Dairui Liu, Tianwei She, Yuang Jiang, and Irene Li. 2024.\n\n\nLarge language models on wikipedia-style survey generation: an evaluation in nlp concepts.\n\n\nPreprint, arXiv:2308.10410.\n\n\n",
      "Gao et\u00a0al. (2023)": "\nGao et\u00a0al. (2023)\n\nLuyu Gao, Zhuyun Dai, Panupong Pasupat, Anthony Chen, Arun\u00a0Tejasvi Chaganty, Yicheng Fan, Vincent\u00a0Y. Zhao, Ni\u00a0Lao, Hongrae Lee, Da-Cheng Juan, and Kelvin Guu. 2023.\n\n\nRarr: Researching and revising what language models say, using language models.\n\n\nPreprint, arXiv:2210.08726.\n\n\n",
      "Gemini Team Google (2024)": "\nGemini Team Google (2024)\n\nGemini Team Google. 2024.\n\n\nGemini 1.5: Unlocking multimodal understanding across millions of tokens of context.\n\n\nPreprint, arXiv:2403.05530.\n\n\n",
      "GLM Team (2024)": "\nGLM Team (2024)\n\nGLM Team. 2024.\n\n\nGLM-4-9b-chat technical report.\n\n\n",
      "Guo et\u00a0al. (2023)": "\nGuo et\u00a0al. (2023)\n\nDaya Guo, Canwen Xu, Nan Duan, Jian Yin, and Julian McAuley. 2023.\n\n\nLongcoder: A long-range pre-trained language model for code completion.\n\n\nPreprint, arXiv:2306.14893.\n\n\n",
      "He et\u00a0al. (2023)": "\nHe et\u00a0al. (2023)\n\nJunqing He, Kunhao Pan, Xiaoqun Dong, Zhuoyang Song, Yibo Liu, Yuxin Liang, Hao Wang, Qianguo Sun, Songxin Zhang, Zejian Xie, and Jiaxing Zhang. 2023.\n\n\nNever lost in the middle: Improving large language models via attention strengthening question answering.\n\n\nPreprint, arXiv:2311.09198.\n\n\n",
      "Hendrycks et\u00a0al. (2021)": "\nHendrycks et\u00a0al. (2021)\n\nDan Hendrycks, Collin Burns, Anya Chen, and Spencer Ball. 2021.\n\n\nCuad: An expert-annotated nlp dataset for legal contract review.\n\n\nPreprint, arXiv:2103.06268.\n\n\n",
      "Ho et\u00a0al. (2020)": "\nHo et\u00a0al. (2020)\n\nXanh Ho, Anh-Khoa Duong\u00a0Nguyen, Saku Sugawara, and Akiko Aizawa. 2020.\n\n\nConstructing a multi-hop QA dataset for comprehensive evaluation of reasoning steps.\n\n\nIn Proceedings of the 28th International Conference on Computational Linguistics, pages 6609\u20136625, Barcelona, Spain (Online). International Committee on Computational Linguistics.\n\n\n",
      "Hsieh et\u00a0al. (2024)": "\nHsieh et\u00a0al. (2024)\n\nCheng-Ping Hsieh, Simeng Sun, Samuel Kriman, Shantanu Acharya, Dima Rekesh, Fei Jia, Yang Zhang, and Boris Ginsburg. 2024.\n\n\nRuler: What\u2019s the real context size of your long-context language models?\n\nPreprint, arXiv:2404.06654.\n\n\n",
      "Hu et\u00a0al. (2023)": "\nHu et\u00a0al. (2023)\n\nYebowen Hu, Timothy Ganter, Hanieh Deilamsalehy, Franck Dernoncourt, Hassan Foroosh, and Fei Liu. 2023.\n\n\nMeetingBank: A benchmark dataset for meeting summarization.\n\n\nIn Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 16409\u201316423, Toronto, Canada. Association for Computational Linguistics.\n\n\n",
      "Huang et\u00a0al. (2021a)": "\nHuang et\u00a0al. (2021a)\n\nLuyang Huang, Shuyang Cao, Nikolaus Parulian, Heng Ji, and Lu\u00a0Wang. 2021a.\n\n\nEfficient attentions for long document summarization.\n\n\nIn Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1419\u20131436, Online. Association for Computational Linguistics.\n\n\n",
      "Huang et\u00a0al. (2021b)": "\nHuang et\u00a0al. (2021b)\n\nLuyang Huang, Shuyang Cao, Nikolaus Parulian, Heng Ji, and Lu\u00a0Wang. 2021b.\n\n\nEfficient attentions for long document summarization.\n\n\nPreprint, arXiv:2104.02112.\n\n\n",
      "Ivgi et\u00a0al. (2023)": "\nIvgi et\u00a0al. (2023)\n\nMaor Ivgi, Uri Shaham, and Jonathan Berant. 2023.\n\n\nEfficient long-text understanding with short-text models.\n\n\nTransactions of the Association for Computational Linguistics, 11:284\u2013299.\n\n\n",
      "Jiang et\u00a0al. (2024)": "\nJiang et\u00a0al. (2024)\n\nAlbert\u00a0Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra\u00a0Singh Chaplot, Diego de\u00a0las Casas, Emma\u00a0Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, L\u00e9lio\u00a0Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven\u00a0Le Scao, Th\u00e9ophile Gervet, Thibaut Lavril, Thomas Wang, Timoth\u00e9e Lacroix, and William\u00a0El Sayed. 2024.\n\n\nMixtral of experts.\n\n\nPreprint, arXiv:2401.04088.\n\n\n",
      "Jiang et\u00a0al. (2023)": "\nJiang et\u00a0al. (2023)\n\nJinhao Jiang, Kun Zhou, Zican Dong, Keming Ye, Xin Zhao, and Ji-Rong Wen. 2023.\n\n\nStructGPT: A general framework for large language model to reason over structured data.\n\n\nIn Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 9237\u20139251, Singapore. Association for Computational Linguistics.\n\n\n",
      "Kamradt (2023)": "\nKamradt (2023)\n\nGregory Kamradt. 2023.\n\n\nNeedle in a haystack - pressure testing LLMs.\n\n\nGitHub.\n\n\n",
      "Koreeda and Manning (2021a)": "\nKoreeda and Manning (2021a)\n\nYuta Koreeda and Christopher Manning. 2021a.\n\n\nContractNLI: A dataset for document-level natural language inference for contracts.\n\n\nIn Findings of the Association for Computational Linguistics: EMNLP 2021, pages 1907\u20131919, Punta Cana, Dominican Republic. Association for Computational Linguistics.\n\n\n",
      "Koreeda and Manning (2021b)": "\nKoreeda and Manning (2021b)\n\nYuta Koreeda and Christopher\u00a0D. Manning. 2021b.\n\n\nContractnli: A dataset for document-level natural language inference for contracts.\n\n\nPreprint, arXiv:2110.01799.\n\n\n",
      "Kornilova and Eidelman (2019)": "\nKornilova and Eidelman (2019)\n\nAnastassia Kornilova and Vladimir Eidelman. 2019.\n\n\nBillSum: A corpus for automatic summarization of US legislation.\n\n\nIn Proceedings of the 2nd Workshop on New Frontiers in Summarization, pages 48\u201356, Hong Kong, China. Association for Computational Linguistics.\n\n\n",
      "Ko\u010disk\u00fd et\u00a0al. (2017)": "\nKo\u010disk\u00fd et\u00a0al. (2017)\n\nTom\u00e1\u0161 Ko\u010disk\u00fd, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl\u00a0Moritz Hermann, G\u00e1bor Melis, and Edward Grefenstette. 2017.\n\n\nThe narrativeqa reading comprehension challenge.\n\n\nPreprint, arXiv:1712.07040.\n\n\n",
      "Ko\u010disk\u00fd et\u00a0al. (2018)": "\nKo\u010disk\u00fd et\u00a0al. (2018)\n\nTom\u00e1\u0161 Ko\u010disk\u00fd, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl\u00a0Moritz Hermann, G\u00e1bor Melis, and Edward Grefenstette. 2018.\n\n\nThe NarrativeQA Reading Comprehension Challenge.\n\n\nTransactions of the Association for Computational Linguistics, 6:317\u2013328.\n\n\n",
      "Kryscinski et\u00a0al. (2022)": "\nKryscinski et\u00a0al. (2022)\n\nWojciech Kryscinski, Nazneen Rajani, Divyansh Agarwal, Caiming Xiong, and Dragomir Radev. 2022.\n\n\nBOOKSUM: A collection of datasets for long-form narrative summarization.\n\n\nIn Findings of the Association for Computational Linguistics: EMNLP 2022, pages 6536\u20136558, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.\n\n\n",
      "Kry\u015bci\u0144ski et\u00a0al. (2022)": "\nKry\u015bci\u0144ski et\u00a0al. (2022)\n\nWojciech Kry\u015bci\u0144ski, Nazneen Rajani, Divyansh Agarwal, Caiming Xiong, and Dragomir Radev. 2022.\n\n\nBooksum: A collection of datasets for long-form narrative summarization.\n\n\nPreprint, arXiv:2105.08209.\n\n\n",
      "Kulkarni et\u00a0al. (2020)": "\nKulkarni et\u00a0al. (2020)\n\nSayali Kulkarni, Sheide Chammas, Wan Zhu, Fei Sha, and Eugene Ie. 2020.\n\n\nAquamuse: Automatically generating datasets for query-based multi-document summarization.\n\n\nPreprint, arXiv:2010.12694.\n\n\n",
      "Kuratov et\u00a0al. (2024)": "\nKuratov et\u00a0al. (2024)\n\nYuri Kuratov, Aydar Bulatov, Petr Anokhin, Dmitry Sorokin, Artyom Sorokin, and Mikhail Burtsev. 2024.\n\n\nIn search of needles in a 11m haystack: Recurrent memory finds what llms miss.\n\n\nPreprint, arXiv:2402.10790.\n\n\n",
      "Kwiatkowski et\u00a0al. (2019)": "\nKwiatkowski et\u00a0al. (2019)\n\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew\u00a0M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. 2019.\n\n\nNatural questions: A benchmark for question answering research.\n\n\nTransactions of the Association for Computational Linguistics, 7:452\u2013466.\n\n\n",
      "Levy et\u00a0al. (2024)": "\nLevy et\u00a0al. (2024)\n\nMosh Levy, Alon Jacoby, and Yoav Goldberg. 2024.\n\n\nSame task, more tokens: the impact of input length on the reasoning performance of large language models.\n\n\nPreprint, arXiv:2402.14848.\n\n\n",
      "Li et\u00a0al. (2023a)": "\nLi et\u00a0al. (2023a)\n\nDacheng Li, Rulin Shao, Anze Xie, Ying Sheng, Lianmin Zheng, Joseph\u00a0E. Gonzalez, Ion Stoica, Xuezhe Ma, and Hao Zhang. 2023a.\n\n\nHow long can open-source LLMs truly promise on context length?\n\n",
      "Li et\u00a0al. (2023b)": "\nLi et\u00a0al. (2023b)\n\nJiaqi Li, Mengmeng Wang, Zilong Zheng, and Muhan Zhang. 2023b.\n\n\nLoogle: Can long-context language models understand long contexts?\n\nPreprint, arXiv:2311.04939.\n\n\n",
      "Liu et\u00a0al. (2024a)": "\nLiu et\u00a0al. (2024a)\n\nHao Liu, Wilson Yan, Matei Zaharia, and Pieter Abbeel. 2024a.\n\n\nWorld model on million-length video and language with ringattention.\n\n\narXiv preprint arXiv:2402.08268.\n\n\n",
      "Liu et\u00a0al. (2024b)": "\nLiu et\u00a0al. (2024b)\n\nNelson\u00a0F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. 2024b.\n\n\nLost in the middle: How language models use long contexts.\n\n\nTransactions of the Association for Computational Linguistics, 12:157\u2013173.\n\n\n",
      "Liu et\u00a0al. (2023a)": "\nLiu et\u00a0al. (2023a)\n\nShuaiqi Liu, Jiannong Cao, Ruosong Yang, and Zhiyuan Wen. 2023a.\n\n\nLong text and multi-table summarization: Dataset and method.\n\n\nPreprint, arXiv:2302.03815.\n\n\n",
      "Liu et\u00a0al. (2023b)": "\nLiu et\u00a0al. (2023b)\n\nTianyang Liu, Canwen Xu, and Julian McAuley. 2023b.\n\n\nRepobench: Benchmarking repository-level code auto-completion systems.\n\n\nPreprint, arXiv:2306.03091.\n\n\n",
      "Malaviya et\u00a0al. (2024)": "\nMalaviya et\u00a0al. (2024)\n\nChaitanya Malaviya, Subin Lee, Sihao Chen, Elizabeth Sieber, Mark Yatskar, and Dan Roth. 2024.\n\n\nExpertQA: Expert-curated questions and attributed answers.\n\n\nIn Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 3025\u20133045, Mexico City, Mexico. Association for Computational Linguistics.\n\n\n",
      "Mohtashami and Jaggi (2023)": "\nMohtashami and Jaggi (2023)\n\nAmirkeivan Mohtashami and Martin Jaggi. 2023.\n\n\nLandmark attention: Random-access infinite context length for transformers.\n\n\nIn Workshop on Efficient Systems for Foundation Models@ ICML2023.\n\n\n",
      "Narayan et\u00a0al. (2018)": "\nNarayan et\u00a0al. (2018)\n\nShashi Narayan, Shay\u00a0B. Cohen, and Mirella Lapata. 2018.\n\n\nDon\u2019t give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization.\n\n\nIn Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1797\u20131807, Brussels, Belgium. Association for Computational Linguistics.\n\n\n",
      "Nguyen et\u00a0al. (2024)": "\nNguyen et\u00a0al. (2024)\n\nChau Nguyen, Phuong Nguyen, Thanh Tran, Dat Nguyen, An\u00a0Trieu, Tin Pham, Anh Dang, and Le-Minh Nguyen. 2024.\n\n\nCaptain at coliee 2023: Efficient methods for legal information retrieval and entailment tasks.\n\n\nPreprint, arXiv:2401.03551.\n\n\n",
      "OpenAI (2024)": "\nOpenAI (2024)\n\nOpenAI. 2024.\n\n\nGPT-4 technical report.\n\n\nPreprint, arXiv:2303.08774.\n\n\n",
      "Pal et\u00a0al. (2023)": "\nPal et\u00a0al. (2023)\n\nArka Pal, Deep Karkhanis, Manley Roberts, Samuel Dooley, Arvind Sundararajan, and Siddartha Naidu. 2023.\n\n\nGiraffe: Adventures in expanding context lengths in llms.\n\n\nPreprint, arXiv:2308.10882.\n\n\n",
      "Pang et\u00a0al. (2022)": "\nPang et\u00a0al. (2022)\n\nRichard\u00a0Yuanzhe Pang, Alicia Parrish, Nitish Joshi, Nikita Nangia, Jason Phang, Angelica Chen, Vishakh Padmakumar, Johnny Ma, Jana Thompson, He\u00a0He, and Samuel Bowman. 2022.\n\n\nQuALITY: Question answering with long input texts, yes!\n\nIn Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 5336\u20135358, Seattle, United States. Association for Computational Linguistics.\n\n\n",
      "Prasad et\u00a0al. (2023)": "\nPrasad et\u00a0al. (2023)\n\nArchiki Prasad, Trung Bui, Seunghyun Yoon, Hanieh Deilamsalehy, Franck Dernoncourt, and Mohit Bansal. 2023.\n\n\nMeetingQA: Extractive question-answering on meeting transcripts.\n\n\nIn Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 15000\u201315025, Toronto, Canada. Association for Computational Linguistics.\n\n\n",
      "Qian et\u00a0al. (2023)": "\nQian et\u00a0al. (2023)\n\nHongjing Qian, Yutao Zhu, Zhicheng Dou, Haoqi Gu, Xinyu Zhang, Zheng Liu, Ruofei Lai, Zhao Cao, Jian-Yun Nie, and Ji-Rong Wen. 2023.\n\n\nWebbrain: Learning to generate factually correct articles for queries by grounding on large web corpus.\n\n\nPreprint, arXiv:2304.04358.\n\n\n",
      "Rae et\u00a0al. (2019)": "\nRae et\u00a0al. (2019)\n\nJack\u00a0W. Rae, Anna Potapenko, Siddhant\u00a0M. Jayakumar, and Timothy\u00a0P. Lillicrap. 2019.\n\n\nCompressive transformers for long-range sequence modelling.\n\n\nPreprint, arXiv:1911.05507.\n\n\n",
      "Raffel et\u00a0al. (2020)": "\nRaffel et\u00a0al. (2020)\n\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter\u00a0J Liu. 2020.\n\n\nExploring the limits of transfer learning with a unified text-to-text transformer.\n\n\nJournal of machine learning research, 21(140):1\u201367.\n\n\n",
      "Reddy et\u00a0al. (2024)": "\nReddy et\u00a0al. (2024)\n\nVarshini Reddy, Rik Koncel-Kedziorski, Viet\u00a0Dac Lai, Michael Krumdick, Charles Lovering, and Chris Tanner. 2024.\n\n\nDocfinqa: A long-context financial reasoning dataset.\n\n\nPreprint, arXiv:2401.06915.\n\n\n",
      "Saunders et\u00a0al. (2022)": "\nSaunders et\u00a0al. (2022)\n\nWilliam Saunders, Catherine Yeh, Jeff Wu, Steven Bills, Long Ouyang, Jonathan Ward, and Jan Leike. 2022.\n\n\nSelf-critiquing models for assisting human evaluators.\n\n\nPreprint, arXiv:2206.05802.\n\n\n",
      "Shaham et\u00a0al. (2023)": "\nShaham et\u00a0al. (2023)\n\nUri Shaham, Maor Ivgi, Avia Efrat, Jonathan Berant, and Omer Levy. 2023.\n\n\nZeroSCROLLS: A zero-shot benchmark for long text understanding.\n\n\nIn Findings of the Association for Computational Linguistics: EMNLP 2023, pages 7977\u20137989, Singapore. Association for Computational Linguistics.\n\n\n",
      "Shaham et\u00a0al. (2022)": "\nShaham et\u00a0al. (2022)\n\nUri Shaham, Elad Segal, Maor Ivgi, Avia Efrat, Ori Yoran, Adi Haviv, Ankit Gupta, Wenhan Xiong, Mor Geva, Jonathan Berant, and Omer Levy. 2022.\n\n\nSCROLLS: Standardized CompaRison over long language sequences.\n\n\nIn Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 12007\u201312021, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.\n\n\n",
      "Sharma et\u00a0al. (2019)": "\nSharma et\u00a0al. (2019)\n\nEva Sharma, Chen Li, and Lu\u00a0Wang. 2019.\n\n\nBIGPATENT: A large-scale dataset for abstractive and coherent summarization.\n\n\nIn Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2204\u20132213, Florence, Italy. Association for Computational Linguistics.\n\n\n",
      "Stylianou et\u00a0al. (2021)": "\nStylianou et\u00a0al. (2021)\n\nNikolaos Stylianou, Panagiotis Kosmoliaptsis, and Ioannis Vlahavas. 2021.\n\n\nImproved biomedical entity recognition via longer context modeling.\n\n\nIn Artificial Intelligence Applications and Innovations: 17th IFIP WG 12.5 International Conference, AIAI 2021, Hersonissos, Crete, Greece, June 25\u201327, 2021, Proceedings 17, pages 45\u201356. Springer.\n\n\n",
      "Su et\u00a0al. (2024)": "\nSu et\u00a0al. (2024)\n\nJianlin Su, Murtadha Ahmed, Yu\u00a0Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. 2024.\n\n\nRoformer: Enhanced transformer with rotary position embedding.\n\n\nNeurocomputing, 568:127063.\n\n\n",
      "Takeshita et\u00a0al. (2024)": "\nTakeshita et\u00a0al. (2024)\n\nSotaro Takeshita, Tommaso Green, Ines Reinig, Kai Eckert, and Simone Ponzetto. 2024.\n\n\nACLSum: A new dataset for aspect-based summarization of scientific publications.\n\n\nIn Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 6660\u20136675, Mexico City, Mexico. Association for Computational Linguistics.\n\n\n",
      "Tay et\u00a0al. (2020)": "\nTay et\u00a0al. (2020)\n\nYi\u00a0Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler. 2020.\n\n\nLong range arena: A benchmark for efficient transformers.\n\n\nPreprint, arXiv:2011.04006.\n\n\n",
      "Trivedi et\u00a0al. (2022)": "\nTrivedi et\u00a0al. (2022)\n\nHarsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. 2022.\n\n\nMusique: Multihop questions via single-hop question composition.\n\n\nPreprint, arXiv:2108.00573.\n\n\n",
      "Tseng et\u00a0al. (2016)": "\nTseng et\u00a0al. (2016)\n\nBo-Hsiang Tseng, Sheng-Syun Shen, Hung-Yi Lee, and Lin-Shan Lee. 2016.\n\n\nTowards machine comprehension of spoken content: Initial toefl listening comprehension test by machine.\n\n\n",
      "Vaswani et\u00a0al. (2017)": "\nVaswani et\u00a0al. (2017)\n\nAshish Vaswani, Noam\u00a0M. Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan\u00a0N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017.\n\n\nAttention is all you need.\n\n\nIn Neural Information Processing Systems.\n\n\n",
      "Wang et\u00a0al. (2022)": "\nWang et\u00a0al. (2022)\n\nAlex Wang, Richard\u00a0Yuanzhe Pang, Angelica Chen, Jason Phang, and Samuel\u00a0R. Bowman. 2022.\n\n\nSQuALITY: Building a long-document summarization dataset the hard way.\n\n\nIn Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 1139\u20131156, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.\n\n\n",
      "Williams et\u00a0al. (2018)": "\nWilliams et\u00a0al. (2018)\n\nAdina Williams, Nikita Nangia, and Samuel Bowman. 2018.\n\n\nA broad-coverage challenge corpus for sentence understanding through inference.\n\n\nIn Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 1112\u20131122, New Orleans, Louisiana. Association for Computational Linguistics.\n\n\n",
      "Wu et\u00a0al. (2024)": "\nWu et\u00a0al. (2024)\n\nYunshu Wu, Hayate Iso, Pouya Pezeshkpour, Nikita Bhutani, and Estevam Hruschka. 2024.\n\n\nLess is more for long document summary evaluation by llms.\n\n\nPreprint, arXiv:2309.07382.\n\n\n",
      "Yang et\u00a0al. (2018)": "\nYang et\u00a0al. (2018)\n\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William\u00a0W. Cohen, Ruslan Salakhutdinov, and Christopher\u00a0D. Manning. 2018.\n\n\nHotpotqa: A dataset for diverse, explainable multi-hop question answering.\n\n\nPreprint, arXiv:1809.09600.\n\n\n",
      "Zhang et\u00a0al. (2024a)": "\nZhang et\u00a0al. (2024a)\n\nJiebin Zhang, Eugene\u00a0J. Yu, Qinyu Chen, Chenhao Xiong, Dawei Zhu, Han Qian, Mingbo Song, Xiaoguang Li, Qun Liu, and Sujian Li. 2024a.\n\n\nRetrieval-based full-length wikipedia generation for emergent events.\n\n\nPreprint, arXiv:2402.18264.\n\n\n",
      "Zhang et\u00a0al. (2024b)": "\nZhang et\u00a0al. (2024b)\n\nXinrong Zhang, Yingfa Chen, Shengding Hu, Zihang Xu, Junhao Chen, Moo\u00a0Khai Hao, Xu\u00a0Han, Zhen\u00a0Leng Thai, Shuo Wang, Zhiyuan Liu, and Maosong Sun. 2024b.\n\n\n\u221e\\infty\u221ebench: Extending long context evaluation beyond 100k tokens.\n\n\nPreprint, arXiv:2402.13718.\n\n\n",
      "Zhao et\u00a0al. (2022)": "\nZhao et\u00a0al. (2022)\n\nYilun Zhao, Yunxiang Li, Chenying Li, and Rui Zhang. 2022.\n\n\nMultiHiertt: Numerical reasoning over multi hierarchical tabular and textual data.\n\n\nIn Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 6588\u20136600, Dublin, Ireland. Association for Computational Linguistics.\n\n\n",
      "Zhao et\u00a0al. (2023)": "\nZhao et\u00a0al. (2023)\n\nYilun Zhao, Yitao Long, Hongjun Liu, Linyong Nan, Lyuhao Chen, Ryo Kamoi, Yixin Liu, Xiangru Tang, Rui Zhang, and Arman Cohan. 2023.\n\n\nDocmath-eval: Evaluating numerical reasoning capabilities of llms in understanding long documents with tabular data.\n\n\nArXiv, abs/2311.09805.\n\n\n",
      "Zhong et\u00a0al. (2021)": "\nZhong et\u00a0al. (2021)\n\nMing Zhong, Da\u00a0Yin, Tao Yu, Ahmad Zaidi, Mutethia Mutuma, Rahul Jha, Ahmed\u00a0Hassan Awadallah, Asli Celikyilmaz, Yang Liu, Xipeng Qiu, and Dragomir Radev. 2021.\n\n\nQMSum: A new benchmark for query-based multi-domain meeting summarization.\n\n\nIn Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 5905\u20135921, Online. Association for Computational Linguistics.\n\n\n",
      "Zhou et\u00a0al. (2023)": "\nZhou et\u00a0al. (2023)\n\nYijie Zhou, Kejian Shi, Wencai Zhang, Yixin Liu, Yilun Zhao, and Arman Cohan. 2023.\n\n\nOdsum: New benchmarks for open domain multi-document summarization.\n\n\nPreprint, arXiv:2309.08960.\n\n\n"
    },
    "domain": "cs.CL",
    "citation_count": 0
  },
  "b11e6ad0-3ecd-4341-8e43-ac62d7b4b59c": {
    "pk": "b11e6ad0-3ecd-4341-8e43-ac62d7b4b59c",
    "authors": [
      "Tianyu Gao",
      "Alexander Wettig",
      "Howard Yen",
      "Danqi Chen"
    ],
    "title": "How to Train Long-Context Language Models (Effectively)",
    "abstract": "We study continued training and supervised fine-tuning (SFT) of a language model (LM) to make effective use of long-context information. We first establish a reliable evaluation protocol to guide model development -- Instead of perplexity or simple needle-in-a-haystack (NIAH) tests, we use a broad set of long-context tasks, and we evaluate models after SFT with instruction data as this better reveals long-context abilities. Supported by our robust evaluations, we run thorough experiments to decide the data mix for continued pre-training, the instruction tuning dataset, and many other design choices. We find that (1) code repositories and books are excellent sources of long data, but it is crucial to combine them with high-quality short data; (2) training with a sequence length beyond the evaluation length boosts long-context performance; (3) for SFT, using only short instruction datasets yields strong performance on long-context tasks. Our final model, ProLong-8B, which is initialized from Llama-3 and trained on 40B tokens, demonstrates state-of-the-art long-context performance among similarly sized models at a length of 128K. ProLong outperforms Llama-3.18B-Instruct on the majority of long-context tasks despite having seen only 5% as many tokens during long-context training. Additionally, ProLong can effectively process up to 512K tokens, one of the longest context windows of publicly available LMs.",
    "url": "http://arxiv.org/abs/2410.02660v1",
    "timestamp": 1727974012,
    "sections": {
      "1 Introduction": "\n\n1 Introduction\n\nThe ability of language models (LMs) to process extremely long inputs (for example, 128K tokens) has enabled new applications, such as book summarization or learning new tasks on the fly from many examples. However, adapting\nLMs to process long contexts is challenging from an infrastructure and data perspective, and many design decisions are not well understood by open-source practitioners.\n\n\nWhile many works have focused on extending the context length of pre-trained LMs with minimal training (Chen et\u00a0al., 2023; Peng et\u00a0al., 2024),\nFu et\u00a0al. (2024) find that\nthe above methods cannot even perform the simple needle-in-a-haystack (NIAH; Kamradt, 2024) task\nand it is necessary to continually train the LM on long documents for billions of tokens. Frontier open-source models, such as Llama-3.1\u00a0(Dubey et\u00a0al., 2024) and Jamba\u00a0(Lenz et\u00a0al., 2024), also employ a long-context continued training stage, followed by supervised fine-tuning (SFT) on instruction data.\nWe adopt the same setting and study continued training and SFT of a pre-trained LM for effective long-context use.\n\n\nWe first establish a reliable evaluation protocol to provide a meaningful signal for model development.\nMost existing works rely on either perplexity or NIAH for ablating training recipes.\nWe demonstrate that neither is robust for guiding the development\nand opt for a broad range of downstream applications, such as retrieval-augmented generation (RAG), long-document summarization, and many-shot in-context learning (ICL).\nImportantly, we also conduct our evaluations after performing SFT,\neven for all our ablation runs on continued pre-training.\nWe observe that, on some long-context tasks, performance gains only emerge after SFT, which means that best design choices can differ before and after SFT.\n\n\nGuided by our evaluation protocol, we run comprehensive experiments with Llama-3-8B\u00a0(8K original context window; Dubey et\u00a0al., 2024) to study each component of long-context continued training, including data mixture, data and length scaling, supervised fine-tuning, and many other design choices such as cross-document attention masking and position extrapolation.\nMany of our findings are surprising or contradictory to existing claims, for example, (1) training only on long data hurts long-context performance,\n(2) training on longer sequences than the evaluation length helps, and (3) SFT on only short instruction data is sufficient for good long-context performance.\nWe outline our main takeaways and the structure of the paper in the takeaway box at the beginning of this section.\n\n\nOur final model, ProLong, achieves the best performance at a 128K context length among 10B-parameter models, while taking only 5%percent55\\%5 % of the data budget compared to Llama-3.1\u2019s long-context training (Dubey et\u00a0al., 2024).\nProLong has a maximum context length of 512K tokens, making it one of the longest-context LMs available.222Throughout the paper, we use binary prefixes K=210absentsuperscript210=2^{10}= 2 start_POSTSUPERSCRIPT 10 end_POSTSUPERSCRIPT, M=220superscript2202^{20}2 start_POSTSUPERSCRIPT 20 end_POSTSUPERSCRIPT, and B=230superscript2302^{30}2 start_POSTSUPERSCRIPT 30 end_POSTSUPERSCRIPT.\n\n",
      "2 Guide Model Development With Meaningful Evaluations": "\n\n2 Guide Model Development With Meaningful Evaluations\n\nA pre-requisite for training a strong LM is having a robust evaluation suite that can guide model development while tracking its utility in real-world applications.\nWhile synthetic benchmarks like needle-in-a-haystack (NIAH; Kamradt, 2024) and RULER (Hsieh et\u00a0al., 2024) have gained much popularity due to their simplicity and controllability, we are interested in a wider range of tasks that reflect practical usage, such as the ability to reason over the whole document.\nIn the following, we describe our evaluation protocols and\nshowcase why they are critical to our model development.\n\n\n\n2.1 Evaluate on diverse and realistic tasks\n\nWe first make the decision to use HELMET (Yen et\u00a0al., 2024b) as our evaluation suite,\nas it is one of the most comprehensive long-context benchmarks, covering the following tasks:\n\n\n\u2022\n\nRecall: Given a JSON file with random key-values pairs, retrieve the value for a key.\n\n\n\n\u2022\n\nRAG: Answer a question given retrieved Wikipedia documents (NQ, HotPotQA, PopQA).\n\n\n\n\u2022\n\nRe-ranking: Produce top-10 rankings from a shuffled list of documents (MSMARCO).\n\n\n\n\u2022\n\nICL: Learn classification tasks from many in-context examples, where the #classes ranges from 6 to 151; average of 5 datasets (TREC coarse/fine, NLU, Banking77, Clinc-150).\n\n\n\n\u2022\n\nQA: Answer a question given a full-length book (NarrativeQA).\n\n\n\n\u2022\n\nSummarization: Summarize long legal documents (Multi-LexSum).\n\n\n\n\n\nOverall, these diverse tasks reflect a range of long-context abilities including recall, reasoning, learning from context, and robustness to noisy inputs.\nYen et\u00a0al. (2024b) also show that HELMET produces model performance trends that are more consistent with human perceptions unlike other long-context benchmarks.\n\n\nWe showcase the importance of a robust evaluation suite in Table\u00a01.\nAs a predecessor of our work, Fu et\u00a0al. (2024) only consider needle-in-a-haystack (NIAH) and perplexity during model development; evaluations on 3 tasks from HELMET reveal major short-comings of their models despite perfect NIAH scores.\nWe also see how NIAH and even the HELMET recall task become saturated for strong models (Llama-3.1-8B vs. 70B) while other task categories continue to detect differences in their long-context abilities.\n\n\n\u2005\n\n\n\nTable 1: HELMET offers a more holistic long-context evaluation. We reproduce Fu et\u00a0al. (2024) on Llama-3-8B with SFT. We report the instruct Llama versions.\n\n\n\n\n\n\n\n\nHELMET\n\n\nModels\nNIAH\nRecall\nRAG\nRe-rank\n\n\nFu et\u00a0al. (2024)\n100\n95.8\n52.1\n23.1\n\n\nLlama-3.1-8B\n100\n99.4\n56.3\n37.0\n\n\nLlama-3.1-70B\n100\n100\n62.1\n49.2\n\n\n\n\n\n\nWe offer more details about the HELMET evaluation, including its careful choice of metrics, in \u00a7A.1.\nWe did not use all tasks from HELMET for efficiency reasons and we also do not want to overfit to HELMET.\nIf not otherwise specified, we average the performance for each category over all datasets and over evaluation lengths of 32K and 64K;\nfor the final long-context score, we macro-average all categories.\n\n\n\nWhy not perplexity?\n\nBesides synthetic recall tasks, many previous works rely on perplexity (PPL) for evaluating long-context extensions of LMs (Chen et\u00a0al., 2023; Fu et\u00a0al., 2024; Lu et\u00a0al., 2024), which is commonly measured on the PG19 books dataset\u00a0(Rae et\u00a0al., 2020).\nWe use the ablation experiment from \u00a73.2 to showcase why perplexity is not an indicative metric for developing long-context models.\nThe experiment studies how the ratio of long documents affects the performance. We report both our evaluation and the perplexity measured on the last 32K tokens of 64K-length documents from PG19.\nAs shown in Figure\u00a01, while using more long data continues to improve PPL,\nit is clear that using 100% long data significantly hurts downstream long-context performance.\n\n\n\n\n\u2005\n\n\n\n\n\n\nFigure 1: Making design decisions based on perplexity (PPL) is not optimal for long-context downstream tasks.\n\n\n\n\n\n\n2.2 Evaluate after supervised fine-tuning\n\nSupervised fine-tuning (SFT; Ouyang et\u00a0al., 2022) is an additional training stage that fine-tunes the model on a small amount of natural-language instructions and corresponding responses; it enables a base LM to address user queries in a chat format and has become a standard step for producing frontier LMs.\nHere, we consider the difference between evaluating a model before or after SFT.\n\n\nIn preliminary experiments, we continue training Llama-3-8B-Base on 5B-token subsets from the data mix by Fu et\u00a0al. (2024). The mix is based on SlimPajama (Soboleva et\u00a0al., 2023) and upsamples long documents to constitute roughly 70% of tokens, while retaining the original domain proportions.\nThen we conduct SFT on several intermediate checkpoints with UltraChat\u00a0(Ding et\u00a0al., 2023).\n\n\nWe show the benchmarking results before and after SFT in Figure\u00a02.\nLong-context evaluation shows clearer signals when it is conducted after SFT:\n(1) SFT shows that the model continues to improve with more training tokens on RAG and re-ranking, while the improvement is less clear or does not exist when evaluated before SFT.\n(2) SFT enables evaluation on realistic applications like QA and summarization, which require instruction following and have low performance before SFT.\nWe also note that the variance from two random training runs is not substantially higher after the additional SFT phase. Therefore, unless otherwise specified, we report the long-context performance after SFT.\n\n\nFigure 2: \nImprovements on RAG and re-ranking tasks are only observed when evaluating models after a supervised fine-tuning (SFT) phase on instruction data. The models are trained on the pre-training data mix by Fu et\u00a0al. (2024). We report the mean and standard deviations over two training runs. \n\n\nWe dive deeper into supervised fine-tuning in \u00a75 and explore different training datasets, as well as the use of synthetic long instruction data. However, we find that simply fine-tuning on UltraChat remains a surprisingly competitive choice.\n\n\n\n\n2.3 Check that short-context performance is preserved\n\nLong-context abilities should not come at the expense of short-context performance, particularly since short-context evaluations cover a wider range of capabilities, e.g., world knowledge, commonsense, and mathematical reasoning.\nHowever, short-context evaluation has largely been neglected by previous long-context research.\nWe report on 5 tasks from the the Open LLM Leaderboard (Beeching et\u00a0al., 2023): HellaSwag (Zellers et\u00a0al., 2019), MMLU (Hendrycks et\u00a0al., 2021), ARC-challenge (Clark et\u00a0al., 2018), WinoGrande (Sakaguchi et\u00a0al., 2021), and GSM8K (Cobbe et\u00a0al., 2021).\nWe evaluate short-context performance before SFT, since this allows for a direct comparison to the base model which was used as initialization for the long-context training.\n\n\n\nPrevious techniques deteriorate short-context performance.\n\nWe show in Table\u00a02 that both training-free position extrapolation, as well as fine-tuning with an existing long data mixture (Fu et\u00a0al., 2024)\ndo not preserve the strong performance of Llama-3-8B on standard short-context tasks.\nThis motivates us to find data sources which retain the initial model\u2019s strong short-context performance.\n\n\n\n\n\u2005\n\n\n\nTable 2: \nApplying position extrapolation (PE) to\nLlama-3-8B by changing the RoPE frequency base (\u00a7B.1)\nor fine-tuning it\non a long-context SlimPajama mixture (Fu et\u00a0al., 2024; Soboleva et\u00a0al., 2023) deteriorates the performance of this top-shelf pre-trained LM on short-context tasks.\n\n\n\n\n\n\n\nHSwag\nMMLU\nARC-c\nWG\nGSM8K\n\n\n\n\nLlama-3-8B\n82.1\n66.5\n59.4\n77.1\n44.7\n\n\n+ PE\n81.5\n64.7\n58.1\n75.5\n40.1\n\n\n+ SlimPajama\n81.0\n63.1\n57.8\n75.1\n40.6\n\n\n\n\n\n\n",
      "3 Long-Context Data Curation": "\n\n3 Long-Context Data Curation\n\nThe quality and composition of training data has been found to be the most important factor for LM pre-training (Penedo et\u00a0al., 2023; Wettig et\u00a0al., 2024; Li et\u00a0al., 2024a) and is therefore\na primary focus of our study. To make data decisions, we perform ablation experiments:\nwe continue to train Llama-3-8B-Base for 5B tokens with a maximum length of 64K tokens and evaluate according to \u00a72. See \u00a7A.4 for more details of our ablation setting.\n\n\nWe aim to boost the long-context task performance while preserving the short-context performance of the original model. Starting from the intuition that the data should be a mixture of long and short documents, we study these choices separately.\nIn our ablations, the long data is comprised of single-document chunks of 64K tokens, whereas for the short data, we construct batches by packing documents until we reach 64K tokens per sequence.\n\n\n\n3.1 Code repositories and books are good sources of long-context data\n\n\nSlimPajama.\n\nWe analyze the quantity of long data in SlimPajama (SP; Soboleva et\u00a0al., 2023). Table\u00a03 shows that books account for the majority of long-context tokens.\nWhen inspecting the long data in CommonCrawl (CC), we observe that though varied in quality, it also contains some book-like content, which future work could identify via data selection methods.\n\n\n\nCode repositories.\n\nWhile only few files from GitHub reach a very long length (which also tend to be lower quality as suggested by Singh et\u00a0al., 2024), we construct an abundant source of long-context data from the Stack (Kocetkov et\u00a0al., 2023) by concatenating all files from a repository to form a single document. Unlike Guo et\u00a0al. (2024), we do not order the files based on dependencies, which should increase the distance between dependent files and reduce recency bias.\n\n\n\n\n\u2005\n\n\n\nTable 3: Long text documents (\u2265\\geq\u226564K tokens) by data sources.\n\n\n\n\n\nData\n#Long\n\n\ntokens\n\n\nCode Repos\n98.8B\n\n\nSP/Books\n33.2B\n\n\nSP/CC\n15.3B\n\n\nSP/Arxiv\n5.2B\n\n\nSP/GitHub\n2.8B\n\n\nSP/Wiki\n0.1B\n\n\nSP/StackEx\n\n<<<0.1B\n\n\nSP/C4\n\n<<<0.1B\n\n\n\n\n\n\nData mixture.\n\nWe train models with 60% of long-context data and 40% of our ShortMix (\u00a73.3).\nTable\u00a04 shows that using code repositories alone performs the best on stress-test recall tasks. Meanwhile, books are more broadly beneficial for in-context learning, summarization and re-ranking.\nAn equal mix of books and code repositories achieves the best overall performance.\nNote that short-context task performance remains consistent due to our high-quality short data mix.\n\n\nTable 4: Impact of different long data sources, while keeping the 40% short data component fixed. Code repositories particularly helps recall, while books are more effective on re-ranking, ICL, and summarization. Mixing the two achieves the overall best performance.\n\n\n\nLong Data (60%)\nLong-Context\nShort-Context\n\n\nRecall\nRAG\nRe-rank\nICL\nQA\nSumm.\nAvg.\nAvg.\n\n\n\n\nCommonCrawl\n84.1\n53.3\n28.1\n67.5\n35.2\n37.0\n50.9\n66.5\n\n\nBooks\n94.9\n53.9\n30.7\n72.2\n33.2\n37.7\n53.8\n65.5\n\n\nCode Repos\n99.2\n53.8\n29.0\n61.2\n34.7\n36.2\n52.3\n65.9\n\n\nBooks/Repos 1:1\n96.0\n54.9\n29.4\n73.9\n35.7\n37.9\n54.6\n65.5\n\n\n\n\n\n\n\n\n3.2 Training only on long data hurts long-context performance\n\nThe ratio between short/long data is another crucial factor for downstream performance.\nPrior work either trains only on long data (Peng et\u00a0al., 2024) or adds some short training data (Yen et\u00a0al., 2024a; Fu et\u00a0al., 2024).\nHowever, we are the first to systematically study the impact of short/long ratio.\n\n\nFigure\u00a03 shows that\nshort task performance monotonically decreases as the long data increases.\nThe trends for long-context vary by tasks and are further complicated by SFT:\nOn tasks like recall and RAG, the performance before SFT prefers high proportions of long data, while the performance after SFT drastically deteriorates with more long data.\nWe hypothesize that specializing the model only on long data makes it a poor initialization for generic SFT\u2014highlighting the importance of evaluating checkpoints after SFT (\u00a72.2).\nWhile some long-context tasks benefit from more long data consistently (ICL) or show no clear pattern (re-ranking), the best average performance is achieved at 60% long data and 40% short data, which we adopt for our final ProLong model.\n\n\nFigure 3: \nImpact of short/long data ratio.\nAll models are trained on books/repos long data and our ShortMix for 5B tokens.\nMore long data initially improves long-context performance, but then\nbecomes impairing. More long data also consistently degrades the short-context performance.\n\n\n\n\n\n3.3 Choosing a high-quality short-context mix is important\n\nWe saw in \u00a72.3 that it is difficult to preserve the strong performance of Llama-3-8B on short-context tasks during long-context fine-tuning.\nWe adopt our best long-context settings (Book/repo data and 60% long/40% short) and study the impact of different short-context training mixes. We experiment with SlimPajama (Soboleva et\u00a0al., 2023), FineWeb-Edu (Penedo et\u00a0al., 2024), DCLM-Baseline (Li et\u00a0al., 2024a), and our own ProLong ShortMix.\nOur ShortMix is inspired by the \u201cstage 2 training\u201d in MiniCPM\u00a0(Hu et\u00a0al., 2024a) and Dolma-1.7 (Soldaini et\u00a0al., 2024), which use more knowledge-intensive, downstream-related data at the end of pre-training.\nTable\u00a05 shows the composition of our ShortMix.333\nSince we do not truncate documents in the short data component unnecessarily, it includes a small percentage of documents longer than 8K. See Table\u00a014 in the appendix for the dataset length statistics.\n\n\n\u2005\n\n\n\nTable 5: Our ShortMix.\n\n\n\n\n\nComponents\n%\n\n\n\n\nFineWeb\n25\n\n\nFineWeb-Edu\n25\n\n\nWikipedia\n10\n\n\nTulu-v2\n10\n\n\nStackExchange\n10\n\n\nArXiv\n10\n\n\nOpenWebMath\n10\n\n\n\n\n\n\nTable 6: Impact of\ndifferent short data sources.\nThe long-context performance is the average of 6 categories at the lengths of 32K and 64K.\n\n\n\n\nShort Data (40%)\nLong-Context\nShort-Context\n\n\nAvg.\nHellaS.\nMMLU\nARC-c\nWG\nGSM8K\nAvg.\n\n\nOriginal model (Llama-3-8B)\n-\n82.1\n66.5\n59.4\n77.1\n44.7\n66.0\n\n\n\n\nSlimPajama\n52.9\n81.2\n63.0\n58.5\n76.2\n41.9\n64.2\n\n\nFineWeb-Edu\n53.0\n81.0\n62.6\n57.7\n74.4\n39.4\n63.0\n\n\nDCLM-Baseline\n52.0\n82.0\n65.6\n59.6\n77.4\n39.4\n64.8\n\n\nProLong ShortMix\n54.6\n81.6\n65.3\n58.0\n76.2\n46.6\n65.5\n\n\n\n\n\nTable\u00a06 demonstrates that the short data component has a substantial impact on both short-context and long-context downstream performance.\nOur curated ShortMix outperforms other short data sources on both short and long-context tasks and our data domains are particularly important for retaining Llama-3-8B\u2019s performance on mathematical reasoning.\nSurprisingly, we find that fine-tuning only using FineWeb-Edu\u2014a dataset that is curated to help with knowledge-intensive tasks like MMLU\u2014performs poorly as a short-context component, and we combine it with more diverse data sources in our ShortMix.\nDCLM-Baseline performs well on all short-context tasks except for GSM8K.\nThis can likely be improved by combining with math-related datasets, but as we added the DCLM-baseline ablation at the conclusion of the project, we leave this exploration to future work.\n\n\n",
      "4 Scaling the Size and Length of the Training Data": "\n\n4 Scaling the Size and Length of the Training Data\n\nTraining for more steps is well-known to improve downstream tasks in regular pre-training, but little analysis has been done in the context of long-context continued training.\nWe incorporate the lessons from our ablation experiments and arrive at the ProLong recipe, which we describe in detail in \u00a76. Notably, we scale up the training budget to longer sequences (up to 512K) and more tokens (20B tokens at a maximum sequence length of 64K and an additional 20B tokens at 512K). We reset the learning rate schedule and increase the RoPE frequency base when switching from 64K to 512K context lengths.\nIn this section, we analyze the impact of these decisions.\n\n\nIncreasing the number of steps helps.\n\nIn Figure\u00a04, we plot the downstream performance of intermediate checkpoints of our 40B-token runs.\nWhile the long-context performance fluctuates throughout training, we observe positive trends on recall, RAG, re-ranking, and summarization.\nFor short-context tasks, we observe the average performance initially drops from the initialization, but gradually recovers. Performance again drops when switching from 64K to 512K sequence length, but also recovers with additional training.\n\n\nFigure 4: Performance (avg. of 32K and 64K) of our ProLong model throughout training.\n\n\n\n\nIncreasing the training length beyond the evaluation length helps.\n\nOne might assume that we should train long-context models on the maximum sequence length that we want the model to support. Many works even emphasize extrapolation to even longer sequences at inference time (Press et\u00a0al., 2022; Xiao et\u00a0al., 2024b; a; Yen et\u00a0al., 2024a; Chen et\u00a0al., 2023).\nIn contrast, we observe that training on a longer sequence length (512K tokens) substantially improves the long-context performance at a shorter evaluation length (64K tokens).\n\n\nWe establish this by initializing\nwith a model that was trained for 20B tokens at 64K and either (1) continuing training at 64K, or (2) switching to the 512K training.\nWe use the same hyperparameters and data mixtures in either experiment.\nWe evaluate a checkpoint after 4B training tokens\nat a evaluation length of 64K.\nComparing the two runs\nin Table\u00a07,\nwe see consistent gains from switching to the 512K training length.444While we demonstrate the benefit of longer data, we note that training with longer sequences is more expensive, and may therefore not be the computationally optimal choice.\n\n\nTable 7: Impact of training models on different sequence lengths.\nAll the results are evaluated at a sequence length of 64K.\nWe see that training at a maximum length beyond the evaluation context window consistently improves the long-context performance.\n\n\n\n\nMax Seq. Length\nRecall\nRAG\nRe-rank\nICL\n\n\n\n\nProLong 64K training (20B)\n96.5\n52.7\n22.8\n70.6\n\n\n+4B 64K training\n95.0\n56.4\n28.0\n78.8\n\n\n+4B 512K training\n98.5\n56.9\n32.9\n79.2\n\n\n\n\n\n",
      "5 Supervised Fine-Tuning for Long-Context LMs": "\n\n5 Supervised Fine-Tuning for Long-Context LMs\n\nIn this section, we study how to best enable long-context language models to follow instructions.\nWe focus on supervised fine-tuning on instruction datasets (Ouyang et\u00a0al., 2022) and\nleave reinforcement learning and preference optimization for future work.\n\n\nAll our experiments in this section use the ProLong base model, which was\ntrained for\n40B tokens at a maximum sequence length of 512K.\nIn comparison, open-source instruction data are very short, e.g.,\nUltraChat (Ding et\u00a0al., 2023) conversations have 1.2K tokens on average and 4.1K tokens maximum.\nTo bridge this gap, several works\u00a0(Xiong et\u00a0al., 2023; Dubey et\u00a0al., 2024; Xiong et\u00a0al., 2024)\nhave proposed to generate long instruction data synthetically.\n\n\nWe consider three popular SFT datasets\u2014UltraChat\u00a0(Ding et\u00a0al., 2023), Tulu-v2\u00a0(Ivison et\u00a0al., 2023), ShareGPT555https://huggingface.co/datasets/RyokoAI/ShareGPT52K.\u2014and three sources of synthetic data: For synthetic QA, we prompt Llama-3-8B-Instruct\nto generate a question-and-answer pair given a random chunk from a long document; we reuse the QA pairs for synthetic RAG but we present\na random list of chunks from the document to mimic retrieved passages; for synthetic summarization, we generate summaries for long books via recursive summarization (Wu et\u00a0al., 2021).\nFor all synthetic data, we write several templates, which we sample at random to increase diversity. More details can be found in \u00a7A.5.\nWe always use a combination of 40% synthetic QA, 30% synthetic RAG, and 30% synthetic summarization in our synthetic instruction dataset.\nThe hyperparameters for the instruction tuning experiments can be found in Table\u00a09.\n\n\nShort-context instruction data yields strong long-context results.\n\nWe first establish that UltraChat outperforms Tulu-v2 and ShareGPT\nin Table\u00a022.\nWe therefore use it when studying the ratio of synthetic long-context instruction data in Table\u00a08.\nSurprisingly, we find that adding synthetic data does not improve the performance on these very long-context tasks, and adding even as little as 1% synthetic data hurts the performance in our setting.\nTherefore, we use only short-context UltraChat data for SFT of our final ProLong model.\n\n\nTable 8: Effect of different ratios of synthetic SFT data (mixed with UltraChat). We report the 32K-and-64K-averaged performance except tasks marked with \u2020, which are evaluated at 512K for stress testing. The number of percentage is based on #tokens, not #samples.\n\n\n\n% Synthetic Data\nJsonKV\u2020\n\nRAG\nRe-rank\nICL\nQA\u2020\n\nSumm.\u2020\n\nAvg.\n\n\n\n\n0%\n65.7\n58.1\n38.5\n80.3\n49.7\n42.1\n55.7\n\n\n1%\n61.5\n57.0\n38.3\n80.8\n45.3\n41.5\n54.1\n\n\n3%\n62.0\n56.4\n37.9\n80.6\n44.8\n39.5\n53.5\n\n\n10%\n70.3\n55.5\n36.1\n80.6\n41.7\n39.4\n53.9\n\n\n50%\n45.8\n48.8\n18.8\n70.5\n42.3\n33.3\n43.3\n\n\n\n\n\nWhy do our conclusions about synthetic data differ from previous work?\nWe offer the following hypotheses:\n(1) Previous work like Xiong et\u00a0al. (2024); Bai et\u00a0al. (2024a) may have insufficient long-context training and the synthetic data acts as additional long-context training data.\n(2) Our instruction dataset is much smaller compared to the private instruction data used for Llama-3.1 (Dubey et\u00a0al., 2024)\u2014it is possible that when using an extensive short instruction dataset, mixing in synthetic long data avoids the model from degenerating on long-context tasks.\n\n\nTable 9: The training recipe for ProLong.\n\n\n\nContinued Long-context Training\n\n\nData\n30% code repos, 30% books, 3% textbooks, 37% ShortMix\n\n\n\nShortMix:\n27% FineWeb-Edu, 27% FineWeb,\n\n\n\n\n11% Tulu-v2, 11% StackExchange,\n\n\n\n\n8% Wikipedia, 8% OpenWebMath, 8% ArXiv\n\n\n\n\nLength\nCurriculum\n\nStage 1 (64K):\nCode repos, books, and textbooks at length 64K\n\n\nStage 2 (512K):\nCode repos: 50% at length 512K, 50% at length 64K\n\n\n\n\nBooks: 17% at length 512K, 83% at length 64K\n\n\n\n\nTextbooks at length 512K\n\n\nSteps\nStage 1: 20B tokens (2.2K H100 hours), \u2005 Stage 2: 20B tokens (12.2K H100 hours)\n\n\nModel\nInitialization:\nLlama-3-8B-Instruct (original RoPE base freq. 5\u00d71055superscript1055\\times 10^{5}5 \u00d7 10 start_POSTSUPERSCRIPT 5 end_POSTSUPERSCRIPT)\n\n\n\nRoPE:\nStage 1: 8\u00d71068superscript1068\\times 10^{6}8 \u00d7 10 start_POSTSUPERSCRIPT 6 end_POSTSUPERSCRIPT, Stage 2: \u2005 1.28\u00d71081.28superscript1081.28\\times 10^{8}1.28 \u00d7 10 start_POSTSUPERSCRIPT 8 end_POSTSUPERSCRIPT\n\n\n\n\nAttention:\nFull attention with cross-document attention masking\n\n\nOptim.\nAdamW (weight decay = 0.10.10.10.1, \u03b21=0.9subscript\ud835\udefd10.9\\beta_{1}=0.9italic_\u03b2 start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT = 0.9, \u03b22=0.95subscript\ud835\udefd20.95\\beta_{2}=0.95italic_\u03b2 start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT = 0.95)\n\n\n\nLR:\n\n1\u2062e\u221251\ud835\udc5251e-51 italic_e - 5 with 10%percent1010\\%10 % warmup and cosine decay to 1\u2062e\u221261\ud835\udc5261e-61 italic_e - 6, each stage\n\n\n\nBatch size:\n4M tokens for stage 1, 8M tokens for stage 2\n\n\nSupervised Fine-tuning (SFT)\n\n\nData\nUltraChat\n\n\n\nSteps\n1B tokens\n\n\n\nOptim.\nAdamW (weight decay = 0.10.10.10.1, \u03b21=0.9subscript\ud835\udefd10.9\\beta_{1}=0.9italic_\u03b2 start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT = 0.9, \u03b22=0.95subscript\ud835\udefd20.95\\beta_{2}=0.95italic_\u03b2 start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT = 0.95)\n\n\n\nLR = 2\u2062e\u221252\ud835\udc5252e-52 italic_e - 5 (cosine decay to 2\u2062e\u221262\ud835\udc5262e-62 italic_e - 6), warmup = 5%percent55\\%5 %\n\n\n\n\nBatch size = 4M tokens\n\n\n\n\n\n",
      "6 The ProLong Model: Recipe and Results": "\n\n6 The ProLong Model: Recipe and Results\n\n\n6.1 Final recipe\n\nWe summarize the training recipe for ProLong in Table\u00a09.\nOur final model starts from the Llama-3-8B-Instruct model and is trained on 64K sequence length for 20B tokens.\nIt is then further trained on 512K sequence length for 20B tokens (ProLong base), which we achieve using sequence parallelism (Li et\u00a0al., 2023).\nWe obtain the final ProLong model via SFT of the base model on UltraChat.\nOne small difference on the data mixture between our ablations and the final model is that we mix in 3% high-quality textbooks\u00a0(Chevalier et\u00a0al., 2024), as book-like data are shown to be beneficial for long-context (\u00a73.1) and textbooks are highly educational. This also slightly changes the proportions of ShortMix.\nYou can find more details about our data processing (\u00a7A.2) and the training stack (\u00a7A.3) in the appendix.\n\n\nIn the following, we elaborate on several carefully ablated design choices in our recipe.\n\n\nRoPE frequency base tuning.\n\nWe find that changing the RoPE\u00a0(Su et\u00a0al., 2021) frequency base to achieve position extrapolation\u00a0(Xiong et\u00a0al., 2023; emozilla, 2023) significantly improves long-context performance, even with a significant amount of training. \u00a7B.1 shows our ablation on the best RoPE base to use. While the original Llama models use a RoPE base of 105superscript10510^{5}10 start_POSTSUPERSCRIPT 5 end_POSTSUPERSCRIPT, we use a base of 8\u00d71068superscript1068\\times 10^{6}8 \u00d7 10 start_POSTSUPERSCRIPT 6 end_POSTSUPERSCRIPT for the 64K setting and 1.28\u00d71081.28superscript1081.28\\times 10^{8}1.28 \u00d7 10 start_POSTSUPERSCRIPT 8 end_POSTSUPERSCRIPT for the 512K setting.\n\n\n\nDisabling cross-document attention.\n\nDing et\u00a0al. (2024a) show that masking out attention across document boundaries improve model performance and this was also used during Llama-3 pre-training (Dubey et\u00a0al., 2024).\nIn \u00a7B.2, we show that disabling cross-document attention in continued training benefits both the short and long-context performance.\nDisabling cross-document attention can also result in higher training throughput, which we describe in more detail in \u00a7A.3.\n\n\n\nStarting from Llama-3-8B-Instruct.\n\nWhile we conduct all our long-context training ablations with the base model of Llama-3-8B,\nwe use Llama-3-8B-Instruct as the initialization for the final ProLong model.\n\u00a7B.3 shows that while slightly improving the long-context performance, Llama-3-8B-Instruct significantly enhances the short-context performance.\n\n\n\n\n\n6.2 ProLong performance\n\nWe present the final HELMET evaluation results of ProLong in Table\u00a010.\nWe compare to a number of frontier long-context LMs, namely MegaBeam666https://huggingface.co/aws-prototyping/MegaBeam-Mistral-7B-512k., Llama-3.1\u00a0(Dubey et\u00a0al., 2024), Qwen2\u00a0(Yang et\u00a0al., 2024a), Phi-3\u00a0(Abdin et\u00a0al., 2024), Mistral-Nemo777https://huggingface.co/mistralai/Mistral-Nemo-Instruct-2407., Jamba-1.5\u00a0(Lenz et\u00a0al., 2024), Claude-3.5-Sonnet\u00a0(Anthropic, 2024), Gemini-1.5\u00a0(Reid et\u00a0al., 2024), and GPT-4o\u00a0(Achiam et\u00a0al., 2023).\n\n\nProLong outperforms all 10B-scale models on our long-context evaluation. Notably, ProLong outperforms Llama-3.1-8B-Instruct\non all categories except summarization.\nProLong achieves this with only 5% of Llama-3.1\u2019s long-context data budget (40B vs. 800B tokens).\nWe also showcase the strength of ProLong with several QA examples in Table\u00a023.\n\n\nTable 10: Our main evaluation results on HELMET (Yen et\u00a0al., 2024b; details in \u00a7A.1).\nAll results are averaged over sequence lengths of 32K, 64K, and 128K. For all models, we use the corresponding instruction version.\nProLong is one of the best performing 10B-scale LMs.\nThe complete set of results can be found in \u00a7C.\n\n\n\n\nModel\nMax Len.\nRecall\nRAG\nICL\nRe-rank\nQA\nSumm.\nAvg.\n\n\nProLong (8B)\n512K\n99.4\n66.0\n81.1\n33.2\n40.8\n40.5\n60.2\n\n\nMegaBeam-Mistral (7B)\n512K\n99.4\n58.1\n82.1\n22.1\n33.7\n43.6\n56.5\n\n\nMeta-Llama-3.1 (8B)\n128K\n98.7\n62.8\n79.7\n26.6\n40.4\n46.1\n59.0\n\n\nQwen2 (7B)\n128K\n34.4\n43.4\n54.8\n4.6\n23.3\n38.5\n33.2\n\n\nPhi-3-small (7B)\n128K\n74.8\n60.6\n82.0\n18.5\n34.1\n42.4\n52.1\n\n\nMistral-Nemo (12B)\n128K\n24.9\n48.1\n82.0\n4.7\n37.7\n37.0\n39.1\n\n\nJamba-1.5-Mini (12B/52B)\n256K\n87.7\n61.3\n88.4\n25.9\n42.0\n38.6\n57.3\n\n\nMeta-Llama-3.1 (70B)\n128K\n98.5\n65.9\n80.0\n39.4\n47.2\n51.1\n63.7\n\n\nClaude-3.5-Sonnet\n200K\n99.4\n44.0\n79.3\n19.9\n38.1\n49.2\n55.0\n\n\nGemini-1.5-Pro\n2M\n94.2\n71.4\n78.9\n65.3\n44.4\n56.2\n68.4\n\n\nGPT-4o\n128K\n99.9\n71.5\n86.7\n59.6\n47.0\n55.7\n70.1\n\n\n\n\n\nSince most existing models do not support more than 128K tokens,\nto showcase ProLong\u2019s 512K context length,\nwe stress test ProLong on the QA and summarization tasks from 32K to 512K888\nIn QA and summarization, we truncate the documents at the evaluation length; hence an effective long-context model should demonstrate better performance on longer lengths.. Table\u00a011 shows that ProLong continues to improve at a longer context window.\n\n\n\u2005\n\n\n\nTable 11: ProLong at 512K.\n\n\n\n\n\n\n32K\n64K\n128K\n512K\n\n\n\n\nQA\n31.7\n43.7\n46.7\n49.7\n\n\nSumm\n40.4\n39.8\n41.5\n42.1\n\n\n\n\n\n\nTable 12: Results\non the NoCha benchmark\u00a0(Karpinska et\u00a0al., 2024).101010https://github.com/marzenakrp/nocha. NoCha has a private test set and all evaluation is done by the NoCha authors. Hence, we report models from Table\u00a010 that are also on the NoCha leaderboard.\nProLong is the only model that achieves above-random performance in the <<<75K category and it consistently beats Llama-3.1. Different from the original NoCha leaderboard, we report the average accuracy over all test instances without filtering the test examples based on the model\u2019s context window lengths.\n\n\n\nModel\nMax Len.\n\n<<<75K\n75K-127K\n127K-180K\n\n>>>180K\n\n\n\n\nProLong (8B)\n512K\n28.4\n17.0\n13.1\n20.3\n\n\nMegaBeam-Mistral (7B)\n512K\n19.8\n18.3\n17.5\n15.6\n\n\nMeta-Llama-3.1 (8B)\n128K\n17.3\n16.4\n0.0\n0.0\n\n\nMistral-Nemo (12B)\n128K\n13.6\n0.4\n0.0\n0.0\n\n\nJamba-1.5-Mini (12B/52B)\n256K\n27.2\n28.0\n24.4\n6.2\n\n\nMeta-Llama-3.1 (70B)\n128K\n42.0\n25.0\n0.0\n0.0\n\n\nGemini-1.5-Pro\n2M\n24.7\n38.8\n35.3\n46.9\n\n\nGPT-4o\n128K\n55.6\n58.4\n0.0\n0.0\n\n\n\n\n\nBesides HELMET, we also evaluate our models on NoCha\u00a0(Karpinska et\u00a0al., 2024)\u2014a claim verification dataset on 67 recently published English fictional books.\nWe chose this dataset because (1) it minimizes the data contamination problem as all the books are unlikely to exist in the model pre-training data; (2) all the claims are written by human readers and require global reasoning. Each test instance contains two contradictory claims, and the models must correctly judge both to pass.\n\n\nfootnote\u00a010 demonstrates the NoCha evaluation results.\nAmong 10B-scale models, ProLong achieves the best accuracy on the extremely long test instances (>>>180K); on test instances <<<75K tokens, ProLong significantly outperforms other models and is the only model that is better than random guessing (25%).\nThis further showcases the strength of our training recipe and the ProLong model.\n\n\n",
      "7 Related Work": "\n\n7 Related Work\n\nAdapting existing LMs for long contexts.\n\nMany works explore\nextending the LM context windows with minimal training,\neither by\nposition extrapolation\u00a0(Chen et\u00a0al., 2023; Peng et\u00a0al., 2024; Chen et\u00a0al., 2024; Ding et\u00a0al., 2024b; Liu et\u00a0al., 2024a; Zhang et\u00a0al., 2024b; Zhu et\u00a0al., 2024; Zhao et\u00a0al., 2024; Wu et\u00a0al., 2024; Hu et\u00a0al., 2024b)\nor\nmanipulating the attention patterns\u00a0(Chen et\u00a0al., 2024; Xiao et\u00a0al., 2024b; a; Bertsch et\u00a0al., 2023; Jin et\u00a0al., 2024).\nYoshida et\u00a0al. (2020); Choromanski et\u00a0al. (2021); Chevalier et\u00a0al. (2023)\ninstead explore the idea of compressing the long contexts into shorter forms.\nHowever, Fu et\u00a0al. (2024); Lu et\u00a0al. (2024) show that using full attention, applying simple position extrapolation, and fine-tuning the model on long documents reach much stronger results.\n\n\nLlama 3.1\u00a0(Dubey et\u00a0al., 2024) and Jamba\u00a0(Lieber et\u00a0al., 2024) achieve long-context capabilities by adding a long-context continued training stage between standard pre-training and supervised fine-tuning, which is the setting we follow.\nFu et\u00a0al. (2024) study the data engineering for this setting and argue that 0.5B tokens of domain-balanced, length-upsampled data is sufficient for acquiring the long-context recall ability\u2014which we show is not sufficient if a more holistic evaluation is taken.\nXiong et\u00a0al. (2023); Dubey et\u00a0al. (2024); Lieber et\u00a0al. (2024); Xiong et\u00a0al. (2024); An et\u00a0al. (2024b); Bai et\u00a0al. (2024a) also adopt synthetically-generated long data in the SFT stage; however,\nwe find that\nusing standard, short-context instruction data achieves the best long-context results in our setting.\n\n\n\nEfficient long-context architectures.\n\nThere have been many efforts in designing more efficient architectures,\nfor example,\nlinear attention/RNNs\u00a0(Gu & Dao, 2023; Dao & Gu, 2024; Ma et\u00a0al., 2022; Sun et\u00a0al., 2023; Peng et\u00a0al., 2023; Yang et\u00a0al., 2024b),\nand\nalternative attention architectures\u00a0(Rubin & Berant, 2023; Sun et\u00a0al., 2024; Yen et\u00a0al., 2024a).\nHowever,\nthey often require training from scratch and many have the inherent limitations in terms of long-context recall (Jelassi et\u00a0al., 2024; Arora et\u00a0al., 2024).\nRecent works explore hybrid models (Waleffe et\u00a0al., 2024; Lieber et\u00a0al., 2024)) or distilling existing LMs into hybrid models (Wang et\u00a0al., 2024) and show promising results.\n\n\n\nLong-context evaluation.\n\nMany benchmarks have been proposed for long-context evaluation\n(Shaham et\u00a0al., 2023; Hsieh et\u00a0al., 2024; Krishna et\u00a0al., 2023; Zhang et\u00a0al., 2024a; An et\u00a0al., 2024a; Bai et\u00a0al., 2024b)\nThere are works studying particular aspects of long-context LMs as well,\nsuch as positional bias (Liu et\u00a0al., 2024b),\nin-context learning\u00a0(Bertsch et\u00a0al., 2024; Li et\u00a0al., 2024b), and book-length summarization\u00a0(Kim et\u00a0al., 2024).\nIn this work, we follow Yen et\u00a0al. (2024b)\nfor its diverse application coverage and reliable evaluations.\n\n\n",
      "8 Conclusion": "\n\n8 Conclusion\n\nWe study the problem of given a short-context pre-trained LM, how to\nmost effectively continually pre-train and SFT the model to be long-context.\nWe conduct thorough ablations on each component and\nmany of our findings contradict existing practices or beliefs.\nWe use all the findings to produce ProLong, a new state-of-the-art long-context LM. We release all our code, data, and models publicly\nand hope that our findings will\nboost research and applications of long-context LMs.\n\n\nLimitations\n\nAlthough we try to ablate the major components of our training recipe,\ndue to resource limitations, we cannot exhaust all aspects, such as the optimization hyperparameters and additional data mixtures.\nWe also limit ourselves to the 10B-scale regime and the Llama-3 models, which may limit the generalizability of our findings and recipe.\nAnother concern is that we are overfitting to the tasks chosen for model development\u2014however, we do not directly train on those datasets and guiding model development with benchmark tasks has become a common practice in pre-trained LM development. We also show that our final recipe and model perform well on an additional evaluation dataset, NoCha.\n\n\n\nAcknowledgments\n\nWe acknowledge Mengzhou Xia, Zexuan Zhong, Samyak Gupta, Dan Friedman, Yihe Dong, Abhishek Panigrah, Adithya Bhaskar, Colin Wang, Carlos Jimenez, and other members of Princeton Language and Intelligence for their helpful feedback and discussion. We also thank Luca Soldaini for providing comments on a draft.\nWe thank Marzena Karpinska, Tanya Goyal, and Mohit Iyyer for their help with the NoCha evaluation.\nTianyu Gao is supported by an IBM PhD Fellowship.\nThis work is gratefully supported by an NSF CAREER award (IIS-2239290), a grant from Intel, and Microsoft Azure credits through the \u201cAccelerate Foundation Models Academic Research\u201d Initiative.\n\n\n",
      "Appendix A Experiment Details": "\n\nAppendix A Experiment Details\n\n\nA.1 Evaluation\n\nTable 13: The details for our long-context evaluation following HELMET (Yen et\u00a0al., 2024b). \n\n\n\nCategory\nMetrics\n\n\nTasks and Datasets\n\n\n\n\n\n\nRecall\nSubEM\n\n\nGiven a randomly-generated long JSON file and a key, retrieve the corresponding value\u00a0(Liu et\u00a0al., 2024b).\n\n\n\n\nRAG\nSubEM\n\n\nGiven a question and many retrieved Wikipedia documents (shuffled), answer the question\u00a0(Liu et\u00a0al., 2024b). Datasets: NaturalQuestion\u00a0(Kwiatkowski et\u00a0al., 2019), HotpotQA\u00a0(Yang et\u00a0al., 2018), and PopQA\u00a0(Mallen et\u00a0al., 2023).\n\n\n\n\nRe-rank\nnDCG@10\n\n\nGiven a query and many retrieved documents (shuffled), re-rank the top-10 documents. Datasets: MSMARCO\u00a0(Bajaj et\u00a0al., 2016).\n\n\n\n\nICL\nAccuracy\n\n\nDatasets selected from Bertsch et\u00a0al. (2024): TREC coarse, TREC fine\u00a0(Hovy et\u00a0al., 2001), NLU\u00a0(Liu et\u00a0al., 2021), Banking77\u00a0(Casanueva et\u00a0al., 2020), and Clinc-150 (Larson et\u00a0al., 2019).\n\n\n\n\nQA\nGPT-4o score\n\n\nGiven a book, answer the question. Datasets (# tokens): NarrativeQA (medium: 73K; max: 518K; Ko\u010disk\u00fd et\u00a0al., 2018).\n\n\n\n\nSumm.\nGPT-4o score\n\n\nSummarize a given legal document. Datasets (# tokens): Multi-LexSum (medium: 90K; max: 5M; Shen et\u00a0al., 2022)\n\n\n\n\n\n\n\nTable\u00a013 shows all the datasets we used for the long-context evaluation from HELMET\u00a0(Yen et\u00a0al., 2024b).\nNote that we did not use all the datasets from HELMET for efficiency reasons and we also do not want to overfit to HELMET.\nWe highlight some of the evaluation protocol improvements that HELMET implemented compared to previous benchmarks here:\n\n\n\n\n\u2022\n\nSufficient context lengths and fine-grained control. HELMET can evaluate models at a context length of 128K tokens and beyond.\nThe evaluation protocol also allows for reporting results at different lengths, giving developers fine-trained controls for different needs of long contexts.\n\n\n\n\u2022\n\nBetter synthetic recall tasks. As shown in HELMET, needle-in-a-haystack\u00a0(Kamradt, 2024) is mostly saturated because of its simplicity\u2014the model only needs to find a needle in some irrelevant context. We instead use the more challenging JSON KV task, first proposed in Liu et\u00a0al. (2024b) and included in HELMET, where the model is required to find the corresponding value to a given key among a large JSON file.\n\n\n\n\u2022\n\nUsing class-balanced demonstrations and abstract labels for ICL. To disentangle models\u2019 ability of learning from demonstrations from their pre-training bias of the task or the dataset label distribution\u00a0(Pan et\u00a0al., 2023), HELMET samples the same number of demonstrations for each class and uses number labels (1, 2, \u2026) instead of natural-language labels (e.g., location, description, \u2026).\n\n\n\n\u2022\n\nModel-based evaluation for long-context QA and summarization. Instead of using traditional metrics like ROUGE (which has shown to be poorly indicative of the real model performance: Deutsch & Roth, 2021; Deutsch et\u00a0al., 2022; Goyal et\u00a0al., 2023; Chang et\u00a0al., 2024), HELMET uses model-based evaluations to compare the reference answer and the model output. For QA, HELMET uses GPT-4o to score the model output given the question and the reference answer at a 0-3 scale.\nFor summarization, HELMET takes a similar approach as Zhang & Bansal (2021); Gao et\u00a0al. (2023): it first uses GPT-4o to decompose the reference summary into atomic claims; then it uses GPT-4o to check whether each reference atomic claim is covered by the model output (recall) and whether each sentence in the model output is covered by the reference summary (precision). Yen et\u00a0al. (2024b) show that the model-based evaluation correlates with human perceptions significantly better than traditional metrics.\n\n\n\n\n\n\n\nA.2 Data processing\n\nData sources.\n\nWe list all the data sources we have explored in our ablations and main experiments here:\nthe Stack\u00a0(Kocetkov et\u00a0al., 2023),\nSlimPajama\u00a0(Together, 2023; Soboleva et\u00a0al., 2023),\nFineWeb (we use the 2023-50 snapshot), FineWeb-Edu (we use a random sample)\u00a0(Penedo et\u00a0al., 2024),\nTulu-v2\u00a0(Ivison et\u00a0al., 2023),\nOpenWebMath\u00a0(Paster et\u00a0al., 2024),\ntextbooks\u00a0(Chevalier et\u00a0al., 2024),\nand Dolma\u00a0(Soldaini et\u00a0al., 2024).\nThe Books, StackExchange, and ArXiv data are from SlimPajama. The Wikipedia data are from Dolma.\n\n\n\nData filtering and packing.\n\nFor the short training data and the SFT data, we randomly sample and concatenate the documents or conversations into 64K chunks. The last document for each chunk is truncated. The truncated part is used as the beginning for the next chunk for the short training data but is discarded for the SFT data.\nFor the long-context training data, we filter out the documents that are shorter than 64K; we do the same for the 512K setting, while making sure that the 64K documents packed to 512K length are distinct from the 512K documents.\n\n\n\nFinal data mixture.\n\nFor 512K length, we use a mix of 64K and 512K long data. For the ratio of 64K/512K data, we choose 50%/50% for code and 83%/17%, which are roughly chosen according to the natural availability of very long data, i.e., there are relatively fewer books of length 512K than code repositories. One benefit of retaining 64K-long documents is that we can process these without sequence parallelism and the associated communication overhead.\nWe use a slightly different long data mixture in our ablations\u00a0(Table\u00a05) and our main ProLong experiment\u00a0(Table\u00a09).\nFor the final model, we mix 3% textbooks into the long-context training data. The textbooks are open-source resources from libretexts.org, collected and made available by Chevalier et\u00a0al. (2024).\nWe pre-process the data by concatenating chapters from the same text books, as well as books from the same subject areas. This results in extremely long sequences which we pack into contexts of either 64K or 512K tokens.\nThough we do not have an ablation for adding this data due to limited resources, we believe that it should have a slight positive effect to the final model performance as textbooks are highly educational long-context data.\n\n\nTable 14: % Proportion of long documents for the short data components used in Table\u00a06. These statistics are computed after packing and truncation and therefore correspond to the document lengths as seen by the model. We highlight that the proportion of documents beyond 32K is below 1% for ShortMix.\n\n\n\n\n\n>>>4K\n\n>>>8K\n\n>>>16K\n\n>>>32K\n\n\n\u2005 FineWeb\n1.4\n0.3\n0.1\n0.0\n\n\n\u2005 FineWeb-Edu\n2.8\n0.8\n0.2\n0.0\n\n\n\u2005 Wikipedia\n1.6\n0.4\n0.0\n0.0\n\n\n\u2005 Tulu-v2\n0.0\n0.0\n0.0\n0.0\n\n\n\u2005 StackExchange\n0.6\n0.1\n0.0\n0.0\n\n\n\u2005 ArXiv\n85.7\n64.0\n30.3\n7.6\n\n\n\u2005 OpenWebMath\n11.1\n4.3\n1.2\n0.3\n\n\nShortMix\n10.9\n7.2\n3.2\n0.8\n\n\nSlimPajama\n11.3\n7.4\n4.9\n3.2\n\n\nFineWeb-Edu\n2.8\n0.8\n0.2\n0.0\n\n\nDCLM-Baseline\n4.9\n1.7\n0.4\n0.1\n\n\n\n\n\n\n\n\nA.3 Implementation details\n\nTechnical stack.\n\nWe use various open-source packages and tools for the ProLong training and evaluation.\nWe use PyTorch (Paszke et\u00a0al., 2019) and Hugging Face transformers\u00a0(Wolf et\u00a0al., 2020) for the model training.\nWe use mosaic-streaming (Mosaic ML, 2022) for loading and mixing the data\nand FlashAttention 2 (Dao, 2024) for efficient attention implementation.\nWe implement sequence parallelism based on DeepSpeed-Ulysses (Jacobs et\u00a0al., 2023) across groups of 8 GPUs on the same node. We only perform distributed attention if it is necessary, i.e., only on sequences of 512K length.\nFor long-context evaluation, we use HELMET\u00a0(Yen et\u00a0al., 2024b) and for short-context evaluation, we use lm-eval-harness (Gao et\u00a0al., 2021).\n\n\n\nAttention and batching.\n\nSince we do document masking in attention\u00a0(\u00a76),\nwe use the variable-length attention implementation from FlashAttention 2\u00a0(Dao, 2024) to speed up long-context training:\nfor sequences that are concatenations of multiple short documents,\ninstead of computing the full attention with masking,\nwe instead compute the attention for each individual document.\nSince the complexity of attention is quadratic to the sequence length, this improves the training speed.\nHowever, the improvement is negligible in a distributed training setting with FSDP, since GPUs processing short sequence batches have to wait on other GPUs processing long sequences.\nWe therefore implement a smart batching algorithm:\nIn our setting, a gradient step usually consists of multiple gradient accumulation steps, where each device processes a smaller minibatch.\nWe sort all the minibatches per training step by the sum of the squared lengths of documents in the sequence.\nThis leads to more balanced sequence lengths across the GPUs and effective speedups, as can be seen in Table\u00a015, without affecting the gradient updates or loss during training. However, the efficiency gains are diminished when training with more GPUs, as this reduces the number of gradient accumulation steps.\n\n\nTable 15: Throughput per device of our ablation runs from Table\u00a020, when training with 8 Nvidia H100 GPUs with FSDP. Our strategy of reordering minibatches is important for realizing the speed benefits from variable-length attention.\n\n\n\n\nThroughput\n\n\n\n(tokens/s/GPU)\n\n\n\n\n64K full attention\n\u00a0\u00a0\u00a02770\n\n\nVariable-length attention\n\u00a0\u00a0\u00a02780(+0.4%)\n\n\n\n\u2005 + Minibatch reordering\n\n\u00a0\u00a0\u00a03095(+11.7%)\n\n\n\n\n\n\n\nToken-averaged loss.\n\nWe found that in the SFT stage,\nthe distribution of the training tokens (in SFT, the tokens from the instructions are masked out and the models are only trained on the responses) on each GPU device can be extremely imbalanced, especially when there is synthetic data (most tokens in a synthetic data instance are from the instruction).\nConventional all-reduce loss in distributed training averages over the sequences instead of valid tokens, which skews the optimization and also our control over the domain proportions.\nInstead, we change the all-reduce loss to be the average over all valid training tokens.\nBai et\u00a0al. (2024a) implements their SFT loss in a similar way.\n\n\n\n\n\nA.4 The ablation setting\n\nFor all our ablations, unless specified, we train the base model of Llama-3-8B (instead of Instruct) on a 64K sequence length for 5B tokens, with the same hyperparameters as specified in Table\u00a09.\nWe choose this context length, as it is the highest power of 2 value for which we can train without sequence parallelism.\nBy default, we use the same training data as the 64K ProLong setting, except that we remove the textbooks and use the ShortMix proportions in Table\u00a05.\nFor SFT, we use the same settings as specified in Table\u00a09.\n\n\n\n\nA.5 Generating synthetic SFT data\n\nWe prompt Llama-3-8B-Instruct to generate the synthetic data and\nTable\u00a016 shows the prompt we used for generating the synthetic QA data for books.\nWe also write predefined templates and randomly sample one for each synthetic instance to increase the diversity, and Table\u00a017 provides some examples.\n\n\nTable 16: \nPrompts for generating synthetic QA data.\n\n\n\n\n\n\nGiven the following snippet of a book, ask a relevant question and provide the answer. The question and the answer should follow the following rules:\n\n\n\n\n\n\n\n\n\n\n\n\n\n(1) The question should be specific enough that it can only be answered with the snippet. The question should also be interesting and intellectual enough that a curious reader of the book would ask about it.\n\n\n\n\n\n\n(2) The question and the answer should be comprehensible given just the whole book without highlighting the snippet. With that being said, the question should NOT refer to the snippet directly (e.g., do NOT say things like \"Question: given the conversation in the snippet, what ...\"). The answer also should not mention \"the snippet \u2026\" explicitly (assuming that the snippet is never provided), but it can copy the snippet content as a reference when answering the question.\n\n\n\n\n\n\n(3) The answer should be concise but also should provide references to the book when needed. For example, \u201cWellington Yueh betrayed the Atreides, as the book mentioned, \u2019...\u2019\".\n\n\n\n\n\n\n\n\n\n\n\n\n\n*** Start of the snippet ***\n\n\n\n\n\n\n\n\n\n\n\n\n\n{sampled snippet}\n\n\n\n\n\n\n\n\n\n\n\n\n\n*** End of the snippet ***\n\n\n\n\n\n\n\n\n\n\n\n\n\nBefore generating the question and the answer, first reason about what this snippet is about. In your generation, stick to the following format:\n\n\n\n\n\n\n\n\n\n\n\n\n\nReasoning: this snippet is about ...\n\n\n\n\n\n\nQuestion: ...\n\n\n\n\n\n\nAnswer: ...\n\n\n\n\n\n\n\nTable 17: \nExamples for question prompts and templates used for generating diverse synthetic QA data. We sample one question prompt and one template each time and combine them with the documents and the generated QA pairs to form a synthetic training example.\n\n\n\n\nExample question prompts for synthetic QA data\n\n\n\n\n\n\nGiven the document, please answer the question.\n\n\n\n\n\n\nHere is a piece of text; answer the following question based on it.\n\n\n\n\n\n\nPlease answer the question using the provided content.\n\n\n\n\n\n\nBased on the given passage, respond to the question.\n\n\n\n\n\n\nRead the snippet and answer the question that follows.\n\n\n\n\n\n\nUsing the provided text, answer the following question.\n\n\n\n\nExample templates for combining questions, answers, and contexts for synthetic QA data\n\n\n\n\n{prompt}\\n\\n{documents}\\n\\nQuestion: {question}\n\n\n\n\n\n\n{prompt}\\n\\n==== document starts ====\\n{documents}\\n==== document ends ====\\n\\nQuestion: {question}\n\n\n\n\n\n\n{prompt}\\n\\n{documents}\\n\\n{question}\n\n\n\n\n\n\n{prompt} Question: {question}\\n\\n{documents}\n\n\n\n\n\n\n{prompt} {question}\\n\\n{documents}\n\n\n\n\n\n\n{prompt}\\n\\n{question}\\n\\n{documents}\n\n\n\n\n\n\n\n",
      "Appendix B More Ablations": "\n\nAppendix B More Ablations\n\n\nB.1 Position extrapolation\n\nXiong et\u00a0al. (2023); emozilla (2023) show that\nchanging the RoPE frequency base to a larger value in\ncontinual long-context pre-training or in inference time can improve the long-context performance.\nemozilla (2023) suggests that one should scale the frequency base by a factor of tdd\u22122superscript\ud835\udc61\ud835\udc51\ud835\udc512t^{\\frac{d}{d-2}}italic_t start_POSTSUPERSCRIPT divide start_ARG italic_d end_ARG start_ARG italic_d - 2 end_ARG end_POSTSUPERSCRIPT, where t\ud835\udc61titalic_t is the ratio between the target sequence length and the original LM length, and d\ud835\udc51ditalic_d is the attention head dimension.\n\n\nWe conduct ablation studies, at both 64K (same as our standard ablation setting as specified in \u00a7A.4) and 512K (starting from ProLong-64K and training with the 512K data mixture for 5B tokens) sequence lengths, on what frequency bases we should use.\n\u00a7B.1 and \u00a7B.1 show the results.\nWe first see that using the original 500,000 frequency base from Llama-3 leads to significant performance degradation. While dynamic NTK suggests 4\u00d71064superscript1064\\times 10^{6}4 \u00d7 10 start_POSTSUPERSCRIPT 6 end_POSTSUPERSCRIPT, we find that further scaling it to 8\u00d71068superscript1068\\times 10^{6}8 \u00d7 10 start_POSTSUPERSCRIPT 6 end_POSTSUPERSCRIPT leads to better performance. Similar, we see that when scaling the 64K model to 512K, while dynamic NTK suggests a 64\u00d710664superscript10664\\times 10^{6}64 \u00d7 10 start_POSTSUPERSCRIPT 6 end_POSTSUPERSCRIPT frequency base, much larger frequency bases (128\u00d7106128superscript106128\\times 10^{6}128 \u00d7 10 start_POSTSUPERSCRIPT 6 end_POSTSUPERSCRIPT and 256\u00d7106256superscript106256\\times 10^{6}256 \u00d7 10 start_POSTSUPERSCRIPT 6 end_POSTSUPERSCRIPT) lead to better performance.\nWe use 8\u00d71068superscript1068\\times 10^{6}8 \u00d7 10 start_POSTSUPERSCRIPT 6 end_POSTSUPERSCRIPT for 64K and 128\u00d7106128superscript106128\\times 10^{6}128 \u00d7 10 start_POSTSUPERSCRIPT 6 end_POSTSUPERSCRIPT for 512K for our final ProLong models.\n\n\nTable 18: Ablation study on RoPE frequency base at a maximum training length of 64K. Dynamic NTK\u00a0(emozilla, 2023) roughly suggests to use 4m as the frequency base.\n\n\n\n\n\nRoPE Base \n(\u00d7106absentsuperscript106\\times 10^{6}\u00d7 10 start_POSTSUPERSCRIPT 6 end_POSTSUPERSCRIPT)\nLong-Context\nShort-Context\n\n\nRecall\nRAG\nRe-rank\nICL\nQA\nSumm.\nAvg.\nAvg.\n\n\n\n\n0.5\n25.8\n37.0\n4.4\n73.8\n17.5\n16.3\n29.1\n65.0\n\n\n4.0\n81.3\n47.8\n18.2\n76.5\n31.8\n36.3\n48.7\n65.3\n\n\n8.0\n96.0\n54.9\n29.4\n73.9\n35.7\n37.9\n54.6\n65.5\n\n\n\n\n\n\n\nTable 19: Ablation study on RoPE frequency base at a maximum training length of 512K. Dynamic NTK\u00a0(emozilla, 2023) roughly suggests to use 64\u00d710664superscript10664\\times 10^{6}64 \u00d7 10 start_POSTSUPERSCRIPT 6 end_POSTSUPERSCRIPT as the frequency base.\n\n\n\n\n\nRoPE Base \n(\u00d7106absentsuperscript106\\times 10^{6}\u00d7 10 start_POSTSUPERSCRIPT 6 end_POSTSUPERSCRIPT)\nLong-Context\nShort-Context\n\n\nRecall\nRAG\nRe-rank\nICL\nQA\nSumm.\nAvg.\nAvg.\n\n\n\n\n64\n98.8\n57.8\n30.4\n82.2\n38.2\n38.3\n57.6\n68.3\n\n\n128\n98.8\n57.4\n30.7\n80.0\n40.4\n38.8\n57.7\n68.6\n\n\n256\n98.8\n56.8\n33.8\n79.8\n37.9\n39.7\n57.8\n68.4\n\n\n\n\n\n\n\n\nB.2 Document masks\n\nWe experiment whether to use document masks in attention in Table\u00a020. Standard training concatenates multiple short documents into a single sequence (in our case, a 64K sequence), uses a special token to separate documents, and performs full attention over the whole sequence. When the document masks are used, we do not allow the attention to cross the document boundaries.\nWe find that using document masks in continual long-context training leads to both better long-context results and short-context performance.\nFor all our other ablations and the main experiment, we use document masks.\n\n\nTable 20: Impact of using document masks in attention.\n\n\n\nAttention\nLong-Context\nShort-Context\n\n\nRecall\nRAG\nRe-rank\nICL\nQA\nSumm.\nAvg.\nAvg.\n\n\n\n\nNo doc masks\n97.4\n53.6\n20.4\n76.6\n37.2\n36.3\n53.6\n64.9\n\n\nDocument masks\n96.0\n54.9\n29.4\n73.9\n35.7\n37.9\n54.6\n65.5\n\n\n\n\n\n\nB.3 Initialization\n\nWe use the base model for Llama-3-8B as the initialization for all our ablations to make sure the findings are generalizable and are not confounded by the Llama instruction tuning.\nHowever, for our final ProLong model, we use Llama-3-8B-Instruct as the initialization to achieve the best performance.\nWe see in Table\u00a021 (using the ablation setting from \u00a7A.4) that using Llama-3-8B-Instruct as the initialization achieves slightly better long-context performance and much stronger short-context performance.\n\n\nTable 21: \nDifferences of using the base Llama-3-8B model vs. Llama-3-8B-Instruct.\n\n\n\n\nBase Model\nLong-Context\nShort-Context\n\n\nAvg.\nHellaS.\nMMLU\nARC-c\nWG\nGSM8K\nAvg.\n\n\n\n\nLlama-3-8B-Base\n54.6\n81.6\n65.3\n58.0\n76.2\n46.6\n65.5\n\n\nLlama-3-8B-Instruct\n55.0\n80.8\n66.1\n58.5\n75.6\n57.7\n67.7\n\n\n\n\n\n\nB.4 Instruction-tuning datasets\n\nInitialized from the ProLong base model, we experiment with different public, short-context SFT datasets.\nAll runs use the same SFT hyperparameters as specified in Table\u00a09.\nTable\u00a022 shows that using UltraChat leads to the best overall results.\nNote that this does not necessarily mean that UltraChat is the best SFT dataset for all base models or applications.\n\n\nTable 22: \nAblations on using different short-context SFT datasets.\nWe report the 32K-and-64K-averaged performance except tasks marked with \u2020, which are evaluated at 512K for stress testing.\n\n\n\n\nSFT Data\nLong-Context\n\n\nRecall\u2020\n\nRAG\nRe-rank\nICL\nQA\u2020\n\nSumm.\u2020\n\nAvg.\n\n\n\n\nUltraChat\n65.7\n58.1\n38.5\n80.3\n49.7\n42.1\n55.7\n\n\nTulu v2\n61.5\n45.4\n25.1\n81.8\n40.4\n40.3\n49.1\n\n\nShareGPT\n40.5\n47.5\n26.7\n79.6\n42.7\n34.4\n45.2\n\n\n\n\n\n\nAppendix C Full Evaluation Results\n\nWe provide all the evaluation results, including specific numbers for each dataset and evaluation length, at https://shorturl.at/pULbx.\n\n\n\n\nAppendix D Examples\n\nTable\u00a023 shows some long-document QA examples with ProLong, Llama-3.1, and GPT-4o.\n\n\nTable 23: \nLong-document QA examples with ProLong (512K), Llama-3.1-8B-Instruct (128K), and GPT-4o (via the ChatGPT interface). Note that\nit is very likely that the ChatGPT-version GPT-4o uses a retrieval component for long documents, hence it can process contexts beyond its 128K context window.\nWe see that ProLong can answer questions effectively with a 300K document while both Llama-3.1 and GPT-4o fail.\nOn a shorter document (Llama-3.1 paper), ProLong can understand very technical questions and answer them accurately.\n\n\n\n\nInstacart S1 file (300K tokens)\n\n\n\n\n\n\nQ: How many shares and at what price did instacard sell their series G redeemable convertible preferred stock in June 2020 and July 2020?\n\n\n\nProLong: Instacart sold an aggregate of  6,757,893 shares of their \u2026 at a purchase price of $48.0919 \u2026\n\n\n\n\n\n\nllama-3.1-8B: The following is a II/5 [collapsed]\n\n\n\n\n\n\nGPT-4o: \u2026 The company sold a total of 6,758,000 shares at a price of $48.0919 per share during this period.\n\n\n\n\nLlama-3.1 paper (94K tokens)\n\n\n\n\nQ: During training of the llama3 model, how many unexpected interruptions are caused by hardware related reasons?\n\n\n\nProLong: Approximately 78% of unexpected \u2026\n\n\n\n\n\n\nllama-3.1-8B: According to Table 5, approximately 58.7% \u2026\n\n\n\n\n\n\nGPT-4o: \u2026 approximately 78% of unexpected interruptions \u2026\n\n\n\n\n\n\nQ: Is Mark Zuckerberg a contributor to the paper?\n\n\n\nProLong: There is no mention of Mark Zuckerberg as a contributor to the paper.\n\n\n\n\n\n\nllama-3.1-8B: Yes, Mark Zuckerberg is listed as a contributor \u2026\n\n\n\n\n\n\nGPT-4o: Mark Zuckerberg is acknowledged in the paper for \u2026, but he is not listed as a core contributor \u2026\n\n\n\n\n\n\nQ: What ratio did the llama 3 team use to mix synthetic long-context data with short data in SFT?\n\n\n\nProLong: The llama 3 team used a ratio of 0.1% synthetic \u2026\n\n\n\n\n\n\nllama-3.1-8B: 0.10%\n\n\n\n\n\n\nGPT-4o: The Llama 3 team used a mix ratio of 30% synthetic \u2026\n\n\n\n\n\n\nQ: What benchmark did they evaluate on for \u201dinstruction-following\u201d ability? Provide references too.\n\n\n\nProLong: They evaluated on IFEval (Zhou et al., 2023) for instruction-following ability. Reference: Jeffrey Zhou, \u2026 Instruction-following evaluation for large language models. arXiv preprint \u2026\n\n\n\n\n\n\nllama-3.1-8B: The benchmark they evaluated on for \u201dinstruction-following\u201d ability is IFEval. Reference: Section 5.2.2, \u201dGeneral Knowledge and Instruction-Following Benchmarks\u201d\n\n\n\n\n\n\nGPT-4o: The benchmark they used to evaluate \u201dinstruction- following\u201d ability is IFEval. This is mentioned in the context of assessing how Direct Preference Optimization (DPO) \u2026\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",
      "Appendix C Full Evaluation Results": "\n\nAppendix C Full Evaluation Results\n\nWe provide all the evaluation results, including specific numbers for each dataset and evaluation length, at https://shorturl.at/pULbx.\n\n",
      "Appendix D Examples": "\n\nAppendix D Examples\n\nTable\u00a023 shows some long-document QA examples with ProLong, Llama-3.1, and GPT-4o.\n\n\nTable 23: \nLong-document QA examples with ProLong (512K), Llama-3.1-8B-Instruct (128K), and GPT-4o (via the ChatGPT interface). Note that\nit is very likely that the ChatGPT-version GPT-4o uses a retrieval component for long documents, hence it can process contexts beyond its 128K context window.\nWe see that ProLong can answer questions effectively with a 300K document while both Llama-3.1 and GPT-4o fail.\nOn a shorter document (Llama-3.1 paper), ProLong can understand very technical questions and answer them accurately.\n\n\n\n\nInstacart S1 file (300K tokens)\n\n\n\n\n\n\nQ: How many shares and at what price did instacard sell their series G redeemable convertible preferred stock in June 2020 and July 2020?\n\n\n\nProLong: Instacart sold an aggregate of  6,757,893 shares of their \u2026 at a purchase price of $48.0919 \u2026\n\n\n\n\n\n\nllama-3.1-8B: The following is a II/5 [collapsed]\n\n\n\n\n\n\nGPT-4o: \u2026 The company sold a total of 6,758,000 shares at a price of $48.0919 per share during this period.\n\n\n\n\nLlama-3.1 paper (94K tokens)\n\n\n\n\nQ: During training of the llama3 model, how many unexpected interruptions are caused by hardware related reasons?\n\n\n\nProLong: Approximately 78% of unexpected \u2026\n\n\n\n\n\n\nllama-3.1-8B: According to Table 5, approximately 58.7% \u2026\n\n\n\n\n\n\nGPT-4o: \u2026 approximately 78% of unexpected interruptions \u2026\n\n\n\n\n\n\nQ: Is Mark Zuckerberg a contributor to the paper?\n\n\n\nProLong: There is no mention of Mark Zuckerberg as a contributor to the paper.\n\n\n\n\n\n\nllama-3.1-8B: Yes, Mark Zuckerberg is listed as a contributor \u2026\n\n\n\n\n\n\nGPT-4o: Mark Zuckerberg is acknowledged in the paper for \u2026, but he is not listed as a core contributor \u2026\n\n\n\n\n\n\nQ: What ratio did the llama 3 team use to mix synthetic long-context data with short data in SFT?\n\n\n\nProLong: The llama 3 team used a ratio of 0.1% synthetic \u2026\n\n\n\n\n\n\nllama-3.1-8B: 0.10%\n\n\n\n\n\n\nGPT-4o: The Llama 3 team used a mix ratio of 30% synthetic \u2026\n\n\n\n\n\n\nQ: What benchmark did they evaluate on for \u201dinstruction-following\u201d ability? Provide references too.\n\n\n\nProLong: They evaluated on IFEval (Zhou et al., 2023) for instruction-following ability. Reference: Jeffrey Zhou, \u2026 Instruction-following evaluation for large language models. arXiv preprint \u2026\n\n\n\n\n\n\nllama-3.1-8B: The benchmark they evaluated on for \u201dinstruction-following\u201d ability is IFEval. Reference: Section 5.2.2, \u201dGeneral Knowledge and Instruction-Following Benchmarks\u201d\n\n\n\n\n\n\nGPT-4o: The benchmark they used to evaluate \u201dinstruction- following\u201d ability is IFEval. This is mentioned in the context of assessing how Direct Preference Optimization (DPO) \u2026\n\n\n\n\n\n\n\n"
    },
    "bibliography": {
      "Abdin et\u00a0al. (2024)": "\nAbdin et\u00a0al. (2024)\n\nMarah Abdin, Sam\u00a0Ade Jacobs, Ammar\u00a0Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Harkirat Behl, et\u00a0al.\n\n\nPhi-3 technical report: A highly capable language model locally on your phone.\n\n\narXiv preprint arXiv:2404.14219, 2024.\n\n\n",
      "Achiam et\u00a0al. (2023)": "\nAchiam et\u00a0al. (2023)\n\nJosh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia\u00a0Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et\u00a0al.\n\n\nGpt-4 technical report.\n\n\narXiv preprint arXiv:2303.08774, 2023.\n\n\n",
      "An et\u00a0al. (2024a)": "\nAn et\u00a0al. (2024a)\n\nChenxin An, Shansan Gong, Ming Zhong, Xingjian Zhao, Mukai Li, Jun Zhang, Lingpeng Kong, and Xipeng Qiu.\n\n\nL-eval: Instituting standardized evaluation for long context language models.\n\n\nIn Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp.\u00a0 14388\u201314411, 2024a.\n\n\n",
      "An et\u00a0al. (2024b)": "\nAn et\u00a0al. (2024b)\n\nShengnan An, Zexiong Ma, Zeqi Lin, Nanning Zheng, and Jian-Guang Lou.\n\n\nMake your llm fully utilize the context.\n\n\narXiv preprint arXiv:2404.16811, 2024b.\n\n\n",
      "Anthropic (2024)": "\nAnthropic (2024)\n\nAI\u00a0Anthropic.\n\n\nClaude 3.5 sonnet model card addendum.\n\n\nClaude-3.5 Model Card, 2024.\n\n\n",
      "Arora et\u00a0al. (2024)": "\nArora et\u00a0al. (2024)\n\nSimran Arora, Sabri Eyuboglu, Michael Zhang, Aman Timalsina, Silas Alberti, James Zou, Atri Rudra, and Christopher Re.\n\n\nSimple linear attention language models balance the recall-throughput tradeoff.\n\n\nIn Forty-first International Conference on Machine Learning, 2024.\n\n\n",
      "Bai et\u00a0al. (2024a)": "\nBai et\u00a0al. (2024a)\n\nYushi Bai, Xin Lv, Jiajie Zhang, Yuze He, Ji\u00a0Qi, Lei Hou, Jie Tang, Yuxiao Dong, and Juanzi Li.\n\n\nLongalign: A recipe for long context alignment of large language models.\n\n\narXiv preprint arXiv:2401.18058, 2024a.\n\n\n",
      "Bai et\u00a0al. (2024b)": "\nBai et\u00a0al. (2024b)\n\nYushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li.\n\n\nLongBench: A bilingual, multitask benchmark for long context understanding.\n\n\nIn Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp.\u00a0 3119\u20133137, 2024b.\n\n\n",
      "Bajaj et\u00a0al. (2016)": "\nBajaj et\u00a0al. (2016)\n\nPayal Bajaj, Daniel Campos, Nick Craswell, Li\u00a0Deng, Jianfeng Gao, Xiaodong Liu, Rangan Majumder, Andrew McNamara, Bhaskar Mitra, Tri Nguyen, et\u00a0al.\n\n\nMs marco: A human generated machine reading comprehension dataset.\n\n\narXiv preprint arXiv:1611.09268, 2016.\n\n\n",
      "Beeching et\u00a0al. (2023)": "\nBeeching et\u00a0al. (2023)\n\nEdward Beeching, Cl\u00e9mentine Fourrier, Nathan Habib, Sheon Han, Nathan Lambert, Nazneen Rajani, Omar Sanseviero, Lewis Tunstall, and Thomas Wolf.\n\n\nOpen llm leaderboard, 2023.\n\n\n",
      "Bertsch et\u00a0al. (2023)": "\nBertsch et\u00a0al. (2023)\n\nAmanda Bertsch, Uri Alon, Graham Neubig, and Matthew\u00a0R. Gormley.\n\n\nUnlimiformer: Long-range transformers with unlimited length input.\n\n\nIn Advances in Neural Information Processing Systems (NeurIPS), 2023.\n\n\n",
      "Bertsch et\u00a0al. (2024)": "\nBertsch et\u00a0al. (2024)\n\nAmanda Bertsch, Maor Ivgi, Uri Alon, Jonathan Berant, Matthew\u00a0R Gormley, and Graham Neubig.\n\n\nIn-context learning with long-context models: An in-depth exploration.\n\n\narXiv preprint arXiv:2405.00200, 2024.\n\n\n",
      "Casanueva et\u00a0al. (2020)": "\nCasanueva et\u00a0al. (2020)\n\nI\u00f1igo Casanueva, Tadas Tem\u010dinas, Daniela Gerz, Matthew Henderson, and Ivan Vuli\u0107.\n\n\nEfficient intent detection with dual sentence encoders.\n\n\nIn Proceedings of the 2nd Workshop on Natural Language Processing for Conversational AI, pp.\u00a0 38\u201345, 2020.\n\n\n",
      "Chang et\u00a0al. (2024)": "\nChang et\u00a0al. (2024)\n\nYapei Chang, Kyle Lo, Tanya Goyal, and Mohit Iyyer.\n\n\nBooookscore: A systematic exploration of book-length summarization in the era of LLMs.\n\n\nIn The Twelfth International Conference on Learning Representations, 2024.\n\n\n",
      "Chen et\u00a0al. (2023)": "\nChen et\u00a0al. (2023)\n\nShouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian.\n\n\nExtending context window of large language models via positional interpolation, 2023.\n\n\n",
      "Chen et\u00a0al. (2024)": "\nChen et\u00a0al. (2024)\n\nYukang Chen, Shengju Qian, Haotian Tang, Xin Lai, Zhijian Liu, Song Han, and Jiaya Jia.\n\n\nLongloRA: Efficient fine-tuning of long-context large language models.\n\n\nIn The Twelfth International Conference on Learning Representations, 2024.\n\n\n",
      "Chevalier et\u00a0al. (2023)": "\nChevalier et\u00a0al. (2023)\n\nAlexis Chevalier, Alexander Wettig, Anirudh Ajith, and Danqi Chen.\n\n\nAdapting language models to compress contexts.\n\n\nIn Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp.\u00a0 3829\u20133846, Singapore, December 2023. Association for Computational Linguistics.\n\n\n",
      "Chevalier et\u00a0al. (2024)": "\nChevalier et\u00a0al. (2024)\n\nAlexis Chevalier, Jiayi Geng, Alexander Wettig, Howard Chen, Sebastian Mizera, Toni Annala, Max Aragon, Arturo\u00a0Rodriguez Fanlo, Simon Frieder, Simon Machado, Akshara Prabhakar, Ellie Thieu, Jiachen\u00a0T. Wang, Zirui Wang, Xindi Wu, Mengzhou Xia, Wenhan Xia, Jiatong Yu, Junjie Zhu, Zhiyong Ren, Sanjeev Arora, and Danqi Chen.\n\n\nLanguage models as science tutors.\n\n\nIn Forty-first International Conference on Machine Learning, 2024.\n\n\n",
      "Choromanski et\u00a0al. (2021)": "\nChoromanski et\u00a0al. (2021)\n\nKrzysztof\u00a0Marcin Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared\u00a0Quincy Davis, Afroz Mohiuddin, Lukasz Kaiser, David\u00a0Benjamin Belanger, Lucy\u00a0J Colwell, and Adrian Weller.\n\n\nRethinking attention with performers.\n\n\nIn International Conference on Learning Representations, 2021.\n\n\nURL https://openreview.net/forum?id=Ua6zuk0WRH.\n\n\n",
      "Clark et\u00a0al. (2018)": "\nClark et\u00a0al. (2018)\n\nPeter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord.\n\n\nThink you have solved question answering? Try ARC, the AI2 reasoning challenge.\n\n\narXiv preprint arXiv:1803.05457, 2018.\n\n\n",
      "Cobbe et\u00a0al. (2021)": "\nCobbe et\u00a0al. (2021)\n\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et\u00a0al.\n\n\nTraining verifiers to solve math word problems.\n\n\narXiv preprint arXiv:2110.14168, 2021.\n\n\n",
      "Dao (2024)": "\nDao (2024)\n\nTri Dao.\n\n\nFlashattention-2: Faster attention with better parallelism and work partitioning.\n\n\nIn The Twelfth International Conference on Learning Representations, 2024.\n\n\n",
      "Dao & Gu (2024)": "\nDao & Gu (2024)\n\nTri Dao and Albert Gu.\n\n\nTransformers are ssms: Generalized models and efficient algorithms through structured state space duality.\n\n\narXiv preprint arXiv:2405.21060, 2024.\n\n\n",
      "Deutsch & Roth (2021)": "\nDeutsch & Roth (2021)\n\nDaniel Deutsch and Dan Roth.\n\n\nUnderstanding the extent to which content quality metrics measure the information quality of summaries.\n\n\nIn Arianna Bisazza and Omri Abend (eds.), Proceedings of the 25th Conference on Computational Natural Language Learning, pp.\u00a0 300\u2013309, 2021.\n\n\n",
      "Deutsch et\u00a0al. (2022)": "\nDeutsch et\u00a0al. (2022)\n\nDaniel Deutsch, Rotem Dror, and Dan Roth.\n\n\nRe-examining system-level correlations of automatic summarization evaluation metrics.\n\n\nIn Marine Carpuat, Marie-Catherine de\u00a0Marneffe, and Ivan\u00a0Vladimir Meza\u00a0Ruiz (eds.), Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp.\u00a0 6038\u20136052, Seattle, United States, July 2022. Association for Computational Linguistics.\n\n\ndoi: 10.18653/v1/2022.naacl-main.442.\n\n\nURL https://aclanthology.org/2022.naacl-main.442.\n\n\n",
      "Ding et\u00a0al. (2024a)": "\nDing et\u00a0al. (2024a)\n\nHantian Ding, Zijian Wang, Giovanni Paolini, Varun Kumar, Anoop Deoras, Dan Roth, and Stefano Soatto.\n\n\nFewer truncations improve language modeling.\n\n\nIn Forty-first International Conference on Machine Learning, 2024a.\n\n\nURL https://openreview.net/forum?id=kRxCDDFNpp.\n\n\n",
      "Ding et\u00a0al. (2023)": "\nDing et\u00a0al. (2023)\n\nNing Ding, Yulin Chen, Bokai Xu, Yujia Qin, Shengding Hu, Zhiyuan Liu, Maosong Sun, and Bowen Zhou.\n\n\nEnhancing chat language models by scaling high-quality instructional conversations.\n\n\nIn Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp.\u00a0 3029\u20133051, Singapore, December 2023. Association for Computational Linguistics.\n\n\n",
      "Ding et\u00a0al. (2024b)": "\nDing et\u00a0al. (2024b)\n\nYiran Ding, Li\u00a0Lyna Zhang, Chengruidong Zhang, Yuanyuan Xu, Ning Shang, Jiahang Xu, Fan Yang, and Mao Yang.\n\n\nLongroPE: Extending LLM context window beyond 2 million tokens.\n\n\nIn Forty-first International Conference on Machine Learning, 2024b.\n\n\n",
      "Dubey et\u00a0al. (2024)": "\nDubey et\u00a0al. (2024)\n\nAbhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et\u00a0al.\n\n\nThe Llama 3 herd of models.\n\n\narXiv preprint arXiv:2407.21783, 2024.\n\n\n",
      "emozilla (2023)": "\nemozilla (2023)\n\nemozilla.\n\n\nDynamically scaled rope further increases performance of long context llama with zero fine-tuning, 2023.\n\n\nURL https://www.reddit.com/r/LocalLLaMA/comments/14mrgpr/dynamically_scaled_rope_further_increases/.\n\n\n",
      "Fu et\u00a0al. (2024)": "\nFu et\u00a0al. (2024)\n\nYao Fu, Rameswar Panda, Xinyao Niu, Xiang Yue, Hannaneh Hajishirzi, Yoon Kim, and Hao Peng.\n\n\nData engineering for scaling language models to 128k context.\n\n\nIn International Conference on Machine Learning (ICML), 2024.\n\n\n",
      "Gao et\u00a0al. (2021)": "\nGao et\u00a0al. (2021)\n\nLeo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff, Jason Phang, Laria Reynolds, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou.\n\n\nA framework for few-shot language model evaluation, September 2021.\n\n\n",
      "Gao et\u00a0al. (2023)": "\nGao et\u00a0al. (2023)\n\nTianyu Gao, Howard Yen, Jiatong Yu, and Danqi Chen.\n\n\nEnabling large language models to generate text with citations.\n\n\nIn Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp.\u00a0 6465\u20136488, Singapore, December 2023. Association for Computational Linguistics.\n\n\n",
      "Goyal et\u00a0al. (2023)": "\nGoyal et\u00a0al. (2023)\n\nTanya Goyal, Junyi\u00a0Jessy Li, and Greg Durrett.\n\n\nNews summarization and evaluation in the era of gpt-3, 2023.\n\n\nURL https://arxiv.org/abs/2209.12356.\n\n\n",
      "Gu & Dao (2023)": "\nGu & Dao (2023)\n\nAlbert Gu and Tri Dao.\n\n\nMamba: Linear-time sequence modeling with selective state spaces, 2023.\n\n\n",
      "Guo et\u00a0al. (2024)": "\nGuo et\u00a0al. (2024)\n\nDaya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang, Guanting Chen, Xiao Bi, Yu\u00a0Wu, YK\u00a0Li, et\u00a0al.\n\n\nDeepseek-coder: When the large language model meets programming\u2013the rise of code intelligence.\n\n\narXiv preprint arXiv:2401.14196, 2024.\n\n\n",
      "Hendrycks et\u00a0al. (2021)": "\nHendrycks et\u00a0al. (2021)\n\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt.\n\n\nMeasuring massive multitask language understanding.\n\n\nIn International Conference on Learning Representations, 2021.\n\n\n",
      "Hovy et\u00a0al. (2001)": "\nHovy et\u00a0al. (2001)\n\nEduard Hovy, Laurie Gerber, Ulf Hermjakob, Chin-Yew Lin, and Deepak Ravichandran.\n\n\nToward semantics-based answer pinpointing.\n\n\nIn Proceedings of the First International Conference on Human Language Technology Research, 2001.\n\n\n",
      "Hsieh et\u00a0al. (2024)": "\nHsieh et\u00a0al. (2024)\n\nCheng-Ping Hsieh, Simeng Sun, Samuel Kriman, Shantanu Acharya, Dima Rekesh, Fei Jia, and Boris Ginsburg.\n\n\nRULER: What\u2019s the real context size of your long-context language models?\n\n\nIn First Conference on Language Modeling, 2024.\n\n\n",
      "Hu et\u00a0al. (2024a)": "\nHu et\u00a0al. (2024a)\n\nShengding Hu, Yuge Tu, Xu\u00a0Han, Chaoqun He, Ganqu Cui, Xiang Long, Zhi Zheng, Yewei Fang, Yuxiang Huang, Weilin Zhao, et\u00a0al.\n\n\nMinicpm: Unveiling the potential of small language models with scalable training strategies.\n\n\narXiv preprint arXiv:2404.06395, 2024a.\n\n\n",
      "Hu et\u00a0al. (2024b)": "\nHu et\u00a0al. (2024b)\n\nZhiyuan Hu, Yuliang Liu, Jinman Zhao, Suyuchen Wang, Yan Wang, Wei Shen, Qing Gu, Anh\u00a0Tuan Luu, See-Kiong Ng, Zhiwei Jiang, et\u00a0al.\n\n\nLongrecipe: Recipe for efficient long context generalization in large languge models.\n\n\narXiv preprint arXiv:2409.00509, 2024b.\n\n\n",
      "Ivison et\u00a0al. (2023)": "\nIvison et\u00a0al. (2023)\n\nHamish Ivison, Yizhong Wang, Valentina Pyatkin, Nathan Lambert, Matthew Peters, Pradeep Dasigi, Joel Jang, David Wadden, Noah\u00a0A. Smith, Iz\u00a0Beltagy, and Hannaneh Hajishirzi.\n\n\nCamels in a changing climate: Enhancing lm adaptation with Tulu 2, 2023.\n\n\n",
      "Jacobs et\u00a0al. (2023)": "\nJacobs et\u00a0al. (2023)\n\nSam\u00a0Ade Jacobs, Masahiro Tanaka, Chengming Zhang, Minjia Zhang, Shuaiwen\u00a0Leon Song, Samyam Rajbhandari, and Yuxiong He.\n\n\nDeepspeed ulysses: System optimizations for enabling training of extreme long sequence transformer models.\n\n\narXiv preprint arXiv:2309.14509, 2023.\n\n\n",
      "Jelassi et\u00a0al. (2024)": "\nJelassi et\u00a0al. (2024)\n\nSamy Jelassi, David Brandfonbrener, Sham\u00a0M. Kakade, and Eran Malach.\n\n\nRepeat after me: Transformers are better than state space models at copying, 2024.\n\n\n",
      "Jin et\u00a0al. (2024)": "\nJin et\u00a0al. (2024)\n\nHongye Jin, Xiaotian Han, Jingfeng Yang, Zhimeng Jiang, Zirui Liu, Chia-Yuan Chang, Huiyuan Chen, and Xia Hu.\n\n\nLLM maybe longLM: Selfextend LLM context window without tuning.\n\n\nIn Forty-first International Conference on Machine Learning, 2024.\n\n\n",
      "Kamradt (2024)": "\nKamradt (2024)\n\nGarrett Kamradt.\n\n\nNeedle in a haystack - pressure testing LLMs, 2024.\n\n\n",
      "Karpinska et\u00a0al. (2024)": "\nKarpinska et\u00a0al. (2024)\n\nMarzena Karpinska, Katherine Thai, Kyle Lo, Tanya Goyal, and Mohit Iyyer.\n\n\nOne thousand and one pairs: A\u201d novel\u201d challenge for long-context language models.\n\n\narXiv preprint arXiv:2406.16264, 2024.\n\n\n",
      "Kim et\u00a0al. (2024)": "\nKim et\u00a0al. (2024)\n\nYekyung Kim, Yapei Chang, Marzena Karpinska, Aparna Garimella, Varun Manjunatha, Kyle Lo, Tanya Goyal, and Mohit Iyyer.\n\n\nFABLES: Evaluating faithfulness and content selection in book-length summarization.\n\n\nIn First Conference on Language Modeling, 2024.\n\n\n",
      "Kocetkov et\u00a0al. (2023)": "\nKocetkov et\u00a0al. (2023)\n\nDenis Kocetkov, Raymond Li, Loubna\u00a0Ben allal, Jia LI, Chenghao Mou, Yacine Jernite, Margaret Mitchell, Carlos\u00a0Mu\u00f1oz Ferrandis, Sean Hughes, Thomas Wolf, Dzmitry Bahdanau, Leandro\u00a0Von Werra, and Harm de\u00a0Vries.\n\n\nThe stack: 3 TB of permissively licensed source code.\n\n\nTransactions on Machine Learning Research, 2023.\n\n\n",
      "Ko\u010disk\u00fd et\u00a0al. (2018)": "\nKo\u010disk\u00fd et\u00a0al. (2018)\n\nTom\u00e1\u0161 Ko\u010disk\u00fd, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl\u00a0Moritz Hermann, G\u00e1bor Melis, and Edward Grefenstette.\n\n\nThe NarrativeQA reading comprehension challenge.\n\n\nTransactions of the Association for Computational Linguistics, 6:317\u2013328, 2018.\n\n\n",
      "Krishna et\u00a0al. (2023)": "\nKrishna et\u00a0al. (2023)\n\nKalpesh Krishna, Erin Bransom, Bailey Kuehl, Mohit Iyyer, Pradeep Dasigi, Arman Cohan, and Kyle Lo.\n\n\nLongEval: Guidelines for human evaluation of faithfulness in long-form summarization.\n\n\nIn Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, pp.\u00a0 1650\u20131669, 2023.\n\n\n",
      "Kwiatkowski et\u00a0al. (2019)": "\nKwiatkowski et\u00a0al. (2019)\n\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew\u00a0M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov.\n\n\nNatural questions: A benchmark for question answering research.\n\n\nTransactions of the Association for Computational Linguistics, 7:452\u2013466, 2019.\n\n\n",
      "Larson et\u00a0al. (2019)": "\nLarson et\u00a0al. (2019)\n\nStefan Larson, Anish Mahendran, Joseph\u00a0J. Peper, Christopher Clarke, Andrew Lee, Parker Hill, Jonathan\u00a0K. Kummerfeld, Kevin Leach, Michael\u00a0A. Laurenzano, Lingjia Tang, and Jason Mars.\n\n\nAn evaluation dataset for intent classification and out-of-scope prediction.\n\n\nIn Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp.\u00a0 1311\u20131316, 2019.\n\n\n",
      "Lenz et\u00a0al. (2024)": "\nLenz et\u00a0al. (2024)\n\nBarak Lenz, Alan Arazi, Amir Bergman, Avshalom Manevich, Barak Peleg, Ben Aviram, Chen Almagor, Clara Fridman, Dan Padnos, et\u00a0al.\n\n\nJamba-1.5: Hybrid transformer-mamba models at scale.\n\n\narXiv preprint arXiv:2408.12570, 2024.\n\n\n",
      "Li et\u00a0al. (2024a)": "\nLi et\u00a0al. (2024a)\n\nJeffrey Li, Alex Fang, Georgios Smyrnis, Maor Ivgi, Matt Jordan, Samir Gadre, Hritik Bansal, Etash Guha, Sedrick Keh, Kushal Arora, Saurabh Garg, Rui Xin, Niklas Muennighoff, Reinhard Heckel, Jean Mercat, Mayee Chen, Suchin Gururangan, Mitchell Wortsman, Alon Albalak, Yonatan Bitton, Marianna Nezhurina, Amro Abbas, Cheng-Yu Hsieh, Dhruba Ghosh, Josh Gardner, Maciej Kilian, Hanlin Zhang, Rulin Shao, Sarah Pratt, Sunny Sanyal, Gabriel Ilharco, Giannis Daras, Kalyani Marathe, Aaron Gokaslan, Jieyu Zhang, Khyathi Chandu, Thao Nguyen, Igor Vasiljevic, Sham Kakade, Shuran Song, Sujay Sanghavi, Fartash Faghri, Sewoong Oh, Luke Zettlemoyer, Kyle Lo, Alaaeldin El-Nouby, Hadi Pouransari, Alexander Toshev, Stephanie Wang, Dirk Groeneveld, Luca Soldaini, Pang\u00a0Wei Koh, Jenia Jitsev, Thomas Kollar, Alexandros\u00a0G. Dimakis, Yair Carmon, Achal Dave, Ludwig Schmidt, and Vaishaal Shankar.\n\n\nDatacomp-lm: In search of the next generation of training sets for language models.\n\n\narXiv preprint arXiv:2406.11794, 2024a.\n\n\n",
      "Li et\u00a0al. (2023)": "\nLi et\u00a0al. (2023)\n\nShenggui Li, Fuzhao Xue, Chaitanya Baranwal, Yongbin Li, and Yang You.\n\n\nSequence parallelism: Long sequence training from system perspective.\n\n\nIn Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (eds.), Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp.\u00a0 2391\u20132404, Toronto, Canada, July 2023. Association for Computational Linguistics.\n\n\ndoi: 10.18653/v1/2023.acl-long.134.\n\n\nURL https://aclanthology.org/2023.acl-long.134.\n\n\n",
      "Li et\u00a0al. (2024b)": "\nLi et\u00a0al. (2024b)\n\nTianle Li, Ge\u00a0Zhang, Quy\u00a0Duc Do, Xiang Yue, and Wenhu Chen.\n\n\nLong-context llms struggle with long in-context learning.\n\n\narXiv preprint arXiv:2404.02060, 2024b.\n\n\n",
      "Lieber et\u00a0al. (2024)": "\nLieber et\u00a0al. (2024)\n\nOpher Lieber, Barak Lenz, Hofit Bata, Gal Cohen, Jhonathan Osin, Itay Dalmedigos, Erez Safahi, Shaked Meirom, Yonatan Belinkov, Shai Shalev-Shwartz, et\u00a0al.\n\n\nJamba: A hybrid transformer-mamba language model.\n\n\narXiv preprint arXiv:2403.19887, 2024.\n\n\n",
      "Liu et\u00a0al. (2024a)": "\nLiu et\u00a0al. (2024a)\n\nJiaheng Liu, Zhiqi Bai, Yuanxing Zhang, Chenchen Zhang, Yu\u00a0Zhang, Ge\u00a0Zhang, Jiakai Wang, Haoran Que, Yukang Chen, Wenbo Su, et\u00a0al.\n\n\nE^ 2-llm: Efficient and extreme length extension of large language models.\n\n\narXiv preprint arXiv:2401.06951, 2024a.\n\n\n",
      "Liu et\u00a0al. (2024b)": "\nLiu et\u00a0al. (2024b)\n\nNelson\u00a0F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang.\n\n\nLost in the Middle: How Language Models Use Long Contexts.\n\n\nTransactions of the Association for Computational Linguistics, 12:157\u2013173, 02 2024b.\n\n\n",
      "Liu et\u00a0al. (2021)": "\nLiu et\u00a0al. (2021)\n\nXingkun Liu, Arash Eshghi, Pawel Swietojanski, and Verena Rieser.\n\n\nBenchmarking natural language understanding services for building conversational agents.\n\n\nIn Increasing naturalness and flexibility in spoken dialogue interaction: 10th international workshop on spoken dialogue systems, pp.\u00a0 165\u2013183. Springer, 2021.\n\n\n",
      "Lu et\u00a0al. (2024)": "\nLu et\u00a0al. (2024)\n\nYi\u00a0Lu, Jing\u00a0Nathan Yan, Songlin Yang, Justin\u00a0T Chiu, Siyu Ren, Fei Yuan, Wenting Zhao, Zhiyong Wu, and Alexander\u00a0M Rush.\n\n\nA controlled study on long context extension and generalization in llms.\n\n\narXiv preprint arXiv:2409.12181, 2024.\n\n\n",
      "Ma et\u00a0al. (2022)": "\nMa et\u00a0al. (2022)\n\nXuezhe Ma, Chunting Zhou, Xiang Kong, Junxian He, Liangke Gui, Graham Neubig, Jonathan May, and Luke Zettlemoyer.\n\n\nMega: moving average equipped gated attention.\n\n\narXiv preprint arXiv:2209.10655, 2022.\n\n\n",
      "Mallen et\u00a0al. (2023)": "\nMallen et\u00a0al. (2023)\n\nAlex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Daniel Khashabi, and Hannaneh Hajishirzi.\n\n\nWhen not to trust language models: Investigating effectiveness of parametric and non-parametric memories.\n\n\nIn Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (eds.), Association for Computational Linguistics (ACL), pp.\u00a0 9802\u20139822, Toronto, Canada, July 2023. Association for Computational Linguistics.\n\n\n",
      "Mosaic ML (2022)": "\nMosaic ML (2022)\n\nMosaic ML.\n\n\nstreaming.\n\n\n<https://github.com/mosaicml/streaming/>, 2022.\n\n\n",
      "Ouyang et\u00a0al. (2022)": "\nOuyang et\u00a0al. (2022)\n\nLong Ouyang, Jeffrey Wu, Xu\u00a0Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et\u00a0al.\n\n\nTraining language models to follow instructions with human feedback.\n\n\nAdvances in Neural Information Processing Systems (NeurIPS), 35:27730\u201327744, 2022.\n\n\nURL https://proceedings.neurips.cc/paper_files/paper/2022/file/b1efde53be364a73914f58805a001731-Paper-Conference.pdf.\n\n\n",
      "Pan et\u00a0al. (2023)": "\nPan et\u00a0al. (2023)\n\nJane Pan, Tianyu Gao, Howard Chen, and Danqi Chen.\n\n\nWhat in-context learning \u201clearns\u201d in-context: Disentangling task recognition and task learning.\n\n\nIn Findings of the Association for Computational Linguistics: ACL 2023, pp.\u00a0 8298\u20138319. Association for Computational Linguistics, 2023.\n\n\n",
      "Paster et\u00a0al. (2024)": "\nPaster et\u00a0al. (2024)\n\nKeiran Paster, Marco\u00a0Dos Santos, Zhangir Azerbayev, and Jimmy Ba.\n\n\nOpenwebmath: An open dataset of high-quality mathematical web text.\n\n\nIn The Twelfth International Conference on Learning Representations, 2024.\n\n\n",
      "Paszke et\u00a0al. (2019)": "\nPaszke et\u00a0al. (2019)\n\nAdam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et\u00a0al.\n\n\nPytorch: An imperative style, high-performance deep learning library.\n\n\nAdvances in neural information processing systems, 32, 2019.\n\n\n",
      "Penedo et\u00a0al. (2023)": "\nPenedo et\u00a0al. (2023)\n\nGuilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Hamza Alobeidli, Alessandro Cappelli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay.\n\n\nThe refinedweb dataset for falcon llm: Outperforming curated corpora with web data only.\n\n\nIn A.\u00a0Oh, T.\u00a0Naumann, A.\u00a0Globerson, K.\u00a0Saenko, M.\u00a0Hardt, and S.\u00a0Levine (eds.), Advances in Neural Information Processing Systems, volume\u00a036, pp.\u00a0 79155\u201379172. Curran Associates, Inc., 2023.\n\n\n",
      "Penedo et\u00a0al. (2024)": "\nPenedo et\u00a0al. (2024)\n\nGuilherme Penedo, Hynek Kydl\u00ed\u010dek, Loubna\u00a0Ben allal, Anton Lozhkov, Margaret Mitchell, Colin Raffel, Leandro\u00a0Von Werra, and Thomas Wolf.\n\n\nThe fineweb datasets: Decanting the web for the finest text data at scale, 2024.\n\n\n",
      "Peng et\u00a0al. (2023)": "\nPeng et\u00a0al. (2023)\n\nBo\u00a0Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Stella Biderman, Huanqi Cao, Xin Cheng, Michael Chung, Leon Derczynski, Xingjian Du, Matteo Grella, Kranthi Gv, Xuzheng He, Haowen Hou, Przemyslaw Kazienko, Jan Kocon, Jiaming Kong, Bart\u0142omiej Koptyra, Hayden Lau, Jiaju Lin, Krishna Sri\u00a0Ipsit Mantri, Ferdinand Mom, Atsushi Saito, Guangyu Song, Xiangru Tang, Johan Wind, Stanis\u0142aw Wo\u017aniak, Zhenyuan Zhang, Qinghua Zhou, Jian Zhu, and Rui-Jie Zhu.\n\n\nRWKV: Reinventing RNNs for the transformer era.\n\n\nIn Findings of the Association for Computational Linguistics: EMNLP 2023, pp.\u00a0 14048\u201314077, 2023.\n\n\n",
      "Peng et\u00a0al. (2024)": "\nPeng et\u00a0al. (2024)\n\nBowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole.\n\n\nYaRN: Efficient context window extension of large language models.\n\n\nIn The Twelfth International Conference on Learning Representations, 2024.\n\n\n",
      "Press et\u00a0al. (2022)": "\nPress et\u00a0al. (2022)\n\nOfir Press, Noah Smith, and Mike Lewis.\n\n\nTrain short, test long: Attention with linear biases enables input length extrapolation.\n\n\nIn International Conference on Learning Representations (ICLR), 2022.\n\n\n",
      "Rae et\u00a0al. (2020)": "\nRae et\u00a0al. (2020)\n\nJack\u00a0W. Rae, Anna Potapenko, Siddhant\u00a0M. Jayakumar, Chloe Hillier, and Timothy\u00a0P. Lillicrap.\n\n\nCompressive transformers for long-range sequence modelling.\n\n\nIn International Conference on Learning Representations, 2020.\n\n\n",
      "Reid et\u00a0al. (2024)": "\nReid et\u00a0al. (2024)\n\nMachel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Jean-baptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, et\u00a0al.\n\n\nGemini 1.5: Unlocking multimodal understanding across millions of tokens of context.\n\n\narXiv preprint arXiv:2403.05530, 2024.\n\n\n",
      "Rubin & Berant (2023)": "\nRubin & Berant (2023)\n\nOhad Rubin and Jonathan Berant.\n\n\nLong-range language modeling with self-retrieval, 2023.\n\n\n",
      "Sakaguchi et\u00a0al. (2021)": "\nSakaguchi et\u00a0al. (2021)\n\nKeisuke Sakaguchi, Ronan\u00a0Le Bras, Chandra Bhagavatula, and Yejin Choi.\n\n\nWinogrande: An adversarial winograd schema challenge at scale.\n\n\nCommunications of the ACM, 64(9):99\u2013106, 2021.\n\n\n",
      "Shaham et\u00a0al. (2023)": "\nShaham et\u00a0al. (2023)\n\nUri Shaham, Maor Ivgi, Avia Efrat, Jonathan Berant, and Omer Levy.\n\n\nZeroSCROLLS: A zero-shot benchmark for long text understanding.\n\n\nIn Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Findings of the Association for Computational Linguistics: EMNLP 2023, pp.\u00a0 7977\u20137989, Singapore, December 2023. Association for Computational Linguistics.\n\n\n",
      "Shen et\u00a0al. (2022)": "\nShen et\u00a0al. (2022)\n\nZejiang Shen, Kyle Lo, Lauren Yu, Nathan Dahlberg, Margo Schlanger, and Doug Downey.\n\n\nMulti-lexsum: Real-world summaries of civil rights lawsuits at multiple granularities.\n\n\nIn Advances in Neural Information Processing Systems, volume\u00a035, pp.\u00a0 13158\u201313173. Curran Associates, Inc., 2022.\n\n\n",
      "Singh et\u00a0al. (2024)": "\nSingh et\u00a0al. (2024)\n\nAaditya\u00a0K Singh, Yu\u00a0Yang, Kushal Tirumala, Mostafa Elhoushi, and Ari\u00a0S Morcos.\n\n\nBrevity is the soul of wit: Pruning long files for code generation.\n\n\narXiv preprint arXiv:2407.00434, 2024.\n\n\n",
      "Soboleva et\u00a0al. (2023)": "\nSoboleva et\u00a0al. (2023)\n\nDaria Soboleva, Faisal Al-Khateeb, Robert Myers, Jacob\u00a0R Steeves, Joel Hestness, and Nolan Dey.\n\n\nSlimPajama: A 627B token cleaned and deduplicated version of RedPajama, 2023.\n\n\n",
      "Soldaini et\u00a0al. (2024)": "\nSoldaini et\u00a0al. (2024)\n\nLuca Soldaini, Rodney Kinney, Akshita Bhagia, Dustin Schwenk, David Atkinson, Russell Authur, Ben Bogin, Khyathi Chandu, Jennifer Dumas, Yanai Elazar, Valentin Hofmann, Ananya Jha, Sachin Kumar, Li\u00a0Lucy, Xinxi Lyu, Nathan Lambert, Ian Magnusson, Jacob Morrison, Niklas Muennighoff, Aakanksha Naik, Crystal Nam, Matthew Peters, Abhilasha Ravichander, Kyle Richardson, Zejiang Shen, Emma Strubell, Nishant Subramani, Oyvind Tafjord, Evan Walsh, Luke Zettlemoyer, Noah Smith, Hannaneh Hajishirzi, Iz\u00a0Beltagy, Dirk Groeneveld, Jesse Dodge, and Kyle Lo.\n\n\nDolma: an open corpus of three trillion tokens for language model pretraining research.\n\n\nIn Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp.\u00a0 15725\u201315788, Bangkok, Thailand, August 2024. Association for Computational Linguistics.\n\n\ndoi: 10.18653/v1/2024.acl-long.840.\n\n\nURL https://aclanthology.org/2024.acl-long.840.\n\n\n",
      "Su et\u00a0al. (2021)": "\nSu et\u00a0al. (2021)\n\nJianlin Su, Yu\u00a0Lu, Shengfeng Pan, Ahmed Murtadha, Bo\u00a0Wen, and Yunfeng Liu.\n\n\nRoformer: Enhanced transformer with rotary position embedding, 2021.\n\n\n",
      "Sun et\u00a0al. (2023)": "\nSun et\u00a0al. (2023)\n\nYutao Sun, Li\u00a0Dong, Shaohan Huang, Shuming Ma, Yuqing Xia, Jilong Xue, Jianyong Wang, and Furu Wei.\n\n\nRetentive network: A successor to transformer for large language models.\n\n\narXiv preprint arXiv:2307.08621, 2023.\n\n\n",
      "Sun et\u00a0al. (2024)": "\nSun et\u00a0al. (2024)\n\nYutao Sun, Li\u00a0Dong, Yi\u00a0Zhu, Shaohan Huang, Wenhui Wang, Shuming Ma, Quanlu Zhang, Jianyong Wang, and Furu Wei.\n\n\nYou only cache once: Decoder-decoder architectures for language models.\n\n\narXiv preprint arXiv:2405.05254, 2024.\n\n\n",
      "Together (2023)": "\nTogether (2023)\n\nTogether.\n\n\nRedpajama: An open source recipe to reproduce llama training dataset, April 2023.\n\n\n",
      "Waleffe et\u00a0al. (2024)": "\nWaleffe et\u00a0al. (2024)\n\nRoger Waleffe, Wonmin Byeon, Duncan Riach, Brandon Norick, Vijay Korthikanti, Tri Dao, Albert Gu, Ali Hatamizadeh, Sudhakar Singh, Deepak Narayanan, et\u00a0al.\n\n\nAn empirical study of mamba-based language models.\n\n\narXiv preprint arXiv:2406.07887, 2024.\n\n\n",
      "Wang et\u00a0al. (2024)": "\nWang et\u00a0al. (2024)\n\nJunxiong Wang, Daniele Paliotta, Avner May, Alexander\u00a0M Rush, and Tri Dao.\n\n\nThe mamba in the llama: Distilling and accelerating hybrid models.\n\n\narXiv preprint arXiv:2408.15237, 2024.\n\n\n",
      "Wettig et\u00a0al. (2024)": "\nWettig et\u00a0al. (2024)\n\nAlexander Wettig, Aatmik Gupta, Saumya Malik, and Danqi Chen.\n\n\nQuRating: Selecting high-quality data for training language models.\n\n\nIn Ruslan Salakhutdinov, Zico Kolter, Katherine Heller, Adrian Weller, Nuria Oliver, Jonathan Scarlett, and Felix Berkenkamp (eds.), Proceedings of the 41st International Conference on Machine Learning, volume 235 of Proceedings of Machine Learning Research, pp.\u00a0 52915\u201352971. PMLR, 21\u201327 Jul 2024.\n\n\n",
      "Wolf et\u00a0al. (2020)": "\nWolf et\u00a0al. (2020)\n\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le\u00a0Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander Rush.\n\n\nTransformers: State-of-the-art natural language processing.\n\n\nIn Qun Liu and David Schlangen (eds.), Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pp.\u00a0 38\u201345, Online, October 2020. Association for Computational Linguistics.\n\n\ndoi: 10.18653/v1/2020.emnlp-demos.6.\n\n\n",
      "Wu et\u00a0al. (2021)": "\nWu et\u00a0al. (2021)\n\nJeff Wu, Long Ouyang, Daniel\u00a0M Ziegler, Nisan Stiennon, Ryan Lowe, Jan Leike, and Paul Christiano.\n\n\nRecursively summarizing books with human feedback.\n\n\narXiv preprint arXiv:2109.10862, 2021.\n\n\n",
      "Wu et\u00a0al. (2024)": "\nWu et\u00a0al. (2024)\n\nWenhao Wu, Yizhong Wang, Yao Fu, Xiang Yue, Dawei Zhu, and Sujian Li.\n\n\nLong context alignment with short instructions and synthesized positions.\n\n\narXiv preprint arXiv:2405.03939, 2024.\n\n\n",
      "Xiao et\u00a0al. (2024a)": "\nXiao et\u00a0al. (2024a)\n\nChaojun Xiao, Pengle Zhang, Xu\u00a0Han, Guangxuan Xiao, Yankai Lin, Zhengyan Zhang, Zhiyuan Liu, Song Han, and Maosong Sun.\n\n\nInfLLM: Unveiling the intrinsic capacity of LLMs for understanding extremely long sequences with training-free memory.\n\n\narXiv preprint arXiv:2402.04617, 2024a.\n\n\n",
      "Xiao et\u00a0al. (2024b)": "\nXiao et\u00a0al. (2024b)\n\nGuangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis.\n\n\nEfficient streaming language models with attention sinks.\n\n\nIn The Twelfth International Conference on Learning Representations, 2024b.\n\n\n",
      "Xiong et\u00a0al. (2023)": "\nXiong et\u00a0al. (2023)\n\nWenhan Xiong, Jingyu Liu, Igor Molybog, Hejia Zhang, Prajjwal Bhargava, Rui Hou, Louis Martin, Rashi Rungta, Karthik\u00a0Abinav Sankararaman, Barlas Oguz, Madian Khabsa, Han Fang, Yashar Mehdad, Sharan Narang, Kshitiz Malik, Angela Fan, Shruti Bhosale, Sergey Edunov, Mike Lewis, Sinong Wang, and Hao Ma.\n\n\nEffective long-context scaling of foundation models, 2023.\n\n\n",
      "Xiong et\u00a0al. (2024)": "\nXiong et\u00a0al. (2024)\n\nZheyang Xiong, Vasilis Papageorgiou, Kangwook Lee, and Dimitris Papailiopoulos.\n\n\nFrom artificial needles to real haystacks: Improving retrieval capabilities in llms by finetuning on synthetic data.\n\n\narXiv preprint arXiv:2406.19292, 2024.\n\n\n",
      "Yang et\u00a0al. (2024a)": "\nYang et\u00a0al. (2024a)\n\nAn\u00a0Yang, Baosong Yang, Binyuan Hui, Bo\u00a0Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, et\u00a0al.\n\n\nQwen2 technical report.\n\n\narXiv preprint arXiv:2407.10671, 2024a.\n\n\n",
      "Yang et\u00a0al. (2024b)": "\nYang et\u00a0al. (2024b)\n\nSonglin Yang, Bailin Wang, Yikang Shen, Rameswar Panda, and Yoon Kim.\n\n\nGated linear attention transformers with hardware-efficient training.\n\n\nIn Forty-first International Conference on Machine Learning, 2024b.\n\n\n",
      "Yang et\u00a0al. (2018)": "\nYang et\u00a0al. (2018)\n\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher\u00a0D. Manning.\n\n\nHotpotQA: A dataset for diverse, explainable multi-hop question answering.\n\n\nIn Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pp.\u00a0 2369\u20132380, 2018.\n\n\n",
      "Yen et\u00a0al. (2024a)": "\nYen et\u00a0al. (2024a)\n\nHoward Yen, Tianyu Gao, and Danqi Chen.\n\n\nLong-context language modeling with parallel context encoding.\n\n\nIn Association for Computational Linguistics (ACL), pp.\u00a0 2588\u20132610, 2024a.\n\n\n",
      "Yen et\u00a0al. (2024b)": "\nYen et\u00a0al. (2024b)\n\nHoward Yen, Tianyu Gao, Minmin Hou, Ke\u00a0Ding, Daniel Fleischer, Peter Izsak, Moshe Wasserblat, and Danqi Chen.\n\n\nHelmet: How to evaluate long-context language models effectively and thoroughly.\n\n\n2024b.\n\n\n",
      "Yoshida et\u00a0al. (2020)": "\nYoshida et\u00a0al. (2020)\n\nDavis Yoshida, Allyson Ettinger, and Kevin Gimpel.\n\n\nAdding recurrence to pretrained transformers for improved efficiency and context size, 2020.\n\n\n",
      "Zellers et\u00a0al. (2019)": "\nZellers et\u00a0al. (2019)\n\nRowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi.\n\n\nHellaSwag: Can a machine really finish your sentence?\n\n\nIn Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp.\u00a0 4791\u20134800, 2019.\n\n\n",
      "Zhang & Bansal (2021)": "\nZhang & Bansal (2021)\n\nShiyue Zhang and Mohit Bansal.\n\n\nFinding a balanced degree of automation for summary evaluation.\n\n\nIn Empirical Methods in Natural Language Processing (EMNLP), pp.\u00a0 6617\u20136632, 2021.\n\n\n",
      "Zhang et\u00a0al. (2024a)": "\nZhang et\u00a0al. (2024a)\n\nXinrong Zhang, Yingfa Chen, Shengding Hu, Zihang Xu, Junhao Chen, Moo Hao, Xu\u00a0Han, Zhen Thai, Shuo Wang, Zhiyuan Liu, and Maosong Sun.\n\n\n\u221e\\infty\u221eBench: Extending long context evaluation beyond 100K tokens.\n\n\nIn Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp.\u00a0 15262\u201315277, 2024a.\n\n\n",
      "Zhang et\u00a0al. (2024b)": "\nZhang et\u00a0al. (2024b)\n\nYikai Zhang, Junlong Li, and Pengfei Liu.\n\n\nExtending llms\u2019 context window with 100 samples.\n\n\narXiv preprint arXiv:2401.07004, 2024b.\n\n\n",
      "Zhao et\u00a0al. (2024)": "\nZhao et\u00a0al. (2024)\n\nLiang Zhao, Tianwen Wei, Liang Zeng, Cheng Cheng, Liu Yang, Peng Cheng, Lijie Wang, Chenxia Li, Xuejie Wu, Bo\u00a0Zhu, et\u00a0al.\n\n\nLongskywork: A training recipe for efficiently extending context length in large language models.\n\n\narXiv preprint arXiv:2406.00605, 2024.\n\n\n",
      "Zhu et\u00a0al. (2024)": "\nZhu et\u00a0al. (2024)\n\nDawei Zhu, Nan Yang, Liang Wang, Yifan Song, Wenhao Wu, Furu Wei, and Sujian Li.\n\n\nPoSE: Efficient context window extension of LLMs via positional skip-wise training.\n\n\nIn The Twelfth International Conference on Learning Representations, 2024.\n\n\n"
    },
    "domain": "cs.CL",
    "citation_count": 0
  }
}